@article{PAN2025114348,
title = {A Dual-Matching Framework for Visual Entity Linking Enhanced by Large Language Models},
journal = {Knowledge-Based Systems},
volume = {329},
pages = {114348},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.114348},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125013875},
author = {Dijing Pan and Runhe Qiu and Xueqin Jiang and Shaohua Tao},
keywords = {Knowledge Graph, Large Language Model, Multimodal Entity Linking, Visual Entity Linking},
abstract = {Multimodal entity linking (MEL) accurately links ambiguous textual mentions in a multimodal context to unambiguous entities within a knowledge graph (KG) or knowledge base (KB). It plays a substantial role in various application domains, such as KG construction and semantic retrieval. However, currently, this task primarily aims to enhance the textual semantic level and fails to fully leverage multimodal context to enrich the semantic depth of a KG. We address this limitation by proposing a new task called visual entity linking (VEL), which is similar to traditional MEL. The key difference is that VEL aims to jointly map ambiguous textual mentions and their corresponding visual objects to entities in the KG. To this end, we introduce DMVEL, a dual-matching framework for VEL. (1) The optimal visual object can be obtained by utilizing a multi-instance feature alignment and classification mechanism that fully leverages both coarse-grained (textual and image) and fine-grained (textual and visual object) information. (2) Pre-designed prompt templates are employed to guide large language models (LLMs) in generating entity-focused descriptions for all entities in the KG, minimizing noise from irrelevant information. (3) A dual matching strategy comprising two key components is proposed. The first component entails applying an innovative filter to align ambiguous mentions with entities at the macro level of the overall semantics. The second component is the re-ranker, which performs fine-grained matching between local features and enhanced entity feature representations at the granular level, ensuring global semantic alignment while emphasizing local semantics. Extensive experiments on three public benchmarks demonstrate that the proposed method achieves state-of-the-art performance, paving the way toward an efficient and general solution to utilize LLMs to perform VEL.}
}
@article{SHE2025128207,
title = {Multi-view syntax-semantics information bottleneck for dependency-driven relation extraction},
journal = {Expert Systems with Applications},
volume = {288},
pages = {128207},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.128207},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425018275},
author = {Wei She and Xiwang Li and Linpu Lv and Youwei Wang and Honghui Dong and Zhao Tian},
keywords = {Relation extraction, Dependency tree, Information bottleneck, Syntax information, Semantics information},
abstract = {Dependency-driven relation extraction (DDRE) intends to improve the classification performance by exploring the syntax information contained in dependency tree. Although recent works have made impressive advances, they usually suffer severe challenges of losing sequential semantics information due to the pruning strategy. In addition, task-irrelevant information in representations such as noise or entity appositive information might be dominant in the procedure of representation learning, which interferes with the final performance of relation classification. To address these challenges, we propose a novel multi-view syntax-semantics information bottleneck (MS2IB) model, which aims at exploring supplementary information from a newly incorporated view and captures minimum sufficient information for the DDRE task. Specifically, MS2IB treats the DDRE task as a multi-view information learning procedure, where the sequential semantics information is supplemented under the guidance of representation learning with multiple views. Meanwhile, the minimum sufficient task-relevant information is extracted from the aspect of information compression and information preservation simultaneously. Finally, we formulate the objective of MS2IB as an information loss function based on the measurement of mutual information, where a new variational approach is presented to ensure its local optimum. Experiments on benchmark datasets and self-constructed dataset are conducted to show the superiority of MS2IB over the state-of-the-art models. For example, the proposed MS2IB achieves 91.42 % Precision, 89.21 % Recall and 90.28 % F1-score on the SemEval-2010 dataset. The MS2IB model further demonstrates strong generalization on domain datasets and robust performance on large-scale and noisy dataset. With the promising performance on both universal and domain datasets, the proposed model can be applied into various practical applications, such as information extraction and inference of transportation incidents/accidents.}
}
@article{CHEN2025113118,
title = {KG-prompt: Interpretable knowledge graph prompt for pre-trained language models},
journal = {Knowledge-Based Systems},
volume = {311},
pages = {113118},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113118},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125001650},
author = {Liyi Chen and Jie Liu and Yutai Duan and Runze Wang},
keywords = {Knowledge graph, Prompt learning, Knowledge injection, Pre-trained language models},
abstract = {Knowledge graphs (KGs) can provide rich factual knowledge for language models, enhancing reasoning ability and interpretability. However, existing knowledge injection methods usually ignore the structured information in KGs. Using structured knowledge to enhance pre-trained language models (PLMs) still has a set of challenging issues, including resource consumption of knowledge retraining, heterogeneous information, and knowledge noise. To address these issues, we explore how to flexibly inject structured knowledge into frozen PLMs. Inspired by prompt learning, we propose a novel method Knowledge Graph Prompt (KG-Prompt), which for the first time encodes the KG as structured prompts to enhance the knowledge expression ability of PLMs. KG-Prompt consists of a compressed subgraph construction module and a KG prompt generation module. In the compressed subgraph construction module, we construct compressed subgraphs based on a path-weighting strategy to reduce knowledge noise. In the KG prompt generation module, we propose a multi-hop consistency optimization strategy to learn the representation of compressed subgraphs, and then generate KG prompts based on a knowledge mapper to solve the heterogeneous information problem. The KG prompts can be inserted into the input of PLMs expediently, which decouples from PLMs and the downstream model without knowledge retraining and reduces computational resources. Extensive experiments on three knowledge-driven natural language understanding tasks demonstrate that our approach effectively improves the knowledge reasoning ability of PLMs. Furthermore, we provide a detailed analysis of different KG prompts and discuss the interpretability and generalizability of the proposed method.}
}
@article{GUSMITA2026102504,
title = {ELEVATE-ID: Extending Large Language Models for End-to-End Entity Linking Evaluation in Indonesian},
journal = {Data & Knowledge Engineering},
volume = {161},
pages = {102504},
year = {2026},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2025.102504},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X25000990},
author = {Ria Hari Gusmita and Asep Fajar Firmansyah and Hamada M. Zahera and Axel-Cyrille {Ngonga Ngomo}},
keywords = {LLMs, Evaluation, End-to-end EL, Indonesian},
abstract = {Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, their effectiveness in low-resource languages remains underexplored, particularly in complex tasks such as end-to-end Entity Linking (EL), which requires both mention detection and disambiguation against a knowledge base (KB). In earlier work, we introduced IndEL — the first end-to-end EL benchmark dataset for the Indonesian language — covering both a general domain (news) and a specific domain (religious text from the Indonesian translation of the Quran), and evaluated four traditional end-to-end EL systems on this dataset. In this study, we propose ELEVATE-ID, a comprehensive evaluation framework for assessing LLM performance on end-to-end EL in Indonesian. The framework evaluates LLMs under both zero-shot and fine-tuned conditions, using multilingual and Indonesian monolingual models, with Wikidata as the target KB. Our experiments include performance benchmarking, generalization analysis across domains, and systematic error analysis. Results show that GPT-4 and GPT-3.5 achieve the highest accuracy in zero-shot and fine-tuned settings, respectively. However, even fine-tuned GPT-3.5 underperforms compared to DBpedia Spotlight — the weakest of the traditional model baselines — in the general domain. Interestingly, GPT-3.5 outperforms Babelfy in the specific domain. Generalization analysis indicates that fine-tuned GPT-3.5 adapts more effectively to cross-domain and mixed-domain scenarios. Error analysis uncovers persistent challenges that hinder LLM performance: difficulties with non-complete mentions, acronym disambiguation, and full-name recognition in formal contexts. These issues point to limitations in mention boundary detection and contextual grounding. Indonesian-pretrained LLMs, Komodo and Merak, reveal core weaknesses: template leakage and entity hallucination, respectively—underscoring architectural and training limitations in low-resource end-to-end EL.11Code and dataset are available at https://github.com/dice-group/ELEVATE-ID.}
}
@article{GUO2024121448,
title = {A method for constructing a machining knowledge graph using an improved transformer},
journal = {Expert Systems with Applications},
volume = {237},
pages = {121448},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121448},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423019504},
author = {Liang Guo and Xinling Li and Fu Yan and Yuqian Lu and Wenping Shen},
keywords = {Knowledge Engineering, Machining, Transformer, Knowledge Graph},
abstract = {Process knowledge base is a core component in the intelligent process, which determines the intelligent degree of product manufacturing and directly affects the production efficiency of products. However, traditional process knowledge base is often constructed manually, which is difficult and time-consuming. In addition, in the field of machining, there is a large amount of unstructured invisible process knowledge, which is not effectively organized and managed. To make use of this knowledge and provide knowledge support for downstream production and maintenance, a process knowledge base construction framework is proposed by using Knowledge Graph (KG) technology. Firstly, the ontology rules of process knowledge are designed from the perspective of the processing method of process characteristics according to the particularity of knowledge in the machining field. The process KG schema layer is then constructed. Secondly, a neural network BERT–Improved TRANSFORMER–CRF (BITC) model is proposed for the machining knowledge extraction task, and the data layer is constructed. Then, entity linking and knowledge fusion are performed by using the word vector cosine similarity algorithm and stored in Neo4j. The process KG is then constructed. Finally, the effectiveness of the proposed method is verified by using an aero-engine casing of an enterprise as an example. Under the same dataset, the BITC model scored higher than several other classical models. The Precision, Recall, and F1-score were 85.27%, 86.40%, and 85.83 %, respectively.}
}
@article{DELANGHE2025100184,
title = {Position-aware end-to-end cross-document event coreference resolution for Dutch},
journal = {Natural Language Processing Journal},
volume = {13},
pages = {100184},
year = {2025},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2025.100184},
url = {https://www.sciencedirect.com/science/article/pii/S2949719125000603},
author = {Loic {De Langhe} and Orphée {De Clercq} and Veronique Hoste},
keywords = {Coreference Resolution, Discourse Processing, Events},
abstract = {Natural language understanding entails the ability to comprehend the relations between various people, objects or events throughout one, or multiple, text(s). Event coreference resolution (ECR) is a discourse-based natural language processing (NLP) task which aims to link those textual events, be they real or fictional, that refer to the same conceptual event. In this paper, we introduce a novel end-to-end approach for cross-document ECR which combines expert-level positional knowledge and graph-based representations in order to create a memory-efficient and accurate system meant for the detection and resolution of events in large document collections. We make three fundamental architectural changes to a current state-of-the-art cross-document ECR system and show that our approach outperforms this earlier model (+ 4% CONLL F1) on a large Dutch ECR dataset. Moreover, we show through in-depth qualitative and quantitative analysis that our proposed approach consistently detects more relevant events and suffers notably less from the typical issues models exhibit when predicting coreference chains.}
}
@article{LI20243825,
title = {Survey and Prospect for Applying Knowledge Graph in Enterprise Risk Management},
journal = {Computers, Materials and Continua},
volume = {78},
number = {3},
pages = {3825-3865},
year = {2024},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2024.046851},
url = {https://www.sciencedirect.com/science/article/pii/S1546221824003618},
author = {Pengjun Li and Qixin Zhao and Yingmin Liu and Chao Zhong and Jinlong Wang and Zhihan Lyu},
keywords = {Knowledge graph, enterprise risk, risk identification, risk management, review},
abstract = {Enterprise risk management holds significant importance in fostering sustainable growth of businesses and in serving as a critical element for regulatory bodies to uphold market order. Amidst the challenges posed by intricate and unpredictable risk factors, knowledge graph technology is effectively driving risk management, leveraging its ability to associate and infer knowledge from diverse sources. This review aims to comprehensively summarize the construction techniques of enterprise risk knowledge graphs and their prominent applications across various business scenarios. Firstly, employing bibliometric methods, the aim is to uncover the developmental trends and current research hotspots within the domain of enterprise risk knowledge graphs. In the succeeding section, systematically delineate the technical methods for knowledge extraction and fusion in the standardized construction process of enterprise risk knowledge graphs. Objectively comparing and summarizing the strengths and weaknesses of each method, we provide recommendations for addressing the existing challenges in the construction process. Subsequently, categorizing the applied research of enterprise risk knowledge graphs based on research hotspots and risk category standards, and furnishing a detailed exposition on the applicability of technical routes and methods. Finally, the future research directions that still need to be explored in enterprise risk knowledge graphs were discussed, and relevant improvement suggestions were proposed. Practitioners and researchers can gain insights into the construction of technical theories and practical guidance of enterprise risk knowledge graphs based on this foundation.}
}
@article{REN2023126893,
title = {Tuning N-ary relation extraction as Machine Reading Comprehension},
journal = {Neurocomputing},
volume = {562},
pages = {126893},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.126893},
url = {https://www.sciencedirect.com/science/article/pii/S0925231223010160},
author = {Pengrui Ren and Tianyu Xu and Jianfeng Qu and Yu Sang and Zhixu Li and Junhua Fang and Pengpeng Zhao and Guilin Ma},
keywords = {-ary relation extraction, Machine reading comprehension, Continuous prompting questions, Additional knowledge, Multi-perspective},
abstract = {Compared with conventional binary relation extraction, n-ary relation extraction is a particularly challenging task due to the presence of multiple entities that span across sentences. Although current methods have achieved remarkable results in this area, they often rely on complex modeling like dependency parsing, which easily suffers from error propagation. To address this predicament, this paper proposes a novel framework for n-ary relation extraction that utilizes Machine Reading Comprehension (MRC) as its foundation. In particular, considering the unnameable relations or sub-relations between multiple entities, we resort to learning continuous prompting questions to make up for the deficiency of natural language questions. Additionally, to alleviate the high semantic similarity between close relation classes, we obtain supplementary prompt messages (i.e., additional knowledge) according to the statistical results for each relation class so as to equip the model with a better capacity to make distinctions. Finally, since the one-turn mechanism of MRC is prone to mistakes, especially in those challenging n-ary tasks, we design a double-check mechanism that generates questions from multi-perspective to ascertain the final relation between all the entities by aggregating the answers to all questions. Our method has demonstrated the most advanced results on n-ary relation extraction datasets through extensive experimentation.}
}
@article{LAVRINOVICS2025100844,
title = {Knowledge Graphs, Large Language Models, and Hallucinations: An NLP Perspective},
journal = {Journal of Web Semantics},
volume = {85},
pages = {100844},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100844},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000301},
author = {Ernests Lavrinovics and Russa Biswas and Johannes Bjerva and Katja Hose},
keywords = {LLM, Factuality, Knowledge Graphs, Hallucinations},
abstract = {Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) based applications including automated text generation, question answering, chatbots, and others. However, they face a significant challenge: hallucinations, where models produce plausible-sounding but factually incorrect responses. This undermines trust and limits the applicability of LLMs in different domains. Knowledge Graphs (KGs), on the other hand, provide a structured collection of interconnected facts represented as entities (nodes) and their relationships (edges). In recent research, KGs have been leveraged to provide context that can fill gaps in an LLM’s understanding of certain topics offering a promising approach to mitigate hallucinations in LLMs, enhancing their reliability and accuracy while benefiting from their wide applicability. Nonetheless, it is still a very active area of research with various unresolved open problems. In this paper, we discuss these open challenges covering state-of-the-art datasets and benchmarks as well as methods for knowledge integration and evaluating hallucinations. In our discussion, we consider the current use of KGs in LLM systems and identify future directions within each of these challenges.}
}
@article{HARUNA2025103578,
title = {AddManBERT: A combinatorial triples extraction and classification task for establishing a knowledge graph to facilitate design for additive manufacturing},
journal = {Advanced Engineering Informatics},
volume = {67},
pages = {103578},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103578},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625004719},
author = {Auwal Haruna and Khandaker Noman and Yongbo Li and Xin Wang and Md Junayed Hasan and Ahmad Bala Alhassan},
keywords = {Knowledge graph, BERT model, Textual Data, Additive manufacturing, Triples extraction and classification},
abstract = {In recent years, triple extraction and classification have received attention in the context of Additive Manufacturing (AM). However, the lack of a formalized process to extract and classify triple from textual data poses challenges for the effective embedding learning techniques in utilizing AM’s product innovation and manufacturing capabilities. Hence, the AM field’s manual cognitive process hinders the broader adoption of Design for AM (DFAM) in manufacturing. Aiming to solve these challenging problems, this research proposes a Natural Language Processing (NLP) and Knowledge Graph (KG) methodology for triple extraction and classification from textual data to provide an embedding learning approach. Initially, multi-source textual data for triple extraction and classification is developed. Then, AM Bidirectional Encoder Representation from the Transformers (AddManBERT) is used for triple extraction and classification. The AddManBERT utilizes dependency parsing to determine the semantic relations between the entities for triple extraction and classification. Consequently, the AddManBERT transformed each extracted piece of knowledge from the textual data into a 768-dimensional vector structure by analyzing the projected probability of the output within the center word based on the token embedding surrounding the input. The triples extracted and classified are then saved in the Neo4j database and displayed as graph nodes. An experiment and an application case study verify the proposed method’s efficacy. The experiment results indicate that the proposed method outperforms the traditional centralized approaches in responsiveness, classification accuracy, and prediction efficiency.}
}
@article{GALLEGO2025113211,
title = {Enhancing cross-encoders using knowledge graph hierarchy for medical entity linking in zero- and few-shot scenarios},
journal = {Knowledge-Based Systems},
volume = {314},
pages = {113211},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113211},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125002588},
author = {Fernando Gallego and Pedro Ruas and Francisco M. Couto and Francisco J. Veredas},
keywords = {Knowledge enrichment, Medical entity linking, Contrastive-learning, Knowledge bases, Candidate reranking, Zero-few shot},
abstract = {Medical Entity Linking (MEL) is a common task in natural language processing, focusing on the normalization of recognized entities from clinical texts using large knowledge bases (KBs). This task presents significant challenges, especially when working with electronic health records that often lack annotated clinical notes, even in languages like English. The difficulty increases in few-shot or zero-shot scenarios, where models must operate with minimal or no training data, a common issue when dealing with less-documented languages such as Spanish. Existing solutions that combine contrastive learning with external sources, like the Unified Medical Language System (UMLS), have shown competitive results. However, most of these methods focus on individual concepts from the KBs, ignoring relationships such as synonymy or hierarchical links between concepts. In this paper, we propose leveraging these relationships to enrich the training triplets used for contrastive learning, improving performance in MEL tasks. Specifically, we fine-tune several BERT-based cross-encoders using enriched triplets on three clinical corpora in Spanish : DisTEMIST, MedProcNER, and SympTEMIST. Our approach addresses the complexity of real-world data, where unseen mentions and concepts are frequent. The results show a notable improvement in lower top-k accuracies, surpassing the state-of-the-art by up to 5.5 percentage points for unseen mentions and by up to 5.9 points for unseen concepts. This improvement reduces the number of candidate concepts required for cross-encoders, enabling more efficient semi-automatic annotation and decreasing human effort. Additionally, our findings underscore the importance of leveraging not only the concept-level information in KBs but also the relationships between those concepts.}
}
@article{REHMAN2025610,
title = {Intelligent configuration management in modular production systems: Integrating operational semantics with knowledge graphs},
journal = {Journal of Manufacturing Systems},
volume = {80},
pages = {610-625},
year = {2025},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2025.03.017},
url = {https://www.sciencedirect.com/science/article/pii/S0278612525000780},
author = {Hamood Ur Rehman and Fan Mo and Jack C. Chaplin and Leszek Zarzycki and Mark Jones and Antonio Maffei and Svetan Ratchev},
keywords = {Configuration management, Intelligent manufacturing, Knowledge graphs, Operational semantics, Modular production systems, Real-time reconfiguration, Data-driven manufacturing},
abstract = {This paper presents an innovative approach to integrating data-driven strategies into intelligent manufacturing systems, specifically targeting the challenges of configuration management in modular production environments. To address the distinct and evolving requirements of customized products, we propose a dynamic configuration management methodology that automatically adjusts system settings in real-time. This approach utilizes operational semantics to formalize the interactions between production modules, capturing essential operational information for intelligent decision-making. A novel control mechanism is developed, using knowledge graphs to semantically represent and manage the relationships between production system components and settings. By mapping these, the system can determine optimal configurations based on real-time data and specific operational requirements. The interaction between the control mechanism and the knowledge graph ensures continuous adaptability, enabling the system to reconfigure dynamically in response to changes. This method was validated in an industrial dry-air leak testing scenario, demonstrating its effectiveness in adaptability.}
}
@article{NIE2025100866,
title = {What can knowledge graph do for few-shot named entity recognition},
journal = {Journal of Web Semantics},
volume = {86},
pages = {100866},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2025.100866},
url = {https://www.sciencedirect.com/science/article/pii/S157082682500006X},
author = {Binling Nie and Yiming Shao and Yigang Wang},
keywords = {Few-shot NER, Knowledge graph, Knowledge fusion},
abstract = {Due to its extensive applicability in various downstream domains, few-shot named entity recognition (NER) has attracted increasing attention, particularly in areas where acquiring sufficient labeled data poses a significant challenge. Recent studies have highlighted the potential of knowledge graphs (KGs) in enhancing natural language processing (NLP) tasks. However, a comprehensive understanding of whether and how KGs can effectively improve the NER performance under low-resource conditions remains elusive. In this paper, for the first time, we quantitatively investigate the effects of different kinds of extra KG features for few-shot NER. We enable our analysis by aggregating extra KG features into an NER framework. Through extensive experiments, we find that incorporating class features yields the best performance. To fully explore the potential of class features from KGs, we propose a novel network architecture, named KGen, to jointly leverage KG-based knowledge from both the input sentence side and the label semantic side for few-shot NER.The efficacy of our proposed method is validated through extensive experiments on five challenging datasets.}
}
@article{TONG2025110790,
title = {MVIFSA: Enhancing relation detection in knowledge base question answering through multi-view information fusion and self-attention},
journal = {Engineering Applications of Artificial Intelligence},
volume = {152},
pages = {110790},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.110790},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625007900},
author = {Guoxiang Tong and Maolin Wang and Qi Hu},
keywords = {Knowledge base question answering, Relation detection, Bi-directional long short-term memory, Self-attention, Residual learning},
abstract = {In the domain of knowledge base question answering systems, relation detection plays a pivotal role in interpreting entities and their intricate networks. Traditional relation detection approaches model primarily from a question and relation-level perspective without taking full advantage of word-by-word relation representations. Existing hierarchical relation detection models typically use a unidirectional, layer-specific learning manner that ignores training losses across layers. They do not take into account the critical fraction of feature vectors from the input questions and relations. Towards this, we propose MVIFSA, a multi-view information fusion model based on self-attention for relation matching. Our approach employs a multi-view relational embedding strategy, which increases the word-by-word relational embedding dimension to capture the correlations between questions and relations more efficiently. The residual learning layer is designed to reduce training loss and avoid local optimum. This is achieved by constructing residual connections between the output of the information fusion layer and the complex relation representation layer. In addition, the introduction of the self-attention mechanism allows the model to selectively pay attention to the vectors of input questions and relations, thus improving its ability to align questions with the corresponding relations. Experimental results show that MVIFSA outperforms state-of-the-art methods on benchmark datasets such as SimpleQuestions, WebQuestionsSP, and PathQuestionLarge.}
}
@article{ZHAO2024103524,
title = {A survey on cybersecurity knowledge graph construction},
journal = {Computers & Security},
volume = {136},
pages = {103524},
year = {2024},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2023.103524},
url = {https://www.sciencedirect.com/science/article/pii/S0167404823004340},
author = {Xiaojuan Zhao and Rong Jiang and Yue Han and Aiping Li and Zhichao Peng},
keywords = {Cybersecurity knowledge graphs, Knowledge graph construction, Cybersecurity ontology, Named entity recognition, Relation extraction},
abstract = {The development of key technologies of knowledge graph (KG) has promoted the development of machine cognition technology, and the combination of KG and industry as well as scenario-based landing have also made breakthroughs in succession. In the field of cybersecurity, with the intelligent upgrading of defense technology, there is an urgent need for a mature and effective technical system to provide knowledge and intelligent reasoning support for the offensive and defensive games in strong adversarial and high dynamic environment. As a domain KG, cybersecurity KG (CKG) just meets this requirement. KG for cybersecurity is not a recent invention; its predecessor is the earliest semantic networks and ontologies, and the KG is small in scale and the relations are relatively simple. Through investigation, we found that most studies that explicitly mentioned CKG, still tend to construct a cybersecurity ontology first, and then extract semantic triples based on ontology. Of course, there are also some studies that try to build such a graph from a higher dimension to express the rich semantics. In order to apply mature techniques of KG construction and reasoning, we believe that the construction of CKG should also follow the Open-domain KG. Therefore, we conducted a comprehensive review and detailed comparison of CKG-related works, discussed the dilemma of CKG application, and then proposed future research opportunities for CKG. This work can help researchers keep up with recent research trends.}
}
@article{YACOUBIAYADI2024100484,
title = {A unified approach to publish semantic annotations of agricultural documents as knowledge graphs},
journal = {Smart Agricultural Technology},
volume = {8},
pages = {100484},
year = {2024},
issn = {2772-3755},
doi = {https://doi.org/10.1016/j.atech.2024.100484},
url = {https://www.sciencedirect.com/science/article/pii/S2772375524000893},
author = {Nadia {Yacoubi Ayadi} and Stephan Bernard and Robert Bossy and Marine Courtin and Bill Gates {Happi Happi} and Pierre Larmande and Franck Michel and Claire Nédellec and Catherine Roussey and Catherine Faron},
keywords = {Agriculture, Knowledge graphs, Semantic modelling, RDF transformation, Natural language processing, Annotations, Semantic resources, Named entity recognition and linking},
abstract = {The research results presented in this paper were obtained as part of the D2KAB project (Data to Knowledge in Agriculture and Biodiversity) which aims to develop semantic web-based tools to describe and make agronomical data actionable and accessible following the FAIR principles. We focus on constructing domain-specific Knowledge Graphs (KGs) from textual data sources, using Natural Language Processing (NLP) techniques to extract and structure relevant entities. Our approach is based on the formalization of a semantic data model using common linked open vocabularies such as the Web Annotation Ontology (OA) and the Provenance Ontology (PROV). The model was developed by formulating motivating scenarios and competency questions from domain experts. This model has been used to construct three different KGs from three distinct corpora: PubMed scientific publications on wheat and rice genetics and phenotyping, and French agricultural alert bulletins. The named entities to be recognized include genes, phenotypes, traits, genetic markers, taxa and phenological stages normalized using semantic resources such as the Wheat Trait and Phenotype Ontology (WTO), the French Crop Usage (FCU) thesaurus and the Plant Phenological Description Ontology (PPDO). Named entities were extracted using different NLP approaches and tools. The relevance of the semantic model was validated by implementing experts questions as SPARQL queries to be answered on the constructed RDF knowledge graphs. Our work demonstrates how domain-specific vocabularies and systematic querying of KGs can reveal hidden interactions and support agronomists in navigating vast amounts of data. The resources and transformation pipelines developed are publicly available in Git repositories.}
}
@article{MORENOSCHNEIDER2022101966,
title = {Lynx: A knowledge-based AI service platform for content processing, enrichment and analysis for the legal domain},
journal = {Information Systems},
volume = {106},
pages = {101966},
year = {2022},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2021.101966},
url = {https://www.sciencedirect.com/science/article/pii/S0306437921001563},
author = {Julián {Moreno Schneider} and Georg Rehm and Elena Montiel-Ponsoda and Víctor Rodríguez-Doncel and Patricia Martín-Chozas and María Navas-Loro and Martin Kaltenböck and Artem Revenko and Sotirios Karampatakis and Christian Sageder and Jorge Gracia and Filippo Maganza and Ilan Kernerman and Dorielle Lonke and Andis Lagzdins and Julia {Bosque Gil} and Pieter Verhoeven and Elsa {Gomez Diaz} and Pascual {Boil Ballesteros}},
keywords = {Text analytics, Tools, Systems, Applications, Knowledge discovery/representation},
abstract = {The EU-funded project Lynx focuses on the creation of a knowledge graph for the legal domain (Legal Knowledge Graph, LKG) and its use for the semantic processing, analysis and enrichment of documents from the legal domain. This article describes the use cases covered in the project, the entire developed platform and the semantic analysis services that operate on the documents.}
}
@article{MURALI2023104403,
title = {Towards electronic health record-based medical knowledge graph construction, completion, and applications: A literature study},
journal = {Journal of Biomedical Informatics},
volume = {143},
pages = {104403},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2023.104403},
url = {https://www.sciencedirect.com/science/article/pii/S1532046423001247},
author = {Lino Murali and G. Gopakumar and Daleesha M. Viswanathan and Prema Nedungadi},
keywords = {EHR data, Health knowledge graph, Clinical decision making},
abstract = {With the growth of data and intelligent technologies, the healthcare sector opened numerous technology that enabled services for patients, clinicians, and researchers. One major hurdle in achieving state-of-the-art results in health informatics is domain-specific terminologies and their semantic complexities. A knowledge graph crafted from medical concepts, events, and relationships acts as a medical semantic network to extract new links and hidden patterns from health data sources. Current medical knowledge graph construction studies are limited to generic techniques and opportunities and focus less on exploiting real-world data sources in knowledge graph construction. A knowledge graph constructed from Electronic Health Records (EHR) data obtains real-world data from healthcare records. It ensures better results in subsequent tasks like knowledge extraction and inference, knowledge graph completion, and medical knowledge graph applications such as diagnosis predictions, clinical recommendations, and clinical decision support. This review critically analyses existing works on medical knowledge graphs that used EHR data as the data source at (i) representation level, (ii) extraction level (iii) completion level. In this investigation, we found that EHR-based knowledge graph construction involves challenges such as high complexity and dimensionality of data, lack of knowledge fusion, and dynamic update of the knowledge graph. In addition, the study presents possible ways to tackle the challenges identified. Our findings conclude that future research should focus on knowledge graph integration and knowledge graph completion challenges.}
}
@article{DESSI2022109945,
title = {SCICERO: A deep learning and NLP approach for generating scientific knowledge graphs in the computer science domain},
journal = {Knowledge-Based Systems},
volume = {258},
pages = {109945},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.109945},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122010383},
author = {Danilo Dessí and Francesco Osborne and Diego {Reforgiato Recupero} and Davide Buscaldi and Enrico Motta},
keywords = {Knowledge graph, Scholarly domain, Scientific facts, Artificial intelligence},
abstract = {Science communication has a number of bottlenecks that include the rising number of published research papers and its non-machine-accessible and document-based paradigm, which makes the exploration, reading, and reuse of research outcomes rather inefficient. Recently, Knowledge Graphs (KG), i.e., semantic interlinked networks of entities, have been proposed as a new core technology to describe and curate scholarly information with the goal to make it machine readable and understandable. However, the main drawback of the use of such a technology is that researchers are asked to manually annotate their research papers and add their contributions within the KGs. To address this problem, in this paper we propose SCICERO, a novel KG generation approach that takes in input text from research articles and generates a KG of research entities. SCICERO uses Natural Language Processing techniques to parse the content of scientific papers to discover entities and relationships, exploits state-of-the-art Deep Learning Transformer models to make sense and validate extracted information, and uses Semantic Web best practices to formally represent the extracted entities and relationships, making the written content of research papers machine-actionable. SCICERO has been tested on a dataset of 6.7M papers about Computer Science generating a KG of about 10M entities. It has been evaluated on a manually generated gold standard of 3,600 triples that cover three Computer Science subdomains (Information Retrieval, Natural Language Processing, and Machine Learning) obtaining remarkable results.}
}
@article{FRENCH2023104252,
title = {An overview of biomedical entity linking throughout the years},
journal = {Journal of Biomedical Informatics},
volume = {137},
pages = {104252},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2022.104252},
url = {https://www.sciencedirect.com/science/article/pii/S153204642200257X},
author = {Evan French and Bridget T. McInnes},
keywords = {Natural language processing, Entity linking, Normalization},
abstract = {Biomedical Entity Linking (BEL) is the task of mapping of spans of text within biomedical documents to normalized, unique identifiers within an ontology. This is an important task in natural language processing for both translational information extraction applications and providing context for downstream tasks like relationship extraction. In this paper, we will survey the progression of BEL from its inception in the late 80s to present day state of the art systems, provide a comprehensive list of datasets available for training BEL systems, reference shared tasks focused on BEL, discuss the technical components that comprise BEL systems, and discuss possible directions for the future of the field.}
}
@article{WANG20254141,
title = {Label-Guided Scientific Abstract Generation with a Siamese Network Using Knowledge Graphs},
journal = {Computers, Materials and Continua},
volume = {83},
number = {3},
pages = {4141-4166},
year = {2025},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2025.062806},
url = {https://www.sciencedirect.com/science/article/pii/S1546221825004369},
author = {Haotong Wang and Yves Lepage},
keywords = {Graph-to-text generation, knowledge graph, siamese network, scientific abstract},
abstract = {Knowledge graphs convey precise semantic information that can be effectively interpreted by neural networks, and generating descriptive text based on these graphs places significant emphasis on content consistency. However, knowledge graphs are inadequate for providing additional linguistic features such as paragraph structure and expressive modes, making it challenging to ensure content coherence in generating text that spans multiple sentences. This lack of coherence can further compromise the overall consistency of the content within a paragraph. In this work, we present the generation of scientific abstracts by leveraging knowledge graphs, with a focus on enhancing both content consistency and coherence. In particular, we construct the ACL Abstract Graph Dataset (ACL-AGD) which pairs knowledge graphs with text, incorporating sentence labels to guide text structure and diverse expressions. We then implement a Siamese network to complement and concretize the entities and relations based on paragraph structure by accomplishing two tasks: graph-to-text generation and entity alignment. Extensive experiments demonstrate that the logical paragraphs generated by our method exhibit entities with a uniform position distribution and appropriate frequency. In terms of content, our method accurately represents the information encoded in the knowledge graph, prevents the generation of irrelevant content, and achieves coherent and non-redundant adjacent sentences, even with a shared knowledge graph.}
}
@article{HAMEL2024108689,
title = {Deep sequence to sequence semantic embedding with attention for entity linking in context of incomplete linked data},
journal = {Engineering Applications of Artificial Intelligence},
volume = {134},
pages = {108689},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.108689},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624008479},
author = {Oussama Hamel and Messaouda Fareh},
keywords = {Links prediction, Semantic links, Linked data, Attention mechanism, Deep learning},
abstract = {In contemporary times, Linked Data has emerged as a prominent approach for publishing data on the internet. This data is typically represented in the form of RDF (Resource Description Framework) triples, which are interconnected, thus enhancing the relevance of search results for users. Despite its advantages, Linked Data suffers from various limitations, such as erroneous data, imprecise information, and missing links between resources. Existing solutions to address the issue of missing links in RDF triples often overlook the semantic relationship between the subject and object. To address this gap, we present a novel approach called LinkED-S2S (Linking Entities deeply with Sequence To Sequence model), which employs an encoder–decoder model with an attention mechanism. Our proposed model incorporates an embedding layer to enhance data representation, along with GRU (Gated Recurrent Unit) cells to mitigate the vanishing gradient problem. This work demonstrates significant improvement in predicting missing links compared to baseline models. We evaluated our model’s performance using a comprehensive set of metrics on the widely-used DBpedia dataset and standard benchmark datasets. Our model achieved very good results on these metrics, highlighting its effectiveness in predicting missing links.}
}
@article{CARTA20242235,
title = {A Zero-Shot Strategy for Knowledge Graph Engineering Using GPT-3.5},
journal = {Procedia Computer Science},
volume = {246},
pages = {2235-2243},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.573},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924026231},
author = {Salvatore Carta and Alessandro Giuliani and Marco Manolo Manca and Leonardo Piano and Alessandro Sebastian Podda and Livio Pompianu and Sandro Gabriele Tiddia},
keywords = {Knowledge Engineering, Knowledge Graphs, Large Language Models},
abstract = {In the recent digitization era, capturing, representing, and understanding knowledge is essential in countless real-world scenarios. Knowledge graphs emerged as a powerful tool for representing information through an adequately interconnected and interpretable structure in such a context. Nevertheless, generating proper knowledge graphs usually requires significant manual effort and domain expertise, resulting in graphs often affected by human subjectivity, limited scalability, or inability to capture implicit knowledge or handle heterogeneity. This paper proposes an innovative zero-shot strategy tailored to uncover reliable knowledge from text leveraging the recent highly effective generative large language models, with a particular focus on the GPT-3.5 model. Our proposal aims to create a suitable knowledge graph or improve existing ones by discovering missing qualitative triples. To assess the effectiveness of our methodology, we performed experiments on domain-specific datasets, confirming its potential for scalable and versatile knowledge discovery.}
}
@article{YERRAGUNTA2025110142,
title = {Bayesian-error-informed contrastive learning for knowledge-based question answering systems},
journal = {Computers and Electrical Engineering},
volume = {123},
pages = {110142},
year = {2025},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2025.110142},
url = {https://www.sciencedirect.com/science/article/pii/S0045790625000850},
author = {Sudarshan Yerragunta and Rajendra Prasath and G.N. Girish},
keywords = {Question answering, Knowledge-based QA, Incomplete knowledge base, Knowledge aware text reader, Gating mechanisms, Contrastive learning},
abstract = {The Knowledge-base Question Answering (KBQA) system aims to answer a question based on a knowledge base (KB). However, incomplete knowledge bases (KBs) limit the performance of KBQA systems. To address this issue, we propose a contrastive regularization method that considers two modules to tackle this problem: knowledge expansion and a contrastive loss function, Bayesian-error-informed Contrastive Learning (BeCoL). These modules leverage latent knowledge from context KBs and their associated question–answer pairs to generate more such pairs. Additionally, we use these question–answer pairs for informative representation learning, which makes hard positive pairs attract and hard negative pairs separate. This approach will enhance the ability of the system to distinguish the pairs better, ultimately improving the systems performance. We evaluate our proposed approach on the WebQuestionSP (WebQSP), ComplexWebQuestions (CompWebQ), and GrailQA datasets. The results indicate that our approach outperforms existing methods across different KB settings in the WebQSP dataset at 10%, 30%, 50%, and 100% with Hits@1 scores of 43.8, 49.7, 61.3, and 73.7 respectively, and with F1-scores of 28.2, 32.5, 44.3, and 61.1 respectively. Similarly, we achieved Hits@1 score of 52.7 and F1-score of 44.2 on the CompWebQ dataset with 100% KB setting. For the GrailQA dataset under the 100% KB setting, our method attained an Exact Match (EM) score of 67.5 and an F1-score of 76.4. The findings demonstrate the proposed methods capacity to address low-resource settings and significantly improve the performance of KBQA systems. The code is available at https://github.com/ysudarshan-collab/BeCoL.}
}
@article{BAKHSHI2022107626,
title = {SParseQA: Sequential word reordering and parsing for answering complex natural language questions over knowledge graphs},
journal = {Knowledge-Based Systems},
volume = {235},
pages = {107626},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107626},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121008881},
author = {Mahdi Bakhshi and Mohammadali Nematbakhsh and Mehran Mohsenzadeh and Amir Masoud Rahmani},
keywords = {RDF complex question answering, Uncertain question graph construction, Sequential word reordering and parsing, Relation pattern-based similarity},
abstract = {One of the effective approaches for answering natural language questions (NLQs) over knowledge graphs consists of two main stages. It first creates a query graph based on the NLQ and then matches this graph over the knowledge graph to construct a structured query. An obstacle in the first stage is the need to build question interpretations with candidate resources, even if some implicit phrases exist in the sentence. In the second stage, a serious problem is to map diverse NLQ relations to their corresponding predicates. To overcome these problems, in this paper, we propose a novel sequential word parsing-based method to construct and refine an uncertain question graph that is disambiguated directly over the knowledge graph. Instead of relying on the syntactic dependency relations and some predefined rules that recognize the relations and their arguments, we consider the identified entities and variables in the NLQ as well as their corresponding place in the structure of a query graph pattern to build question triples. First, by leveraging the ordered dependency tree of an NLQ, sentence words are reordered. Then the question graph structure is constructed by parsing the new sequence backward, starting from the identified items. Subsequently, the question graph is refined by eliminating the useless elements. Additionally, to improve the relation similarity measure in the graph similarity process, we exploit the knowledge hidden in a relation pattern taxonomy. Experimental studies over several benchmarks demonstrate that our proposed approach is effective as it achieves promising results in answering the complex NLQs.}
}
@article{SUI2024112386,
title = {Multi-level feature interaction for open knowledge base canonicalization},
journal = {Knowledge-Based Systems},
volume = {303},
pages = {112386},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112386},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124010207},
author = {Xuhui Sui and Ying Zhang and Kehui Song and Baohang Zhou and Xiaojie Yuan},
keywords = {Open knowledge base canonicalization, Knowledge representation, Data mining, Feature interaction},
abstract = {Open Information Extraction (OpenIE) aims to construct expansive open knowledge bases (OKBs) by extracting triples (noun phrase, relation phrase, noun phrase) from unstructured text. One critical problem in OKBs is the lack of canonicalization for noun phrases and relation phrases, leading to the storage of redundant and ambiguous facts. Consequently, open knowledge base canonicalization, which clusters synonymous phrases into the same group, has emerged as an active research area. Existing approaches either leverage fact triples or source context in isolation, or at best, interact them at the clustering level. However, these approaches lack explicit interaction or only loosely couple the two types of knowledge, resulting in the potential loss of valuable intermediate information. In this paper, we propose MuFIC, a novel unsupervised framework that interacts the fact triples and source context at the feature level to address these limitations. In order to capture and integrate fine-grained fact and context knowledge, we design three levels of feature interaction: low-level context-guided feature interaction, mid-level fact-guided feature interaction, and high-level gated fusion feature interaction. Furthermore, we introduce an additional objective function via contrastive learning to improve the quality of extracted features and reduce knowledge-specific noise. Finally, we design a bidirectional feedback mechanism to better guide the learning process of joint features by harnessing side information prototype learning, and to dynamically optimize side information based on the clustering results formed by joint features. Extensive experiments on three public benchmark datasets demonstrate the superiority of our proposed framework.}
}
@article{CORDEIRO2024105714,
title = {Petro NLP: Resources for natural language processing and information extraction for the oil and gas industry},
journal = {Computers & Geosciences},
volume = {193},
pages = {105714},
year = {2024},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2024.105714},
url = {https://www.sciencedirect.com/science/article/pii/S0098300424001973},
author = {Fábio Corrêa Cordeiro and Patrícia Ferreira {da Silva} and Alexandre Tessarollo and Cláudia Freitas and Elvis {de Souza} and Diogo {da Silva Magalhaes Gomes} and Renato Rocha Souza and Flávio Codeço Coelho},
keywords = {Natural language processing, Information extraction, Ontology, Knowledge graphs, Linguistic corpora},
abstract = {Most companies struggle to find and extract relevant information from their technical documents. In particular, the Oil and Gas (O&G) industry faces the challenge of dealing with large amounts of data hidden within old and new geoscientific reports collected over decades of operation. Making this information available in a structured format can unlock valuable information among these mountains of data, which is crucial to support a wide range of industrial and academic applications. However, most natural language processing resources were built from general domain corpora extracted from the Internet and primarily written in English. This paper presents Petro NLP, a comprehensive set of natural language processing and information extraction resources for the oil and gas industry in Portuguese. We connected an interdisciplinary team of geoscientists, linguists, computer scientists, petroleum engineers, librarians, and ontologists to build a knowledge graph and several annotated corpora. The Petro NLP resources comprise: (i) Petro KGraph– a knowledge graph populated with entities and relations commonly found on technical reports; and (ii) Petrolês, PetroGold, PetroNER, and PetroRE– sets of corpora containing raw text and documents annotated with morphosyntactic labels, named entities, and relations. These resources are fundamental infrastructure for future research in natural language processing and information extraction in the oil industry. Our ongoing research uses these datasets to train and enhance pre-trained machine learning models that automatically extract information from geoscientific technical documents.}
}
@article{BAG2025114526,
title = {Enhancing cybersecurity risk assessment using temporal knowledge graph-based explainable decision support system},
journal = {Decision Support Systems},
volume = {198},
pages = {114526},
year = {2025},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2025.114526},
url = {https://www.sciencedirect.com/science/article/pii/S0167923625001277},
author = {Subhajit Bag and Sobhan Sarkar and Indranil Bose},
keywords = {Decision support, Cybersecurity policy assessment, Temporal knowledge graph, Attention mechanism, Interpretability},
abstract = {Assessing cybersecurity policies is crucial for any organization to combat evolving cyber threats. The absence of a comprehensive dataset has prevented previous studies from analyzing the risk of organizations’ cybersecurity policies. Past studies have not considered temporal information in the policies. Analysis of cybersecurity policies using attention mechanism requires automated determination of optimal number of attention units which remains unaddressed. Moreover, absence of interpretation in cybersecurity studies creates a barrier to understanding policy vulnerabilities and developing targeted solutions. To address these challenges, we develop a decision support system which (i) enhances risk classification of organization’s cybersecurity policies, (ii) develops a comprehensive cybersecurity policy dataset from the websites of 190 companies, transformed into a knowledge graph to capture entity relationships among various policies, (iii) integrates temporal information into the knowledge graph by incorporating time stamps from event sequences in cyberattack information, (iv) develops Explainable Factor Analysis based Multi-Head Attention mechanism, which automates the determination of the optimal number of attention units and optimizes data allocation across attention units using factor analysis, and (v) utilizes attention heatmaps and shapley values for interpretability. Our cybersecurity policy dataset is used as a case study with four benchmark datasets for further validation. Results reveal that our model outperforms the other state-of-the-art, achieving an 87.78% F1 score, followed by robustness checking and statistical significance testing. Finally, Shapley values are used to interpret the model’s output to identify vulnerabilities within the organizational policies, providing crucial insights enabling decision-makers to enhance their cybersecurity policies and mitigate potential threats.}
}
@article{BABAIHA2024100095,
title = {Rationalism in the face of GPT hypes: Benchmarking the output of large language models against human expert-curated biomedical knowledge graphs},
journal = {Artificial Intelligence in the Life Sciences},
volume = {5},
pages = {100095},
year = {2024},
issn = {2667-3185},
doi = {https://doi.org/10.1016/j.ailsci.2024.100095},
url = {https://www.sciencedirect.com/science/article/pii/S2667318524000023},
author = {Negin Sadat Babaiha and Sathvik Guru Rao and Jürgen Klein and Bruce Schultz and Marc Jacobs and Martin Hofmann-Apitius},
keywords = {Large language models (LLMs), Natural language processing (NLP), Biomedical text mining, Biomedical knowledge graphs, Biological expression language (BEL)},
abstract = {Biomedical knowledge graphs (KGs) hold valuable information regarding biomedical entities such as genes, diseases, biological processes, and drugs. KGs have been successfully employed in challenging biomedical areas such as the identification of pathophysiology mechanisms or drug repurposing. The creation of high-quality KGs typically requires labor-intensive multi-database integration or substantial human expert curation, both of which take time and contribute to the workload of data processing and annotation. Therefore, the use of automatic systems for KG building and maintenance is a prerequisite for the wide uptake and utilization of KGs. Technologies supporting the automated generation and updating of KGs typically make use of Natural Language Processing (NLP), which is optimized for extracting implicit triples described in relevant biomedical text sources. At the core of this challenge is how to improve the accuracy and coverage of the information extraction module by utilizing different models and tools. The emergence of pre-trained large language models (LLMs), such as ChatGPT which has grown in popularity dramatically, has revolutionized the field of NLP, making them a potential candidate to be used in text-based graph creation as well. So far, no previous work has investigated the power of LLMs on the generation of cause-and-effect networks and KGs encoded in Biological Expression Language (BEL). In this paper, we present initial studies towards one-shot BEL relation extraction using two different versions of the Generative Pre-trained Transformer (GPT) models and evaluate its performance by comparing the extracted results to a highly accurate, manually curated BEL KG curated by domain experts.}
}
@article{CHEN2025128864,
title = {Distantly supervised relation extraction with a Meta-Relation enhanced Contrastive learning framework},
journal = {Neurocomputing},
volume = {617},
pages = {128864},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128864},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224016357},
author = {Chuanshu Chen and Shuang Hao and Jian Liu},
keywords = {Distant supervision, Relation extraction, Meta relation pattern, Contrastive learning, Mixup},
abstract = {Distantly supervised relation extraction employs the alignment of unstructured corpora with knowledge bases to automatically generate labeled data. This method, however, often introduces significant label noise. To address this, multi-instance learning has been widely utilized over the past decade, aiming to extract reliable features from a bag of sentences. Yet, multi-instance learning struggles to effectively distinguish between clean and noisy instances within a bag, thereby hindering the full utilization of informative instances and the reduction of the impact of incorrectly labeled instances. In this paper, we propose a new Meta-Relation enhanced Contrastive learning based method for distantly supervised Relation Extraction named MRConRE. Specifically, we generate a “meta relation pattern” (MRP) for each bag, based on its semantic content, to differentiate between clean and noisy instances. Noisy instances are then transformed into beneficial bag-level instances through relabeling. Subsequently, contrastive learning is employed to develop precise sentence representations, forming the overall representation of the bag. Finally, we utilize a mixup strategy to integrate bag-level information for model training. Our method’s effectiveness is validated through experiments on various benchmarks.}
}
@article{ZU2024121759,
title = {SRSCL: A strong-relatedness-sequence-based fine-grained collective entity linking method for heterogeneous information networks},
journal = {Expert Systems with Applications},
volume = {238},
pages = {121759},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121759},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423022613},
author = {Lizheng Zu and Lin Lin and Jie Liu and Song Fu and Changsheng Tong and Hao Guo},
keywords = {Collective entity linking, Heterogeneous information network, Overall relatedness, Strong relatedness sequence, Knowledge representation},
abstract = {The development of efficient methods for mining information from heterogeneous information networks (HINs) has become essential for improving the accuracy of collective entity linking in the absence of third-party knowledge bases. Currently, there remain three major challenges in the latest research: (1) The objective function of collective entity linking does not fully embrace the concept of “collective linking”. (2) The objective function employs the mean value rather than the maximum value of the entity relatedness as a link parameter, while discounting the importance of the strong logical associations between the text and language for meaning recognition. (3) The objective function utilizes only one type of 2-hop path to contribute to entity relatedness, thereby disregarding other types of 2-hop path that exist in actual HINs. To address the aforementioned issues, this paper proposes a strong-relatedness-sequence-based fine-grained collective entity linking method (SRSCL). The SRSCL is capable of capturing the contextual information of the entity in the HINs, thereby providing improved accuracy in entity linking. Specifically, SRSCL constructs a knowledge representation learning model and proposes an overall semantic similarity model for entity mentions and candidate entities to solve the objective function and thereby reflect the idea of “collective linking”. Additionally, a strong-relatedness-sequence-based overall relatedness measurement model is proposed for candidate entities to emphasize the strong logical associations between them. Furthermore, SRSCL defines three types of 2-hop path and evaluates the importance of each path to accurately measure the relatedness of entities. Finally, the experimental results demonstrate that the proposed SRSCL is more effective in capturing the overall relatedness of entities than the latest model. Particularly, when the number of entity mentions contained in one sliding window is greater than 6, the proposed SRSCL improves the precision, recall and F1 score by more than 10% compared with the latest model.}
}
@article{SU2024332,
title = {Chinese Nested Named Entity Recognition Algorithm Based on Knowledge Graph},
journal = {Procedia Computer Science},
volume = {243},
pages = {332-339},
year = {2024},
note = {The 4th International Conference on Machine Learning and Big Data Analytics for IoT Security and Privacy},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.041},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924020489},
author = {Qianqian Su and Zhenyu Yan and He Wang},
keywords = {Chinese Nested, Named Entity Recognition, Knowledge Graph, Information Extraction, Recognition Accuracy},
abstract = {Nested named entity recognition has been widely used in natural language processing, information extraction and other fields. However, the multiple boundaries of nested named entities make the recognition of single entity face great challenges. This article used the information in the knowledge graph to extract and annotate entities in the Chinese nested named entity recognition algorithm, and built a new named entity recognition model based on this. Experimental results showed that this article applied the knowledge graph to the Chinese nested named entity recognition task, and its accuracy could reach up to 95.3%, which has obvious advantages in complex structure processing, relationship extraction and entity links.}
}
@article{AHMED2024103579,
title = {CyberEntRel: Joint extraction of cyber entities and relations using deep learning},
journal = {Computers & Security},
volume = {136},
pages = {103579},
year = {2024},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2023.103579},
url = {https://www.sciencedirect.com/science/article/pii/S0167404823004893},
author = {Kashan Ahmed and Syed Khaldoon Khurshid and Sadaf Hina},
keywords = {Cyber threat intelligence, Deep learning, Named entity recognition, Relation extraction, Knowledge graph},
abstract = {The cyber threat intelligence (CTI) knowledge graph is beneficial for making robust defense strategies for security professionals. These are built from cyber threat intelligence data based on relation triples where each relation triple contains two entities associated with one relation. The main problem is that the CTI data is increasing more rapidly than expected and existing techniques are becoming ineffective for extracting the CTI information. This work mainly focuses on the extraction of cyber relation triples in an effective way using the joint extraction technique, which resolves the issues in the classical pipeline technique. Firstly, the ‘BIEOS’ tagging scheme was applied to CTI data using the joint tagging technique and then the relation triples were jointly extracted. This study utilized the attention-based RoBERTa-BiGRU-CRF model for sequential tagging. Finally, the relation triples were extracted using the relation-matching technique after matching the best suitable relation for the two predicted entities. The experimental results showed that this technique outperformed the state-of-the-art models in knowledge triple extraction on CTI data. Furthermore, a 7% increase in the F1 score also proved the effectiveness of this technique for the information extraction task on CTI data.}
}
@article{CUI2025104861,
title = {A review on knowledge graphs for healthcare: Resources, applications, and promises},
journal = {Journal of Biomedical Informatics},
volume = {169},
pages = {104861},
year = {2025},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2025.104861},
url = {https://www.sciencedirect.com/science/article/pii/S1532046425000905},
author = {Hejie Cui and Jiaying Lu and Ran Xu and Shiyu Wang and Wenjing Ma and Yue Yu and Shaojun Yu and Xuan Kan and Chen Ling and Liang Zhao and Zhaohui S. Qin and Joyce C. Ho and Tianfan Fu and Jing Ma and Mengdi Huai and Fei Wang and Carl Yang},
keywords = {Knowledge graph, Healthcare, language models, Multimodality, Interpretable AI},
abstract = {Objective:
This comprehensive review aims to provide an overview of the current state of Healthcare Knowledge Graphs (HKGs), including their construction, utilization models, and applications across various healthcare and biomedical research domains.
Methods:
We thoroughly analyzed existing literature on HKGs, covering their construction methodologies, utilization techniques, and applications in basic science research, pharmaceutical research and development, clinical decision support, and public health. The review encompasses both model-free and model-based utilization approaches and the integration of HKGs with large language models (LLMs).
Results:
We searched Google Scholar for relevant papers on HKGs and classified them into the following topics: HKG construction, HKG utilization, and their downstream applications in various domains. We also discussed their special challenges and the promise for future work.
Discussion:
The review highlights the potential of HKGs to significantly impact biomedical research and clinical practice by integrating vast amounts of biomedical knowledge from multiple domains. The synergy between HKGs and LLMs offers promising opportunities for constructing more comprehensive knowledge graphs and improving the accuracy of healthcare applications.
Conclusions:
HKGs have emerged as a powerful tool for structuring medical knowledge, with broad applications across biomedical research, clinical decision-making, and public health. This survey serves as a roadmap for future research and development in the field of HKGs, highlighting the potential of combining knowledge graphs with advanced machine learning models for healthcare transformation.}
}
@article{KHATUN2025113756,
title = {Classification of triple extraction and RDF generation using boosted BERT fused attention convolutional Bi-directional LSTM with optimization},
journal = {Knowledge-Based Systems},
volume = {324},
pages = {113756},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113756},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125008020},
author = {Rubaya Khatun and Arup Sarkar},
keywords = {Resource description frameworks, Triples, Bidirectional encoder representations from transformers, Bi-LSTM, Bald eagle search optimization},
abstract = {Knowledge and expertise are the two most frequent ways of sharing information in a structured manner. A depth of knowledge is typically made up of triples of Resource Description Frameworks (RDF) that characterize the entities and their relationships. Some of the current limitations include higher computational costs, longer execution times, and more complexity in achieving better results. The machine learning based techniques generate lower performance compared to deep learning approaches. Initially, the data are collected using BBC News, Kaggle, and Lonely Planet datasets. After the collection of data, pre-processing steps like data cleaning, Parts of Speech (PoS) tagging, tokenization, stop word removal, and stemming are applied to the data to enhance further performance. The entity and attribute features are extracted by Boosted Bidirectional Encoder Representations from Transformers (B-BERT). The classification can be performed using fused attention with a convolutional Bidirectional LSTM (AcBL) approach. An improved Bald Eagle Search Optimization (EBesO) algorithm is used to optimize the loss derived from the classification phase to improve model accuracy. The overall performance outcomes of the proposed method attain 99.1 % accuracy on BBC, 99.7 % accuracy on Kaggle, and 99 % accuracy on Lonely Planet. The proposed technique acquires 97.9 % precision on BBC, 98.7 % precision on Kaggle, and 97.9 % precision on Lonely Planet. The F1 score of the proposed methodology accomplishes 97.9 % on BBC, 98.6 % on Kaggle, and 97.8 % on Lonely Planet. Recall that the proposed model achieves 98 % on BBC, 98.5 % on Kaggle, and 97.6 % on Lonely Planet.}
}
@article{BI2025861,
title = {Syntax-Enhanced Entity Relation Extraction with Complex Knowledge},
journal = {Computers, Materials and Continua},
volume = {83},
number = {1},
pages = {861-876},
year = {2025},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2025.060517},
url = {https://www.sciencedirect.com/science/article/pii/S1546221825003005},
author = {Mingwen Bi and Hefei Chen and Zhenghong Yang},
keywords = {Entity relation extraction, complex knowledge, syntax-enhanced, semantic interaction, pre-trained BERT},
abstract = {Entity relation extraction, a fundamental and essential task in natural language processing (NLP), has garnered significant attention over an extended period., aiming to extract the core of semantic knowledge from unstructured text, i.e., entities and the relations between them. At present, the main dilemma of Chinese entity relation extraction research lies in nested entities, relation overlap, and lack of entity relation interaction. This dilemma is particularly prominent in complex knowledge extraction tasks with high-density knowledge, imprecise syntactic structure, and lack of semantic roles. To address these challenges, this paper presents an innovative “character-level” Chinese part-of-speech (CN-POS) tagging approach and incorporates part-of-speech (POS) information into the pre-trained model, aiming to improve its semantic understanding and syntactic information processing capabilities. Additionally, A relation reference filling mechanism (RF) is proposed to enhance the semantic interaction between relations and entities, utilize relations to guide entity modeling, improve the boundary prediction ability of entity models for nested entity phenomena, and increase the cascading accuracy of entity-relation triples. Meanwhile, the “Queue” sub-task connection strategy is adopted to alleviate triplet cascading errors caused by overlapping relations, and a Syntax-enhanced entity relation extraction model (SE-RE) is constructed. The model showed excellent performance on the self-constructed E-commerce Product Information dataset (EPI) in this article. The results demonstrate that integrating POS enhancement into the pre-trained encoding model significantly boosts the performance of entity relation extraction models compared to baseline methods. Specifically, the F1-score fluctuation in subtasks caused by error accumulation was reduced by 3.21%, while the F1-score for entity-relation triplet extraction improved by 1.91%.}
}
@article{ISAEE2025121643,
title = {Addressing the challenges of open n-ary relation extraction with a deep learning-driven approach},
journal = {Information Sciences},
volume = {692},
pages = {121643},
year = {2025},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.121643},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524015573},
author = {Mitra Isaee and Afsaneh Fatemi and Mohammadali Nematbakhsh},
keywords = {Natural language processing, Open relation extraction, Open-domain text, N-ary relation, Entity embedding, SpanBERT},
abstract = {Open relation extraction is a critical task in natural language processing aimed at automatically extracting relations between entities in open-domain corpora. Most existing systems focus on extracting binary relations (relations between two entities) while extracting more complex n-ary relations (involving more than two entities) remains a significant challenge. Additionally, many previous systems rely on hand-crafted patterns and natural language processing tools, which result in error accumulation and reduced accuracy. The current study proposes a novel approach to open n-ary relation extraction that leverages recent advancements in deep learning architectures. This approach addresses the limitations of existing open relation extraction systems, particularly their reliance on hand-crafted patterns and their focus on binary relations. It utilizes SpanBERT to capture relational patterns from text data directly and introduces entity embedding vectors to create distinct representations of entities within sentences. These vectors enhance the proposed system’s understanding of the entities within the input sentence, leading to more accurate relation extraction. Notably, the proposed system in the present study achieves an F1-score of 89.79 and 92.67 on the LSOIE-wiki and OpenIE4 datasets, outperforming the best existing models by over 12% and 10%, respectively. These results highlight the effectiveness of the proposed approach in addressing the challenges of open n-ary relation extraction.}
}
@article{TANEJA2023104341,
title = {Developing a Knowledge Graph for Pharmacokinetic Natural Product-Drug Interactions},
journal = {Journal of Biomedical Informatics},
volume = {140},
pages = {104341},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2023.104341},
url = {https://www.sciencedirect.com/science/article/pii/S153204642300062X},
author = {Sanya B. Taneja and Tiffany J. Callahan and Mary F. Paine and Sandra L. Kane-Gill and Halil Kilicoglu and Marcin P. Joachimiak and Richard D. Boyce},
keywords = {Biomedical ontology, Interactions, Knowledge graph, Knowledge representation, Literature-based discovery, Natural products, Pharmacokinetics},
abstract = {Background
Pharmacokinetic natural product-drug interactions (NPDIs) occur when botanical or other natural products are co-consumed with pharmaceutical drugs. With the growing use of natural products, the risk for potential NPDIs and consequent adverse events has increased. Understanding mechanisms of NPDIs is key to preventing or minimizing adverse events. Although biomedical knowledge graphs (KGs) have been widely used for drug-drug interaction applications, computational investigation of NPDIs is novel. We constructed NP-KG as a first step toward computational discovery of plausible mechanistic explanations for pharmacokinetic NPDIs that can be used to guide scientific research.
Methods
We developed a large-scale, heterogeneous KG with biomedical ontologies, linked data, and full texts of the scientific literature. To construct the KG, biomedical ontologies and drug databases were integrated with the Phenotype Knowledge Translator framework. The semantic relation extraction systems, SemRep and Integrated Network and Dynamic Reasoning Assembler, were used to extract semantic predications (subject-relation-object triples) from full texts of the scientific literature related to the exemplar natural products green tea and kratom. A literature-based graph constructed from the predications was integrated into the ontology-grounded KG to create NP-KG. NP-KG was evaluated with case studies of pharmacokinetic green tea- and kratom-drug interactions through KG path searches and meta-path discovery to determine congruent and contradictory information in NP-KG compared to ground truth data. We also conducted an error analysis to identify knowledge gaps and incorrect predications in the KG.
Results
The fully integrated NP-KG consisted of 745,512 nodes and 7,249,576 edges. Evaluation of NP-KG resulted in congruent (38.98% for green tea, 50% for kratom), contradictory (15.25% for green tea, 21.43% for kratom), and both congruent and contradictory (15.25% for green tea, 21.43% for kratom) information compared to ground truth data. Potential pharmacokinetic mechanisms for several purported NPDIs, including the green tea-raloxifene, green tea-nadolol, kratom-midazolam, kratom-quetiapine, and kratom-venlafaxine interactions were congruent with the published literature.
Conclusion
NP-KG is the first KG to integrate biomedical ontologies with full texts of the scientific literature focused on natural products. We demonstrate the application of NP-KG to identify known pharmacokinetic interactions between natural products and pharmaceutical drugs mediated by drug metabolizing enzymes and transporters. Future work will incorporate context, contradiction analysis, and embedding-based methods to enrich NP-KG. NP-KG is publicly available at https://doi.org/10.5281/zenodo.6814507. The code for relation extraction, KG construction, and hypothesis generation is available at https://github.com/sanyabt/np-kg.}
}
@article{QU20243583,
title = {A Review of Knowledge Graph in Traditional Chinese Medicine: Analysis, Construction, Application and Prospects},
journal = {Computers, Materials and Continua},
volume = {81},
number = {3},
pages = {3583-3616},
year = {2024},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2024.055671},
url = {https://www.sciencedirect.com/science/article/pii/S1546221824008294},
author = {Xiaolong Qu and Ziwei Tian and Jinman Cui and Ruowei Li and Dongmei Li and Xiaoping Zhang},
keywords = {Systematic review, traditional Chinese medicine, knowledge graph, deep learning, medical applications},
abstract = {As an advanced data science technology, the knowledge graph systematically integrates and displays the knowledge framework within the field of traditional Chinese medicine (TCM). This not only contributes to a deeper comprehension of traditional Chinese medical theories but also provides robust support for the intelligent decision systems and medical applications of TCM. Against this backdrop, this paper aims to systematically review the current status and development trends of TCM knowledge graphs, offering theoretical and technical foundations to facilitate the inheritance, innovation, and integrated development of TCM. Firstly, we introduce the relevant concepts and research status of TCM knowledge graphs. Secondly, we conduct an in-depth analysis of the challenges and trends faced by key technologies in TCM knowledge graph construction, such as knowledge representation, extraction, fusion, and reasoning, and classifies typical knowledge graphs in various subfields of TCM. Next, we comprehensively outline the current medical applications of TCM knowledge graphs in areas such as information retrieval, diagnosis, question answering, recommendation, and knowledge mining. Finally, the current research status and future directions of TCM knowledge graphs are concluded and discussed. We believe this paper contributes to a deeper understanding of the research dynamics in TCM knowledge graphs and provides essential references for scholars in related fields.}
}
@article{TSANEVA2025104145,
title = {Knowledge graph validation by integrating LLMs and human-in-the-loop},
journal = {Information Processing & Management},
volume = {62},
number = {5},
pages = {104145},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2025.104145},
url = {https://www.sciencedirect.com/science/article/pii/S030645732500086X},
author = {Stefani Tsaneva and Danilo Dessì and Francesco Osborne and Marta Sabou},
keywords = {Knowledge graph validation, Large language models, Hybrid human-AI workflows},
abstract = {Ensuring the quality of knowledge graphs (KGs) is crucial for the success of the intelligent applications they support. Recent advances in large language models (LLMs) have demonstrated human-level performance across various tasks, raising the question of their potential for KG validation. In this work, we explore the role of LLMs in human-centric KG validation workflows, examining different collaboration strategies between LLMs and domain experts. We propose and evaluate nine distinct approaches, ranging from fully automated validation to hybrid methods that combine expert oversight with AI assistance. These workflows are tested within a real-world KG construction pipeline used to generate the Computer Science Knowledge Graph (CS-KG), a large-scale resource designed to support scientometric tasks such as trend forecasting and hypothesis generation. CS-KG comprises 41 million statements represented as 350 million triples within the Computer Science domain. Our findings show that integrating LLMs into the CS-KG verification process enhances precision by 12%, improving alignment with expert-level validation. However, this comes at the cost of recall, resulting in a 5% decrease in the overall F1 score. In contrast, a hybrid approach which involves both human-in-the-loop and LLM modules, yields the best overall results, improving F1 score by 5% with minimal human involvement.}
}
@article{LIANG2025130771,
title = {Verbalization diversification for relation extraction with long-tailed entity augmentation},
journal = {Neurocomputing},
volume = {649},
pages = {130771},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.130771},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225014432},
author = {Yiping Liang and Tianbao Jiang and Xin Yi and Yan Cai and Linlin Wang and Xiaoling Wang and Liang He},
keywords = {Relation extraction, Tail generalization, Few-shot learning},
abstract = {In recent years, there has been growing interest in few-shot relation extraction on real-world datasets with long-tailed distribution. However, existing approaches often lack sufficient generalization abilities with limited annotations. In this paper, we propose an easy-and-effective architecture that gives sufficient consideration to relation verbalization diversification and tail generalization. Specifically, we focus on optimizing and diversifying verbalizations of relation labels to improve the performance of sentence-level relation extraction in low-resource settings. With augmented data, the architecture can benefit from the knowledge and patterns of common entities contained within the pre-trained model when facing long-tailed entities. The experimental results on TACRED, TACREV, Re-TACRED show that our model can be significantly and consistently comparable to existing state-of-the-art baselines.}
}
@article{CHEN2025131230,
title = {Knowledge graph and large language model integration with focus on educational applications: A survey},
journal = {Neurocomputing},
volume = {654},
pages = {131230},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.131230},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225019022},
author = {Guanyu Chen and Tao Song and Quanyu Wang and Zheng Ma and Jun Hu and Qi Li and Chunming Wu},
keywords = {Knowledge graph, Large language model, Educational application, Retrieval augmented generation, Pre-training},
abstract = {In recent years, artificial intelligence (AI) technology has made significant advancements, particularly in the areas of large language models (LLMs) and knowledge graphs (KGs). KGs excel at structured knowledge representation and reasoning, offering interpretability; however, they are costly to construct, have limited coverage, and lack natural language processing capabilities. Conversely, LLMs possess powerful language understanding and generation abilities, but they rely heavily on vast amounts of data, are prone to “hallucinations," and lack interpretability. The integration of these two approaches is an inevitable trend for achieving stronger and more reliable AI applications, and has become a hot topic of research. Simultaneously, the combination of LLMs and KGs perfectly aligns with the pressing needs of the education field for precise reasoning and personalized services, addressing the shortcomings of traditional teaching methods and providing support for intelligent education. In light of this, this paper undertakes work in the following three key areas. Firstly, the concepts and technologies of both LLMs and KGs, along with their applications in education, are introduced. On this basis, the paper then delves into a discussion of the methods for integrating LLMs and KGs, and reviews related research progress. Finally, the paper focuses on specific educational scenarios, such as intelligent tutoring systems, intelligent learning companions, and intelligent evaluation systems, to explore the collaborative application of LLMs and KGs. The aim of this paper is to provide researchers in the field with a systematic understanding of LLMs and KGs, and to offer valuable references for future AI-driven educational innovation.}
}
@article{BAI2026102503,
title = {Time-Aware Complex Question Answering over Temporal Knowledge Graph},
journal = {Data & Knowledge Engineering},
volume = {161},
pages = {102503},
year = {2026},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2025.102503},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X25000989},
author = {Luyi Bai and Tongyue Zhang and Guangchen Feng},
keywords = {Attention mechanism, Question answering, Temporal knowledge graph, Temporal knowledge graph embedding},
abstract = {Knowledge Graph Question Answering (KGQA) is a crucial topic in Knowledge Graphs (KGs), with the objective of retrieving the corresponding facts from KGs to answer given questions. In practical applications, facts in KGs usually have time constraints, thus, question answering on Temporal Knowledge Graphs (TKGs) has attracted extensive attention. Existing Temporal Knowledge Graph Question Answering (TKGQA) methods focus on dealing with complex questions involving multiple facts, and mainly face two challenges. First, these methods only consider matching questions with facts in TKGs to identify the answer, ignoring the temporal order between different facts, which makes it challenging to solve the questions involving temporal order. Second, they usually focus on the representation of the question text while neglecting the rich semantic information within the questions, which leads to certain limitations in understanding question. To address the above challenges, this research proposes a model named Time-Aware Complex Question Answering (TA-CQA). Specifically, we extend the Temporal Knowledge Graph Embedding (TKGE) model by incorporating temporal order information into the embedding vectors, ensuring that the model can distinguish the temporal order of different facts. To enhance the semantic representation of the question, we integrate question information using attention mechanism and learnable encoder. Different from the previous TKGQA methods, we propose time relevance measurement to further enhance the accuracy of answer prediction by better capturing the correlation between question information and time information. Multiple sets of experiments on CronQuestions and TimeQuestions demonstrate our model’s superior performance across all question types. In particular, for complex questions involving multiple facts, the hit@1 values are increased by 3.2% and 3.5% respectively.}
}
@article{ZHU2022108445,
title = {SwitchNet: A modular neural network for adaptive relation extraction},
journal = {Computers and Electrical Engineering},
volume = {104},
pages = {108445},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.108445},
url = {https://www.sciencedirect.com/science/article/pii/S0045790622006607},
author = {Hongyin Zhu and Prayag Tiwari and Yazhou Zhang and Deepak Gupta and Meshal Alharbi and Tri Gia Nguyen and Shahram Dehdashti},
keywords = {Relation extraction, Modular neural network, Information flow, Joint optimization, Entity pair},
abstract = {This paper presents a portable toolkit, SwitchNet, for extracting relations from textual input. We summarize four data protocols for relation extraction tasks, including relation classification, relation extraction, triple extraction, and distant supervision relation extraction. This neural architecture is modular, so it can take as input data at different stages of the information extraction process (simple text, text and entities or entity pairs as relation candidates) and compute the rest of the process (named entity recognition and relation classification). We systematically design four information flows to integrate the above protocols by sharing network building blocks and switching different information flows. This framework can extract multiple triples (subject, predicate, object) in one pass. This framework enhances the use of relation classification models in end-to-end triple extraction by inferring pairs of entities of interest and using the shared representation mechanism.}
}
@article{CHAUDHARY2025127612,
title = {Fact retrieval from knowledge graphs through semantic and contextual attention},
journal = {Expert Systems with Applications},
volume = {282},
pages = {127612},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127612},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425012345},
author = {Akhil Chaudhary and Enayat Rajabi and Somayeh Kafaie and Evangelos Milios},
keywords = {Querying, Knowledge graphs, Fact retrieval, Information extraction, BERT, Zero shot},
abstract = {Knowledge Graphs (KGs), such as DBpedia and ConceptNet, enhance Natural Language Processing (NLP) applications by providing structured information. However, extracting accurate data from KGs is challenging due to issues in entity detection, disambiguation, and relation classification, which often lead to errors and inefficiencies. We introduce Attention2Query (A2Q), an attention-driven approach that directly ranks and selects the most relevant facts, thus minimizing error propagation. A2Q centres on three key contributions: (1) Focused Node Selection, which streamlines graph traversal; (2) Global Attention Alignment, improving retrieval by comparing facts against the query text; and (3) Contextual Re-ranking, enabling on-the-fly adjustments of fact importance based on evolving query context. Experimental results across multiple tasks and datasets show that A2Q substantially outperforms baseline methods, including those in zero-shot settings, achieving higher retrieval accuracy with reduced computational overhead.}
}
@article{YANG2023110823,
title = {Generating paraphrase sentences for multimodal entity-category-sentiment triple extraction},
journal = {Knowledge-Based Systems},
volume = {278},
pages = {110823},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.110823},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123005737},
author = {Li Yang and Jieming Wang and Jin-Cheon Na and Jianfei Yu},
keywords = {Entity-based sentiment analysis, Fine-grained opinion mining, Multimodal sentiment analysis},
abstract = {Multimodal entity-based sentiment analysis (MEBSA) is an emerging task in sentiment analysis that aims to identify three key elements (entity, entity category, and sentiment polarity) from a pair of sentence and image. However, most existing studies have primarily focused on one or two MEBSA subtasks, ignoring the fact that these subtasks are closely related with one another. Moreover, previous studies focused on the detection of coarse-grained entity categories, which failed to provide sufficient information to disambiguate entities. To address these two issues, we introduced a new task called multimodal entity-category-sentiment triple extraction (MECSTE) to extract entities, their corresponding fine-grained entity categories, and sentiment polarities simultaneously. We constructed two datasets for this task based on two existing Twitter corpora. Moreover, we developed a generative multimodal approach based on a pre-trained sequence-to-sequence model that formulates the MECSTE task as a paraphrase generation problem by linearising all the entity-category-sentiment triples into a natural language sentence. Extensive experiments on the annotated Twitter datasets demonstrate the superiority of the proposed method.}
}
@article{LAKSHIKA2025102451,
title = {ECS-KG: An event-centric semantic knowledge graph for event-related news articles},
journal = {Data & Knowledge Engineering},
volume = {159},
pages = {102451},
year = {2025},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2025.102451},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X25000461},
author = {MVPT Lakshika and HA Caldera and TNK {De Zoysa}},
keywords = {Semantic KG, Event-centric KG, Contextual embedding, Graph attention networks, Temporal graph neural networks},
abstract = {Recent advances in deep learning techniques and contextual understanding render Knowledge Graphs (KGs) valuable tools for enhancing accessibility and news comprehension. Conventional and news-specific KGs frequently lack the specificity for efficient news-related tasks, leading to limited relevance and static data representation. To fill the gap, this study proposes an Event-Centric Semantic Knowledge Graph (ECS-KG) model that combines deep learning approaches with contextual embeddings to improve the procedural and dynamic knowledge representation observed in news articles. The ECS-KG incorporates several information extraction techniques, a temporal Graph Neural Network (GNN), and a Graph Attention Network (GAT), yielding significant improvements in news representation. Several gold-standard datasets, comprising CNN/Daily Mail, TB-Dense, and ACE 2005, revealed that the proposed model outperformed the most advanced models. By integrating temporal reasoning and semantic insights, ECS-KG not only enhances user understanding of news significance but also meets the evolving demands of news consumers. This model advances the field of event-centric semantic KGs and provides valuable resources for applications in news information processing.}
}
@article{ATKINSON2022100115,
title = {Evolutionary natural-language coreference resolution for sentiment analysis},
journal = {International Journal of Information Management Data Insights},
volume = {2},
number = {2},
pages = {100115},
year = {2022},
issn = {2667-0968},
doi = {https://doi.org/10.1016/j.jjimei.2022.100115},
url = {https://www.sciencedirect.com/science/article/pii/S2667096822000581},
author = {John Atkinson and Alex Escudero},
keywords = {Natural-language processing, Sentiment analysis, Coreference resolution, Information management, Social media analysis, Evolutionary computation},
abstract = {Communicating messages on social media usually conveys much implicit linguistic knowledge, which makes it difficult to process texts for further analysis. One of the major problems, the linguistic coreference resolution task involves detecting coreference chains of entities and pronouns that coreference them. It has mostly been addressed for formal and full-sized text in which a relatively clear discourse structure can be discovered, using Natural-Language Processing techniques. However, texts in social media are short, informal and lack a lot of underlying linguistic information to make decisions so traditional methods can not be applied. Furthermore, this may significantly impact the performance of several tasks on social media applications such as opinion mining, network analysis, sentiment analysis, text categorization. In order to deal with these issues, this research address the task of linguistic co-referencing using an evolutionary computation approach. It combines discourse coreference analysis techniques, domain-based heuristics (i.e., syntactic, semantic and world knowledge), graph representation methods, and evolutionary computation algorithms to resolving implicit co-referencing within informal opinion texts. Experiments were conducted to assess the ability of the model to find implicit referents on informal messages, showing the promise of our approach when compared to related methods.}
}
@article{TAN2023120721,
title = {CLRN: A reasoning network for multi-relation question answering over Cross-lingual Knowledge Graphs},
journal = {Expert Systems with Applications},
volume = {231},
pages = {120721},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120721},
url = {https://www.sciencedirect.com/science/article/pii/S095741742301223X},
author = {Yiming Tan and Xinyu Zhang and Yongrui Chen and Zafar Ali and Yuncheng Hua and Guilin Qi},
keywords = {Question answering, Cross-lingual knowledge graphs, Multi-hop reasoning, Entity alignment},
abstract = {Cross-lingual Knowledge Graphs-based Question Answering (CLKGQA) requires the question answering (QA) system to combine the knowledge graphs (KGs) in different languages to obtain answers to input questions. In previous works, the common idea is to merge Cross-lingual Knowledge Graphs (CLKGs) into a single KG through aligned entity pairs and then treat it as a traditional KG-based QA. However, as demonstrated by Tan et al. (2023), existing Entity Alignment (EA) models cannot generate highly accurate aligned entity pairs for CLKGs. Therefore, two issues need to be addressed in the CLKGQA task: (1) Remove the dependency of the QA model on the fused KG; (2) Improve the performance of the EA model in obtaining aligned entity pairs from locally isomorphic CLKGs. To solve the above two issues, this paper presents Cross-lingual Reasoning Network (CLRN), a novel multi-hop QA model that allows switching knowledge graphs at any stage of the multi-hop reasoning. Furthermore, we establish an iterative framework that combines CLRN and EA model, in which CLRN is used for extracting potential alignment triple pairs from CLKGs during the QA process. The extracted triple pairs provide pseudo-aligned entities, and the additional aligned entity pairs are used to mine missing relations between entities in CLKGs. These pseudo-aligned entity pairs and relations improve the performance of the EA model, resulting in higher accuracy in QA. Extensive experiments demonstrate the effectiveness of the proposed model, which outperforms the baseline approaches. Through iterative enhancement, the performance of the EA model has also been improved by > 1.0 % in Hit@1 and Hit@10, and the improvement is statistically significant in the confidence interval of p<0.01. Moreover, our work discusses the correlation between QA and EA from the side of QA, which has reference value for the follow-up exploration of related communities. We have open-sourced our dataset and code, which is available at the URL https://github.com/tan92hl/Cross-lingual-Reasoning-Network-for-CLKGQA.}
}
@article{CARTA20232224,
title = {SailGenie: SAiling expertIse to knowLedge Graph through opEN Information Extraction},
journal = {Procedia Computer Science},
volume = {225},
pages = {2224-2233},
year = {2023},
note = {27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.213},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923013716},
author = {Salvatore Carta and Pietro Fariello and Alessandro Giuliani and Leonardo Piano and Alessandro Sebastian Podda and Sandro Gabriele Tiddia},
keywords = {Artificial Intelligence, Knowledge graphs, Sailing, Domain-specific dataset},
abstract = {This work is focused on the sailing domain, for which several innovative technologies are being adopted to improve sailing efficiency, performance, and safety. In this context a knowledge graph could be used, for example, to represent information about different types of boats, sailing techniques, maritime safety, or weather conditions. Although numerous construction methods or ready-to-go knowledge graphs have been proposed in many fields, the sailing domain still needs to be explored. As the most effective methods rely on domain-specific datasets, the absence of suitable and available sailing datasets is one of the main challenges. Although several Open Information Extraction (OpenIE) methods may generate relevant triplets (the elementary units composing a knowledge graph) from arbitrary text without any additional information about its topic, such methods usually generate many incorrect triplets. In this paper, we aim (i) to address the aforementioned problem by proposing an innovative method that combines in an improved and strengthened way different OpenIE tools to generate proper triplets from domain-specific sources and, in particular, (ii) to build and release a suitable dataset for the sailing domain. Results confirm that our proposal can maximize the extracted information and infer unique information irretrievable by the classical OpenIE tools and, furthermore, that the generated dataset is significantly valuable for the sailing scenario.}
}
@article{WEN2023e20390,
title = {A systematic knowledge graph-based smart management method for operations: A case study of standardized management},
journal = {Heliyon},
volume = {9},
number = {10},
pages = {e20390},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e20390},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023075989},
author = {Peihan Wen and Yiming Zhao and Jin Liu},
keywords = {Knowledge graph, Operation management, Quality management system, Knowledge management},
abstract = {Standardized routine operation management (SROM) has been widely accepted and applied by kinds of enterprises and played a key supporting role. With full use of the emerging knowledge-based smart management technology, SROM will further increase comprehensive efficiency and save human resources greatly at the same time, especially for small and medium enterprises (SMEs). Hence, we propose a systematic knowledge-based smart management method to transfer SROM activities from human operations to automatic response by means of knowledge explicitation, organization, sharing and reusing, which can be further achieved by employing knowledge graph. We took a typical SROM instance, ISO 9000 implementation management, as an example to validate the transformation from human activities to knowledge graph-based automatic operation. We firstly analyzed characteristics of domain knowledge and constructed an ontology model according to the knowledge stability. Secondly, a hybrid knowledge graph construction and dynamic updating framework together with related algorithms were designed by deliberately integrating semantic similarity calculation and natural language processing. Thirdly, we developed a question-answering mechanism and reasoning system based on the ISO 9000 implementation knowledge graph to support automatic decision and feedback for ISO 9000 routine operation management including knowledge learning and processes auditing. Finally, the practicability and effectiveness of SROM knowledge graph has been validated in a SME in China, realizing the application of question-answering, job responsibility recommendation, conflict detection, semantic detection, multidimensional statistical analysis. The proposed method can also be generalized to support auxiliary optimization decision, vertical risk control, operation mode analysis, optimization model improvement experience and so on.}
}
@article{LEONG2025102331,
title = {MERMaid: Universal multimodal mining of chemical reactions from PDFs using vision-language models},
journal = {Matter},
pages = {102331},
year = {2025},
issn = {2590-2385},
doi = {https://doi.org/10.1016/j.matt.2025.102331},
url = {https://www.sciencedirect.com/science/article/pii/S2590238525003741},
author = {Shi Xuan Leong and Sergio Pablo-García and Brandon Wong and Alán Aspuru-Guzik},
keywords = {data mining, digitization, database, electro-organic synthesis, knowledge graphs, organic synthesis, photocatalysis, retrieval-augmented generation, vision large language models},
abstract = {Summary
Data digitization of scientific literature is essential for creating machine-actionable knowledge bases to advance data-driven research and integrate with self-driving laboratories. It is especially critical to extract, interpret, and structure data from graphical elements, the primary medium for conveying complex scientific insights. However, this remains challenging due to the inherent lack of semantic structure in the prevalent PDF format, the complexity of visual content, and the need for multimodal integration. We present MERMaid (multimodal aid for reaction mining), an end-to-end pipeline that converts disparate visual data across PDFs into a coherent knowledge graph. Leveraging the emergent visual cognition and reasoning capabilities of vision-language models, MERMaid demonstrates chemical context awareness, self-directed context completion, and robust coreference resolution to achieve 87% end-to-end accuracy across three chemical domains. Its modular design facilitates future application to diverse data beyond reaction mining, promising to unlock the full potential of scientific literature for knowledge-intensive applications.}
}
@article{YANG2025110285,
title = {Alzheimer's disease knowledge graph enhances knowledge discovery and disease prediction},
journal = {Computers in Biology and Medicine},
volume = {192},
pages = {110285},
year = {2025},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2025.110285},
url = {https://www.sciencedirect.com/science/article/pii/S0010482525006365},
author = {Yue Yang and Kaixian Yu and Shan Gao and Sheng Yu and Di Xiong and Chuanyang Qin and Huiyuan Chen and Jiarui Tang and Niansheng Tang and Hongtu Zhu},
keywords = {Alzheimer's disease, Disease prediction, Knowledge graph construction, Link prediction},
abstract = {Objective
To construct an Alzheimer's Disease Knowledge Graph (ADKG) by extracting and integrating relationships among Alzheimer's disease (AD), genes, variants, chemicals, drugs, and other diseases from biomedical literature, aiming to identify existing treatments, potential targets, and diagnostic methods for AD.
Methods
We annotated 800 PubMed abstracts (ADERC corpus) with 20,886 entities and 4935 relationships, augmented via GPT-4. A SpERT model (SciBERT-based) trained on this data extracted relations from PubMed abstracts, supported by biomedical databases and entity linking refined via abbreviation resolution/string matching. The resulting knowledge graph trained embedding models to predict novel relationships. ADKG's utility was validated by integrating it with UK Biobank data for predictive modeling.
Results
The ADKG contained 3,199,276 entity mentions and 633,733 triplets, linking >5K unique entities and capturing complex AD-related interactions. Its graph embedding models produced evidence-supported predictions, enabling testable hypotheses. In UK Biobank predictive modeling, ADKG-enhanced models achieved higher AUROC of 0.928 comparing to 0.903 without ADKG enhancement.
Conclusion
By synthesizing literature-derived insights into a computable framework, ADKG bridges molecular mechanisms to clinical phenotypes, advancing precision medicine in Alzheimer's research. Its structured data and predictive utility underscore its potential to accelerate therapeutic discovery and risk stratification.}
}
@article{DOST2022101975,
title = {Aligning and linking entity mentions in image, text, and knowledge base},
journal = {Data & Knowledge Engineering},
volume = {138},
pages = {101975},
year = {2022},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2021.101975},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X2100094X},
author = {Shahi Dost and Luciano Serafini and Marco Rospocher and Lamberto Ballan and Alessandro Sperduti},
keywords = {AI, NLP, Computer Vision, Machine Learning, Knowledge Representation, Semantic Web, Entity recognition and linking},
abstract = {A picture is worth a thousand words, the adage reads. However, pictures cannot replace words in terms of their ability to efficiently convey clear (mostly) unambiguous and concise knowledge. Images and text, indeed reveal different and complementary information that, if combined will result in more information than the sum of that contained in a single media. The combination of visual and textual information can be obtained by linking the entities mentioned in the text with those shown in the pictures. To further integrate this with the agent’s background knowledge, an additional step is necessary. That is, either finding the entities in the agent knowledge base that correspond to those mentioned in the text or shown in the picture or, extending the knowledge base with the newly discovered entities. We call this complex task Visual-Textual-Knowledge Entity Linking (VTKEL). In this article, after providing a precise definition of the VTKEL task, we present two datasets called VTKEL1k* and VTKEL30k. These datasets consisting of images and corresponding captions, in which the image and textual mentions are both annotated with the corresponding entities typed according to the YAGO ontology. The datasets can be used for training and evaluating algorithms of the VTKEL task. Successively, we introduce a baseline algorithm called VT-LinKEr (Visual-Textual-Knowledge Entity Linker) for the solution of the VTKEL task. We evaluate the performances of VT-LinKEr on both datasets. We then contribute a supervised algorithm called ViTKan (Visual-Textual-Knowledge Alignment Network). We trained the ViTKan algorithm using features data of the VTKEL1k* dataset. The experimental results on VTKEL1k* and VTKEL30k datasets show that ViTKan substantially outperforms the baseline algorithm.}
}
@article{YANG2025113503,
title = {A comprehensive survey on integrating large language models with knowledge-based methods},
journal = {Knowledge-Based Systems},
volume = {318},
pages = {113503},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113503},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125005490},
author = {Wenli Yang and Lilian Some and Michael Bain and Byeong Kang},
keywords = {LLMs, Knowledge-based, Knowledge integration, RAG, KG},
abstract = {The rapid development of artificial intelligence has led to marked progress in the field. One interesting direction for research is whether Large Language Models (LLMs) can be integrated with structured knowledge-based systems. This approach aims to combine the generative language understanding of LLMs and the precise knowledge representation systems by which they are integrated. This article surveys the relationship between LLMs and knowledge bases, looks at how they can be applied in practice, and discusses related technical, operational, and ethical challenges. Utilizing a comprehensive examination of the literature, the study both identifies important issues and assesses existing solutions. It demonstrates the merits of incorporating generative AI into structured knowledge-base systems concerning data contextualization, model accuracy, and utilization of knowledge resources. The findings give a full list of the current situation of research, point out the main gaps, and propose helpful paths to take. These insights contribute to advancing AI technologies and support their practical deployment across various sectors.}
}
@article{GAO2025,
title = {Leveraging Medical Knowledge Graphs Into Large Language Models for Diagnosis Prediction: Design and Application Study},
journal = {JMIR AI},
volume = {4},
year = {2025},
issn = {2817-1705},
doi = {https://doi.org/10.2196/58670},
url = {https://www.sciencedirect.com/science/article/pii/S2817170525000092},
author = {Yanjun Gao and Ruizhe Li and Emma Croxford and John Caskey and Brian W Patterson and Matthew Churpek and Timothy Miller and Dmitriy Dligach and Majid Afshar},
keywords = {knowledge graph, natural language processing, machine learning, electronic health record, large language model, diagnosis prediction, graph model, artificial intelligence},
abstract = {Background
Electronic health records (EHRs) and routine documentation practices play a vital role in patients’ daily care, providing a holistic record of health, diagnoses, and treatment. However, complex and verbose EHR narratives can overwhelm health care providers, increasing the risk of diagnostic inaccuracies. While large language models (LLMs) have showcased their potential in diverse language tasks, their application in health care must prioritize the minimization of diagnostic errors and the prevention of patient harm. Integrating knowledge graphs (KGs) into LLMs offers a promising approach because structured knowledge from KGs could enhance LLMs’ diagnostic reasoning by providing contextually relevant medical information.
Objective
This study introduces DR.KNOWS (Diagnostic Reasoning Knowledge Graph System), a model that integrates Unified Medical Language System–based KGs with LLMs to improve diagnostic predictions from EHR data by retrieving contextually relevant paths aligned with patient-specific information.
Methods
DR.KNOWS combines a stack graph isomorphism network for node embedding with an attention-based path ranker to identify and rank knowledge paths relevant to a patient’s clinical context. We evaluated DR.KNOWS on 2 real-world EHR datasets from different geographic locations, comparing its performance to baseline models, including QuickUMLS and standard LLMs (Text-to-Text Transfer Transformer and ChatGPT). To assess diagnostic reasoning quality, we designed and implemented a human evaluation framework grounded in clinical safety metrics.
Results
DR.KNOWS demonstrated notable improvements over baseline models, showing higher accuracy in extracting diagnostic concepts and enhanced diagnostic prediction metrics. Prompt-based fine-tuning of Text-to-Text Transfer Transformer with DR.KNOWS knowledge paths achieved the highest ROUGE-L (Recall-Oriented Understudy for Gisting Evaluation–Longest Common Subsequence) and concept unique identifier F1-scores, highlighting the benefits of KG integration. Human evaluators found the diagnostic rationales of DR.KNOWS to be aligned strongly with correct clinical reasoning, indicating improved abstraction and reasoning. Recognized limitations include potential biases within the KG data, which we addressed by emphasizing case-specific path selection and proposing future bias-mitigation strategies.
Conclusions
DR.KNOWS offers a robust approach for enhancing diagnostic accuracy and reasoning by integrating structured KG knowledge into LLM-based clinical workflows. Although further work is required to address KG biases and extend generalizability, DR.KNOWS represents progress toward trustworthy artificial intelligence–driven clinical decision support, with a human evaluation framework focused on diagnostic safety and alignment with clinical standards.}
}
@article{SONG2025647,
title = {Construction of Question Answering System Based on English Pre-Trained Language Model Enhanced by Knowledge Graph},
journal = {Procedia Computer Science},
volume = {261},
pages = {647-655},
year = {2025},
note = {The 5th International Conference on Machine Learning and Big Data Analytics for IoT Security and Privacy (SPIoT2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.04.256},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925013584},
author = {Pei Song},
keywords = {NLP technology, knowledge graph enhancement, BERT model, English pre-training, language model},
abstract = {With the continuous development of technology, question answering systems based on pre trained language models have become an important component of intelligent applications. However, traditional question-answering systems often face challenges in terms of understanding accuracy and insufficient knowledge coverage when dealing with open-domain questions. To this end, this paper uses a method for building a question-answering system based on a knowledge graph-enhanced BERT pre-trained language model. First, this paper trains the basic model based on a large-scale BERT pre-trained language model. Then, this paper adopts knowledge graph technology to introduce structured knowledge information into the model by integrating domain-specific knowledge bases. Finally, in order to effectively integrate knowledge graph information, this paper uses graph neural network (GNN) to model graph data, and combines the self-attention mechanism in the BERT model to optimize the weighted fusion process of knowledge graph information. Experimental results show that the BERT model enhanced with knowledge graph performs well in multiple question-answering tasks. On the simple question of the first experiment, the average accuracy of the enhanced model increased from 84.6% of the standard BERT model to 89.8%, and the F1 score increased from 0.86 to 0.91. In complex reasoning tasks, the BERT+knowledge graph model demonstrates stronger reasoning ability and higher knowledge coverage. In the experimental conclusion, the introduction of the knowledge graph significantly improves the model’s reasoning ability and knowledge coverage, especially in professional field problems and multi-step reasoning tasks, the enhanced model shows stronger capabilities. This research provides a new construction method for question-answering systems, demonstrates the great potential of knowledge graphs in natural language processing, and has broad application prospects.}
}
@article{ZHAO2025102769,
title = {Hypergraph convolutional networks with multi-ordering relations for cross-document event coreference resolution},
journal = {Information Fusion},
volume = {115},
pages = {102769},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102769},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524005475},
author = {Wenbin Zhao and Yuhang Zhang and Di Wu and Feng Wu and Neha Jain},
keywords = {Event coreference resolution, Hypergraph convolutional neural network, Pretraining language models, Attention aggregation, Clustering},
abstract = {Recognizing the coreference relationship between different event mentions in the text (i.e., event coreference resolution) is an important task in natural language processing. It helps to understand the association between various events in the text, and plays an important role in information extraction, question answering systems, and reading comprehension. Existing research has made progress in improving the performance of event coreference resolution, but there are also some shortcomings. For example, most of the existing methods analyze the event data in the document in a serial processing mode, without considering the complex relationship between events, and it is difficult to mine the deep semantics of events. To solve these problems, this paper proposes a cross-document event co-reference resolution method (HGCN-ECR) based on hypergraph convolutional neural networks. Firstly, the BiLSTM-CRF model was used to label the semantic role of the events extracted from a number of documents. According to the labeling results, the trigger words and non-trigger words of the event were determined, and the multi-document event hypergraph was constructed around the event trigger words. Then hypergraph convolutional neural networks are used to learn higher-order semantic information in multi-document event hypergraphs, and multi-head attention mechanisms are introduced to understand the hidden features of different event relationship types by treating each event relationship as a set of separate attention mechanisms. Finally, the feed-forward neural network and the average link clustering method are used to calculate the coreference score of events and complete the coreference event clustering, and the cross-document event coreference resolution is realized. The experimental results show that the cross-document event co-reference resolution method is superior to the baseline model.}
}
@article{LIU2025128721,
title = {Research on application of knowledge graph in industrial control system security situation awareness and decision-making: A survey},
journal = {Neurocomputing},
volume = {613},
pages = {128721},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128721},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224014929},
author = {Lixin Liu and Peihang Xu and Kefeng Fan and Mingyan Wang},
keywords = {Knowledge graph, Knowledge association, Knowledge reasoning, Security situation awareness, Decision-making},
abstract = {Knowledge graph as a powerful tool for knowledge organization and representation, can integrate scattered data into a unified knowledge network, enabling knowledge association and knowledge reasoning, thus improving the accuracy of security situation awareness. In the context of industrial control system security decision-making, knowledge graphs can provide comprehensive knowledge support, assisting decision-makers in making intelligent decisions. This paper reviews the current research status of knowledge graphs in the field of industrial control system security situation awareness and decision-making, covering data-driven techniques, rule-based methods, and knowledge graph-based approaches. Existing knowledge graph technologies and their practical applications in industrial control system security situation awareness and decision-making are discussed. Finally, a series of challenges in the application of knowledge graphs in industrial control system security situation awareness and decision are summarized, such as data dispersion, correlation and information security, and the future prospects.}
}
@article{WAN2024103,
title = {Making knowledge graphs work for smart manufacturing: Research topics, applications and prospects},
journal = {Journal of Manufacturing Systems},
volume = {76},
pages = {103-132},
year = {2024},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2024.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S0278612524001572},
author = {Yuwei Wan and Ying Liu and Zheyuan Chen and Chong Chen and Xinyu Li and Fu Hu and Michael Packianather},
keywords = {Smart manufacturing, Industry 4.0, Knowledge graph, Semantic modelling under industry 4.0, Knowledge reasoning},
abstract = {Smart manufacturing (SM) confronts several challenges inherently suited to knowledge graphs (KGs) capabilities. The first key challenge lies in the synthesis of complex and varied data surrounding the manufacturing context, which demands advanced semantic analysis and inference capabilities. The second main limitation is the contextualization of manufacturing systems and the exploitation of manufacturing domain knowledge, which requires a dynamic and holistic representation of knowledge. The last major obstacle arises from the facilitation of intricate decision-making processes towards correlated manufacturing ecosystems, which benefit from interconnected data structures that KGs excel at organizing. However, the existing survey studies concentrated on distinct facets of SM and offered isolated insights into KG applications while overlooking the interconnections between various KG technologies and their application across multiple domains. What specific role KGs should play in SM towards the aforementioned challenges, how to effectively harness KGs for these challenges, and the essential topics and methodologies required to make KGs functional remain underexplored. To explore the potential of KGs in SM, this study adopts a systematic approach to investigate, evaluate, and analyse current research on KGs, identifying core advancements and their implications for future manufacturing practices. Firstly, cutting-edge developments in the challenge-driven roles of KGs and KG techniques are identified, from knowledge extraction and mining to techniques for KG construction and updates, further extending to KG embedding, fusion, and reasoning—central to driving SM ecosystems. Specifically, the KG technologies for SM are depicted holistically, emphasizing the interplay of diverse KG techniques with a comprehensive framework. Subsequently, this foundation outlines and discusses key application scenarios of KGs from engineering design to predictive maintenance, covering the main representative stages of the manufacturing life cycle. Lastly, this study explores the intricate interplay of the practical challenges and advantages of KGs in manufacturing systems, pointing to emerging research avenues.}
}
@article{SUI2025106783,
title = {Introducing high correlation and high quality instances for few-shot entity linking},
journal = {Neural Networks},
volume = {181},
pages = {106783},
year = {2025},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2024.106783},
url = {https://www.sciencedirect.com/science/article/pii/S089360802400707X},
author = {Xuhui Sui and Ying Zhang and Kehui Song and Baohang Zhou and Xiaojie Yuan},
keywords = {Entity linking, Few-shot learning, Deep neural learning, Natural language processing},
abstract = {Entity linking, the process of connecting textual mentions in documents to canonical entities within a knowledge base, plays an integral role in a myriad of natural language processing tasks. A significant challenge prevalent within the field is the scarcity of resources, particularly for multiple specialized domains, which accentuates the importance of few-shot entity linking in real-world scenarios. Previous works address the problem of lacking in-domain labeled data by generating synthetic data. However, we argue that the synthetic data is frequently far from high-quality, such low-quality instances will introduce noise and diminish the ability of entity linking models to comprehend the semantic consistency between mentions and entities. In this paper, we propose a H2FEL framework to introduce high correlation and high quality instances for few-shot entity linking. We argue that there are rich high-quality labeled data in general domains and some of them are highly correlated to the target domain. Thus, we first design an adversarial instance extraction module to extract such high-correlation instances without depending on additional manually annotated data. To further mitigate the negative effects brought by low-correlation instances, we train our entity linking model via a variant of curriculum learning. Experimental results on the few-shot entity linking dataset demonstrate the effectiveness of our proposed H2FEL framework and it achieves state-of-the-art performance.}
}
@article{SUN2024953,
title = {Research on Digital Book Resource Recommendation Algorithm Based on Knowledge Graph},
journal = {Procedia Computer Science},
volume = {247},
pages = {953-962},
year = {2024},
note = {The 11th International Conference on Applications and Techniques in Cyber Intelligence},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.10.115},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924029156},
author = {Yan Sun},
keywords = {Knowledge Graph, Digital Book Resource, Recommendation Algorithm, Mathematical Model, Attention Mechanism, Simulation Experiment},
abstract = {Knowledge graph based search is becoming increasingly close to users' reading habits, providing a more user-friendly interactive experience. Integrating knowledge graphs as auxiliary information into recommendation algorithms can model and integrate semantic information of learning resources, providing more diverse personalized recommendation results and better alleviating the problem of information overload. In order to solve the problem of readers searching for suitable digital book resources and improve reading efficiency, the first step is to study the construction of knowledge graphs, including core tasks such as knowledge extraction, knowledge representation, and knowledge storage; Then, a mathematical model is constructed, based on the optimized graph convolutional network layer and the quantification algorithm of knowledge point centrality, to construct a universal computational framework for attention mechanism; Finally, simulation experiments were conducted to compare and analyze different models through evaluation indicators, in order to verify the advantages of the model constructed in this paper. The experimental results show that the research method proposed in this paper is superior to conventional methods such as FM, CKE, DNN, NFM, CFKG, KGAT, and KGCN.}
}
@article{HAO2025,
title = {Knowledge graph–based safety risk evaluation method for hazardous behaviors of road transport vehicles},
journal = {Traffic Injury Prevention},
year = {2025},
issn = {1538-9588},
doi = {https://doi.org/10.1080/15389588.2025.2540554},
url = {https://www.sciencedirect.com/science/article/pii/S1538958825001407},
author = {Yadi Hao and Gen Li and Jiwei Lu and Wanrong Cheng and Quan Yuan and Zhihong Yao},
keywords = {Knowledge graph, hazardous driving behaviors, risk prediction, traffic safety management},
abstract = {Objective
This study aims to develop a knowledge graph (KG)–based framework to quantify and analyze the impact of hazardous driving behaviors on road transport safety.
Method
A top-down approach was adopted to construct a multilayered KG incorporating seven categories of hazardous behavior factors (C1–C7). Multisource accident datasets were integrated to map the relationships among hazardous behavior factors, accident types, and accident causes. The Criteria Importance Through Intercriteria Correlation (CRITIC) method was applied to calculate the safety risk levels of various hazardous behaviors. Cosine similarity analysis was used to quantify correlations between hazardous behavioral factors and calculated risk metrics. Furthermore, KG-based path reasoning was used to trace causal chains linking hazardous behaviors to accidents.
Results
Dangerous driving (C5) and driver technical competency (C1) emerged as the two most influential risk factor categories, with correlation coefficients of 0.995 and 0.987, respectively. Rear-end collisions were identified as the most probable accident type caused by C5, with a conditional probability of 0.5. Fatigue and speeding were identified as the most common behavioral triggers. KG pathway analysis effectively traced risk propagation paths, highlighting key links in accident causation.
Conclusions
This study integrates the multidimensional correlation analysis of knowledge graphs with the weighting advantages of the CRITIC method, explicitly expressing the causal chain of “hazardous behavior–accident type–accident cause” through graph structures to comprehensively analyze the behavioral mechanisms of traffic accidents.}
}
@article{GAO2023110195,
title = {Exploiting global context and external knowledge for distantly supervised relation extraction},
journal = {Knowledge-Based Systems},
volume = {261},
pages = {110195},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.110195},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122012916},
author = {Jianwei Gao and Huaiyu Wan and Youfang Lin},
keywords = {Relation extraction, Distant supervision, Knowledge representation, Word embedding, Gating mechanism},
abstract = {Distantly supervised relation extraction aims to obtain relational facts from unstructured texts. Although distant supervision can automatically generate labeled training instances, it inevitably suffers from the wrong-label problem. Most of the current work is based on the bag-level for solving the noise problem, where a bag is composed of multiple sentences containing mentions of the same entity pair. However, previous studies mostly represent sentences from a single perspective, wherein insufficient modeling of global information restricts the effectiveness of denoising. In this study, we propose a novel distantly supervised relation extraction approach that incorporates the global contextual information of sentences to guide the denoising process and generate an effective bag-level representation. Simultaneously, knowledge-aware word embeddings were generated to enrich sentence-level representations by introducing both structured knowledge from external knowledge graphs and semantic knowledge from the corpus. The experimental results demonstrate that our proposed approach outperforms state-of-the-art methods on both versions of the large-scale benchmark New York Times dataset. In addition, the differences between the two versions of the dataset were investigated through further comparative experiments.}
}
@article{LI2022100723,
title = {Translational relation embeddings for multi-hop knowledge base question answering},
journal = {Journal of Web Semantics},
volume = {74},
pages = {100723},
year = {2022},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2022.100723},
url = {https://www.sciencedirect.com/science/article/pii/S1570826822000154},
author = {Ziyan Li and Haofen Wang and Wenqiang Zhang},
keywords = {Knowledge base question answering, Knowledge graph embedding, Multi-hop relation extraction},
abstract = {Multi-hop Knowledge Base Question Answering (KBQA) aims to predict answers that require multi-hop reasoning from the topic entity in the question over the Knowledge Base (KB). Relation extraction is a core step in KBQA, which extracts the relation path from the topic entity to the answer entity. Compared with single-hop questions, multi-hop ones have more complex syntactic structures to understand, and multi-hop relation paths lead to a larger search space, which makes it much more challenging to extract the correct relation paths. To tackle the above challenges, most existing relation extraction approaches focus on the semantic similarity between questions and relation paths. However, those approaches only consider the word semantics of the relation names but ignore the graph semantics inside the knowledge base. As a result, their generalization ability relying on the naming rules of the relations, making it more difficult to generalize over large knowledge bases. To address the current limitations and take advantage of the graph semantics of relations, we propose a novel translational embedding-based relation extractor that utilizes pretrained embeddings from TransE. In particular, we treat the multi-hop relation path as a translation from the first relation to the last one in the semantic space of TransE. Then we map the question into this space under the supervision of the path embeddings. To take full advantage of the pretrained graph semantics in TransE, we propose a KBQA framework that leverages pretrained relation semantics in relation extraction and pretrained entity semantics in answer selection. Our approach achieves state-of-the-art performance on two benchmark datasets, WebQuestionSP and MetaQA, demonstrating its effectiveness on the multi-hop KBQA task.}
}
@article{GIORDANO20241170,
title = {POPCORN: Fictional and Synthetic Intelligence Reports for Named Entity Recognition and Relation Extraction Tasks},
journal = {Procedia Computer Science},
volume = {246},
pages = {1170-1180},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.542},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924025870},
author = {Bastien Giordano and Maxime Prieur and Nakanyseth Vuth and Sylvain Verdy and Kévin Cousot and Gilles Sérasset and Guillaume Gadek and Didier Schwab and Cédric Lopez},
keywords = {Synthetic Data Generation, Dataset, Natural Language Processing, Large Language Models, Information Extraction},
abstract = {POPCORN is a research project aiming at maturing Information Extraction (IE) solutions for intelligence services. Due to defense security constraints, reports analyzed by intelligence services are not to be accessible to the scientific community. To address this challenge, we propose a dataset made of “fictional” (handcrafted) and “synthetic” (AI generated) French reports. Those synthetic reports are produced by an innovative approach that generates texts closely resembling real-world intelligence reports, facilitating the training and evaluation of IE tasks such as Entity and Relation Extraction. Experiments demonstrate the interest of synthetic reports to enhance the performance of IE models, showcasing their potential to augment real-world intelligence operations.}
}
@article{ZHAO2023225,
title = {A Survey of Knowledge Graph Construction Using Machine Learning},
journal = {CMES - Computer Modeling in Engineering and Sciences},
volume = {139},
number = {1},
pages = {225-257},
year = {2023},
issn = {1526-1492},
doi = {https://doi.org/10.32604/cmes.2023.031513},
url = {https://www.sciencedirect.com/science/article/pii/S152614922300098X},
author = {Zhigang Zhao and Xiong Luo and Maojian Chen and Ling Ma},
keywords = {Knowledge graph (KG), semantic network, relation extraction, entity linking, knowledge reasoning},
abstract = {Knowledge graph (KG) serves as a specialized semantic network that encapsulates intricate relationships among real-world entities within a structured framework. This framework facilitates a transformation in information retrieval, transitioning it from mere string matching to far more sophisticated entity matching. In this transformative process, the advancement of artificial intelligence and intelligent information services is invigorated. Meanwhile, the role of machine learning method in the construction of KG is important, and these techniques have already achieved initial success. This article embarks on a comprehensive journey through the last strides in the field of KG via machine learning. With a profound amalgamation of cutting-edge research in machine learning, this article undertakes a systematical exploration of KG construction methods in three distinct phases: entity learning, ontology learning, and knowledge reasoning. Especially, a meticulous dissection of machine learning-driven algorithms is conducted, spotlighting their contributions to critical facets such as entity extraction, relation extraction, entity linking, and link prediction. Moreover, this article also provides an analysis of the unresolved challenges and emerging trajectories that beckon within the expansive application of machine learning-fueled, large-scale KG construction.}
}
@article{PRAMANIK2024100833,
title = {Uniqorn: Unified question answering over RDF knowledge graphs and natural language text},
journal = {Journal of Web Semantics},
volume = {83},
pages = {100833},
year = {2024},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100833},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000192},
author = {Soumajit Pramanik and Jesujoba Alabi and Rishiraj Saha Roy and Gerhard Weikum},
keywords = {Complex question answering, Heterogeneous sources, Group Steiner Trees},
abstract = {Question answering over RDF data like knowledge graphs has been greatly advanced, with a number of good systems providing crisp answers for natural language questions or telegraphic queries. Some of these systems incorporate textual sources as additional evidence for the answering process, but cannot compute answers that are present in text alone. Conversely, the IR and NLP communities have addressed QA over text, but such systems barely utilize semantic data and knowledge. This paper presents a method for complex questions that can seamlessly operate over a mixture of RDF datasets and text corpora, or individual sources, in a unified framework. Our method, called Uniqorn, builds a context graph on-the-fly, by retrieving question-relevant evidences from the RDF data and/or a text corpus, using fine-tuned BERT models. The resulting graph typically contains all question-relevant evidences but also a lot of noise. Uniqorn copes with this input by a graph algorithm for Group Steiner Trees, that identifies the best answer candidates in the context graph. Experimental results on several benchmarks of complex questions with multiple entities and relations, show that Uniqorn significantly outperforms state-of-the-art methods for heterogeneous QA – in a full training mode, as well as in zero-shot settings. The graph-based methodology provides user-interpretable evidence for the complete answering process.}
}
@article{WANG2024102820,
title = {Knowledge graph of agricultural engineering technology based on large language model},
journal = {Displays},
volume = {85},
pages = {102820},
year = {2024},
issn = {0141-9382},
doi = {https://doi.org/10.1016/j.displa.2024.102820},
url = {https://www.sciencedirect.com/science/article/pii/S0141938224001847},
author = {Haowen Wang and Ruixue Zhao},
keywords = {LLM, Knowledge graph},
abstract = {Agriculture is an industry that has evolved alongside human evolution and has faithfully fulfilled its core mission of food supply. With the reduction of rural labor, the progress of artificial intelligence and the development of Internet of Things technology, it is hoped that the efficiency and productivity of the agricultural industry can be improved. Recently, with the development of information and intelligent technology, agricultural production and management have been significantly enhanced. However, there is still a considerable challenge in effectively integrating the vast amount of fragmented information for downstream applications. An agricultural knowledge graph (AGKG) will serve as the foundation for achieving these goals. Knowledge graphs can be general or domain-specific, and are the basis for many applications, such as search engines, online question-and-answer services, and knowledge inference. Therefore, there are many knowledge graphs, including Wikidata and DBpedia, for accessing structured knowledge. Although some general knowledge graphs contain some entities and relationships related to agriculture, there are no domain-specific knowledge graphs specifically for agricultural applications. Therefore, this paper proposes an agricultural knowledge graph (AGKG) for automatically integrating large amounts of agricultural data from the Internet. By applying natural language processing and deep learning technologies, AGKG can automatically identify agricultural entities from unstructured text and connect them to form a knowledge graph. In addition, we have described the typical scenarios of our AGKG and validated it through real-world applications such as agricultural entity retrieval and agricultural question-answering.}
}
@article{BADENESOLMEDO2023104382,
title = {Lessons learned to enable question answering on knowledge graphs extracted from scientific publications: A case study on the coronavirus literature},
journal = {Journal of Biomedical Informatics},
volume = {142},
pages = {104382},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2023.104382},
url = {https://www.sciencedirect.com/science/article/pii/S153204642300103X},
author = {Carlos Badenes-Olmedo and Oscar Corcho},
keywords = {Question-answering, Knowledge graphs, Ontology, Evidences},
abstract = {The article presents a workflow to create a question-answering system whose knowledge base combines knowledge graphs and scientific publications on coronaviruses. It is based on the experience gained in modeling evidence from research articles to provide answers to questions in natural language. The work contains best practices for acquiring scientific publications, tuning language models to identify and normalize relevant entities, creating representational models based on probabilistic topics, and formalizing an ontology that describes the associations between domain concepts supported by the scientific literature. All the resources generated in the domain of coronavirus are available openly as part of the Drugs4COVID initiative, and can be (re)-used independently or as a whole. They can be exploited by scientific communities conducting research related to SARS-CoV-2/COVID-19 and also by therapeutic communities, laboratories, etc., wishing to find and understand relationships between symptoms, drugs, active ingredients and their documentary evidence.}
}
@article{OTTERSEN2024100334,
title = {Triplet extraction leveraging sentence transformers and dependency parsing},
journal = {Array},
volume = {21},
pages = {100334},
year = {2024},
issn = {2590-0056},
doi = {https://doi.org/10.1016/j.array.2023.100334},
url = {https://www.sciencedirect.com/science/article/pii/S2590005623000590},
author = {Stuart Gallina Ottersen and Flávio Pinheiro and Fernando Bação},
keywords = {Triplet extraction, NLP, Natural language processing, Knowledge Graph},
abstract = {Knowledge Graphs are a tool to structure (entity, relation, entity) triples. One possible way to construct these knowledge graphs is by extracting triples from unstructured text. The aim when doing this is to maximise the number of useful triples while minimising the triples containing no or useless information. Most previous work in this field uses supervised learning techniques that can be expensive both computationally and in that they require labelled data. While the existing unsupervised methods often produce an excessive amount of triples with low value, base themselves on empirical rules when extracting triples or struggle with the order of the entities relative to the relation. To address these issues this paper suggests a new model: Unsupervised Dependency parsing Aided Semantic Triple Extraction (UDASTE) that leverages sentence structure and allows defining restrictive triple relation types to generate high-quality triples while removing the need for mapping extracted triples to relation schemas. This is done by leveraging pre-trained language models. UDASTE is compared with two baseline models on three datasets. UDASTE outperforms the baselines on all three datasets. Its limitations and possible further work are discussed in addition to the implementation of the model in a computational intelligence context.}
}
@article{MOUICHE2025104120,
title = {Entity and relation extractions for threat intelligence knowledge graphs},
journal = {Computers & Security},
volume = {148},
pages = {104120},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.104120},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824004255},
author = {Inoussa Mouiche and Sherif Saad},
keywords = {Threat intelligence knowledge graphs (TiKG), Cyber threat intelligence (CTI), Cyber knowledge graphs (CKGs), Pipeline extraction, Joint extraction, Entity-relation extraction, Knowledge Ontology, NER Datasets},
abstract = {Advanced persistent threats (APTs) represent a complex challenge in cybersecurity as they infiltrate networks stealthily to conduct espionage, steal data, and maintain a long-term presence. To combat these threats, security professionals increasingly rely on cyber knowledge graphs (CKGs), which provide scalable solutions to analyze and structure vast amounts of cyber threat intelligence (CTI) from diverse sources in real-time, enabling the automation of proactive security measures. Developing CKGs requires extracting entity and their relationships from unstructured CTI reports. However, existing approaches face significant limitations, such as difficulties with the nuances of cybersecurity language, diverse threat terminologies, and high rates of error propagation, resulting in low accuracy and poor generalizability. This paper introduces a novel Threat Intelligence Knowledge Graph (TiKG) pipeline designed to address these challenges. The TiKG framework leverages SecureBERT, a domain-specific transformer-based model optimized for cybersecurity, and integrates it with an attention-based BiLSTM to capture the context and nuances of security texts, reducing error propagation and improving extraction accuracy. Additionally, the pipeline incorporates a domain-specific ontology and inference model to ensure precise relation mapping in relation extraction. Using three large-scale TI open-source datasets (DNRTI, STUCCO, and CYNER) and a curated CTI dataset, extensive evaluations demonstrate the effectiveness of our framework, showing significant improvements over existing methods in detecting and linking cyber threats. These contributions provide a robust platform for security professionals to analyze and predict potential attacks, develop effective defenses, and enhance the strategic capabilities of cybersecurity operations.}
}
@article{CHEN2024122007,
title = {An effective relation-first detection model for relational triple extraction},
journal = {Expert Systems with Applications},
volume = {238},
pages = {122007},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122007},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423025095},
author = {Juan Chen and Jie Hu and Tianrui Li and Fei Teng and Shengdong Du},
keywords = {Information extraction, Relational triple extraction, Overlapping triples, Error propagation},
abstract = {Relational triple extraction is a crucial task in the field of information extraction, which attempts to identify all triples from natural language text. Existing methods primarily focus on addressing the issue of overlapping triples. However, the majority of studies need to perform the same operation on all predefined relations when solving this problem, which will lead to relation redundancy. In addition, most methods have the problem of error propagation. During training, they use the ground truth labels as a priori knowledge to predict at different stages, while during inference, they must use the labels predicted in the previous stage to predict in the following stages. To address these problems, we propose an effective relation-first detection model for relational triple extraction (ERFD-RTE). The proposed model first detects the potential relations in the sentence and then performs entity recognition for each specific relation, which aims to solve the overlapping triples issue and avoid additional calculations for redundant relations. We design a random label error strategy for the error propagation problem in the training phase, which balances the difference between training and inference. Experiment results demonstrate that ERFD-RTE is superior to other baselines by improving the F1 score to 92.7% (+0.7%) on NYT-P and NYT-E, 92.9% (+0.3%) on WebNLG-P, 89.3% (+0.9%) on WebNLG-E and 83.71% (+1.5%) on ADE. Additional analysis shows that ERFD-RTE can effectively extract overlapping triples.}
}
@article{ZHANG2025103705,
title = {A knowledge graphs construction method enhanced by multimodal large language model for industrial equipment operation and maintenance},
journal = {Advanced Engineering Informatics},
volume = {68},
pages = {103705},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103705},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625005981},
author = {Zhengping Zhang and Junyuan Yu and Bo Yang and Kaze Du and Shilong Wang and Xing Qi},
keywords = {Equipment operation and maintenance, Multi-modal knowledge graph, Multimodal large language modals, Attention mechanism},
abstract = {The industrial equipment Operation and Maintenance (O&M) is a core component in ensuring production safety and efficiency, urgently requiring the support of intelligent technologies. Knowledge graphs, which represent equipment and faults in graph structures, are widely utilized to enable efficient association and rapid retrieval of maintenance knowledge, thereby being extensively applied in intelligent decision-making for the equipment O&M. Traditional knowledge graph construction methods, which rely on a single textual modality, are confronted with challenges such as the scarcity of annotated samples, difficulties in dynamically associating old and new equipment, and insufficient parsing of complex equipment relationships. As a result, issues like missing graph entities and broken causal chains are often encountered, thereby negatively impacting the quality of maintenance decision-making. Therefore, a dual-attention model enhanced by multimodal large language models (MllmDA-KGC) is proposed in this paper. Multimodal large language model(MLLM) is introduced to fully utilize multi-modal knowledge from the O&M domain, thereby enabling a more effective understanding and modeling of complex O&M knowledge. As a result, the quality of knowledge graph construction is significantly improved. In the MllmDA-KGC framework, first, QWEN2-VL is introduced into a dual-stream Transformer architecture to achieve dynamic alignment between images and text while supplementing semantics. As a result, the precision of identifying relationships between parts and problem entities in the O&M domain is significantly enhanced; second, the MT-Transformer module is proposed, which integrates causal convolution, dilated convolution, and Memory_Bank mechanisms to achieve cross-modal temporal embedding fusion. As a result, the continuity of associations between new and old parts, as well as the precision of causal chain embeddings, is significantly improved; third, a multimodal dynamic weight attention-guided module is designed, in which weighted key-value guided attention mechanisms are introduced to focus on critical aspects. Schematic diagrams and textual features are fused to enhance the precision of entity relationship modeling between parts and faults; finally, to fully leverage the multimodal understanding capabilities of the MLLM, the image embeddings and positional embeddings generated and marked by MLLM are integrated into the feature embeddings of RoBERTa. Subsequently, CRF and Softmax are combined to accomplish the MNER (Multimodal Named Entity Recognition) and MRE (Multimodal Relation Extraction) tasks, thereby enabling the construction of a multimodal equipment O&M knowledge graph. In this paper, the vehicle O&M dataset from an automobile company was utilized to validate the proposed method. The experimental results showed that the F1 scores of the model in the MNER and MRE tasks reached 88.40% and 93.79%, respectively, demonstrating its effectiveness in constructing a multimodal knowledge graph for equipment O&M. Furthermore, during the process of graph inference, the performance of multimodal graph inference was significantly better than that of the unimodal approach, further confirming the superiority of the multimodal knowledge graph.}
}
@article{PROBIERZ20234324,
title = {Knowledge graphs to an analysis and visualization of texts from scientific articles},
journal = {Procedia Computer Science},
volume = {225},
pages = {4324-4333},
year = {2023},
note = {27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.429},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923015879},
author = {Barbara Probierz and Jan Kozak},
keywords = {knowledge graphs, natural language processing, analysis of texts, scientific articles},
abstract = {Reviewing the literature is one of the key elements of scientific research that allows you to identify existing solutions and research niches. However, it can be difficult for researchers to find relevant scientific articles related to the research topic. A limited number of available sources of information, diverse ways of describing them, and a multitude of scientific publications mean that scientists often have to spend a lot of time and effort to find the information they need. For this reason, we propose a solution for text analysis and its presentation in the form of a graph visualization. In our research, we used Natural Language Processing (NLP) methods and word weighting measures such as TF and TF-IDF. In addition, knowledge graphs were used to present the results visually. The conducted research was based on the analysis of the content of scientific articles, which allowed to draw important conclusions related to the presentation of texts in graphic form. Experimental results also identified potential methods and suggestions for literature reviews in specific fields of science. The analysis of the methods used and the results obtained allowed for a better understanding of the potential of natural language processing and graph text representation in the analysis of scientific articles.}
}
@article{HE2025129752,
title = {Few-shot medical relation extraction via prompt tuning enhanced pre-trained language model},
journal = {Neurocomputing},
volume = {633},
pages = {129752},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.129752},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225004242},
author = {Guoxiu He and Chen Huang},
keywords = {Information extraction, Medical relation extraction, Few-shot learning, Prompt tuning, Pre-trained language model},
abstract = {Medical relation extraction is crucial for developing structured information to support intelligent healthcare systems. However, acquiring large volumes of labeled medical data is challenging due to the specialized nature of medical knowledge and privacy constraints. To address this, we propose a prompt-enhanced few-shot relation extraction (FSRE) model that leverages few-shot and prompt learning techniques to improve performance with minimal data. Our approach introduces a hard prompt concatenated to the original input, enabling contextually enriched learning. We calculate prototype representations by averaging the intermediate states of each relation class in the support set, and classify relations by finding the shortest distance between the query instance and class prototypes. We evaluate our model against existing deep learning based FSRE models using three biomedical datasets: the 2010 i2b2/VA challenge dataset, the CHEMPROT corpus, and the BioRED dataset, focusing on few-shot scenarios with limited training data. Our model demonstrates exceptional performance, achieving the highest accuracy across all datasets in most training configurations under a 3-way-5-shot condition and significantly surpassing the current state-of-the-art. Particularly, it achieves improvements ranging from 1.25% to 11.25% on the 2010 i2b2/VA challenge dataset, 3.4% to 20.2% on the CHEMPROT dataset, and 2.73% to 10.98% on the BioRED dataset compared to existing models. These substantial gains highlight the model’s robust generalization ability, enabling it to effectively handle previously unseen relations during testing. The demonstrated effectiveness of this approach underscores its potential for diverse medical applications, particularly in scenarios where acquiring extensive labeled data is challenging.}
}
@incollection{DHAMENIYA2025227,
title = {12 - Knowledge graph-based question answering (KG-QA) using natural language processing},
editor = {Rajesh Kumar Dhanaraj and M. Nalini and Malathy Sathyamoorthy and Manar Mohaisen},
booktitle = {Knowledge Graph-Based Methods for Automated Driving},
publisher = {Elsevier},
pages = {227-249},
year = {2025},
isbn = {978-0-443-30040-0},
doi = {https://doi.org/10.1016/B978-0-443-30040-0.00012-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780443300400000126},
author = {Priyanshu Dhameniya and Rashmi Yadav},
keywords = {Natural language processing, Knowledge graphsand question answering, Information retrieval, Unstructured, Entity-resolution models, Semantic-parsing, Semantic annotations, Recurrent neural networks, Entity recognition, RDF-based knowledge graphs, SPARQL},
abstract = {This chapter begins by providing an overview of knowledge graphs, their construction, and their role in representing relationships between entities. It then delves into the challenges posed by question-answering tasks and how knowledge graphs can enhance traditional QA systems by offering a structured and context-aware information base. The core of this chapter focuses on the integration of NLP techniques in KG-QA systems. It discusses the processing of natural language queries, entity recognition, and relationship extraction to bridge the gap between user queries and the structured information encapsulated in knowledge graphs. Special attention is given to semantic parsing and understanding, enabling the extraction of nuanced information from unstructured text.}
}
@article{ZHANG2025104527,
title = {More intelligent knowledge graph: A large language model-driven method for knowledge representation in geospatial digital twins},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {139},
pages = {104527},
year = {2025},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2025.104527},
url = {https://www.sciencedirect.com/science/article/pii/S1569843225001748},
author = {Jinbin Zhang and Jun Zhu and Zhihao Guo and Jianlin Wu and Yukun Guo and Jianbo Lai and Weilian Li},
keywords = {Geospatial digital twins, Knowledge graph, Large language model, Knowledge extraction, Dynamic update},
abstract = {Knowledge graphs (KGs) can describe the nature and relationships of geographic entities and are an essential knowledge base for realizing geospatial digital twins (GDTs). However, existing KGs make it challenging to describe dynamic geographic entities under geographic spatiotemporal evolution accurately. Furthermore, they are constrained by the professional backgrounds of their users, which hinders updates and communication. Therefore, the research constructed an “event-object-state” three-domain associated GDT-oriented KG, proposed a large language model (LLM) −driven KG dynamic update algorithm, and established a KG intelligent Q&A method integrating LLM. We developed a prototype system and selected an earthquake disaster as a typical geographic event for experimental analysis. The results showed that the proposed method can reflect the space, time, state, evolution process, and interrelationships of geographic entities in a more comprehensive way, support users to build, update, and query KGs using natural language, with an updating efficiency of less than 1 min, and an updating quality comparable to that of manual updating by experts. Compared with the traditional KGs, our method can represent virtual geographic entities and has significant advantages in intelligence and automation, which effectively breaks down professional barriers and supports the construction of GDTs with the need for rapid updating of knowledge.}
}
@article{XU2026103141,
title = {Automated multimodal process knowledge graph construction for intelligent process planning with Cross-Modal Transformers},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {98},
pages = {103141},
year = {2026},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2025.103141},
url = {https://www.sciencedirect.com/science/article/pii/S0736584525001954},
author = {Qingfeng Xu and Chao Zhang and Dongxu Ma and Jiacheng Li and Jiewu Leng and Guanghui Zhou},
keywords = {Transformer, Multimodal process knowledge graphs, Machining feature, Intelligent process planning},
abstract = {Intelligent process planning is pivotal in modern manufacturing systems, enabling efficient, precise, and flexible production by optimizing resource allocation, enhancing machining accuracy, and shortening production cycles. Knowledge graphs integrate multi-source heterogeneous data to support this process, yet traditional single-modal approaches hinder the exploration of complex relationships in multimodal data, falling short of the needs for complex part planning. This paper examines machining features, the foundational units of process planning, and introduces an automatic construction method for a Multimodal Process Knowledge Graph (MPKG) tailored to intelligent process planning, powered by Cross-Modal Transformers. We developed the MF36 dataset, encompassing 36 machining features with 3D models, engineering views, and descriptive texts. A cross-modal framework integrating LERT-CRF and PA-ViT models automates the extraction and fusion of multimodal process knowledge, with PA-ViT’s pooling attention mechanism markedly boosting machining feature recognition accuracy. Experiments demonstrate superior performance over baselines, achieving F1 scores of 0.895 in entity extraction and 0.877 in image recognition. A case study validates the method’s reliability for precise process recommendations, providing fresh insights into advancing intelligent process planning.}
}
@article{AHMED20226505,
title = {Arabic Knowledge Graph Construction: A close look in the present and into the future},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {9},
pages = {6505-6523},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2022.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S1319157822001240},
author = {Ibrahim A. Ahmed and Fatima N. AL-Aswadi and Khaled M.G. Noaman and Wafa' Za'al Alma'aitah},
keywords = {Arabic Knowledge Graph, Knowledge Graph Construction, Knowledge Representation, Ontology},
abstract = {With the widespread growth of data on the Web, the need for efficient methods to get and arrange valuable information from these big noisy data is increased. The knowledge graph (KG) is a way to represent and organize the data in a more efficient and easy way to modify, use, and understand. Recently, KG has become a new hotspot topic in academic and business research; it is used in many applications such as intelligent question-answering (QA), recommender systems, map navigation, etc. There has been a tendency to construct KG in different languages such as English, Chinese, Persian, or Arabic. Constructing KG faces many challenges and obstacles, especially constructing Arabic Knowledge Graph (AKG) due to the sparse Arabic data in online encyclopedias and academic research, as well as the lack of tools that can process the proprietary nature of the Arabic language effectively, besides other challenges. This research aims to review and discuss KG construction best practices (systems, phases, problems, and challenges) with highlighting the Arabic perspective. Besides, it elaborates a classification of the AKG challenges and investigates the potential solutions and opportunities that might define the key future research directions of constructing AKG.}
}
@article{CENIKJ2023102586,
title = {FooDis: A food-disease relation mining pipeline},
journal = {Artificial Intelligence in Medicine},
volume = {142},
pages = {102586},
year = {2023},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2023.102586},
url = {https://www.sciencedirect.com/science/article/pii/S0933365723001008},
author = {Gjorgjina Cenikj and Tome Eftimov and Barbara {Koroušić Seljak}},
keywords = {Text mining, Relation extraction, Named entity recognition, Named entity linking, Food-disease relations},
abstract = {Nowadays, it is really important and crucial to follow the new biomedical knowledge that is presented in scientific literature. To this end, Information Extraction pipelines can help to automatically extract meaningful relations from textual data that further require additional checks by domain experts. In the last two decades, a lot of work has been performed for extracting relations between phenotype and health concepts, however, the relations with food entities which are one of the most important environmental concepts have never been explored. In this study, we propose FooDis, a novel Information Extraction pipeline that employs state-of-the-art approaches in Natural Language Processing to mine abstracts of biomedical scientific papers and automatically suggests potential cause or treat relations between food and disease entities in different existing semantic resources. A comparison with already known relations indicates that the relations predicted by our pipeline match for 90% of the food-disease pairs that are common in our results and the NutriChem database, and 93% of the common pairs in the DietRx platform. The comparison also shows that the FooDis pipeline can suggest relations with high precision. The FooDis pipeline can be further used to dynamically discover new relations between food and diseases that should be checked by domain experts and further used to populate some of the existing resources used by NutriChem and DietRx.}
}
@article{LIU2025126099,
title = {Document-level relation extraction with structural encoding and entity-pair-level information interaction},
journal = {Expert Systems with Applications},
volume = {268},
pages = {126099},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.126099},
url = {https://www.sciencedirect.com/science/article/pii/S095741742402966X},
author = {Wanlong Liu and Yichen Xiao and Shaohuan Cheng and Dingyi Zeng and Li Zhou and Weishan Kong and Malu Zhang and Wenyu Chen},
keywords = {Relation extraction, Structural transformer, Information extraction, Natural Language Processing},
abstract = {Document-level relation extraction focuses on identifying the relations between entity pairs across the entirety of a document. However, current mainstream methods mainly have two drawbacks: (a) Separating the context encoding stage and the graph reasoning stage. (b) Limiting the reasoning between entity mentions, which ignores the relation correlations in context. In this paper, we introduce the SE-REPI model, which consists of a Structural-enhanced Encoding (SE) module and a Relation-based Entity-Pair-Level Interaction (REPI) module. The SE module effectively injects structural information into the architecture of the Transformer, integrating the graph reasoning stage into the encoding stage. The REPI module constructs entity-pair-level information interaction, which effectively captures the relation correlations among entity pairs. Comprehensive experimental results on four public DocRE datasets DocRED, CRD, GDA, and DWIE underscore the superior performance and efficiency of our model compared to previous leading-edge methods. Further experimental analyses reveal the interpretability of our approach.}
}
@article{ZAVARELLA2024e32479,
title = {Triplétoile: Extraction of knowledge from microblogging text},
journal = {Heliyon},
volume = {10},
number = {12},
pages = {e32479},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e32479},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024085104},
author = {Vanni Zavarella and Sergio Consoli and Diego {Reforgiato Recupero} and Gianni Fenu and Simone Angioni and Davide Buscaldi and Danilo Dessí and Francesco Osborne},
keywords = {Information extraction, Knowledge graphs, Social media analysis, Named entity recognition, Hierarchical clustering, Word embeddings},
abstract = {Numerous methods and pipelines have recently emerged for the automatic extraction of knowledge graphs from documents such as scientific publications and patents. However, adapting these methods to incorporate alternative text sources like micro-blogging posts and news has proven challenging as they struggle to model open-domain entities and relations, typically found in these sources. In this paper, we propose an enhanced information extraction pipeline tailored to the extraction of a knowledge graph comprising open-domain entities from micro-blogging posts on social media platforms. Our pipeline leverages dependency parsing and classifies entity relations in an unsupervised manner through hierarchical clustering over word embeddings. We provide a use case on extracting semantic triples from a corpus of 100 thousand tweets about digital transformation and publicly release the generated knowledge graph. On the same dataset, we conduct two experimental evaluations, showing that the system produces triples with precision over 95% and outperforms similar pipelines of around 5% in terms of precision, while generating a comparatively higher number of triples.}
}
@article{TERHORST2023102491,
title = {Automatic knowledge graph population with model-complete text comprehension for pre-clinical outcomes in the field of spinal cord injury},
journal = {Artificial Intelligence in Medicine},
volume = {137},
pages = {102491},
year = {2023},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2023.102491},
url = {https://www.sciencedirect.com/science/article/pii/S0933365723000052},
author = {Hendrik {ter Horst} and Nicole Brazda and Jessica Schira-Heinen and Julia Krebbers and Hans-Werner Müller and Philipp Cimiano},
keywords = {Information extraction, Pre-clinical outcomes, Structured prediction, Spinal cord injury, Deep knowledge graph population},
abstract = {The paradigm of evidence-based medicine requires that medical decisions are made on the basis of the best available knowledge published in the literature. Existing evidence is often summarized in the form of systematic reviews and/or meta-reviews and is rarely available in a structured form. Manual compilation and aggregation is costly, and conducting a systematic review represents a high effort. The need to aggregate evidence arises not only in the context of clinical trials, but is also important in the context of pre-clinical animal studies. In this context, evidence extraction is important to support translation of the most promising pre-clinical therapies into clinical trials or to optimize clinical trial design. Aiming at developing methods that facilitate the task of aggregating evidence published in pre-clinical studies, in this paper a new system is presented that automatically extracts structured knowledge from such publications and stores it in a so-called domain knowledge graph. The approach follows the paradigm of model-complete text comprehension by relying on guidance from a domain ontology creating a deep relational data-structure that reflects the main concepts, protocol, and key findings of studies. Focusing on the domain of spinal cord injuries, a single outcome of a pre-clinical study is described by up to 103 outcome parameters. Since the problem of extracting all these variables together is intractable, we propose a hierarchical architecture that incrementally predicts semantic sub-structures according to a given data model in a bottom-up fashion. At the heart of our approach is a statistical inference method that relies on conditional random fields to infer the most likely instance of the domain model given the text of a scientific publication as input. This approach allows modeling dependencies between the different variables describing a study in a semi-joint fashion. We present a comprehensive evaluation of our system to understand the extent to which our system can capture a study in the depth required to enable the generation of new knowledge. We conclude the article with a brief description of some applications of the populated knowledge graph and show the potential implications of our work for supporting evidence-based medicine.}
}
@article{LIANG2025883,
title = {A survey of large language model-augmented knowledge graphs for advanced complex product design},
journal = {Journal of Manufacturing Systems},
volume = {80},
pages = {883-901},
year = {2025},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2025.04.016},
url = {https://www.sciencedirect.com/science/article/pii/S0278612525001050},
author = {Xinxin Liang and Zuoxu Wang and Jihong Liu},
keywords = {Knowledge Graph, Large Language Model, Complex Product Design, Intelligent Manufacturing},
abstract = {In the Human-AI collaboration rapid development era, the design and development of knowledge-intensive complex products should enable the design process with the help of advanced AI technology, and enhance the reasoning and application of design domain knowledge. Extracting and reusing domain knowledge would greatly facilitate the success of complex product design. Knowledge graphs (KGs), a powerful knowledge representation and storage technology, have been widely deployed in advanced complex product design because of their advantages in mining and applying large-scale, complex, and specialized domain knowledge. But merely KG and its related reasoning approaches still cannot fully support the ill-defined product design tasks. In the future complex product design, Human-AI collaboration will become a mainstream prevention trend. Large language models (LLMs) have outstanding performance in natural language understanding and generation, showing promising potential to collaborate with KGs in complex product design and development. Till 2024/03/04, only a few studies have systematically reviewed the current status of LLM and KG applications in the engineering field, not to mention a further detailed review in the complex product design field, leaving many issues not covered or fully examined. To fill this gap, 100 articles published in the last 4 years (i.e., 2021–2024) were screened and surveyed. This study provides a statistical analysis of the screened research articles, mainstream techniques of LLM & KG, and LLM & KG applications were analyze. To understand how KG and LLM could support complex product design, a framework of LLMs-augmented KG in advanced complex product design was proposed, which contains data layer, KG & LLM collaboration layer, enhanced design capability layer, and design task layer. Furthermore, we also discussed the challenges and future research directions of the LLM-KG-collaborated complex product design paradigm. As an exploratory review paper, it provides insightful ideas for implementing more specialized domain KGs in product design field.}
}
@article{CHEN2025103124,
title = {Knowledge Graphs for Multi-modal Learning: Survey and Perspective},
journal = {Information Fusion},
volume = {121},
pages = {103124},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2025.103124},
url = {https://www.sciencedirect.com/science/article/pii/S1566253525001976},
author = {Zhuo Chen and Yichi Zhang and Yin Fang and Yuxia Geng and Lingbing Guo and Jiaoyan Chen and Xiaoze Liu and Jeff Z. Pan and Ningyu Zhang and Huajun Chen and Wen Zhang},
keywords = {Knowledge Graphs, Multi-modal Learning, Knowledge-based Information Fusion, Visual Question Answering, Large Language Model, Literature review},
abstract = {Integrated with multi-modal learning, knowledge graphs (KGs) as structured knowledge repositories, can enhance AI for processing and understanding complex, real-world data. This paper provides a comprehensive survey of cutting-edge research on KG-aware multi-modal learning. For these core areas, we provide task definitions, evaluation benchmarks, and comprehensive insights into key breakthroughs, offering detailed explanations critical for conducting related research. Furthermore, we also discuss current challenges, highlighting emerging trends and future research directions. The repository for this paper can be found at https://github.com/zjukg/KG-MM-Survey.}
}
@article{KABAL20242617,
title = {Enhancing Domain-Independent Knowledge Graph Construction through OpenIE Cleaning and LLMs Validation},
journal = {Procedia Computer Science},
volume = {246},
pages = {2617-2626},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.436},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924024761},
author = {Othmane Kabal and Mounira Harzallah and Fabrice Guillet and Ryutaro Ichise},
keywords = {Knowledge graph, From raw text, Domain-independent building, Knowledge graph construction pipeline, LLMs based validation, Information Extraction, LLMs for KG},
abstract = {In the challenging context of Knowledge Graph (KG) construction from text, traditional approaches often rely on Open Information Extraction (OpenIE) pipelines. However, they are prone to generating many incorrect triplets. While domain specific Named Entity Recognition (NER) is commonly used to enhance the results, it compromises the domain independence and misses crucial triplets. To address these limitations, we introduce G-T2KG, a novel pipeline for KG construction that aims to preserve the domain independence while reducing incorrect triplets, thus offering a cost-effective solution without the need for domain-specific adaptations. Our pipeline utilizes state-of-the-art OpenIE combined with both a noun phrase-based cleaning and a LLMs based validation. It is evaluated using gold standards in two distinct domains (i.e., computer science and music) that we have constructed in the context of this study. On computer science corpus, the experimental results demonstrate a higher recall as compared to state-of-the-art approaches, and a higher precision notably increased by the integration of LLMs. Experiments on the music corpus show good performance, underscoring the versatility and effectiveness of G-T2KG in domain-independent KG construction.}
}
@article{COSTELLO2023,
title = {Leveraging Knowledge Graphs and Natural Language Processing for Automated Web Resource Labeling and Knowledge Mobilization in Neurodevelopmental Disorders: Development and Usability Study},
journal = {Journal of Medical Internet Research},
volume = {25},
year = {2023},
issn = {1438-8871},
doi = {https://doi.org/10.2196/45268},
url = {https://www.sciencedirect.com/science/article/pii/S1438887123002388},
author = {Jeremy Costello and Manpreet Kaur and Marek Z Reformat and Francois V Bolduc},
keywords = {knowledge graph, natural language processing, neurodevelopmental disorders, autism spectrum disorder, intellectual disability, attention deficit hyperactivity disorder, named entity recognition, topic modeling, aggregation operator},
abstract = {Background
Patients and families need to be provided with trusted information more than ever with the abundance of online information. Several organizations aim to build databases that can be searched based on the needs of target groups. One such group is individuals with neurodevelopmental disorders (NDDs) and their families. NDDs affect up to 18% of the population and have major social and economic impacts. The current limitations in communicating information for individuals with NDDs include the absence of shared terminology and the lack of efficient labeling processes for web resources. Because of these limitations, health professionals, support groups, and families are unable to share, combine, and access resources.
Objective
We aimed to develop a natural language–based pipeline to label resources by leveraging standard and free-text vocabularies obtained through text analysis, and then represent those resources as a weighted knowledge graph.
Methods
Using a combination of experts and service/organization databases, we created a data set of web resources for NDDs. Text from these websites was scraped and collected into a corpus of textual data on NDDs. This corpus was used to construct a knowledge graph suitable for use by both experts and nonexperts. Named entity recognition, topic modeling, document classification, and location detection were used to extract knowledge from the corpus.
Results
We developed a resource annotation pipeline using diverse natural language processing algorithms to annotate web resources and stored them in a structured knowledge graph. The graph contained 78,181 annotations obtained from the combination of standard terminologies and a free-text vocabulary obtained using topic modeling. An application of the constructed knowledge graph is a resource search interface using the ordered weighted averaging operator to rank resources based on a user query.
Conclusions
We developed an automated labeling pipeline for web resources on NDDs. This work showcases how artificial intelligence–based methods, such as natural language processing and knowledge graphs for information representation, can enhance knowledge extraction and mobilization, and could be used in other fields of medicine.}
}
@article{LI2024123921,
title = {Self-supervised commonsense knowledge learning for document-level relation extraction},
journal = {Expert Systems with Applications},
volume = {250},
pages = {123921},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.123921},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424007875},
author = {Rongzhen Li and Jiang Zhong and Zhongxuan Xue and Qizhu Dai and Xue Li},
keywords = {Document-level relation extraction, Self-supervised learning, Commonsense knowledge},
abstract = {Compared to sentence-level relation extraction, practical document-level relation extraction (DocRE) is a more challenging task for which multi-entity problems need to be resolved. It aims at extracting relationships between two entities over multiple sentences at once while taking into account significant cross-sentence features. Learning long-distance semantic relation representation across sentences in a document, however, is a widespread and difficult task. To address the above issues, we propose a novel Self-supervised Commonsense-enhanced DocRE approach, named as SCDRE, bypassing the need for external knowledge. The methodology begins by harnessing self-supervised learning to capture the commonsense knowledge pertaining to each entity within an entity pair, drawing insights from the commonsense entailed text. This acquired knowledge subsequently serves as the foundation for transforming cross-sentence entity pairs into alias counterparts achieved by the coreference commonsense replacement. The focus then shifts to semantic relation representation learning, applied to these alias entity pairs. Through a process of entity pair rich attention fusion, these alias pairs are seamlessly and automatically translated back into the target entity pairs. This innovation harnesses self-supervised learning and contextual commonsense to distinguish SCDRE as a unique and self-contained approach, promising an enhanced ability to extract relationships from documents. We examine our model on three publicly accessible datasets, DocRED, DialogRE and MPDD, and the results show that it performs significantly better than strong baselines by 2.03% F1, and commonsense knowledge has an important contribution to the DocRE by the ablation experimental analysis.}
}
@article{CUI2025127430,
title = {KLLMs4Rec: Knowledge graph-enhanced LLMs sentiment extraction for personalized recommendations},
journal = {Expert Systems with Applications},
volume = {282},
pages = {127430},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127430},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425010528},
author = {Yachao Cui and Kaiguang Wang and Hongli Yu and Xiaoxu Guo and Han Cao},
keywords = {Large Language Models, Text sentiment analysis, Knowledge graphs, Personalized recommendation},
abstract = {Recommendation algorithms typically leverage auxiliary information such as user reviews and knowledge graphs to enhance algorithm performance, thereby alleviating data sparsity and cold start issues. Recently, researchers have increasingly employed large language models, which boast powerful natural language understanding capabilities, to further improve recommendation systems. However, these models often suffer from hallucination problems. Moreover, integrating heterogeneous information, such as reviews and knowledge graphs, can introduce new noise, potentially impairing recommendation performance. Knowledge graphs, as tightly organized structured knowledge bases, can assist in addressing the hallucination problem and heterogeneous information fusion problem of LLMs. To effectively address the aforementioned issues, we propose the Knowledge Graph-Enhanced Large Language Model Sentiment Extraction for the Personalized Recommendation Model (KLLMs4Rec). It aims to solve the LLMs hallucination problem and the noise problem caused by the fusion of heterogeneous information in recommender systems, and provide users with more accurate, diverse and novel personalized recommendations. To address the hallucination problem when extracting user sentiments from reviews with LLMs, we designed a knowledge graph-enhanced prompt template. It is worth noting that this scheme also solves the noise issue of heterogeneous information fusion. Additionally, to further expand user preferences extracted from reviews, this paper proposes a new hierarchical sentiment attention graph convolutional network, which utilizes three sentiment weight schemes to propagate user personalized preferences on the knowledge graph. Extensive experiments on the Movielens-20 m, Amazon-book, and Yelp datasets demonstrate that our model surpasses current leading methods while effectively addressing the hallucination problem of LLMs and the noise problem of heterogeneous information fusion.}
}
@article{CHENG2024109361,
title = {A link prediction method for Chinese financial event knowledge graph based on graph attention networks and convolutional neural networks},
journal = {Engineering Applications of Artificial Intelligence},
volume = {138},
pages = {109361},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.109361},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624015197},
author = {Haitao Cheng and Ke Wang and Xiaoying Tan},
keywords = {Chinese financial event knowledge graph, Link prediction, Graph attention network, Convolutional neural network, Large language model},
abstract = {Finance is a knowledge-intensive domain in nature, with its data containing a significant amount of interconnected information. Constructing a financial knowledge graph is an important application for transforming financial text/web content into machine-readable data. However, the complexity of Chinese financial knowledge and the dynamic and evolving nature of Chinese financial data often lead to incomplete knowledge graphs. To address this challenge, we propose a novel link prediction method for Chinese financial event knowledge graph based on Graph Attention Networks and Convolutional Neural Networks. Our method begins with the construction of the foundational Chinese financial event knowledge graph using a relational triple extraction module integrated with a large language model framework, along with a Prompting with Iterative Verification (PiVe) module for validation. To enhance the completeness of the knowledge graph, we introduce an encoder-decoder framework, where a graph attention network with joint embeddings of financial event entities and relations acts as the encoder, while a Convolutional Knowledge Base embedding model (ConvKB) serves as the decoder. This framework effectively aggregates crucial neighbor information and captures global relationships among entity and relation embeddings. Extensive comparative experiments demonstrate the utility and accuracy of this method, ultimately enabling the effective completion of Chinese financial event knowledge graphs.}
}
@article{MORIOKA2025762,
title = {Automatic construction of asset knowledge graph with large language model},
journal = {Procedia CIRP},
volume = {135},
pages = {762-767},
year = {2025},
note = {32nd CIRP Conference on Life Cycle Engineering (LCE2025)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2025.01.097},
url = {https://www.sciencedirect.com/science/article/pii/S2212827125004469},
author = {Tomoaki Morioka and Toshiaki Kono and Takehisa Nishida},
keywords = {Maintenance, Knowledge graph construction, Large language model, Reliability centered maintenance},
abstract = {Lifecycle engineering is a critical concept for fostering environmentally sustainable practices within the manufacturing sector. An essential component of lifecycle management for achieving sustainability is reliability-centered maintenance, which enhances various key performance indicators (KPIs), including machine availability and environmental impact. Effective and reliable maintenance necessitates expert knowledge of the equipment. For instance, determining which components and failure modes should be addressed through condition-based maintenance requires insights derived from failure mode and effect analysis (FMEA). However, constructing expert knowledge is labor-intensive, and ensuring its quality presents significant challenges. This study proposes a method for the automated construction of expert knowledge related to maintenance, along with a corresponding tool designed to reduce construction costs and enhance knowledge quality. The proposed method leverages a large language model (LLM) to automatically generate asset knowledge graphs based on FMEA. By combining general knowledge about equipment derived from the pre-trained LLM with specialized information extracted from technical documents, the tool creates knowledge structures such as component trees and failure modes. Subject matter experts can then iteratively refine and validate this knowledge. To evaluate the proposed approach, we assessed the accuracy and coverage of the knowledge generated by the tool in two case studies involving specific types of equipment. The results indicated that the LLM-generated output contained 4.98 times more items than those manually created, with precision ranging from 0.490 to 0.662 and recall ranging from 0.481 to 0.810.}
}
@article{DETROJA2023200244,
title = {A survey on Relation Extraction},
journal = {Intelligent Systems with Applications},
volume = {19},
pages = {200244},
year = {2023},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2023.200244},
url = {https://www.sciencedirect.com/science/article/pii/S2667305323000698},
author = {Kartik Detroja and C.K. Bhensdadia and Brijesh S. Bhatt},
keywords = {Information Extraction (IE), Relation Extraction (RE), Machine Learning (ML), Deep Learning (DL), Convolutional Neural Network (CNN), Recurrent Neural Network (RNN)},
abstract = {With the advent of the Internet, the daily production of digital text in the form of social media, emails, blogs, news items, books, research papers, and Q&A forums has increased significantly. This unstructured or semi-structured text contains a huge amount of information. Information Extraction (IE) can extract meaningful information from text sources and present it in a structured format. The sub-tasks of IE include Named Entity Recognition (NER), Event Extraction, Relation Extraction (RE), Sentiment Extraction, Opinion Extraction, Terminology Extraction, Reference Extraction, and so on. One way to represent information in the text is in the form of entities and relations representing links between entities. The Entity Extraction task identifies entities from the text, and the Relation Extraction (RE) task can identify relationships between those entities. Many NLP applications can benefit from relational information derived from natural language, including Structured Search, Knowledge Base (KB) population, Information Retrieval, Question-Answering, Language Understanding, Ontology Learning, etc. This survey covers (1) basic concepts of Relation Extraction; (2) various Relation Extraction methodologies; (3) Deep Learning techniques for Relation Extraction; and (4) different datasets that can be used to evaluate the RE system.}
}
@article{WAN2023103268,
title = {A deep neural network model for coreference resolution in geological domain},
journal = {Information Processing & Management},
volume = {60},
number = {3},
pages = {103268},
year = {2023},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2023.103268},
url = {https://www.sciencedirect.com/science/article/pii/S0306457323000055},
author = {Bo Wan and Shuai Dong and Deping Chu and Hong Li and Yiyang Liu and Jinming Fu and Fang Fang and Shengwen Li and Dan Zhou},
keywords = {Geological text mining, Coreference resolution, Deeping learning, Chinese geological texts},
abstract = {Coreference resolution of geological entities is an important task in geological information mining. Although the existing generic coreference resolution models can handle geological texts, a dramatic decline in their performance can occur without sufficient domain knowledge. Due to the high diversity of geological terminology, coreference is intricately governed by the semantic and expressive structure of geological terms. In this paper, a framework CorefRoCNN based on RoBERTa and convolutional neural network (CNN) for end-to-end coreference resolution of geological entities is proposed. Firstly, the fine-tuned RoBERTa language model is used to transform words into dynamic vector representations with contextual semantic information. Second, a CNN-based multi-scale structure feature extraction module for geological terms is designed to capture the invariance of geological terms in length, internal structure, and distribution. Thirdly, we incorporate the structural feature and word embedding for further determinations of coreference relations. In addition, attention mechanisms are used to improve the ability of the model to capture valid information in geological texts with long sentence lengths. To validate the effectiveness of the model, we compared it with several state-of-the-art models on the constructed dataset. The results show that our model has the optimal performance with an average F1 value of 79.78%, which is a 1.22% improvement compared to the second-ranked method.}
}
@article{YANG2025,
title = {Large Language Model–Driven Knowledge Graph Construction in Sepsis Care Using Multicenter Clinical Databases: Development and Usability Study},
journal = {Journal of Medical Internet Research},
volume = {27},
year = {2025},
issn = {1438-8871},
doi = {https://doi.org/10.2196/65537},
url = {https://www.sciencedirect.com/science/article/pii/S1438887125004534},
author = {Hao Yang and Jiaxi Li and Chi Zhang and Alejandro Pazos Sierra and Bairong Shen},
keywords = {sepsis, knowledge graph, large language models, prompt engineering, real-world, GPT-4.0},
abstract = {Background
Sepsis is a complex, life-threatening condition characterized by significant heterogeneity and vast amounts of unstructured data, posing substantial challenges for traditional knowledge graph construction methods. The integration of large language models (LLMs) with real-world data offers a promising avenue to address these challenges and enhance the understanding and management of sepsis.
Objective
This study aims to develop a comprehensive sepsis knowledge graph by leveraging the capabilities of LLMs, specifically GPT-4.0, in conjunction with multicenter clinical databases. The goal is to improve the understanding of sepsis and provide actionable insights for clinical decision-making. We also established a multicenter sepsis database (MSD) to support this effort.
Methods
We collected clinical guidelines, public databases, and real-world data from 3 major hospitals in Western China, encompassing 10,544 patients diagnosed with sepsis. Using GPT-4.0, we used advanced prompt engineering techniques for entity recognition and relationship extraction, which facilitated the construction of a nuanced sepsis knowledge graph.
Results
We established a sepsis database with 10,544 patient records, including 8497 from West China Hospital, 690 from Shangjin Hospital, and 357 from Tianfu Hospital. The sepsis knowledge graph comprises of 1894 nodes and 2021 distinct relationships, encompassing nine entity concepts (diseases, symptoms, biomarkers, imaging examinations, etc) and 8 semantic relationships (complications, recommended medications, laboratory tests, etc). GPT-4.0 demonstrated superior performance in entity recognition and relationship extraction, achieving an F1-score of 76.76 on a sepsis-specific dataset, outperforming other models such as Qwen2 (43.77) and Llama3 (48.39). On the CMeEE dataset, GPT-4.0 achieved an F1-score of 65.42 using few-shot learning, surpassing traditional models such as BERT-CRF (62.11) and Med-BERT (60.66). Building upon this, we compiled a comprehensive sepsis knowledge graph, comprising of 1894 nodes and 2021 distinct relationships.
Conclusions
This study represents a pioneering effort in using LLMs, particularly GPT-4.0, to construct a comprehensive sepsis knowledge graph. The innovative application of prompt engineering, combined with the integration of multicenter real-world data, has significantly enhanced the efficiency and accuracy of knowledge graph construction. The resulting knowledge graph provides a robust framework for understanding sepsis, supporting clinical decision-making, and facilitating further research. The success of this approach underscores the potential of LLMs in medical research and sets a new benchmark for future studies in sepsis and other complex medical conditions.}
}
@article{WANG2024104001,
title = {Joint relational triple extraction with enhanced representation and binary tagging framework in cybersecurity},
journal = {Computers & Security},
volume = {144},
pages = {104001},
year = {2024},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.104001},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824003067},
author = {Xiaodi Wang and Zhonglin Liu and Jiayong Liu},
keywords = {Cybersecurity, Open-source intelligence, Relation extraction, Joint extraction, Knowledge graph},
abstract = {The cyber threat intelligence (CTI) knowledge graph is a valuable tool for aiding security practitioners in the identification and analysis of cyberattacks. These graphs are constructed from CTI data, organized into relational triples, where each triple comprises two entities linked by a particular relation. However, as the volume of CTI data is expanding at a faster rate than predicted, existing technologies are unable to extract relational triples quickly and accurately. This work mainly focuses on the extraction of relational triples in CTI data, which is achieved by an enhanced representation and binary tagging framework (ERBTF). Firstly, we introduce embedding representations for relations and concatenate these with word embeddings to obtain the initial hidden representation. Subsequently, we employ a novel dilated convolutional encoder that consists of a dilated convolution neural network, gate mechanism and residual connection to enhance the learned contextual representation. Afterwards, we adopt an attention module that includes multi-head self-attention and position-wise feed-forward neural network to allocate greater attention to words that significantly influence the specific relation. Additionally, we utilize the straightforward yet efficient binary entity tagger to identify subject and object entities under different relations for constructing relational triples. We conduct massive experiments on relational triple extraction from CTI data, the results show that ERBTF is superior to the existing relation extraction models, and achieves state-of-the-art performance.}
}
@article{ZHANG2025293,
title = {A disambiguation method for potential ambiguities in Chinese based on knowledge graphs and large language model},
journal = {Alexandria Engineering Journal},
volume = {126},
pages = {293-302},
year = {2025},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2025.04.089},
url = {https://www.sciencedirect.com/science/article/pii/S1110016825005861},
author = {Dan Zhang and Delong Jia},
keywords = {Chinese ambiguity, Disambiguation model, Knowledge graph, Large language model, Natural language processing},
abstract = {Traditional disambiguation methods struggle to effectively balance and integrate a wide range of contextual information and world knowledge when dealing with potential ambiguities in Chinese. To address this issue, this paper proposes a disambiguation model that integrates knowledge graphs and large language models (LLMs) to tackle lexical ambiguity in Chinese texts. This article uses an attention based disambiguation model, which is fine-tuned using multiple hyperparameter configurations. It optimizes network layers and knowledge graph embedding dimensions to enhance performance. Visualization of the attention mechanism reveals the model's focus on target words, context, and knowledge graph entities. Experiments conducted on a dataset comprising 200,000 sentences demonstrate significant improvements in accuracy and F1 scores, reaching 92.4 % and 91.9 %, respectively, compared to traditional statistical and deep learning models. Visualization of the attention mechanism reveals the model's focus on target words, context, and knowledge graph entities. The findings suggest that integrating knowledge graphs with LLMs offers an innovative approach to complex language tasks. In practical applications such as machine translation and chatbots, this model is expected to enhance both performance and interpretability.}
}
@article{DONG2023132,
title = {Relational distance and document-level contrastive pre-training based relation extraction model},
journal = {Pattern Recognition Letters},
volume = {167},
pages = {132-140},
year = {2023},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2023.02.012},
url = {https://www.sciencedirect.com/science/article/pii/S0167865523000429},
author = {Yihao Dong and Xiaolong Xu},
abstract = {Document-level relation extraction has multi-entity and multi-mention compared to sentence-level, existing sentence-level relation extraction models cannot meet the requirements of document-level relation extraction. Existing graph-based document-level models usually design points and edges manually, which often introduces man-made noise; while the Transformer-based models cannot deeply solve the difficulties such as coreference resolution by designing pre-training tasks or other methods. In this paper, we propose a new Relational Distance and Document-level Contrastive Pre-training (RDDCP) based relation extraction model, which achieves coreference resolution by simple and effective mention replacement; we also introduce the concept of relational distance to achieve document-level contrastive pre-training, and find the most likely relational mention pairs from the plural mention pairs existing in the document-level dataset for contrastive learning; for the relation information in distant mentions ignored by the relational distance, we quantified the distances as weights and incorporated the information with weights into the embedding representation of entities. Each entity presents different entity embedding representations in different entity pairs. We conducted experiments on three popular datasets and the RDDCP model outperformed GAIN, SSAN and ATLOP as well as other baseline models in terms of performance and time complexity.}
}
@article{SCHAFER2024639,
title = {BioKGrapher: Initial evaluation of automated knowledge graph construction from biomedical literature},
journal = {Computational and Structural Biotechnology Journal},
volume = {24},
pages = {639-660},
year = {2024},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2024.10.017},
url = {https://www.sciencedirect.com/science/article/pii/S2001037024003386},
author = {Henning Schäfer and Ahmad Idrissi-Yaghir and Kamyar Arzideh and Hendrik Damm and Tabea M.G. Pakull and Cynthia S. Schmidt and Mikel Bahn and Georg Lodde and Elisabeth Livingstone and Dirk Schadendorf and Felix Nensa and Peter A. Horn and Christoph M. Friedrich},
keywords = {Knowledge graph, Named entity recognition, Entity linking, Clinical guidelines, Software},
abstract = {Background The growth of biomedical literature presents challenges in extracting and structuring knowledge. Knowledge Graphs (KGs) offer a solution by representing relationships between biomedical entities. However, manual construction of KGs is labor-intensive and time-consuming, highlighting the need for automated methods. This work introduces BioKGrapher, a tool for automatic KG construction using large-scale publication data, with a focus on biomedical concepts related to specific medical conditions. BioKGrapher allows researchers to construct KGs from PubMed IDs. Methods The BioKGrapher pipeline begins with Named Entity Recognition and Linking (NER+NEL) to extract and normalize biomedical concepts from PubMed, mapping them to the Unified Medical Language System (UMLS). Extracted concepts are weighted and re-ranked using Kullback-Leibler divergence and local frequency balancing. These concepts are then integrated into hierarchical KGs, with relationships formed using terminologies like SNOMED CT and NCIt. Downstream applications include multi-label document classification using Adapter-infused Transformer models. Results BioKGrapher effectively aligns generated concepts with clinical practice guidelines from the German Guideline Program in Oncology (GGPO), achieving F1-Scores of up to 0.6. In multi-label classification, Adapter-infused models using a BioKGrapher cancer-specific KG improved micro F1-Scores by up to 0.89 percentage points over a non-specific KG and 2.16 points over base models across three BERT variants. The drug-disease extraction case study identified indications for Nivolumab and Rituximab. Conclusion BioKGrapher is a tool for automatic KG construction, aligning with the GGPO and enhancing downstream task performance. It offers a scalable solution for managing biomedical knowledge, with potential applications in literature recommendation, decision support, and drug repurposing.}
}
@article{KAUR2022,
title = {Deciphering the Diversity of Mental Models in Neurodevelopmental Disorders: Knowledge Graph Representation of Public Data Using Natural Language Processing},
journal = {Journal of Medical Internet Research},
volume = {24},
number = {8},
year = {2022},
issn = {1438-8871},
doi = {https://doi.org/10.2196/39888},
url = {https://www.sciencedirect.com/science/article/pii/S143888712200512X},
author = {Manpreet Kaur and Jeremy Costello and Elyse Willis and Karen Kelm and Marek Z Reformat and Francois V Bolduc},
keywords = {concept map, neurodevelopmental disorder, knowledge graph, text analysis, semantic relatedness, PubMed, forums, mental model},
abstract = {Background
Understanding how individuals think about a topic, known as the mental model, can significantly improve communication, especially in the medical domain where emotions and implications are high. Neurodevelopmental disorders (NDDs) represent a group of diagnoses, affecting up to 18% of the global population, involving differences in the development of cognitive or social functions. In this study, we focus on 2 NDDs, attention deficit hyperactivity disorder (ADHD) and autism spectrum disorder (ASD), which involve multiple symptoms and interventions requiring interactions between 2 important stakeholders: parents and health professionals. There is a gap in our understanding of differences between mental models for each stakeholder, making communication between stakeholders more difficult than it could be.
Objective
We aim to build knowledge graphs (KGs) from web-based information relevant to each stakeholder as proxies of mental models. These KGs will accelerate the identification of shared and divergent concerns between stakeholders. The developed KGs can help improve knowledge mobilization, communication, and care for individuals with ADHD and ASD.
Methods
We created 2 data sets by collecting the posts from web-based forums and PubMed abstracts related to ADHD and ASD. We utilized the Unified Medical Language System (UMLS) to detect biomedical concepts and applied Positive Pointwise Mutual Information followed by truncated Singular Value Decomposition to obtain corpus-based concept embeddings for each data set. Each data set is represented as a KG using a property graph model. Semantic relatedness between concepts is calculated to rank the relation strength of concepts and stored in the KG as relation weights. UMLS disorder-relevant semantic types are used to provide additional categorical information about each concept’s domain.
Results
The developed KGs contain concepts from both data sets, with node sizes representing the co-occurrence frequency of concepts and edge sizes representing relevance between concepts. ADHD- and ASD-related concepts from different semantic types shows diverse areas of concerns and complex needs of the conditions. KG identifies converging and diverging concepts between health professionals literature (PubMed) and parental concerns (web-based forums), which may correspond to the differences between mental models for each stakeholder.
Conclusions
We show for the first time that generating KGs from web-based data can capture the complex needs of families dealing with ADHD or ASD. Moreover, we showed points of convergence between families and health professionals’ KGs. Natural language processing–based KG provides access to a large sample size, which is often a limiting factor for traditional in-person mental model mapping. Our work offers a high throughput access to mental model maps, which could be used for further in-person validation, knowledge mobilization projects, and basis for communication about potential blind spots from stakeholders in interactions about NDDs. Future research will be needed to identify how concepts could interact together differently for each stakeholder.}
}