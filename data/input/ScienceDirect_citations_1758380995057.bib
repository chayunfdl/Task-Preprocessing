@article{BAI2025113257,
title = {Question answering over temporal knowledge graphs based on time sensitive graph neural network},
journal = {Applied Soft Computing},
volume = {177},
pages = {113257},
year = {2025},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2025.113257},
url = {https://www.sciencedirect.com/science/article/pii/S156849462500568X},
author = {Luyi Bai and Linshuo Xu and Lin Zhu},
keywords = {Graph neural networks, Temporal knowledge graph question answering, Temporal knowledge graph query},
abstract = {Answering query questions accurately on large-scale knowledge graph has always been the key of question answering system. Recently, answering temporal questions on temporal knowledge graphs has attracted wide attention. However, all rely on predefined temporal knowledge or structured data, which often limits their ability to generalize to more dynamic or unstructured temporal information. These models can also struggle to deal with ambiguous or complex temporal reasoning, especially when dealing with long or overlapping time frames. Therefore, we propose a new Time Sensitive Graph Neural Network Question Answering model (TSGNN-QA), which not only addresses the limitations of existing knowledge graph QA models in learning and representing temporal information, but also enhances the ability to infer and predict implicit temporal information and improve multi-hop node connectivity and node ability, providing a new solution for temporal knowledge graph QA tasks. On the one hand, we use temporal graph neural network to learn temporal knowledge graph, and improve its time encoding part by hyperplane technology to improve the time sensitivity of the model. On the other hand, we use gated recurrent unit to reason and predict the hidden temporal information, so as to alleviate the problem of query accuracy degradation caused by lack of temporal information. In addition, in view of the fact that some question entities and answer entities are not directly connected in the temporal question answering process, it may take multiple entities to find the answer entity. This paper establishes and increases the multi-hop connection between nodes and carries out reasoning scoring, which improves the multi-hop query ability of the model. In the query module of TSGNN-QA model, we optimize and decodes the multi-head attention mechanism and multi-layer perceptron. Finally, experimental results show that the TSGNN-QA model has significant advantages in temporal question-answering queries. Its experimental results for Hits@1 on the CronQuestions dataset and Complex-CronQuestions dataset outperform the best baseline model by 12.03 % and 1.52 %, respectively.}
}
@article{GONG2024124652,
title = {Incorporating multi-perspective information into reinforcement learning to address multi-hop knowledge graph question answering},
journal = {Expert Systems with Applications},
volume = {255},
pages = {124652},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124652},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424015197},
author = {Chuanyang Gong and Zhihua Wei and Rui Wang and Ping Zhu and Jing Chen and Hongyun Zhang and Duoqian Miao},
keywords = {Multi-perspective information fusion, Knowledge graph embedding, Multi-hop knowledge graph question answering, Reinforcement learning, Reasoning interpretability},
abstract = {Knowledge graph question answering (KGQA) aims to answer natural language questions from structured knowledge graphs (KGs). Traditional KQGA methods are usually limited to single-hop queries and cannot handle complex questions involving multi-hop reasoning well. To overcome this issue, multi-hop KGQA based on reinforcement learning (RL) has been proposed. However, multi-hop KGQA based on RL still faces some challenges. Firstly, due to the insufficient availability of latent environmental information during the reasoning process, the agent finds it challenging to make coherent and correct decisions. Secondly, the agent only receives rewards from the environment upon reaching the answer entity during the exploration, leading to slow or even obstructed learning. To address these shortcomings, we construct multi-perspective information based on the state of the environment, and integrate multi-perspective information with RL framework, thereby creating the Multi-Perspective Information Fusion Reasoning Network (MPIFRN). MPIFRN achieves the goal via three steps. (1) We construct three different views of information, i.e., expectation embedding, instruction-guided embedding, and path-aware embedding. These environmental cues provide more reliable support for decision-making. (2) We still adopt the method of mapping entities and relations into the knowledge graph embedding space to answer multi-hop questions. At each step of reasoning, we use a scoring function to measure the plausibility of each “triple” ¡topic entity, question, candidate entity¿ in the embedding space. (3) Furthermore, we employ the asynchronous advantage actor-critic (A3C) algorithm to guide the agent in selecting the most promising entities and to expand the reasoning paths in parallel by updating policy and value network parameters, thereby facilitating multi-hop knowledge graph question answering. We conduct extensive experiments on KGQA benchmark datasets, providing substantial evidence to demonstrate the effectiveness of our approach.}
}
@article{DAI2025114314,
title = {Question answering over spatio-temporal knowledge graph},
journal = {Knowledge-Based Systems},
volume = {329},
pages = {114314},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.114314},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125013516},
author = {Xinbang Dai and Huiying Li and Nan Hu and Yongrui Chen and Rihui Jin and Huikang Hu and Guilin Qi},
keywords = {Spatio-temporal knowledge graph, Knowledge graph question answering},
abstract = {Spatio-temporal knowledge graphs (STKGs) enhance traditional KGs by integrating temporal and spatial annotations, enabling precise reasoning over questions with spatio-temporal dependencies. Despite their potential, research on spatio-temporal knowledge graph question answering (STKGQA) remains limited. This is primarily due to the lack of datasets that simultaneously contain spatio-temporal information, as well as methods capable of handling implicit spatio-temporal reasoning. To bridge this gap, we introduce the spatio-temporal question answering dataset (STQAD), the first comprehensive benchmark comprising 10,000 natural language questions that require both temporal and spatial reasoning. STQAD is constructed with real-world facts containing spatio-temporal information, ensuring that the dataset reflects practical scenarios. Furthermore, our experiments reveal that existing KGQA methods underperform on STQAD, primarily due to their inability to model spatio-temporal interactions. To address this, we propose the spatio-temporal complex question answering (STCQA) method, which jointly embeds temporal and spatial features into KG representations and dynamically filters answers through constraint-aware reasoning. STCQA achieves state-of-the-art performance, significantly outperforming existing baselines. Our work not only provides a valuable resource for future research but also advances the field by offering a robust baseline for answering complex spatio-temporal questions.}
}
@article{SHI2025103523,
title = {A stepwise intelligence generative method for structured maintenance guidance documents based on knowledge graph augmented LLM},
journal = {Advanced Engineering Informatics},
volume = {67},
pages = {103523},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103523},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625004161},
author = {Fangcheng Shi and Liang Chen and Moshi Zhou and Yue Zhao and Yu Zheng},
keywords = {Graph retrieval augmented generation, Chain of thought, Large language model, Maintenance guidance documents, Knowledge graph},
abstract = {Maintenance guidance documents (MGDs) are the basis for engineering maintenance process. At present, application of large language models (LLMs) in the generation of industrial documents have issues with inaccurate content and structure that does not match professional requirements. Therefore, this paper proposes an enhanced method that integrates professional knowledge graph retrieval augmented generation (GraphRAG) and chain-of-thought (CoT) prompts to guide LLMs to intelligently generate structured MGDs step by step. First, the LLM prompt enhancement methods are used to assist in the construction of the professional knowledge graph. Second, a CoT prompt is constructed corresponding to the stepwise characteristics of MGDs. Finally, based on the CoT prompt, the corresponding graph entity content is retrieved step by step to construct a stepwise prompt, enhancing the generation. This method has been experimentally verified in the automatic generation task of Baosteel continuous casting equipment MGDs. Compared with methods that rely solely on prompts and examples, this method significantly improves the structural controllability and content accuracy of the generated results.}
}
@article{CHEN2025113118,
title = {KG-prompt: Interpretable knowledge graph prompt for pre-trained language models},
journal = {Knowledge-Based Systems},
volume = {311},
pages = {113118},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113118},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125001650},
author = {Liyi Chen and Jie Liu and Yutai Duan and Runze Wang},
keywords = {Knowledge graph, Prompt learning, Knowledge injection, Pre-trained language models},
abstract = {Knowledge graphs (KGs) can provide rich factual knowledge for language models, enhancing reasoning ability and interpretability. However, existing knowledge injection methods usually ignore the structured information in KGs. Using structured knowledge to enhance pre-trained language models (PLMs) still has a set of challenging issues, including resource consumption of knowledge retraining, heterogeneous information, and knowledge noise. To address these issues, we explore how to flexibly inject structured knowledge into frozen PLMs. Inspired by prompt learning, we propose a novel method Knowledge Graph Prompt (KG-Prompt), which for the first time encodes the KG as structured prompts to enhance the knowledge expression ability of PLMs. KG-Prompt consists of a compressed subgraph construction module and a KG prompt generation module. In the compressed subgraph construction module, we construct compressed subgraphs based on a path-weighting strategy to reduce knowledge noise. In the KG prompt generation module, we propose a multi-hop consistency optimization strategy to learn the representation of compressed subgraphs, and then generate KG prompts based on a knowledge mapper to solve the heterogeneous information problem. The KG prompts can be inserted into the input of PLMs expediently, which decouples from PLMs and the downstream model without knowledge retraining and reduces computational resources. Extensive experiments on three knowledge-driven natural language understanding tasks demonstrate that our approach effectively improves the knowledge reasoning ability of PLMs. Furthermore, we provide a detailed analysis of different KG prompts and discuss the interpretability and generalizability of the proposed method.}
}
@article{WU2025338,
title = {Design of Intelligent Q&A System Based on Knowledge Graph Combined with Large Language Model},
journal = {Procedia Computer Science},
volume = {262},
pages = {338-347},
year = {2025},
note = {The 5th International Conference on Multi-modal Information Analytics (MMIA)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.05.061},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925019088},
author = {Quanquan Wu},
keywords = {Large Language Model, LLM, Knowledge Graph, KG, Intelligent Question-Answering, Q&A, System Design},
abstract = {LLM can transform natural language questions into structured queries, and the KG-based Q&A system can provide accurate and reliable answers. The combination of LLM and KG is the key technology to support the modern Q&A model. Through the two-wheel drive of structured knowledge and semantic understanding, the processing ability of complex problems is significantly improved, and the Q&A system is jointly promoted from the primary to the advanced intelligence evolution. In this paper, based on LLM and KG, knowledge distillation based LLM fusion KG technology is studied, so that the target model can absorb the advantages of both, not only have the language processing capability of large models, but also use the structured knowledge of KG to improve performance and interpretability. On this basis, the multi-layer architecture of intelligent Q&A system is designed, which is easy for developers to work together. The ClaudeKG model constructed in this paper is compared with DeepSeek and Doubao baseline models, and the function and performance are analyzed.}
}
@article{HYVONEN2025100852,
title = {Serendipitous knowledge discovery on the Web of Wisdom based on searching and explaining interesting relations in knowledge graphs},
journal = {Journal of Web Semantics},
volume = {85},
pages = {100852},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100852},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000386},
author = {Eero Hyvönen},
keywords = {Knowledge graphs, Relational search, Knowledge discovery, Information retrieval, Large Language Models, Generative AI},
abstract = {This paper maintains that the Semantic Web is changing into a kind of Web of Wisdom (WoW) where AI-based problem solving, based on symbolic search and sub-symbolic methods, and Information Retrieval (IR) merge: IR is seen as a process for solving information-related problems of the end user with explanations, a form of knowledge discovery. As a case of example, relational search is concerned, i.e., solving problems of the type “How are X1…Xn related to Y1…Ym?”. For example: how is Pablo Picasso related to Barcelona? The idea is to find explainable “interesting” or even serendipitous associations in Knowledge Graphs (KG) and textual web contents. It is argued that domain knowledge-based symbolic methods based of KGs are needed to complement domain-agnostic graph-based methods and Generative AI (GenAI) boosted by Large Language Models (LLM). By using domain specific knowledge, it is possible to find and explain meaningful reliable textual answers, answer quantitative questions, and use data analyses and visualizations for explaining and studying the relations.}
}
@article{LAVRINOVICS2025100844,
title = {Knowledge Graphs, Large Language Models, and Hallucinations: An NLP Perspective},
journal = {Journal of Web Semantics},
volume = {85},
pages = {100844},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100844},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000301},
author = {Ernests Lavrinovics and Russa Biswas and Johannes Bjerva and Katja Hose},
keywords = {LLM, Factuality, Knowledge Graphs, Hallucinations},
abstract = {Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) based applications including automated text generation, question answering, chatbots, and others. However, they face a significant challenge: hallucinations, where models produce plausible-sounding but factually incorrect responses. This undermines trust and limits the applicability of LLMs in different domains. Knowledge Graphs (KGs), on the other hand, provide a structured collection of interconnected facts represented as entities (nodes) and their relationships (edges). In recent research, KGs have been leveraged to provide context that can fill gaps in an LLM’s understanding of certain topics offering a promising approach to mitigate hallucinations in LLMs, enhancing their reliability and accuracy while benefiting from their wide applicability. Nonetheless, it is still a very active area of research with various unresolved open problems. In this paper, we discuss these open challenges covering state-of-the-art datasets and benchmarks as well as methods for knowledge integration and evaluating hallucinations. In our discussion, we consider the current use of KGs in LLM systems and identify future directions within each of these challenges.}
}
@article{KONDINSKI20242070,
title = {Knowledge graph representation of zeolitic crystalline materials††Electronic supplementary information (ESI) available. See DOI: https://doi.org/10.1039/d4dd00166d},
journal = {Digital Discovery},
volume = {3},
number = {10},
pages = {2070-2084},
year = {2024},
issn = {2635-098X},
doi = {https://doi.org/10.1039/d4dd00166d},
url = {https://www.sciencedirect.com/science/article/pii/S2635098X24001669},
author = {Aleksandar Kondinski and Pavlo Rutkevych and Laura Pascazio and Dan N. Tran and Feroz Farazi and Srishti Ganguly and Markus Kraft},
abstract = {Zeolites are complex and porous crystalline inorganic materials that serve as hosts for a variety of molecular, ionic and cluster species. Formal, machine-actionable representation of this chemistry presents a challenge as a variety of concepts need to be semantically interlinked. This work demonstrates the potential of knowledge engineering in overcoming this challenge. We develop ontologies OntoCrystal and OntoZeolite, enabling the representation and instantiation of crystalline zeolite information into a dynamic, interoperable knowledge graph called The World Avatar (TWA). In TWA, crystalline zeolite instances are semantically interconnected with chemical species that act as guests in these materials. Information can be obtained via custom or templated SPARQL queries administered through a user-friendly web interface. Unstructured exploration is facilitated through natural language processing using the Marie System, showcasing promise for the blended large language model – knowledge graph approach in providing accurate responses on zeolite chemistry in natural language.}
}
@article{SHIM2025103001,
title = {OmEGa(Ω): Ontology-based information extraction framework for constructing task-centric knowledge graph from manufacturing documents with large language model},
journal = {Advanced Engineering Informatics},
volume = {64},
pages = {103001},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.103001},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624006529},
author = {Midan Shim and Hyojun Choi and Heeyeon Koo and Kaehyun Um and Kyong-Ho Lee and Sanghyun Lee},
keywords = {Ontology modeling, Manufacturing and maintenance process, Information extraction, Knowledge graph, Document understanding, Large language model},
abstract = {Manufacturing industry relies heavily on technical documents that encapsulate specialized knowledge essential for optimizing production and maintenance processes. However, extracting meaningful insights from these documents is challenging due to their complex structure, domain-specific terminology, and multimodal content, which includes text, images, and tables. Furthermore, there is a contextual gap between the generic training data of pre-trained language models (PLMs) and the specialized knowledge required for manufacturing documents. To address these issues, a Task-Centric Ontology (TCO) is designed to describe fundamental manufacturing tasks, and develop OmEGa, an Ontology-based Information Extraction Framework for Task-Centric Knowledge Graphs. OmEGa leverages large language models (LLMs) to perform instance recognition and relation classification on multimodal documents. By utilizing spatial embedding and modality linking, OmEGa addresses structural challenges, while TCO-driven reasoning mitigates contextual challenges. Experimental results demonstrate the effectiveness of OmEGa, achieving strong performance on both proprietary and open-source datasets. Additionally, a Knowledge Graph Question Answering (KGQA) system built on the extracted task-centric knowledge shows promise in enhancing communication among domain experts in the manufacturing sector.}
}
@article{ZHOU2025103142,
title = {Augmenting general-purpose large-language models with domain-specific multimodal knowledge graph for question-answering in construction project management},
journal = {Advanced Engineering Informatics},
volume = {65},
pages = {103142},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103142},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625000357},
author = {Shenghua Zhou and Keyan Liu and Dezhi Li and Chun Fu and Yan Ning and Wenying Ji and Xuefan Liu and Bo Xiao and Ran Wei},
keywords = {GLM, Construction Project Management, QA, Multimodal Knowledge Graph},
abstract = {Current studies on Question-Answering of Construction Project Management (CPM-QA) face challenges, including the small-scale CPM-related knowledge repositories, the limited effectiveness of QA methods using grammar rules or tiny machine-learning models, and the shortage of testing sets for comparing QA performance. Hence, this research augments general-purpose large-language models (GLMs) with the multimodal CPM knowledge graph (CPM-KG) for CPM-QA. It encompasses (i) building the multimodal CPM-KG covering 36 CPM subfields, (ii) combining CPM-KG and GLMs through three stages, (iii) developing a 2435-question CPM-QA testing set, and (iv) assessing and comparing CPM-QA accuracies for eight pairs of original and CPM-KG-augmented GLMs. The results demonstrate that CPM-KG-augmented GLMs’ CPM-QA accuracy rate is 30.0 % superior to original GLMs on average, and top-performing CPM-KG-augmented GLMs (e.g., ERNIE-Bot 4.0) pass CRCEEs. Within 36 CPM subfields, CPM-QA accuracy enhancements resulting from CPM-KG are between 12.2 % and 57.8 %. Furthermore, CPM-KG leads to CPM-QA accuracy enhancements of 19.6 % for single-answer, 48.0 % for multiple-answer, 30.6 % for text-only, and 20.4 % for image-embedded questions. The multimodal CPM-KG also outperforms the text-only single-modal CPM-KG in enhancing CPM-QA performance. This work contributes to unveiling the significance of CPM-specific knowledge in augmenting GLMs, sharing a reusable multimodal CPM-KG-formatted knowledge repository, and delivering a testing set of CPM-QA.}
}
@article{BI2022108515,
title = {Unrestricted multi-hop reasoning network for interpretable question answering over knowledge graph},
journal = {Knowledge-Based Systems},
volume = {243},
pages = {108515},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.108515},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122002222},
author = {Xin Bi and Haojie Nie and Xiyu Zhang and Xiangguo Zhao and Ye Yuan and Guoren Wang},
keywords = {Knowledge graph, Question answering over knowledge graph, Unrestricted multi-hop reasoning},
abstract = {Knowledge graphs significantly boost the answer retrieval quality for natural language questions. The knowledge graph based question answering (KGQA) task returns accurate answer entities instead of keyword matches. For the more challenging task of multi-hop KGQA, existing methods either address fixed-length multi-hop reasoning, or perform a delayed detection of termination that requires an extra hop of reasoning. In addition, they suffer from two mapping problems between the question and relations: (1) one-to-many mapping when an individual question word corresponds to multiple hops of reasoning; (2) many-to-one mapping when a single hop of reasoning corresponds to multiple question words. Therefore, in this paper, we address these two issues of delayed determination and mapping problems by proposing an Unrestricted Multi-Hop Reasoning Network for Interpretable KGQA named UMRNet. Specifically, the proposed dynamic update strategy of question embeddings based on our attention redistribution mechanism is capable of handling the mapping problems. Furthermore, to avoid the need for an extra hop of reasoning, we propose a non-delayed termination detection mechanism that performs effective evaluation of the remaining reasoning information based on history attention. Extensive ablation studies and comparative experiments have been conducted on four KGQA benchmark datasets. The results demonstrate that the major modules of UMRNet are effective, and UMRNet outperforms the state-of-the-art methods regarding both accuracy and efficiency.}
}
@article{LIU2023110014,
title = {Prompt-WNQA: A prompt-based complex question answering for wireless network over knowledge graph},
journal = {Computer Networks},
volume = {236},
pages = {110014},
year = {2023},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2023.110014},
url = {https://www.sciencedirect.com/science/article/pii/S1389128623004590},
author = {Pei Liu and Bing Qian and Qi Sun and Longgang Zhao},
keywords = {Wireless network, Complex question answering, Knowledge graph, Prompt learning},
abstract = {With the rapid development of wireless networks, the scale of network devices is constantly expanding. Experts spend vast amounts of time consulting and dealing with various network problems of the same nature during the course of daily network operation and maintenance service, and there is no time to collect statistics and sort their information, which affects the overall operation and maintenance efficiency. This paper investigates complex Knowledge Graph Question Answering (KGQA) in the wireless network domain in order to improve operation and maintenance efficiency. Accordingly, we propose Wireless Network QA over the KG using Prompt learning (Prompt-WNQA), a novel method that deals with both constraint-based and multi-hop complex questions making use of prompt learning. Our method is also helpful in performing complex KGQA over incomplete KGs, which can complete missing relations between unconnected entities. Compared to state-of-the-art approaches, our Prompt-WNQA achieves significant improvement over extensive experiments on both the wireless network QA dataset and two public complex QA datasets.}
}
@article{THEUNER20251125,
title = {Weaving Knowledge Graphs and Large Language Models (LLMs): Leveraging Semantics for Contextualized Design Knowledge Retrieval},
journal = {Procedia CIRP},
volume = {134},
pages = {1125-1130},
year = {2025},
note = {58th CIRP Conference on Manufacturing Systems 2025},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2025.03.073},
url = {https://www.sciencedirect.com/science/article/pii/S2212827125006389},
author = {Katharina Theuner and Tomas Mikael Elmgren and Axel Götling and Marvin Carl May and Haluk Akay},
keywords = {Knowledge Engineering, Large Language Models, Design, Knowledge Graph},
abstract = {Demographic change in Europe challenges companies as retiring employees take valuable expertise with them. To address this, knowledge graphs (KGs) are emerging as tools for structured knowledge representation. Simultaneously, large language models (LLMs) are increasingly being used as innovative solutions for information retrieval. However, LLMs generally process only public knowledge, and recent approaches integrating Retrieval Augmented Generation (RAG) for private knowledge retrieval often lack contextual relevance. To enhance trustworthiness and overcome these limitations, a method is proposed for embedding latent problem-solving structures within design processes into LLM-driven information retrieval systems. Using a case study in energy infrastructure, a KG of design problems was constructed by extracting functional requirements from semi-structured documentation via LLMs. This KG is further utilized by an LLM to answer queries, with results visualized through an interactive interface. Validation through field studies with engineers underscores the approach’s effectiveness in enhancing contextual and trustworthy knowledge dissemination.}
}
@article{FAN2024103646,
title = {CuPe-KG: Cultural perspective–based knowledge graph construction of tourism resources via pretrained language models},
journal = {Information Processing & Management},
volume = {61},
number = {3},
pages = {103646},
year = {2024},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103646},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324000062},
author = {Zhanling Fan and Chongcheng Chen},
keywords = {Knowledge graph, Pretrained language models, Cultural tourism, Cultural type, ChatGPT, Travel intelligence},
abstract = {Tourism knowledge graphs lack cultural content, limiting their usefulness for cultural tourists.This paper presents the development of a cultural perspective-based knowledge graph (CuPe-KG). We evaluated fine-tuning ERNIE 3.0 (FT-ERNIE) and ChatGPT for cultural type recognition to strengthen the relationship between tourism resources and cultures. Our investigation used an annotated cultural tourism resource dataset containing 2,745 items across 16 cultural types. The results showed accuracy scores for FT-ERNIE and ChatGPT of 0.81 and 0.12, respectively, with FT-ERNIE achieving a micro-F1 score of 0.93, a 26 percentage point lead over ChatGPT's score of 0.67. These underscore FT-ERNIE's superior performance (the shortcoming is the need to annotate data) while highlighting ChatGPT's limitations because of insufficient Chinese training data and lower identification accuracy in professional knowledge. A novel ontology was designed to facilitate the construction of CuPe-KG, including elements such as cultural types, historical figures, events, and intangible cultural heritage. CuPe-KG effectively addresses cultural tourism visitors’ information retrieval needs.}
}
@article{ZHENG2025106179,
title = {Automating construction contract review using knowledge graph-enhanced large language models},
journal = {Automation in Construction},
volume = {175},
pages = {106179},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106179},
url = {https://www.sciencedirect.com/science/article/pii/S0926580525002195},
author = {Chunmo Zheng and Saika Wong and Xing Su and Yinqiu Tang and Ahsan Nawaz and Mohamad Kassem},
keywords = {Construction contract, Risk identification, Knowledge graph, Large language model},
abstract = {An effective and efficient review of construction contracts is essential for minimizing construction projects losses, but current methods are time-consuming and error-prone. Studies using methods based on Natural Language Processing (NLP) exist, but their scope is often limited to text classification or segmented label prediction. This paper investigates whether integrating Large Language Models (LLMs) and Knowledge Graphs (KGs) can enhance the accuracy and interpretability of automated contract risk identification. A tuning-free approach is proposed that integrates LLMs with a Nested Contract Knowledge Graph (NCKG) using a Graph Retrieval-Augmented Generation (GraphRAG) framework for contract knowledge retrieval and reasoning. Tested on international EPC contracts, the method achieves more accurate risk evaluation and interpretable risk summaries than baseline models. These findings demonstrate the potential of combining LLMs and KGs for reliable reasoning in tasks that are knowledge-intensive and specialized, such as contract review.}
}
@article{CHEN2022109134,
title = {Temporal knowledge graph question answering via subgraph reasoning},
journal = {Knowledge-Based Systems},
volume = {251},
pages = {109134},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.109134},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122005603},
author = {Ziyang Chen and Xiang Zhao and Jinzhi Liao and Xinyi Li and Evangelos Kanoulas},
keywords = {Question answering, Subgraph reasoning, Temporal knowledge graph},
abstract = {Knowledge graph question answering (KGQA) has recently received a lot of attention and many innovative methods have been proposed in this area, but few have been developed for temporal KGQA. Most of the existing temporal KGQA methods focus on semantic or temporal level matching and lack the ability to reason about time constraints. In this paper we propose a subgraph-based model for answering complex questions over temporal knowledge graphs (TKG), inspired by human cognition. Our method, called SubGraph Temporal Reasoning (SubGTR), consists of three main modules: implicit knowledge extraction, relevant facts search, and subgraph logic reasoning. First, the question is reformulated using background knowledge stored in the temporal knowledge graph to acquire explicit time constraints. Then, the TKG is being searched to identify relevant entities and obtain an initial scoring of them. Finally the time constraints are quantified and applied using temporal logic to reach to the final answer. To evaluate our model we experiment against temporal QA benchmarks. We observe that existing benchmarks contain many pseudo-temporal questions, and we propose Complex-CronQuestions, which a filtered version of CronQuestions and which can better demonstrate the model’s inference ability for complex temporal questions. Experimental results show that SubGTR achieves state-of-the-art performance on both CronQuestions and Complex-CronQuestions. Moreover, our model shows better performance in handling the entity cold-start problem compared to existing temporal KGQA methods.}
}
@article{YAO2022102269,
title = {TERQA: Question answering over knowledge graph considering precise dependencies of temporal information on vectors},
journal = {Displays},
volume = {74},
pages = {102269},
year = {2022},
issn = {0141-9382},
doi = {https://doi.org/10.1016/j.displa.2022.102269},
url = {https://www.sciencedirect.com/science/article/pii/S0141938222000890},
author = {Junping Yao and Yijing Wang and Xiaojun Li and Cong Yuan and Kaiyuan Cheng},
keywords = {Temporal knowledge graph, Knowledge graph question answering, Temporal information enhancement, Precise dependency},
abstract = {Time questions involve explicit and implicit constraints as well as complex time interval interactions, making them critical criteria for measuring the effect of knowledge base question answering. Although attention to temporal questions has spurred the development of temporal knowledge graphs, existing studies have focused on the simple splicing and fusion of temporal information with question or knowledge base embeddings, losing sight of the hidden interaction features between temporal information and embedded vectors. In this paper, we proposed TERQA, a temporal knowledge base question-answering approach to explore precise spatial dependencies between temporal information and embedded vectors. The exploration of the deep dependency between time and embedded vectors was divided into two stages. In the first stage, the Transformer model of depth extraction was employed to extract richer features from questions and the representation was enhanced with temporal information; in the second stage, high-level capsules were adopted to extract the low-level vector features for detailed pose determination, allowing a more precise deep dependency of temporal facts on embedded vectors. We conducted an experiment using two temporal question answering datasets, TempQuestions and CronQuestions, and the results showed that accuracy for TERQA improved 11.3% from baseline on the dataset TempQuestions with higher annotated information. Additionally, the adapted TERQA also showed varying degrees of improvements over the baseline in the larger but simply annotated dataset CronQuestions.}
}
@article{ZHU2024107971,
title = {Event-centric hierarchical hyperbolic graph for multi-hop question answering over knowledge graphs},
journal = {Engineering Applications of Artificial Intelligence},
volume = {133},
pages = {107971},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.107971},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624001295},
author = {Xun Zhu and Wang Gao and Tianyu Li and Wenguang Yao and Hongtao Deng},
keywords = {Question answering, Knowledge graphs, Graph attentive network, Hyperbolic geometry, Contrastive learning},
abstract = {Question Answering over Knowledge Graphs (KGQA) blends natural language processing with structured knowledge representation. While much attention of existing research has been given to entity-centric representations, the significance of events has not been fully explored. This paper introduces a novel Event-centric Hierarchical Hyperbolic Graph system for KGQA that effectively integrates entity and event information from knowledge graphs. Utilizing hyperbolic geometry, our model captures hierarchical structures, offering a refined representation of questions and related knowledge. Additionally, our integration of a Hierarchical Graph Attentive Network (HGAT) with Contrastive Representation Learning enables our model to effectively extract deep semantics and align them with knowledge graph structures. Empirical evaluations on the EventQA dataset demonstrate our approach’s effectiveness, significantly surpassing current leading models by 3% F1 and accuracy. This work not only extends the scope of KGQA but also highlights the importance of event-centric representations in knowledge-based tasks.}
}
@article{BI2023103242,
title = {Boosting question answering over knowledge graph with reward integration and policy evaluation under weak supervision},
journal = {Information Processing & Management},
volume = {60},
number = {2},
pages = {103242},
year = {2023},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2022.103242},
url = {https://www.sciencedirect.com/science/article/pii/S0306457322003430},
author = {Xin Bi and Haojie Nie and Guoliang Zhang and Lei Hu and Yuliang Ma and Xiangguo Zhao and Ye Yuan and Guoren Wang},
keywords = {Knowledge graph-based question answering, Multi-hop reasoning, Weak supervision, Augmented intelligence for decision-making},
abstract = {Among existing knowledge graph based question answering (KGQA) methods, relation supervision methods require labeled intermediate relations for stepwise reasoning. To avoid this enormous cost of labeling on large-scale knowledge graphs, weak supervision methods, which use only the answer entity to evaluate rewards as supervision, have been introduced. However, lacking intermediate supervision raises the issue of sparse rewards, which may result in two types of incorrect reasoning path: (1) incorrectly reasoned relations, even when the final answer entity may be correct; (2) correctly reasoned relations in a wrong order, which leads to an incorrect answer entity. To address these issues, this paper considers the multi-hop KGQA task as a Markov decision process, and proposes a model based on Reward Integration and Policy Evaluation (RIPE). In this model, an integrated reward function is designed to evaluate the reasoning process by leveraging both terminal and instant rewards. The intermediate supervision for each single reasoning hop is constructed with regard to both the fitness of the taken action and the evaluation of the unreasoned information remained in the updated question embeddings. In addition, to lead the agent to the answer entity along the correct reasoning path, an evaluation network is designed to evaluate the taken action in each hop. Extensive ablation studies and comparative experiments are conducted on four KGQA benchmark datasets. The results demonstrate that the proposed model outperforms the state-of-the-art approaches in terms of answering accuracy.}
}
@article{CHEN2026102610,
title = {GlintLM: Graph-Layered Integration with Nodal Topology with Language Models — A Bipartite Approach to Question Answering},
journal = {Information Systems},
volume = {135},
pages = {102610},
year = {2026},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2025.102610},
url = {https://www.sciencedirect.com/science/article/pii/S0306437925000948},
author = {ZhuoFan Chen and Yao Hui Hoon and Renne Ye Kai Ong and Justin Juin Hng Wong},
keywords = {Knowledge graphs (KG), Language models (LM), Question answering (QA), Graph neural networks (GNN)},
abstract = {In modern Question Answering (QA) systems, Language Models (LMs) are often combined with Knowledge Graphs (KGs) to better handle challenges like word ambiguity and complex sentence structures. This combination helps LMs gain a deeper understanding by grounding them in structured knowledge. However, existing approaches often fall short in two areas: (1) they do not fully use the features of Knowledge Graphs and Graph Neural Networks (GNNs) during reasoning, and (2) they miss opportunities to better rank and filter information using the outputs of LMs and GNNs. To address this, we propose GlintLM, a system with two key innovations. First, the Enhanced Topological Node Representation (ETNR) module, which uses graph structure and a custom node feature method to improve reasoning. Second, the Multiplex Contextual Scorer (MCS) module, which combines pre-trained LM outputs with GNN attention to better score and filter relevant nodes. Together, these components create a more effective and adaptable system for QA. GlintLM demonstrates improved performance on common-sense (CommonsenseQA, OpenBookQA) and biomedical (MedQA-USMLE) QA benchmarks, showing improved performance across commonsense and medical domains.22The source code and dataset used will be made publicly available upon publication at https://github.com/Lavender-SP/GlintLM.}
}
@article{HOU2025112622,
title = {Low-resource knowledge graph completion based on knowledge distillation driven by large language models},
journal = {Applied Soft Computing},
volume = {169},
pages = {112622},
year = {2025},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.112622},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624013966},
author = {Wenlong Hou and Weidong Zhao and Ning Jia and Xianhui Liu},
keywords = {Knowledge graph completion, Knowledge reasoning, Link prediction, Large language models},
abstract = {Knowledge graph completion (KGC) refines the existing knowledge graph (KG) by predicting missing entities or relations. Existing methods are mainly based on embeddings or texts but only perform better with abundant labeled data. Hence, KGC in resource-constrained settings is a significant problem, which faces challenges of data imbalance across relations and lack of relation label semantics. Considering that Large Language Models (LLMs) demonstrate powerful reasoning and generation capabilities, this work proposes an LLM-driven Knowledge Graph Completion Distillation (KGCD) model to address low-resource KGC. A two-stage framework is developed, involving teacher-student distillation by using LLM to improve reasoning, followed by fine-tuning on real-world low-resource datasets. To deal with data imbalance, a hybrid prompt design for LLM is proposed, which includes rethink and open prompts. Furthermore, a virtual relation label generation strategy enhances the model’s understanding of triples. Extensive experiments on three benchmarks have shown that KGCD’s effectiveness for low-resource KGC, achieving improvements in Mean Reciprocal Rank (MRR) by 11% and Hits@1 by 10% on the WN18, MRR by 10% and Hits@1 by 14% on the WN18RR, and MRR by 12% and Hits@1 by 11% on the YAGO3-10.}
}
@article{BAHR2025100807,
title = {Knowledge graph enhanced retrieval-augmented generation for failure mode and effects analysis},
journal = {Journal of Industrial Information Integration},
volume = {45},
pages = {100807},
year = {2025},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2025.100807},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X25000317},
author = {Lukas Bahr and Christoph Wehner and Judith Wewerka and José Bittencourt and Ute Schmid and Rüdiger Daub},
keywords = {FMEA, Risk assessment, Knowledge graph, Retrieval-augmented generation, Large language models},
abstract = {Failure mode and effects analysis (FMEA) is an essential tool for mitigating potential failures, particularly during the ramp-up phases of new products. However, its effectiveness is often limited by the reasoning capabilities of the FMEA tools, which are usually tabular structured. Meanwhile, large language models (LLMs) offer novel prospects for advanced natural language processing tasks. However, LLMs face challenges in tasks that require factual knowledge, a gap that retrieval-augmented generation (RAG) approaches aim to fill. RAG retrieves information from a non-parametric data store and uses a language model to generate responses. Building on this concept, we propose to enhance the non-parametric data store with a knowledge graph (KG). By integrating a KG into the RAG framework, we aim to leverage analytical and semantic question-answering capabilities for FMEA data. This paper contributes by presenting set-theoretic standardization and a schema for FMEA data, an algorithm for creating vector embeddings from the FMEA-KG, and a KG-enhanced RAG framework. Our approach is validated through a user experience design study, and we measure the precision and performance of the context retrieval recall.}
}
@article{ZHANG2025103468,
title = {A knowledge graph-enhanced large language model for question answering of hydraulic structure safety management},
journal = {Advanced Engineering Informatics},
volume = {66},
pages = {103468},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103468},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625003611},
author = {Dongliang Zhang and Gang Ma and Tongming Qu and Xudong Wang and Wei Zhou and Xiaomao Wang},
keywords = {Hydraulic structure safety management, Domain knowledge QA, Intent parsing, Knowledge graph, Large language model},
abstract = {Early detection and mitigation of hazards in hydraulic structures are crucial for effectively reducing economic and life losses. However, traditional hydraulic structure safety management methods rely on error-prone individual experience and emergency manuals, which are insufficient for making timely, scientifically informed decisions during crises. To address this challenge, this study presents an AI-driven framework for hydraulic structure safety management based on knowledge-based question answering. First, an ontology model was developed through a detailed analysis of safety management texts. Next, a partition fusion Kolmogorov-arnold network (PFKAN) enhanced with attention mechanisms was designed to jointly extract entities and relational knowledge. A safety management knowledge graph (KG) was then constructed from this knowledge. Subsequently, a large language model (LLM) was employed with a voting strategy to interpret query intent and extract relevant domain-specific knowledge from the KG. Finally, domain knowledge was integrated into the LLM to generate professional responses. Experimental results show that the F1 scores for entity and relation extraction with PFKAN reached 0.91 and 0.90, respectively, and the F1 score for query intent parsing with the voting strategy was 0.95, demonstrating competitive performance. The KG-enhanced LLM significantly improves decision-making in hydraulic structure safety management, providing an accurate and scalable tool for engineering safety managers.}
}
@article{ONG2025126648,
title = {Dynamic link prediction: Using language models and graph structures for temporal knowledge graph completion with emerging entities and relations},
journal = {Expert Systems with Applications},
volume = {272},
pages = {126648},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.126648},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425002702},
author = {Ryan Ong and Jiahao Sun and Yi-Ke Guo and Ovidiu Serban},
keywords = {Dynamic knowledge graphs, Language models, Link prediction},
abstract = {Knowledge graphs (KGs) represent real-world facts through entities and relations. However, static KGs fail to capture continuously emerging entities and relations over time. Temporal knowledge graphs address this by incorporating time information or providing multiple sequential snapshots of a static knowledge graph. Most existing work focuses on static KGs with fixed sets of entities and relations, meaning existing methods still struggle to encode emerging entities and relations. Therefore, we propose a novel methodology of combining language models and graph structure to enable the encoding of unseen entities and relations for temporal KG completion. Specifically, we encode relations with RoBERTa and entities using neighbouring relations alongside the entity’s relation type to provide contextual information. We evaluate our methodology on three datasets with emerging entities and relations over temporal snapshots: LKGE-Hybrid, FB-MBE, and the mergers and acquisitions domain TKGQA dataset. Our experiments show that our model achieves new state-of-the-art results on FB-MBE and LKGE-Hybrid while providing strong benchmark results for the TKGQA dataset. Our ablation studies show us that graph structure information is only beneficial if there is sufficient connectivity with the knowledge graph since sparser knowledge graphs can lead to noisy signals. We also explore the performance of Llama v2 on temporal link prediction, and the results show that current LLMs struggle with domain-specific temporal link prediction. Overall, our work provides an essential advance around effectively encoding continuously emerging entities and relations for temporal link prediction across evolving knowledge graphs over time.}
}
@article{DAI2025113060,
title = {Large language models can better understand knowledge graphs than we thought},
journal = {Knowledge-Based Systems},
volume = {312},
pages = {113060},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113060},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125001078},
author = {Xinbang Dai and Yuncheng Hua and Tongtong Wu and Yang Sheng and Qiu Ji and Guilin Qi},
keywords = {Knowledge graph, Large language model},
abstract = {When we integrate factual knowledge from knowledge graphs (KGs) into large language models (LLMs) to enhance their performance, the cost of injection through training increases with the scale of the models. Consequently, there is significant interest in developing prompt strategies that effectively incorporate KG information into LLMs. However, the community has not yet comprehensively understood how LLMs process and interpret KG information in different input formats and organizations within prompts, and researchers often rely on trial and error. To address this gap, we design extensive experiments to empirically study LLMs’ comprehension of different KG prompts. At the literal level, we reveal LLMs’ preferences for various input formats (from linearized triples to fluent natural language text). At the attention distribution level, we discuss the underlying mechanisms driving these preferences. We then investigate how the organization of structured knowledge impacts LLMs and evaluate LLMs’ robustness in processing and utilizing KG information in practical scenarios. Our experiments show that (1) linearized triples are more effective than fluent NL text in helping LLMs understand KG information and answer fact-intensive questions; (2) Different LLMs exhibit varying preferences for different organizational formats of triples; (3) LLMs with larger scales are more susceptible to noisy, incomplete subgraphs.}
}
@article{TASKIRAN2025109318,
title = {A knowledge-graph-based pharmaceutical engineering chatbot for drug discovery},
journal = {Computers & Chemical Engineering},
volume = {203},
pages = {109318},
year = {2025},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2025.109318},
url = {https://www.sciencedirect.com/science/article/pii/S0098135425003205},
author = {Naz Pinar Taskiran and Chia-En Jacklyn Tsai and Shuxin Huang and Arijit Chakraborty and Venkat Venkatasubramanian},
keywords = {Hybrid AI, Pharmaceutical engineering, Ontology, Knowledge graph, Neo4j, LLMs},
abstract = {Despite their success in day-to-day applications, ChatGPT and other large language models (LLMs) have not covered as much ground in scientific and engineering domains. One key challenge is the abundance of domain-specific terminology, which an LLM is not trained to extract in accordance with the underlying physical laws. Such black-box models can also lead to unreliable results or hallucinations. Hybrid AI, which combines data-driven and symbolic methods, leverages domain knowledge to add explainability and reliability to answers. Our group has previously developed a domain-informed ontology-based information extraction tool called SUSIE, which extracts key terms and their context to present them to the user as knowledge graphs (KGs). Although KGs are used to visualize relationships between different entities, they are not easily accessible for user questions. However, they serve as a structured input for LLMs. Thus, KGs can efficiently query a corpus of pharmaceutical documents, streamlining drug discovery and manufacturing processes. In this work, we propose methods to improve the information extraction capabilities of SUSIE by expanding its knowledge base and improving its ability to understand scientific material through a sentence-restructuring module. Additionally, we present a customized question-and-answer module that enables the user to query from generated KGs and get an answer in natural language. Unlike black-box models such as those purely powered by OpenAI’s models and the LangChain GraphQA packages, combining our KGs with Neo4j limits hallucinations and provides reliable and traceable answers in a user-friendly chatbot interface.}
}
@article{WANG2023110810,
title = {Hic-KGQA: Improving multi-hop question answering over knowledge graph via hypergraph and inference chain},
journal = {Knowledge-Based Systems},
volume = {277},
pages = {110810},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.110810},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123005609},
author = {Jingchao Wang and Weimin Li and Fangfang Liu and Bin Sheng and Wei Liu and Qun Jin},
keywords = {Question answering over knowledge graph, Knowledge graph embedding, Hypergraph, Mutual information},
abstract = {Question answering over knowledge graph (KGQA) aims at answering natural language questions posed over knowledge graphs (KGs). Moreover, multi-hop KGQA requires reasoning across multiple triplets in KGs to get to the answer. Unfortunately, KGs often lack complete information and contain many missing links, which poses huge challenges for multi-hop KGQA. To address this, recent several approaches have introduced KG embedding techniques, which have shown good performance on the multi-hop KGQA task. However, these methods ignore the semantic correlations between paths and questions, and the reasoning process is not easily explained. Furthermore, traditional KG embedding methods consider only low-order pairwise relations and ignore the higher-order relations among entities, leading to a sub-optimal embedding result. To address these problems, we propose Hic-KGQA, a novel hypergraph and inference chain-based model for multi-hop KGQA. Specifically, Hic-KGQA first generates pre-trained entity embeddings with multiple semantics via a hypergraph-based KGC module (HKM). Then, an inference chain modeling module (ICMM) is designed to learn the importance of different inference chains for the question and encode the highest-ranked inference chain into an embedding representation. Finally, two scoring networks are used to evaluate the correlation between the candidate answers and the questions from the perspective of both the triplet facts and the reasoning process to obtain more accurate answers. Furthermore, the mutual information maximization (MIM) is innovatively implemented to capture richer common path features from similar cases to alleviate the missing path problem caused by KG incompleteness. Experiments show that Hic-KGQA significantly outperforms existing state-of-the-art methods and is explainable.}
}
@article{SEQUEDA2025100858,
title = {Knowledge Graphs as a source of trust for LLM-powered enterprise question answering},
journal = {Journal of Web Semantics},
volume = {85},
pages = {100858},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100858},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000441},
author = {Juan Sequeda and Dean Allemang and Bryon Jacob},
keywords = {Knowledge Graph, LLM, Large Language Model, Generative AI, Question answering, Knowledge engineering, SPARQL, SQL, OWL, R2RML},
abstract = {Generative AI provides an innovative and exciting way to manage knowledge and data at any scale; for small projects, at the enterprise level, and even at a world wide web scale. It is tempting to think that Generative AI has made other knowledge-based technologies obsolete; that anything we wanted to do with knowledge-based systems, Knowledge Graphs or even expert systems can instead be done with Generative AI. Our position is counter to that conclusion. Our practical experience on implementing enterprise question answering systems using Generative AI has shown that Knowledge Graphs support this infrastructure in multiple ways: they provide a formal framework to evaluate the validity of a query generated by an LLM, serve as a foundation for explaining results, and offer access to governed and trusted data. In this position paper, we share our experience, present industry needs, and outline the opportunities for future research contributions.}
}
@article{FENG2025332,
title = {Temporal Knowledge Graph Embedding with Pre-trained Language Model},
journal = {Procedia Computer Science},
volume = {264},
pages = {332-345},
year = {2025},
note = {International Neural Network Society Workshop on Deep Learning Innovations and Applications 2025},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.07.144},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925021945},
author = {Wenying Feng and Jianming Li and Haiyan Wang and Zhaoquan Gu},
keywords = {Knowledge graph, knowledge graph representation, temporal knowledge graph, pre-trained language model},
abstract = {Large language models (LLMs) have demonstrated exceptional performance in natural language processing. This also leads to extensive research on knowledge extraction, knowledge fusion, knowledge representation, and knowledge completion using pre-trained language models (PLMs). Most of the existing works focus on static multi-relational knowledge graphs (KGs). In contrast, temporal knowledge graphs (TKGs) incorporate temporal information, whereas lack of research utilizing PLMs or LLMs. In this paper, we introduce PT2KGC, a temporal knowledge graph embedding model which employs the pre-trained language model for TKG completion and extrapolation. We present three modeling approaches of PT2KGC to model temporal knowledge: original knowledge embedding, explicit time modeling, and implicit time modeling. PT2KGC(Org.) relies solely on static knowledge; PT2KGC(Exp.) explicitly incorporates timestamps into quadruples; and PT2KGC(Imp.) models time implicitly through dataset reconstruction. We conduct experiments on two public TKG datasets. The results demonstrate the effectiveness of pre-trained language models for TKG embedding. Experiment results on three types of tasks show that all three modeling methods of PT2KGC outperform existing models. Additionally, we compare the performance of PT2KGC under different time modeling approaches.}
}
@article{MA2025112018,
title = {A knowledge graph dataset for broiler farming automatically constructed based on a large language model},
journal = {Data in Brief},
volume = {62},
pages = {112018},
year = {2025},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2025.112018},
url = {https://www.sciencedirect.com/science/article/pii/S2352340925007401},
author = {Nan Ma and Fantao Kong and Jifang Liu and Chenyang Zhang and Chenxv Zhao and Shanshan Cao and Wei Sun},
keywords = {Broiler farming, Large language model, Extraction of knowledge, Knowledge graph},
abstract = {With the rapid advancement of artificial intelligence, intelligent farming has become a key trend in modern agriculture. In particular, the application of intelligent systems in broiler farming is essential for enhancing production efficiency and optimizing management practices. Broiler farming is a complex process involving multiple interrelated components. However, existing knowledge graphs primarily focus on disease and prevention, making it difficult to capture the intricate interdependencies within the farming process. This limits the effectiveness of knowledge-based support in decision-making. To develop a high-quality broiler farming knowledge system, this study adopts large language modeling technology to integrate a Chinese corpus and construct a comprehensive knowledge graph dataset covering four core dimensions: broiler breeds, farming environment, feeding management, and disease prevention. The construction of the dataset involved three key stages. First, text scanning was used to extract information from farming-related literature, while web crawlers collected data from authoritative online sources. The data were then cleaned and manually validated to ensure accuracy and consistency. Second, the DeepKE knowledge extraction framework is used to automatically extract triples related to broiler farming from the text. These are then used as prompts to guide large-scale pre-trained language models (LLMs) to complete and optimize the knowledge, ultimately constructing a relatively complete knowledge graph of broiler farming. Finally, the structured knowledge was stored in a Neo4j graph database to support efficient querying and reasoning. The dataset not only provides researchers and farms with multidimensional knowledge of the broiler farming domain, but also supports visual management and analysis, enables data-driven inference through large models, and offers new approaches to optimize farming strategies and enhance production efficiency.}
}
@article{QU20243583,
title = {A Review of Knowledge Graph in Traditional Chinese Medicine: Analysis, Construction, Application and Prospects},
journal = {Computers, Materials and Continua},
volume = {81},
number = {3},
pages = {3583-3616},
year = {2024},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2024.055671},
url = {https://www.sciencedirect.com/science/article/pii/S1546221824008294},
author = {Xiaolong Qu and Ziwei Tian and Jinman Cui and Ruowei Li and Dongmei Li and Xiaoping Zhang},
keywords = {Systematic review, traditional Chinese medicine, knowledge graph, deep learning, medical applications},
abstract = {As an advanced data science technology, the knowledge graph systematically integrates and displays the knowledge framework within the field of traditional Chinese medicine (TCM). This not only contributes to a deeper comprehension of traditional Chinese medical theories but also provides robust support for the intelligent decision systems and medical applications of TCM. Against this backdrop, this paper aims to systematically review the current status and development trends of TCM knowledge graphs, offering theoretical and technical foundations to facilitate the inheritance, innovation, and integrated development of TCM. Firstly, we introduce the relevant concepts and research status of TCM knowledge graphs. Secondly, we conduct an in-depth analysis of the challenges and trends faced by key technologies in TCM knowledge graph construction, such as knowledge representation, extraction, fusion, and reasoning, and classifies typical knowledge graphs in various subfields of TCM. Next, we comprehensively outline the current medical applications of TCM knowledge graphs in areas such as information retrieval, diagnosis, question answering, recommendation, and knowledge mining. Finally, the current research status and future directions of TCM knowledge graphs are concluded and discussed. We believe this paper contributes to a deeper understanding of the research dynamics in TCM knowledge graphs and provides essential references for scholars in related fields.}
}
@article{ZHENG2019141,
title = {Interactive natural language question answering over knowledge graphs},
journal = {Information Sciences},
volume = {481},
pages = {141-159},
year = {2019},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2018.12.032},
url = {https://www.sciencedirect.com/science/article/pii/S0020025518309848},
author = {Weiguo Zheng and Hong Cheng and Jeffrey Xu Yu and Lei Zou and Kangfei Zhao},
keywords = {Interactive query, Natural language question and answering, Knowledge graph, Question understanding},
abstract = {As many real-world data are constructed into knowledge graphs, providing effective and convenient query techniques for end users is an urgent and important task. Although structured query languages, such as SPARQL, offer a powerful expression ability to query RDF datasets, they are difficult to use. Keywords are simple but have a very limited expression ability. Natural language question (NLQ) is promising for querying knowledge graphs. A huge challenge is how to understand the question clearly so as to translate the unstructured question into a structured query. In this paper, we present a data + oracle approach to answer NLQs over knowledge graphs. We let users verify the ambiguities during the query understanding. To reduce the interaction cost, we formalize an interaction problem and design an efficient strategy to solve the problem. We also propose a query prefetching technique by exploiting the latency in the interactions with users. Moreover, we devise a hybrid approach that incorporates NLP-based, data-driven, and interaction techniques together to complete the question understanding. Extensive experiments over real datasets demonstrate that our proposed approach is effective as it outperforms state-of-the-art methods significantly.}
}
@article{YANG2023120896,
title = {BERT and hierarchical cross attention-based question answering over bridge inspection knowledge graph},
journal = {Expert Systems with Applications},
volume = {233},
pages = {120896},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120896},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423013982},
author = {Jianxi Yang and Xiaoxia Yang and Ren Li and Mengting Luo and Shixin Jiang and Yue Zhang and Di Wang},
keywords = {Intelligent bridge management, Bridge inspection, Knowledge graph question answering, Hierarchical cross-attention mechanism, BERT},
abstract = {Aiming at the problem of insufficient knowledge service in the field of bridge inspection, this paper proposes a knowledge graph question answering (KGQA) model by using BERT and a novel hierarchical cross-attention mechanism. First, the BERT and static domain dictionaries are used as embedding layer for bridge inspection QA pairs to extract multi-granularity features. Second, in order to extract the topic entities from the domain-specific questions, the bidirectional long short-term memory model is employed to further extract features of the contextual dependency. Finally, the proposed hierarchical cross-attention mechanism realizes information interaction among the questions and knowledge triples, and calculates the similarity from shallow vocabulary and deep semantics. The proposed model is evaluated by using a general KGQA benchmark and a Chinese bridge inspection KGQA dataset. The experimental results show that the proposed approach achieves outstanding performance on both datasets. In addition, a KGQA prototype system is employed as use case to illustrate the application effect in the practical scenario.}
}
@article{DU2023110996,
title = {A contrastive framework for enhancing Knowledge Graph Question Answering: Alleviating exposure bias},
journal = {Knowledge-Based Systems},
volume = {280},
pages = {110996},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.110996},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123007463},
author = {Huifang Du and Xixie Zhang and Meng Wang and Yunwen Chen and Daqi Ji and Jun Ma and Haofen Wang},
keywords = {Question answering, Knowledge graph, Semantic parsing, Contrastive learning, Sampling augmentation},
abstract = {Current encoder–decoders for Knowledge Graph Question Answering (KGQA) commonly utilize teacher-forcing training to accelerate convergence. However, this training approach limits the model’s exposure to ground truths, resulting in exposure bias that hampers generalization performance during autoregressive inference. To alleviate the issue, we propose a contrastive framework that enables the model to access a variety of positive and negative examples, thereby enhancing generalization. Firstly, we introduce a sampling augmentation strategy to construct contrastive samples, which can ensure explicit semantic consistency of positive pairs and inconsistency of negative pairs. Secondly, we augment the training process by incorporating “hard” negatives to enhance the contrastive objective, along with augmented positives to improve the generation objective. Finally, we also sample multiple logical forms for each question during the inference to reduce the bias potential and train a contrastive ranking model to obtain the target logical form. We achieve improvements of 1.95% and 1% over the previous state-of-the-art methods on the KQA Pro and OVERNIGHT benchmarks, respectively. Furthermore, our approach obtains competitive results on the WebQSP dataset. These findings validate the efficacy of our contrastive framework for advancing KGQA performance.}
}
@article{JAYAKUMAR20171,
title = {A knowledge graph based speech interface for question answering systems},
journal = {Speech Communication},
volume = {92},
pages = {1-12},
year = {2017},
issn = {0167-6393},
doi = {https://doi.org/10.1016/j.specom.2017.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167639316301443},
author = {Ashwini {Jaya Kumar} and Christoph Schmidt and Joachim Köhler},
keywords = {Spoken question answering, Knowledge graphs, Automatic speech recognition, Spoken language understanding, Spoken interface, Linked data},
abstract = {Speech interfaces to conversational systems have been a focus in academia and industry for over a decade due to its applicability as a natural interface. Speech recognition and speech synthesis constitute the important input and output modules respectively for such spoken interface systems. In this paper, the speech recognition interface for question answering applications is reviewed, and existing limitations are discussed. The existing spoken question answering (QA) systems use an automatic speech recogniser by adapting acoustic and language models for the speech interface and off-the-shelf language processing systems for question interpretation. In the process, the impact of recognition errors and language processing inaccuracies is neglected. It is illustrated in the paper how a semantically rich knowledge graph can be used to solve automatic speech recognition and language processing specific problems. A simple concatenation of a speech recogniser and a natural language processing system is a shallow method for a speech interface. An effort beyond merely concatenating these two units is required to develop a successful spoken question answering system. It is illustrated in this paper how a knowledge graph based structured data can be used to build a unified system combining speech recognition and language understanding. This facilitates the use of a semantically rich data model for speech interface.}
}
@article{CHEN2025131230,
title = {Knowledge graph and large language model integration with focus on educational applications: A survey},
journal = {Neurocomputing},
volume = {654},
pages = {131230},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.131230},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225019022},
author = {Guanyu Chen and Tao Song and Quanyu Wang and Zheng Ma and Jun Hu and Qi Li and Chunming Wu},
keywords = {Knowledge graph, Large language model, Educational application, Retrieval augmented generation, Pre-training},
abstract = {In recent years, artificial intelligence (AI) technology has made significant advancements, particularly in the areas of large language models (LLMs) and knowledge graphs (KGs). KGs excel at structured knowledge representation and reasoning, offering interpretability; however, they are costly to construct, have limited coverage, and lack natural language processing capabilities. Conversely, LLMs possess powerful language understanding and generation abilities, but they rely heavily on vast amounts of data, are prone to “hallucinations," and lack interpretability. The integration of these two approaches is an inevitable trend for achieving stronger and more reliable AI applications, and has become a hot topic of research. Simultaneously, the combination of LLMs and KGs perfectly aligns with the pressing needs of the education field for precise reasoning and personalized services, addressing the shortcomings of traditional teaching methods and providing support for intelligent education. In light of this, this paper undertakes work in the following three key areas. Firstly, the concepts and technologies of both LLMs and KGs, along with their applications in education, are introduced. On this basis, the paper then delves into a discussion of the methods for integrating LLMs and KGs, and reviews related research progress. Finally, the paper focuses on specific educational scenarios, such as intelligent tutoring systems, intelligent learning companions, and intelligent evaluation systems, to explore the collaborative application of LLMs and KGs. The aim of this paper is to provide researchers in the field with a systematic understanding of LLMs and KGs, and to offer valuable references for future AI-driven educational innovation.}
}
@article{BAI2026102503,
title = {Time-Aware Complex Question Answering over Temporal Knowledge Graph},
journal = {Data & Knowledge Engineering},
volume = {161},
pages = {102503},
year = {2026},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2025.102503},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X25000989},
author = {Luyi Bai and Tongyue Zhang and Guangchen Feng},
keywords = {Attention mechanism, Question answering, Temporal knowledge graph, Temporal knowledge graph embedding},
abstract = {Knowledge Graph Question Answering (KGQA) is a crucial topic in Knowledge Graphs (KGs), with the objective of retrieving the corresponding facts from KGs to answer given questions. In practical applications, facts in KGs usually have time constraints, thus, question answering on Temporal Knowledge Graphs (TKGs) has attracted extensive attention. Existing Temporal Knowledge Graph Question Answering (TKGQA) methods focus on dealing with complex questions involving multiple facts, and mainly face two challenges. First, these methods only consider matching questions with facts in TKGs to identify the answer, ignoring the temporal order between different facts, which makes it challenging to solve the questions involving temporal order. Second, they usually focus on the representation of the question text while neglecting the rich semantic information within the questions, which leads to certain limitations in understanding question. To address the above challenges, this research proposes a model named Time-Aware Complex Question Answering (TA-CQA). Specifically, we extend the Temporal Knowledge Graph Embedding (TKGE) model by incorporating temporal order information into the embedding vectors, ensuring that the model can distinguish the temporal order of different facts. To enhance the semantic representation of the question, we integrate question information using attention mechanism and learnable encoder. Different from the previous TKGQA methods, we propose time relevance measurement to further enhance the accuracy of answer prediction by better capturing the correlation between question information and time information. Multiple sets of experiments on CronQuestions and TimeQuestions demonstrate our model’s superior performance across all question types. In particular, for complex questions involving multiple facts, the hit@1 values are increased by 3.2% and 3.5% respectively.}
}
@article{MISHRA2025104045,
title = {PageLLM: Incremental approach for updating a Security Knowledge Graph by using Page ranking and Large language model},
journal = {Information Processing & Management},
volume = {62},
number = {3},
pages = {104045},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.104045},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324004047},
author = {Chinmaya Mishra and Himangshu Sarma and Saravanan M.},
keywords = {Security knowledge graph, Knowledge graph, Knowledge representation learning, Page ranking, Embedding, Generative AI, Large language models (LLMs), Static knowledge graph (SKG), Incremental knowledge graph (IKG), Full knowledge graph (FKG)},
abstract = {Due to increase in cyber crime and evolution of sophisticated tools and techniques, Threat Intelligence plays a critical role. It helps defenders to stay ahead of attackers by developing the right defense mechanism to invade those attacks. In this regards security knowledge graph plays a critical role which can be used to signify complex entities and their relationship in a graphical structure. Further projecting those entities and relationships in to the lower dimension using several embedding techniques such as TransE help in many down streaming task. The learned embedding can be used to predict new cyber threat which is very helpful for defenders to stay alert and develop necessary weapons to stay ahead of an attack. One of the major challenge security knowledge graph has its dynamic nature of changing intelligence. Active learning can be used to only update the substantial portion of embedding rather than retraining the knowledge graph from scratch which has higher time and space complexity. Also given the rise in generative AI and large language models which are super rich in context, there is a scope of utilizing those for building a robust and good quality security knowledge graph. We will discuss a novel methodology called PageLLM which utilizes page ranking and LLMs to enable active learning in an incremental way and will improve the quality of knowledge graph through enriched context.}
}
@article{HAO2025105669,
title = {Uncovering compound urban crises with large language model-assisted knowledge graph construction},
journal = {International Journal of Disaster Risk Reduction},
volume = {127},
pages = {105669},
year = {2025},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2025.105669},
url = {https://www.sciencedirect.com/science/article/pii/S2212420925004935},
author = {Haiyan Hao and Xiaorui Chen and Yudi Chen and Nan Li},
abstract = {Urban areas frequently face an array of crises, including natural hazards, public health emergencies, and technological disruptions, which are becoming more interconnected due to the projected growing trend of climate change and rapid urbanization. While prior studies have investigated compound events, much of the focus has been on weather- and climate-related events, leaving a gap in understanding compound urban crises that integrate diverse types of crises at the city level. To address this, we propose a novel method for mining location-specific knowledge of compound urban crises using large language models (LLMs), enhanced with techniques such as Retrieval-Augmented Generation (RAG), iterative self-refinement, and prompt engineering. By applying the method to 1941 news articles collected from Shenzhen, China, we constructed knowledge graphs (KGs) that reveal the interconnections and compounding patterns among 13 distinct types of urban crises. The results demonstrate the effectiveness of the proposed method in identifying and mapping compound urban crises at the city level. The findings offer practical implications for urban crisis management, equipping cities with tailored knowledge to better anticipate and respond to complex, interrelated crisis scenarios.}
}
@article{ZHU2025106995,
title = {Knowledge graph based question-answering model with subgraph retrieval optimization},
journal = {Computers & Operations Research},
volume = {177},
pages = {106995},
year = {2025},
issn = {0305-0548},
doi = {https://doi.org/10.1016/j.cor.2025.106995},
url = {https://www.sciencedirect.com/science/article/pii/S0305054825000231},
author = {Rui Zhu and Bo Liu and Qiuyu Tian and Ruwen Zhang and Shengxiang Zhang and Yanna Hu and Jiuxin Cao},
keywords = {Subgraph retrieval, Entity disambiguation, Intelligent question-answering},
abstract = {Knowledge graph-based question answering (QA) is a critical domain within natural language processing, aimed at delivering precise and efficient responses to user queries. Current research predominantly focuses on minimizing subgraph sizes to enhance the efficiency and compactness of the search space. However, natural language queries often exhibit ambiguities, and merely reducing subgraph sizes may overlook relevant answer entities. Additionally, redundant relationships among entities in the knowledge graph can adversely affect QA model performance. To address these limitations, this paper introduces a novel QA model that optimizes subgraph retrieval. The proposed model enhances entity linking and subgraph retrieval by leveraging contextual features from both questions and entities. It disambiguates entities using relevant contextual features and refines the search process through entity relation merging and entity ranking strategies. This methodology improves entity recognition and linking, reduces subgraph dimensions, and broadens answer coverage, resulting in substantial improvements in QA performance. Experimental results on the CCKS2019-CKBQA dataset demonstrate the modelś effectiveness, showing an average F1 score improvement of 2.99% over the leading baseline model. Furthermore, the model’s application in the field of ocean engineering underscores its practical utility and significance.}
}
@article{SHAN2025103655,
title = {Large language Models-empowered automatic knowledge graph development based on multi-modal data for building health resilience},
journal = {Advanced Engineering Informatics},
volume = {68},
pages = {103655},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103655},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625005488},
author = {Tianlong Shan and Fan Zhang and Albert P.C. Chan and Shiyao Zhu and Kaijian Li},
keywords = {Building health resilience, Knowledge graph, Large language models, Multi-modal data, Rainstorm},
abstract = {Improving the health resilience of building (BHR) helps keep stable health status of both the building and its occupants under disasters. As BHR is an emerging concept, there is no structured knowledge graph to understand the whole process of BHR under disasters. Therefore, this study aims to build a structured BHR knowledge graph based on multi-modal data, providing sufficient structured knowledge for BHR enhancement. An automated knowledge graph construction approach is proposed to empower the ontology design and triple extraction by large language models (LLMs), and validation processes based on In-context Learning (ICL) prompts. A case study is conducted to construct the knowledge graph of BHR under rainstorms in Hong Kong. The performance of the proposed LLMs-empowered knowledge extraction is also validated based on natural language processing metrics and LLMs-based Evaluation (LLMs-Eval). BHR knowledge graph indicates the potential relations between disasters, factors, response actions, and the health status of the building and occupants, and provides insight to guide the BHR enhancement. The superiority of the proposed LLMs-empowered automated knowledge graph construction approach is proven, implying LLMs have great potential in knowledge graph construction, not only for BHR but also for other concepts that require structured knowledge for further explorations and analyses.}
}
@article{WANG2025100268,
title = {A lightweight knowledge graph-driven question answering system for field-based mineral resource survey},
journal = {Applied Computing and Geosciences},
volume = {27},
pages = {100268},
year = {2025},
issn = {2590-1974},
doi = {https://doi.org/10.1016/j.acags.2025.100268},
url = {https://www.sciencedirect.com/science/article/pii/S2590197425000503},
author = {Mingguo Wang and Chengbin Wang and Jianguo Chen and Bo Wang and Wei Wang and Xiaogang Ma and Jiangtao Ren and Zichen Li and Yicai Ye and Jiakai Zhang and Yue Wang},
keywords = {Knowledge graph-driven question answering, Question answering, Sentence transformer, Mineral resource survey, Intelligent service},
abstract = {Geoscience data associated with mineral resource surveys have become essential digital assets for governments and mining companies. The rapid increase in the volume of geoscience data makes it challenging to acquire knowledge quickly. In this study, we proposed and built a workflow that employs knowledge graph techniques, deep learning, question templates, and matching algorithms to provide a lightweight question-answering service for field-based geologists involved in mineral resource surveys. Initially, we utilized deep-learning-based geological entities and their semantic relation recognition, along with relational data mapping, to construct the mineral resource survey knowledge graph based on the ontology model. We then employed question template matching, a geological entity recognition model, and a sentence transformer to determine the optimal question template and generate a query statement for knowledge acquisition from a knowledge graph based on the Cypher language. Subsequently, we utilized a subgraph and a short abstract to express the results. The comparison with large language models and retrieval-augmented generation indicates that our solution is suitable for field-based mineral source surveys in a poor network environment with low-performance devices, data privacy concerns, and narrowly focused topics. The results also suggest that further studies on geoscience pre-trained models, an informative library of question templates, and multimodal knowledge graphs are necessary to improve the performance of the knowledge graph-driven question-answering system.}
}
@article{XUE2025114320,
title = {HSAE: Hierarchical structure augment embedding for various knowledge graph completion},
journal = {Knowledge-Based Systems},
volume = {329},
pages = {114320},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.114320},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125013607},
author = {Yifan Xue and Wanqiang Cai and Yingyao Ma and Lotfi Senhadji and Huazhong Shu and Jiasong Wu},
keywords = {Knowledge graph completion, Knowledge hypergraph, Generative language model},
abstract = {Knowledge Graph Completion (KGC) addresses the task of reasoning over existing facts to predict missing relationships, serving as a fundamental component for downstream applications including question answering systems and personalized recommendation engines. Over the years, the KGC field has evolved into specialized tasks, including static KGC, temporal KGC, hyper KGC, and few-shot KGC, each requiring specialized methodologies. Although previous methods have utilized Generative Language Models (GLMs) to theoretically support multi task compatibility, their performance remains suboptimal compared to task-specific models. This limitation stems from their inability to effectively integrate structural and textual information, leading to a fine-grained structure-text gap. To address this challenge, we propose HSAE, a novel two-stage framework that hierarchically aligns structural and textual modalities, first at the coarse-grained entity level and then at the fine-grained token level. In the first stage, Entity-Level Structure Augment, we transform structural embeddings into tree-shaped entity classifications, enriching entity representations with explicit structural information. This augmentation provides global structural guidance during beam search, ensuring that generated sequences adhere to the underlying knowledge graph topology. In the second stage, Token-Level Structure Augment, we introduce a cross-modal alignment module that dynamically fuses structural embeddings with token-level predictions. By aligning structural and textual representations at the token level, HSAE ensures that each decoding step is informed by both structural and textual coherence. Experiments on eight benchmarks demonstrate that HSAE outperforms competitive baselines across multiple KGC tasks. The data and code are released at https://anonymous.4open.science/r/HSAE-main/README.md.}
}
@article{TAN2023120721,
title = {CLRN: A reasoning network for multi-relation question answering over Cross-lingual Knowledge Graphs},
journal = {Expert Systems with Applications},
volume = {231},
pages = {120721},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120721},
url = {https://www.sciencedirect.com/science/article/pii/S095741742301223X},
author = {Yiming Tan and Xinyu Zhang and Yongrui Chen and Zafar Ali and Yuncheng Hua and Guilin Qi},
keywords = {Question answering, Cross-lingual knowledge graphs, Multi-hop reasoning, Entity alignment},
abstract = {Cross-lingual Knowledge Graphs-based Question Answering (CLKGQA) requires the question answering (QA) system to combine the knowledge graphs (KGs) in different languages to obtain answers to input questions. In previous works, the common idea is to merge Cross-lingual Knowledge Graphs (CLKGs) into a single KG through aligned entity pairs and then treat it as a traditional KG-based QA. However, as demonstrated by Tan et al. (2023), existing Entity Alignment (EA) models cannot generate highly accurate aligned entity pairs for CLKGs. Therefore, two issues need to be addressed in the CLKGQA task: (1) Remove the dependency of the QA model on the fused KG; (2) Improve the performance of the EA model in obtaining aligned entity pairs from locally isomorphic CLKGs. To solve the above two issues, this paper presents Cross-lingual Reasoning Network (CLRN), a novel multi-hop QA model that allows switching knowledge graphs at any stage of the multi-hop reasoning. Furthermore, we establish an iterative framework that combines CLRN and EA model, in which CLRN is used for extracting potential alignment triple pairs from CLKGs during the QA process. The extracted triple pairs provide pseudo-aligned entities, and the additional aligned entity pairs are used to mine missing relations between entities in CLKGs. These pseudo-aligned entity pairs and relations improve the performance of the EA model, resulting in higher accuracy in QA. Extensive experiments demonstrate the effectiveness of the proposed model, which outperforms the baseline approaches. Through iterative enhancement, the performance of the EA model has also been improved by > 1.0 % in Hit@1 and Hit@10, and the improvement is statistically significant in the confidence interval of p<0.01. Moreover, our work discusses the correlation between QA and EA from the side of QA, which has reference value for the follow-up exploration of related communities. We have open-sourced our dataset and code, which is available at the URL https://github.com/tan92hl/Cross-lingual-Reasoning-Network-for-CLKGQA.}
}
@article{ZHANG202599,
title = {Osteosarcoma knowledge graph question answering system: deep learning-based knowledge graph and large language model fusion},
journal = {Intelligent Medicine},
volume = {5},
number = {2},
pages = {99-110},
year = {2025},
issn = {2667-1026},
doi = {https://doi.org/10.1016/j.imed.2024.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S2667102625000269},
author = {Lulu Zhang and Weisong Zhao and Zhiwei Cheng and Yafei Jiang and Kai Tian and Jia Shi and Zhenyu Jiang and Yingqi Hua},
keywords = {Osteosarcoma, Knowledge graph, Large language model, Text mining},
abstract = {Objective
Osteosarcoma is a prevalent primary malignant bone tumor in children and adolescents, accounting for approximately 5 % of childhood malignancies. Because of its rarity and biological complexity, treatment breakthroughs for osteosarcoma have been limited. To advance research in this field, we aimed to construct the first comprehensive osteosarcoma knowledge graph (OSKG) using the PubMed database.
Methods
A systematic search of PubMed (2003–2023) using the keyword “osteosarcoma” yielded 25,415 abstracts. Leveraging BioBERT, pretrained on biomedical corpora and fine-tuned with osteosarcoma-specific manual annotations, we identified 16 entity types and 17 biological relationships. The extracted elements were synthesized to create the OSKG, resulting in a deep learning-based knowledge base to explore osteosarcoma pathogenesis and molecular mechanisms. We then developed a specialized question-answering system (knowledge graph question answering (KGQA)) powered by ChatGLM3. This system employs advanced natural language processing and incorporates the OSKG to ensure optimal response quality and accuracy.
Results
The pretrained BioBERT averaged > 92 % accuracy in entity and relationship training. Evaluation using 100 pairs of gold-standard quizzes showed that the final quiz system outperformed other large language models in accuracy and robustness.
Conclusion
The system is designed to provide accurate disease-related queries and answers, effectively facilitating knowledge acquisition and reasoning in medical research and clinical practice. This project offers a robust tool for osteosarcoma research and promotes the deep integration of knowledge graphs and artificial intelligence technologies in the medical field.}
}
@article{LIU2025113916,
title = {TEQA: Temporal knowledge graph enhanced question answering},
journal = {Knowledge-Based Systems},
volume = {325},
pages = {113916},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113916},
url = {https://www.sciencedirect.com/science/article/pii/S095070512500961X},
author = {Qian Liu and Siling Feng and Mengxing Huang},
keywords = {Temporal knowledge graph, Knowledge-based question answering, Temporal context modeling, Knowledge-enhanced multimodal embedding network, Dynamic subgraph reasoner},
abstract = {In the data-driven era, Temporal Knowledge Graphs (TKGs) have emerged as knowledge-driven tools for modeling temporal information. However, existing knowledge-based question answering (KBQA) systems struggle to integrate temporal context with structured knowledge, limiting their decision support capabilities for time-sensitive queries. To address this, we propose the Temporal knowledge graph Enhanced Question Answering (TEQA) framework, which enhances dynamic knowledge representation through optimized temporal context modeling and explainable subgraph reasoning. TEQA comprises three knowledge-aware modules: (1) A temporal context modeling module that extracts temporal-entity relationships from questions to build context graphs; (2) A knowledge-enhanced multimodal embedding network that aligns question semantics with TKGs via joint embedding spaces; (3) A dynamic subgraph reasoner that performs constraint-based knowledge inference using temporal-aware graph pruning. By unifying these components, TEQA generates interpretable decision pathways with confidence estimation. Experiments on CronQuestions and TimeQuestions benchmarks demonstrate TEQA’s superiority, achieving 100% Hits@1/10 on complex queries. Notably, TEQA’s knowledge grounding mechanism provides traceable evidence for predictions, making it viable for time-critical decision support systems in healthcare and finance.}
}
@article{ZHU2025128994,
title = {RTA: A reinforcement learning-based temporal knowledge graph question answering model},
journal = {Neurocomputing},
volume = {617},
pages = {128994},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128994},
url = {https://www.sciencedirect.com/science/article/pii/S092523122401765X},
author = {Yu Zhu and Tinghuai Ma and Shengjie Sun and Huan Rong and Yexin Bian and Kai Huang},
keywords = {Temporal knowledge graph reasoning, Path reasoning, Reinforcement learning},
abstract = {Temporal Knowledge Graph Question Answering (TKGQA) is crucial research, focusing on finding an entity or a timestamp to answer temporal questions in the corresponding temporal knowledge graph. Currently, the main challenge in the temporal KGQA task is answering complex temporal questions, often necessitating complex multi-hop temporal reasoning in the TKG. In this paper, we propose a method for the TKGQA task called Reinforcement learning Temporal knowledge graph question Answering (RTA). First, in the question understanding stage, our model extracts context information to select topic entities of the given question, which can effectively deal with scenarios involving multiple entities in complex temporal questions. Furthermore, reasoning complexity escalates significantly with complex temporal questions, as varying timestamps alter the relations between entities. Therefore, we introduce reinforcement learning into the reasoning process. In the policy network, a dynamic path-matching module is specifically included to aggregate the features of relational paths to effectively capture the dynamic changes of the relations between entities on the reasoning paths. At the same time, the weights are calculated to obtain the degree of attention of each candidate action. Then the score of each candidate action is obtained through a weighted summation mechanism which helps the agent learn the optimal path reasoning policy for effective exploration. Finally, we evaluate our method on the CRONQUESTIONS dataset and validate its superiority over all baseline methods. Specifically, our approach proves effective in handling complex temporal questions.}
}
@article{MA2025109117,
title = {f-KGQA: A fuzzy question answering system for knowledge graphs},
journal = {Fuzzy Sets and Systems},
volume = {498},
pages = {109117},
year = {2025},
issn = {0165-0114},
doi = {https://doi.org/10.1016/j.fss.2024.109117},
url = {https://www.sciencedirect.com/science/article/pii/S016501142400263X},
author = {Ruizhe Ma and Yunxing Liu and Zongmin Ma},
keywords = {Knowledge graphs, Question answering, Fuzzy term, Fuzzy question answering system},
abstract = {The wide usage of large-scale knowledge graphs (KGs) motivates the development of user-friendly interfaces so that knowledge graphs become more readily accessible to a larger population. Natural language-based question answering (QA) systems are widely investigated and developed in the context of KGs, which can provide users with a natural means to retrieve the information they need from KGs without expecting them to know the query language. It is very common that natural language contains linguistic terms (fuzzy terms), and fuzzy (flexible) query has been widely investigated in the context of databases. This paper contributes a QA system with fuzzy terms over KGs called f-KGQA. f-KGQA can deal with different types of questions, including simple questions, complex questions, and questions with fuzzy terms. More importantly, users are provided with a channel to flexibly define their fuzzy terms based on their understanding. Our experimental results demonstrate the effectiveness and applicability of f-KGQA in handling questions with fuzzy terms.}
}
@article{CHEN2024104804,
title = {Enhancing emergency decision-making with knowledge graphs and large language models},
journal = {International Journal of Disaster Risk Reduction},
volume = {113},
pages = {104804},
year = {2024},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2024.104804},
url = {https://www.sciencedirect.com/science/article/pii/S2212420924005661},
author = {Minze Chen and Zhenxiang Tao and Weitong Tang and Tingxin Qin and Rui Yang and Chunli Zhu},
keywords = {Emergency decision support, Large language model, Knowledge graph, Decision support system},
abstract = {Emergency management urgently requires comprehensive knowledge while having a high possibility to go beyond individuals’ cognitive scope. Therefore, artificial intelligence(AI) supported decision-making under that circumstance is of vital importance. Recent emerging large language models (LLM) provide a new direction for enhancing targeted machine intelligence. However, the utilization of LLM directly would inevitably introduce unreliable output for its inherent issue of hallucination and poor reasoning skills. In this work, we develop a system called Enhancing Emergency decision-making with Knowledge Graph and LLM (E-KELL), which provides evidence-based decision-making in various emergency stages. The study constructs a structured emergency knowledge graph and guides LLMs to reason over it via a prompt chain. In real-world evaluations, E-KELL demonstrates significant improvement over baseline models in various emergency response scenarios, as rated by emergency commanders and firefighters. This work introduces a novel approach to applying LLMs to enhance emergency decision-making.}
}
@article{SONG2025647,
title = {Construction of Question Answering System Based on English Pre-Trained Language Model Enhanced by Knowledge Graph},
journal = {Procedia Computer Science},
volume = {261},
pages = {647-655},
year = {2025},
note = {The 5th International Conference on Machine Learning and Big Data Analytics for IoT Security and Privacy (SPIoT2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.04.256},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925013584},
author = {Pei Song},
keywords = {NLP technology, knowledge graph enhancement, BERT model, English pre-training, language model},
abstract = {With the continuous development of technology, question answering systems based on pre trained language models have become an important component of intelligent applications. However, traditional question-answering systems often face challenges in terms of understanding accuracy and insufficient knowledge coverage when dealing with open-domain questions. To this end, this paper uses a method for building a question-answering system based on a knowledge graph-enhanced BERT pre-trained language model. First, this paper trains the basic model based on a large-scale BERT pre-trained language model. Then, this paper adopts knowledge graph technology to introduce structured knowledge information into the model by integrating domain-specific knowledge bases. Finally, in order to effectively integrate knowledge graph information, this paper uses graph neural network (GNN) to model graph data, and combines the self-attention mechanism in the BERT model to optimize the weighted fusion process of knowledge graph information. Experimental results show that the BERT model enhanced with knowledge graph performs well in multiple question-answering tasks. On the simple question of the first experiment, the average accuracy of the enhanced model increased from 84.6% of the standard BERT model to 89.8%, and the F1 score increased from 0.86 to 0.91. In complex reasoning tasks, the BERT+knowledge graph model demonstrates stronger reasoning ability and higher knowledge coverage. In the experimental conclusion, the introduction of the knowledge graph significantly improves the model’s reasoning ability and knowledge coverage, especially in professional field problems and multi-step reasoning tasks, the enhanced model shows stronger capabilities. This research provides a new construction method for question-answering systems, demonstrates the great potential of knowledge graphs in natural language processing, and has broad application prospects.}
}
@article{ZHOU2025108009,
title = {Integrating machine learning and a large language model to construct a domain knowledge graph for reducing the risk of fall-from-height accidents},
journal = {Accident Analysis & Prevention},
volume = {215},
pages = {108009},
year = {2025},
issn = {0001-4575},
doi = {https://doi.org/10.1016/j.aap.2025.108009},
url = {https://www.sciencedirect.com/science/article/pii/S0001457525000958},
author = {Zhipeng Zhou and Xinhui Yu and Joseph Jonathan Magoua and Jianqiang Cui and Haiying Luan and Dong Lin},
keywords = {Fall-from-height, Domain knowledge graph, Knowledge extraction, Natural language process, Named entity recognition, Large language model},
abstract = {Fall-from-height (FFH) accidents remain a major source of workplace injuries and fatalities. Fall protection systems (FPS) are critical for preventing falls in the work-at-height (WAH) environment. However, challenges in designing and selecting effective FPS persist across various industries, and existing tools often lack practical references. This study aims to develop an FFH-specific knowledge graph (FFH-KG) to support FPS design. By structuring accident data, the FFH-KG provides empirical insights to help designers improve FPS frameworks, aiding safety planning and decision-making. It serves as a decision support system for FPS designers and safety professionals, guiding the selection and design of appropriate protection solutions for diverse WAH scenarios. The FFH-KG was constructed using a hybrid natural language processing approach, combining manual extraction, entity recognition, text segmentation, and rule-based relation extraction. It was grounded in a schema layer (i.e., ontology) established by experts. A text-mining approach, integrating machine learning with a large language model, facilitated the categorization of fall types, refinement of WAH scenarios, and identification of fall causes, enhancing the content and applicability of knowledge graph. A total of 2,200 entities and 4,820 relationships were created based on fall protection equipment standard documents and fall-from-height accident investigation reports, forming a foundation for developing countermeasures. The retrieval performance of FFH-KG was validated through three case studies. This research has also made significant progress in intelligent safety engineering and management across industries.}
}
@article{GONG2025110396,
title = {The application progress and research trends of knowledge graphs and large language models in agriculture},
journal = {Computers and Electronics in Agriculture},
volume = {235},
pages = {110396},
year = {2025},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2025.110396},
url = {https://www.sciencedirect.com/science/article/pii/S0168169925005022},
author = {Ruizi Gong and Xinxing Li},
keywords = {Knowledge graphs, Large language models, Agricultural knowledge intelligent services},
abstract = {Enhancing agricultural productivity remains a crucial issue in agriculture, wherein agricultural knowledge intelligent services can improve the scientific and intelligent level of agricultural production through technologies such as knowledge coupling and inference decision-making. The application of knowledge graphs (KGs) in agriculture effectively supports the structured representation of agricultural data and helps manage agricultural data. On the other hand, the recent emergence of large language models (LLMs), with their strong language understanding and generation capabilities, provides new methods and ideas for knowledge services in agriculture. Though KGs and LLMs each have different strengths and limitations, their integration is believed to complement each other and has great potential to promote the development of agricultural knowledge intelligent services. In this paper, we review the current status of the application of KGs and LLMs in agriculture. We also discuss their complementary fusion as well as the prospect of their agricultural application, hoping to provide some references for the future development of agricultural knowledge intelligent services.}
}
@article{CUI2023745,
title = {Incorporating anticipation embedding into reinforcement learning framework for multi-hop knowledge graph question answering},
journal = {Information Sciences},
volume = {619},
pages = {745-761},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.11.042},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522013317},
author = {Hai Cui and Tao Peng and Feng Xiao and Jiayu Han and Ridong Han and Lu Liu},
keywords = {Knowledge graph, Question answering, Reinforcement learning, Sparse rewards, Weak supervision},
abstract = {Multi-hop knowledge graph question answering (KGQA) aims to pinpoint answer entities by reasoning across multiple triples in knowledge graphs (KGs). To enhance model interpretability, reinforcement learning (RL) based methods are proposed. However, RL-based solutions suffer from two major issues. The first is aimless exploration due to the limited observation information. The second is delayed and sparse rewards caused by weak supervision. To bridge the gap, we propose an interpretable model based on RL for multi-hop KGQA with weak supervision, which we refer to as Anticipation Reasoning Network (ARN) that incorporates KG embeddings as anticipation information into RL framework to avoid aimlessly traversing in KG. ARN achieves the goal via three steps. (1) KG embedding models represent the entities and relations as low-dimensional vectors. (2) We propose a knowledge embedding based question answering (KEQA) framework to measure the plausibility of the “triple” <topic entity, question, answer entity> in embedding space. (3) A policy-guided agent is designed to perform effective path reasoning by selecting the most promising action at each step. Moreover, we utilize reward shaping strategy and actor-critic optimization to alleviate the problem of low-quality rewards. Extensive experiments well demonstrate the effectiveness of our method footnoteFor reproducibility, we publicly release the code at https://github.com/iDylanCui/ARN.}
}
@article{XIONG2021106954,
title = {Knowledge Graph Question Answering with semantic oriented fusion model},
journal = {Knowledge-Based Systems},
volume = {221},
pages = {106954},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.106954},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121002173},
author = {Haobo Xiong and Shuting Wang and Mingrong Tang and Liping Wang and Xuemin Lin},
keywords = {Knowledge graph question answering, Topic entity recognition, Path generation, Model fusion},
abstract = {Knowledge Graph Question Answering (KGQA) is a major branch of question answering tasks, which can answer fact questions effectively by using the reasonable characteristics of the knowledge graph. Currently, lots of related works combined with a variety of deep learning models are presented for the KGQA task. However, there are still some challenges, such as topic entity recognition under ambiguity expression, semantic level representation of natural language, efficient construction of searching space for answers, etc. In this paper, we propose a comprehensive approach for complex question answering over KG. Firstly, during the stage of topic entity recognition, a deep transition model is constructed to extract topic entities, and an efficient entity linking strategy is presented, which combines character matching and entity disambiguation model. Secondly, for candidate path ranking, a dynamic candidate path generation algorithm is proposed to efficiently create the candidate answer set. And four dedicated similarity calculation models are designed to handle the intricate condition of complex questions with long sequence and diversity expression. Moreover, a fusion policy is proposed to make decision for the final correct answer. We evaluate our approach on CKBQA, a Chinese knowledge base question answering dataset, from CCKS2019 competition. Experimental results demonstrate that the improvements in each process are effective and our approach achieves better performance than the best team in CCKS2019 competition.}
}
@article{SHEN2025112315,
title = {A self-supervised method for learning path-augmented knowledge graph embedding},
journal = {Engineering Applications of Artificial Intelligence},
volume = {162},
pages = {112315},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.112315},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625023231},
author = {Tong Shen and Fu Zhang and Jingwei Cheng},
keywords = {Knowledge graph, Knowledge graph embedding, Self-supervised learning},
abstract = {Knowledge graphs (KGs) consist of factual triples that describe relations between entities in the real world. Knowledge graph embedding (KGE) aims to map entities and relations into constantly low-dimensional vectors, which is important for lots of downstream tasks (e.g., KG completion and information retrieval). Current KGE methods primarily rely on explicit structural patterns, neglecting latent contextual semantics behind those structures and resulting in sub-optimal performance. While some methods incorporate additional data (e.g., textual descriptions), such dependencies limit applicability due to additional data requirements. Furthermore, most KGE models suffer from limited supervision with sparse labeled triples, restricting their capacity to learn comprehensive semantic features. Inspired by the simple but effective self-supervised language model word2vec, one interesting question is: Can KGE be performed as a simple self-supervised language model? To achieve this, we innovatively propose a self-supervised KGE framework that learns entity and relation embeddings by adapting word2vec’s skip-gram objective to path sequences extracted from KGs. Our framework employs separate embedding spaces for entities and relations with an entity-relation mapping mechanism for effective interaction between the two embedding spaces. Further, to enhance the training efficiency, we introduce a markov chain-based negative sampling strategy, which generates semantically meaningful negative samples by preserving the structural contexts along KG paths. Our framework, which is the first attempt to follow the context-based self-supervised idea of language models to conduct KGE tasks, addresses the constraints of label-dependent supervised KGE techniques and obviates the requirement for external information, while simultaneously enabling effective extraction of the implicit contextual semantics inherent in triple structures. Experiments on two widely-used KGE datasets show state-of-the-art performance, demonstrating our framework’s ability to learn semantically rich representations solely from graph structure.}
}
@article{ZHENG2025110479,
title = {Thoughtful and cautious reasoning: A fine-tuned knowledge graph-based multi-hop question answering framework},
journal = {Engineering Applications of Artificial Intelligence},
volume = {150},
pages = {110479},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.110479},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625004798},
author = {Yinghao Zheng and Ling Lu and Yang Hu and Yinong Chen and Aijuan Wang},
keywords = {Knowledge graph, Multi-hop question answering, Graph neural network, Local semantic information},
abstract = {The aim of Knowledge Graph Question Answering (KGQA) is to find the answer entity by utilizing the Knowledge Graph (KG). Despite remarkable successes in recent years, the existing multi-hop KGQA research still faces numerous challenges. First, a multi-hop question often contains multiple entities and their relationships, and the semantic information is complex. The current methods extract the semantics of the question through an encoder that cannot completely extract the complex and rich semantic information in the multi-hop questions. Second, current question answering models use the coarse information filtering mechanism in the process of reasoning, which lead to the loss of effective information and introduce additional noise. To address these issues, we propose a Thoughtful and Cautious Reasoning framework for Knowledge Graph Question Answering (TCR-KGQA). We design a new question encoder that can extract and fully fuse the local semantic information of the question at different levels, focusing on the unique local features of the multi-hop question text. Based on the advantages of Gated Recurrent Unit (GRU) for information filtering, we propose a loop instruction update framework based on residual-GRU to effectively capture key information in the reasoning process. Extensive experiments on three broad benchmark datasets demonstrate the effectiveness of our model on KGQA tasks, and it also yields excellent results in the case of incomplete knowledge graphs with missing question–answer pairs.}
}
@article{GU2025101094,
title = {Construction of Q&A methods based on knowledge graphs and large language models-improving the accuracy of landscape pest and disease Q&A},
journal = {Smart Agricultural Technology},
volume = {12},
pages = {101094},
year = {2025},
issn = {2772-3755},
doi = {https://doi.org/10.1016/j.atech.2025.101094},
url = {https://www.sciencedirect.com/science/article/pii/S2772375525003272},
author = {Zhixin Gu and Ting Long and Shuairan Wang and Xiaowei Shang and Weizheng Shen and Xiaoli Wei and Kaihong Xu},
keywords = {Knowledge graph, Large language model, ERNIE, Garden diseases and pests},
abstract = {With the development of urban landscaping, the problem of garden diseases and pests is becoming increasingly severe. Large language models have garnered significant attention for their ability to understand user intent and provide answers. The introduction of knowledge graphs has provided a high quality knowledge base for large language models. This study combines knowledge graphs (KGs), large language models (LLMs) and other technologies to design an intelligent question-answering (Q&A) model for garden pests and diseases. The main work carried out is as follows: Build a knowledge graph for garden diseases and pests by collecting high-quality data through web crawling and literature analysis. Identify key entities and relationships to construct a conceptual pattern layer. Applying the ERNIE-BiLSTM-CRF model to extract knowledge from unstructured data. Through experiments, it is found that the accuracy, recall and F1 value of the knowledge extraction model proposed in this study are all more than 92%, superior to other models. Propose a Q&A method that integrates the garden pest and disease KG with the ERNIE-Bot-turbo model. By vectorizing the knowledge and using similarity matching, the most relevant data is retrieved, combined with the question to form prompts, and input into the language model to generate natural language answers. Experiments comparing our method with ERNIE-Bot-turbo and ChatGLM-6B showed that our approach performs well on simple, moderate, and complex problems, avoiding misleading answers for irrelevant questions. It outperforms both models in accuracy, achieving a 90% accuracy rate for simple questions.}
}
@article{MA2024128490,
title = {A review of graph neural networks and pretrained language models for knowledge graph reasoning},
journal = {Neurocomputing},
volume = {609},
pages = {128490},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128490},
url = {https://www.sciencedirect.com/science/article/pii/S092523122401261X},
author = {Jiangtao Ma and Bo Liu and Kunlin Li and Chenliang Li and Fan Zhang and Xiangyang Luo and Yaqiong Qiao},
keywords = {Knowledge graph reasoning, Graph neural networks, Pretrained language models, Logic rules},
abstract = {Knowledge Graph (KG) stores human knowledge facts in an intuitive graphical structure but faces challenges such as incomplete construction or inability to handle new knowledge. Knowledge Graph Reasoning (KGR) can make KGs more accurate, complete, and trustworthy to support various artificial intelligence applications better. Currently, the popular KGR methods are based on graph neural networks (GNNs). Recent studies have shown that hybrid logic rules and synergized pre-trained language models (PLMs) can enhance the GNN-based KGR methods. These methods mainly focus on data sparsity, insufficient knowledge evolution patterns, multi-modal fusion, and few-shot reasoning. Although many studies have been conducted, there are still few review papers that comprehensively summarize and explore KGR methods related to GNNs, logic rules, and PLMs. Therefore, this paper provides a comprehensive review of GNNs and PLMs for KGR based on a large number of high-quality papers. To present a clear overview of KGR, we propose a general framework. Specifically, we first introduce the KG preparation. Then we provide an overview of KGR methods, in which we categorize KGR methods into GNNs-based, logic rules-enhanced, and pre-trained language models-enhanced KGR methods. Furthermore, we also compare and analyze the GNN-based KGR methods in two scenarios. Moreover, we also present the application of KGR in different fields. Finally, we discuss the current challenges and future research directions for KGR.}
}
@article{SUI2022108943,
title = {Causality-aware Enhanced Model for Multi-hop Question Answering over Knowledge Graphs},
journal = {Knowledge-Based Systems},
volume = {250},
pages = {108943},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.108943},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122004567},
author = {Yuan Sui and Shanshan Feng and Huaxiang Zhang and Jian Cao and Liang Hu and Nengjun Zhu},
keywords = {Knowledge graph-based question answering, Causal representation learning, Constraint pairwise-based clustering, Knowledge graph embedding, Confounding bias},
abstract = {To improve the performance of knowledge graph-based question answering system (KGQA), several approaches have been developed to construct a semantic parser based on entity linking, relation identification and logical/numerical structure identification. However, existing methods arrive at answers only by maximizing the data likelihood only on the sparse or imbalanced explicit relations, ignoring the potentially large number of latent relations. It makes KGQA suffer from a high level of spurious entity relations and missing link challenge. In this paper, we propose a causal filter (CF) model for KGQA (CF-KGQA), which performs causal interference on the relation representation space to reduce the spurious relation representation in a data-driven manner, i.e., the goal of this work is to comprehensively discover disentangled latent factors to alleviate the spurious correlation problem in KGQA. The model comprises a causal pairwise aggregator (AP) and a disentangled latent factor aggregator (AC). The former filters out most spurious entity relations inconsistent to their dense groups’ neighborhood, and generates a causal pairwise matrix among all the candidate relations. The latter learns the latent relation representation via an encoder–decoder on the causal pairwise matrix. It disconnects the latent factor and the causal confounder beneath the knowledge embedding space by causal intervention. To prove the effectiveness and efficiency of the proposed approach, we test CF-KGQA and other state-of-the-art methods on four public real-world datasets. The experiments indicate that our approach outperforms the recent methods and is also less sensitive to the spurious correlation problem, thus demonstrating the robustness of CF-KGQA.}
}
@article{LIU2025111625,
title = {Knowledge graph reasoning: Mainstream methods, applications and prospects},
journal = {Engineering Applications of Artificial Intelligence},
volume = {159},
pages = {111625},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.111625},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625016276},
author = {Han Liu and Shaojie Yang and Guokai Shi and Zongcheng Miao},
keywords = {Knowledge graph, Knowledge graph reasoning, Deep learning, Reinforcement learning},
abstract = {Knowledge graphs have emerged as a leading paradigm for knowledge representation due to their robust capacity to mine, organize, and manage massive datasets. Their versatility has led to extensive research and application in various advanced fields. Knowledge graph reasoning plays a critical role in this context by reasoning new facts from existing ones, thereby completing and refining the knowledge base. In this paper, we provide a comprehensive review of knowledge graph reasoning by researching its definition, foundational concepts, and methodological approaches. We systematically categorize reasoning methods into five groups: reasoning based on ontology, reasoning based on rules, neural rule reasoning based on distributed representations, neural rule reasoning based on deep learning, and hybrid reasoning. Furthermore, we analyze the applications of knowledge graph reasoning, discuss their current limitations, and outline promising directions for future research to enhance both performance and scalability in this rapidly evolving field.}
}
@article{ZHU2025100315,
title = {Text-augmented long-term relation dependency learning for knowledge graph representation},
journal = {High-Confidence Computing},
volume = {5},
number = {4},
pages = {100315},
year = {2025},
issn = {2667-2952},
doi = {https://doi.org/10.1016/j.hcc.2025.100315},
url = {https://www.sciencedirect.com/science/article/pii/S2667295225000194},
author = {Quntao Zhu and Mengfan Li and Yuanjun Gao and Yao Wan and Xuanhua Shi and Hai Jin},
keywords = {Knowledge graph representation, Graph attention network, Pre-trained language model, Attention-based recurrent network, Masked autoencoder, Contrastive learning},
abstract = {Knowledge graph (KG) representation learning aims to map entities and relations into a low-dimensional representation space, showing significant potential in many tasks. Existing approaches follow two categories: (1) Graph-based approaches encode KG elements into vectors using structural score functions. (2) Text-based approaches embed text descriptions of entities and relations via pre-trained language models (PLMs), further fine-tuned with triples. We argue that graph-based approaches struggle with sparse data, while text-based approaches face challenges with complex relations. To address these limitations, we propose a unified Text-Augmented Attention-based Recurrent Network, bridging the gap between graph and natural language. Specifically, we employ a graph attention network based on local influence weights to model local structural information and utilize a PLM based prompt learning to learn textual information, enhanced by a mask-reconstruction strategy based on global influence weights and textual contrastive learning for improved robustness and generalizability. Besides, to effectively model multi-hop relations, we propose a novel semantic-depth guided path extraction algorithm and integrate cross-attention layers into recurrent neural networks to facilitate learning the long-term relation dependency and offer an adaptive attention mechanism for varied-length information. Extensive experiments demonstrate that our model exhibits superiority over existing models across KG completion and question-answering tasks.}
}
@article{XIONG2025101001,
title = {SheepDoctor: A knowledge graph enhanced large language model for sheep disease diagnosis},
journal = {Smart Agricultural Technology},
volume = {11},
pages = {101001},
year = {2025},
issn = {2772-3755},
doi = {https://doi.org/10.1016/j.atech.2025.101001},
url = {https://www.sciencedirect.com/science/article/pii/S2772375525002345},
author = {Jiayi Xiong and Yong Zhou and Fang Tian and Fuchuan Ni and Liang Zhao},
keywords = {Intelligent diagnose, Knowledge graph, Large language model, Retrieval-augmented generation},
abstract = {Traditional methods for diagnosing sheep diseases often fail to provide timely and accurate results, particularly in emergency situations, and public awareness of these diseases remains limited. This hinders effective prevention and control efforts. To address these challenges, a domain-specific large language model, SheepDoctor, was developed for sheep disease diagnosis. A comprehensive question-and-answer (Q&A) dataset was constructed using prompt techniques, resulting in 5987 samples covering 207 sheep diseases. This dataset included detailed symptom descriptions, treatments, and related information, which were also structured into a knowledge graph. The pre-trained LLaMA2–13B-Chinese model was fine-tuned using Low-Rank Adaptation (LoRA), with integration of the knowledge graph to enhance its diagnostic capabilities and response accuracy. Evaluation using BLEU, ROUGE, and BERTScore metrics demonstrates that SheepDoctor outperforms general-purpose models such as GPT-4o and Kimi on sheep-related diagnostic tasks. The proposed method exhibits strong domain expertise and holds significant potential for improving the efficiency and reliability of sheep disease diagnosis.}
}
@article{BAKHSHI2020113205,
title = {Data-driven construction of SPARQL queries by approximate question graph alignment in question answering over knowledge graphs},
journal = {Expert Systems with Applications},
volume = {146},
pages = {113205},
year = {2020},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.113205},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420300312},
author = {Mahdi Bakhshi and Mohammadali Nematbakhsh and Mehran Mohsenzadeh and Amir Masoud Rahmani},
keywords = {Knowledge graph, Answering natural language questions, Disambiguation of interpretations, Pairwise graph alignment},
abstract = {As increasingly more semantic real-world data is stored in knowledge graphs, providing intuitive and effective query methods for end-users is a fundamental and challenging task. Since there is a gap between the plain natural language question (NLQ) and structured data, most RDF question/answering (Q/A) systems construct SPARQL queries from NLQs and obtain precise answers from knowledge graphs. A major challenge is how to disambiguate the mapping of phrases and relations in a question to the dataset items, especially in complex questions. In this paper, we propose a novel data-driven graph similarity framework for RDF Q/A to extract the query graph patterns directly from the knowledge graph instead of constructing them with semantically mapped items. An uncertain question graph is presented to model the interpretations of an NLQ, based on which our problem is reduced to a graph alignment problem. In formulating the alignment, both the lexical and structural similarity of graphs are considered, hence, the target RDF subgraph is used as a query graph pattern to construct the final query. We create a pruned entity graph dynamically based on the complexity of an input question to reduce the search space on the knowledge graph. Moreover, to reduce the calculating cost of the graph similarity, we compute the similarity scores only for same-distance graph elements and equip the process with an edge association-aware surface form extraction method. Empirical studies over real datasets indicate that our proposed approach is flexible and effective as it outperforms state-of-the-art methods significantly.}
}
@article{SUNIL2024102665,
title = {The gene function prediction challenge: Large language models and knowledge graphs to the rescue},
journal = {Current Opinion in Plant Biology},
volume = {82},
pages = {102665},
year = {2024},
issn = {1369-5266},
doi = {https://doi.org/10.1016/j.pbi.2024.102665},
url = {https://www.sciencedirect.com/science/article/pii/S1369526624001560},
author = {Rohan Shawn Sunil and Shan Chun Lim and Manoj Itharajula and Marek Mutwil},
abstract = {Elucidating gene function is one of the ultimate goals of plant science. Despite this, only ∼15 % of all genes in the model plant Arabidopsis thaliana have comprehensively experimentally verified functions. While bioinformatical gene function prediction approaches can guide biologists in their experimental efforts, neither the performance of the gene function prediction methods nor the number of experimental characterization of genes has increased dramatically in recent years. In this review, we will discuss the status quo and the trajectory of gene function elucidation and outline the recent advances in gene function prediction approaches. We will then discuss how recent artificial intelligence advances in large language models and knowledge graphs can be leveraged to accelerate gene function predictions and keep us updated with scientific literature.}
}
@article{LI20221595,
title = {Research on the construction of smart care question answering system based on knowledge graph},
journal = {Procedia Computer Science},
volume = {214},
pages = {1595-1602},
year = {2022},
note = {9th International Conference on Information Technology and Quantitative Management},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.11.348},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922020609},
author = {Aihua Li and Qinyan Wei and Che Han and Xinzhu Xing},
keywords = {Smart care, Knowledge graph, Question answering system},
abstract = {With the deepening of aging in China, the demand of intelligent care system for the elderly is increasing day by day. The rapid development of information technology and artificial intelligence technology have gradually promoted the transformation of traditional care services from artificial to intelligent. Based on knowledge graph technology, this study built a knowledge graph model for elderly chronic disease smart care. The specific process includes ontology design, knowledge extraction, knowledge graph construction and question answering system design. The uniqueness of the knowledge graph in this paper is that it uses specific information about the elderly and data from professional care books. It is well guaranteed in practicality and reliability.With the help of the knowledge map constructed in this paper, primary care staff can effectively query high-quality and customized chronic disease care knowledge.}
}
@article{YANG2025122135,
title = {OD-Mind: An ocean drilling expert knowledge query system driven by knowledge graph},
journal = {Ocean Engineering},
volume = {339},
pages = {122135},
year = {2025},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2025.122135},
url = {https://www.sciencedirect.com/science/article/pii/S0029801825018190},
author = {Yulong Yang and Weihua Cao and Yupeng Li and Runzhou Chang and Gangcheng Yang and Chao Gan and Min Wu},
keywords = {Ocean drilling, Knowledge extraction, Knowledge graph, Large language model, Retrieval-augmented generation},
abstract = {The textual publications generated by the Scientific Ocean Drilling compile a substantial body of knowledge. However, these publications present significant analytical challenges due to their technical language and rapidly expanding volume. To address these challenges, this study presents Ocean Drilling Mind (OD-Mind), a comprehensive system designed to automatically extract key knowledge from large-scale publications and support question answering in the Scientific Ocean Drilling domain. The contributions are threefold: 1) A knowledge extraction method is adapted to capture the knowledge related to complex multi-word compound named entities in ocean drilling; 2) A knowledge graph refinement method is proposed to consolidate the weak-connected knowledge into a high-density ocean drilling knowledge graph; 3) A query system is designed to answer specialized queries in the domain of ocean drilling based on the knowledge graph. In the case study, OD-Mind demonstrated its potential by constructing a comprehensive knowledge graph on 116 publications. Preliminary results showed that the system performed better than general-purpose large language models, particularly in answering specialized ocean drilling domain queries with improved speed and accuracy.}
}
@article{YANG20251118,
title = {SeedLLM·Rice: A large language model integrated with rice biological knowledge graph},
journal = {Molecular Plant},
volume = {18},
number = {7},
pages = {1118-1129},
year = {2025},
issn = {1674-2052},
doi = {https://doi.org/10.1016/j.molp.2025.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S1674205225001728},
author = {Fan Yang and Huanjun Kong and Jie Ying and Zihong Chen and Tao Luo and Wanli Jiang and Zhonghang Yuan and Zhefan Wang and Zhaona Ma and Shikuan Wang and Wanfeng Ma and Xiaoyi Wang and Xiaoying Li and Zhengyin Hu and Xiaodong Ma and Minguo Liu and Xiqing Wang and Fan Chen and Nanqing Dong},
keywords = {LLM, large language model, knowledge graph, multiomics data integration, GPT, DeepSeek},
abstract = {Rice biology research involves complex decision-making, requiring researchers to navigate a rapidly expanding body of knowledge encompassing extensive literature and multiomics data. The exponential increase in biological data and scientific publications presents significant challenges for efficiently extracting meaningful insights. Although large language models (LLMs) show promise for knowledge retrieval, their application to rice-specific research has been limited by the absence of specialized models and the challenge of synthesizing multimodal data integral to the field. Moreover, the lack of standardized evaluation frameworks for domain-specific tasks impedes the effective assessment of model performance. To address these challenges, we introduce SeedLLM·Rice (SeedLLM), a 7-billion-parameter model trained on 1.4 million rice-related publications, representing nearly 98.24% of global rice research output. Additionally, we present a novel human-centric evaluation framework designed to assess LLM performance in rice biology tasks. Initial evaluations demonstrate that SeedLLM outperforms general-purpose models, including OpenAI GPT-4o1 and DeepSeek-R1, achieving win rates of 57% to 88% on rice-specific tasks. Furthermore, SeedLLM is integrated with the Rice Biological Knowledge Graph (RBKG), which consolidates genome annotations for Nipponbare and large-scale synthesis of transcriptomic and proteomic information from over 1800 studies. This integration enhances the ability of SeedLLM to address complex research questions requiring the fusion of textual and multiomics data. To facilitate global collaboration, we provide free access to SeedLLM and the RBKG via an interactive web portal (https://seedllm.org.cn/). SeedLLM represents a transformative tool for rice biology research, enabling unprecedented discoveries in crop improvement and climate adaptation through advanced reasoning and comprehensive data integration.}
}
@article{VARSHNEY2025110929,
title = {Med-KGMA: A novel AI-driven medical support system leveraging knowledge graphs and medical advisors},
journal = {Computers in Biology and Medicine},
volume = {197},
pages = {110929},
year = {2025},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2025.110929},
url = {https://www.sciencedirect.com/science/article/pii/S0010482525012818},
author = {Sona Varshney and Bhawna Jain and Prerna Singh and Drishti Rani and Saumya Mehra},
keywords = {Medical decision support system, Healthcare, Machine learning, Knowledge graph},
abstract = {Healthcare systems worldwide face a growing burden, struggling to provide timely diagnosis and personalized care due to resource constraints. The increasing demand for medical expertise often results in delayed interventions, making automated decision support crucial. However, existing medical question-answering (QnA) systems struggle with hallucinations, limited contextual understanding, and difficulty handling complex queries, which often results in unreliable responses. To address these challenges, this study proposes Med-KGMA, an artificial intelligence-driven medical QnA system that leverages the proposed SequentialRotatE, a knowledge graph embedding model, along with the Mixture-of-Medical-Advisors (MoMA) framework to enhance diagnostic accuracy and treatment recommendations. Unlike conventional methods, SequentialRotatE effectively captures contextual relationships between medical entities, improving the system’s reasoning capabilities. Additionally, the MoMA framework, which dynamically routes queries to specialized advisors based on complexity and relevance, ensures more precise recommendations. Experimental results demonstrate that Med-KGMA achieves 91.32% accuracy, outperforming state-of-the-art baselines. This approach advances medical knowledge representation through optimized query processing, tailored knowledge graphs, and intelligent advisor selection, providing an efficient, scalable solution. By addressing initial symptom analysis and reducing healthcare load, Med-KGMA empowers users with reliable medical insights, bridging the gap between patients and timely care.}
}
@article{SHIMIZU2025100862,
title = {Accelerating knowledge graph and ontology engineering with large language models},
journal = {Journal of Web Semantics},
volume = {85},
pages = {100862},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2025.100862},
url = {https://www.sciencedirect.com/science/article/pii/S1570826825000022},
author = {Cogan Shimizu and Pascal Hitzler},
keywords = {Knowledge graph engineering, Ontology engineering, Large language models, Modular ontologies, Ontology modeling, Ontology population, Ontology alignment, Entity disambiguation},
abstract = {Large Language Models bear the promise of significant acceleration of key Knowledge Graph and Ontology Engineering tasks, including ontology modeling, extension, modification, population, alignment, as well as entity disambiguation. We lay out LLM-based Knowledge Graph and Ontology Engineering as a new and coming area of research, and argue that modular approaches to ontologies will be of central importance.}
}
@article{PRAMANIK2024100833,
title = {Uniqorn: Unified question answering over RDF knowledge graphs and natural language text},
journal = {Journal of Web Semantics},
volume = {83},
pages = {100833},
year = {2024},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100833},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000192},
author = {Soumajit Pramanik and Jesujoba Alabi and Rishiraj Saha Roy and Gerhard Weikum},
keywords = {Complex question answering, Heterogeneous sources, Group Steiner Trees},
abstract = {Question answering over RDF data like knowledge graphs has been greatly advanced, with a number of good systems providing crisp answers for natural language questions or telegraphic queries. Some of these systems incorporate textual sources as additional evidence for the answering process, but cannot compute answers that are present in text alone. Conversely, the IR and NLP communities have addressed QA over text, but such systems barely utilize semantic data and knowledge. This paper presents a method for complex questions that can seamlessly operate over a mixture of RDF datasets and text corpora, or individual sources, in a unified framework. Our method, called Uniqorn, builds a context graph on-the-fly, by retrieving question-relevant evidences from the RDF data and/or a text corpus, using fine-tuned BERT models. The resulting graph typically contains all question-relevant evidences but also a lot of noise. Uniqorn copes with this input by a graph algorithm for Group Steiner Trees, that identifies the best answer candidates in the context graph. Experimental results on several benchmarks of complex questions with multiple entities and relations, show that Uniqorn significantly outperforms state-of-the-art methods for heterogeneous QA – in a full training mode, as well as in zero-shot settings. The graph-based methodology provides user-interpretable evidence for the complete answering process.}
}
@article{WANG2024102820,
title = {Knowledge graph of agricultural engineering technology based on large language model},
journal = {Displays},
volume = {85},
pages = {102820},
year = {2024},
issn = {0141-9382},
doi = {https://doi.org/10.1016/j.displa.2024.102820},
url = {https://www.sciencedirect.com/science/article/pii/S0141938224001847},
author = {Haowen Wang and Ruixue Zhao},
keywords = {LLM, Knowledge graph},
abstract = {Agriculture is an industry that has evolved alongside human evolution and has faithfully fulfilled its core mission of food supply. With the reduction of rural labor, the progress of artificial intelligence and the development of Internet of Things technology, it is hoped that the efficiency and productivity of the agricultural industry can be improved. Recently, with the development of information and intelligent technology, agricultural production and management have been significantly enhanced. However, there is still a considerable challenge in effectively integrating the vast amount of fragmented information for downstream applications. An agricultural knowledge graph (AGKG) will serve as the foundation for achieving these goals. Knowledge graphs can be general or domain-specific, and are the basis for many applications, such as search engines, online question-and-answer services, and knowledge inference. Therefore, there are many knowledge graphs, including Wikidata and DBpedia, for accessing structured knowledge. Although some general knowledge graphs contain some entities and relationships related to agriculture, there are no domain-specific knowledge graphs specifically for agricultural applications. Therefore, this paper proposes an agricultural knowledge graph (AGKG) for automatically integrating large amounts of agricultural data from the Internet. By applying natural language processing and deep learning technologies, AGKG can automatically identify agricultural entities from unstructured text and connect them to form a knowledge graph. In addition, we have described the typical scenarios of our AGKG and validated it through real-world applications such as agricultural entity retrieval and agricultural question-answering.}
}
@article{BADENESOLMEDO2023104382,
title = {Lessons learned to enable question answering on knowledge graphs extracted from scientific publications: A case study on the coronavirus literature},
journal = {Journal of Biomedical Informatics},
volume = {142},
pages = {104382},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2023.104382},
url = {https://www.sciencedirect.com/science/article/pii/S153204642300103X},
author = {Carlos Badenes-Olmedo and Oscar Corcho},
keywords = {Question-answering, Knowledge graphs, Ontology, Evidences},
abstract = {The article presents a workflow to create a question-answering system whose knowledge base combines knowledge graphs and scientific publications on coronaviruses. It is based on the experience gained in modeling evidence from research articles to provide answers to questions in natural language. The work contains best practices for acquiring scientific publications, tuning language models to identify and normalize relevant entities, creating representational models based on probabilistic topics, and formalizing an ontology that describes the associations between domain concepts supported by the scientific literature. All the resources generated in the domain of coronavirus are available openly as part of the Drugs4COVID initiative, and can be (re)-used independently or as a whole. They can be exploited by scientific communities conducting research related to SARS-CoV-2/COVID-19 and also by therapeutic communities, laboratories, etc., wishing to find and understand relationships between symptoms, drugs, active ingredients and their documentary evidence.}
}
@article{BI2024127044,
title = {Relphormer: Relational Graph Transformer for Knowledge Graph Representations},
journal = {Neurocomputing},
volume = {566},
pages = {127044},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.127044},
url = {https://www.sciencedirect.com/science/article/pii/S0925231223011670},
author = {Zhen Bi and Siyuan Cheng and Jing Chen and Xiaozhuan Liang and Feiyu Xiong and Ningyu Zhang},
keywords = {Knowledge graph, Knowledge graph representation, Transformer},
abstract = {Transformers have achieved remarkable performance in widespread fields, including natural language processing, computer vision and graph mining. However, vanilla Transformer architectures have not yielded promising improvements in the Knowledge Graph (KG) representations, where the translational distance paradigm dominates this area. Note that vanilla Transformer architectures struggle to capture the intrinsically heterogeneous structural and semantic information of knowledge graphs. To this end, we propose a new variant of Transformer for knowledge graph representations dubbed Relphormer. Specifically, we introduce Triple2Seq which can dynamically sample contextualized sub-graph sequences as the input to alleviate the heterogeneity issue. We propose a novel structure-enhanced self-attention mechanism to encode the relational information and keep the semantic information within entities and relations. Moreover, we utilize masked knowledge modeling for general knowledge graph representation learning, which can be applied to various KG-based tasks including knowledge graph completion, question answering, and recommendation. Experimental results on six datasets show that Relphormer can obtain better performance compared with baselines.22Code is available in https://github.com/zjunlp/Relphormer.}
}
@incollection{DHAMENIYA2025227,
title = {12 - Knowledge graph-based question answering (KG-QA) using natural language processing},
editor = {Rajesh Kumar Dhanaraj and M. Nalini and Malathy Sathyamoorthy and Manar Mohaisen},
booktitle = {Knowledge Graph-Based Methods for Automated Driving},
publisher = {Elsevier},
pages = {227-249},
year = {2025},
isbn = {978-0-443-30040-0},
doi = {https://doi.org/10.1016/B978-0-443-30040-0.00012-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780443300400000126},
author = {Priyanshu Dhameniya and Rashmi Yadav},
keywords = {Natural language processing, Knowledge graphsand question answering, Information retrieval, Unstructured, Entity-resolution models, Semantic-parsing, Semantic annotations, Recurrent neural networks, Entity recognition, RDF-based knowledge graphs, SPARQL},
abstract = {This chapter begins by providing an overview of knowledge graphs, their construction, and their role in representing relationships between entities. It then delves into the challenges posed by question-answering tasks and how knowledge graphs can enhance traditional QA systems by offering a structured and context-aware information base. The core of this chapter focuses on the integration of NLP techniques in KG-QA systems. It discusses the processing of natural language queries, entity recognition, and relationship extraction to bridge the gap between user queries and the structured information encapsulated in knowledge graphs. Special attention is given to semantic parsing and understanding, enabling the extraction of nuanced information from unstructured text.}
}
@article{LI20252790,
title = {DNMKG: A method for constructing domain of nonferrous metals knowledge graph based on multiple corpus},
journal = {Transactions of Nonferrous Metals Society of China},
volume = {35},
number = {8},
pages = {2790-2802},
year = {2025},
issn = {1003-6326},
doi = {https://doi.org/10.1016/S1003-6326(25)66848-8},
url = {https://www.sciencedirect.com/science/article/pii/S1003632625668488},
author = {Hai-liang LI and Hai-dong WANG},
keywords = {knowledge graph, nonferrous metals, thesaurus, word vector model, multi-source heterogeneous corpus},
abstract = {To address the underutilization of Chinese research materials in nonferrous metals, a method for constructing a domain of nonferrous metals knowledge graph (DNMKG) was established. Starting from a domain thesaurus, entities and relationships were mapped as resource description framework (RDF) triples to form the graph’s framework. Properties and related entities were extracted from open knowledge bases, enriching the graph. A large-scale, multi- source heterogeneous corpus of over 1×109 words was compiled from recent literature to further expand DNMKG. Using the knowledge graph as prior knowledge, natural language processing techniques were applied to the corpus, generating word vectors. A novel entity evaluation algorithm was used to identify and extract real domain entities, which were added to DNMKG. A prototype system was developed to visualize the knowledge graph and support human−computer interaction. Results demonstrate that DNMKG can enhance knowledge discovery and improve research efficiency in the nonferrous metals field.}
}
@article{ZHANG2025104527,
title = {More intelligent knowledge graph: A large language model-driven method for knowledge representation in geospatial digital twins},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {139},
pages = {104527},
year = {2025},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2025.104527},
url = {https://www.sciencedirect.com/science/article/pii/S1569843225001748},
author = {Jinbin Zhang and Jun Zhu and Zhihao Guo and Jianlin Wu and Yukun Guo and Jianbo Lai and Weilian Li},
keywords = {Geospatial digital twins, Knowledge graph, Large language model, Knowledge extraction, Dynamic update},
abstract = {Knowledge graphs (KGs) can describe the nature and relationships of geographic entities and are an essential knowledge base for realizing geospatial digital twins (GDTs). However, existing KGs make it challenging to describe dynamic geographic entities under geographic spatiotemporal evolution accurately. Furthermore, they are constrained by the professional backgrounds of their users, which hinders updates and communication. Therefore, the research constructed an “event-object-state” three-domain associated GDT-oriented KG, proposed a large language model (LLM) −driven KG dynamic update algorithm, and established a KG intelligent Q&A method integrating LLM. We developed a prototype system and selected an earthquake disaster as a typical geographic event for experimental analysis. The results showed that the proposed method can reflect the space, time, state, evolution process, and interrelationships of geographic entities in a more comprehensive way, support users to build, update, and query KGs using natural language, with an updating efficiency of less than 1 min, and an updating quality comparable to that of manual updating by experts. Compared with the traditional KGs, our method can represent virtual geographic entities and has significant advantages in intelligence and automation, which effectively breaks down professional barriers and supports the construction of GDTs with the need for rapid updating of knowledge.}
}
@article{OYELADE2025127455,
title = {SMAR + NIE IdeaGen: A knowledge graph based node importance estimation with analogical reasoning on large language model for idea generation},
journal = {Expert Systems with Applications},
volume = {279},
pages = {127455},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127455},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425010772},
author = {Olaide N. Oyelade and Hui Wang and Karen Rafferty},
keywords = {Knowledge graphs (KGs), Large language model (LLMs), Idea generation, Novelty, Analogical reasoning, Node importance estimation, Natural language processing (NLP), Isomorphic subgraphs},
abstract = {Idea generation describes a creative process involving reasoning over some knowledge to derive new information. Traditional approaches such as mind-map and brainstorming are limited and often fail due to lack of quality ideas and ineffective methods. The reasoning capability of large language models (LLMs) have been investigated for ideation tasks and have reported interesting performance. However, these models suffer from limited logical reasoning capability which hinders the use of structural and factual real-world knowledge in discovery of latent insight and predict possible outcome when applied to ideation. In addition, the possibility of LLMs regurgitating knowledge learnt from datasets might adversely impact the degree of novel ideas the models can generate. In this paper, a two-stage logical reasoning approach is applied to initiate the search for candidate idea pathways based on the knowledge graphs (KGs) to address the problem of reasoning, domain-specificity and novelty. The divergence stage this reasoning explores utilizes a new node importance estimation (NIE) technique over KGs to discover latent connections supporting idea generation. In the convergence stage of this reasoning, subgraph matching using analogical reasoning (SMAR) is applied to find matching patterns to describe a new idea. The use of SMAR + NIE and KGs helps to achieve an improvement in reasoning over KGs before transferring such reasoning to LLMs for translation of idea into natural language. To evaluate the degree of novelty of ideas generated, a relevance-to-novelty scoring metrics is proposed based on multiple premise entailment (MPE). We combined this metric with other popular metrics to evaluate the performance of SMAR + NIE on benchmark datasets, and as well on the quality of ideas generated. Findings from the study showed that this approach demonstrates competitive performance with mainstream LLMs in idea generation tasks.}
}
@article{BASHIR2025102406,
title = {Logic-infused knowledge graph QA: Enhancing large language models for specialized domains through Prolog integration},
journal = {Data & Knowledge Engineering},
volume = {157},
pages = {102406},
year = {2025},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2025.102406},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X25000011},
author = {Aneesa Bashir and Rong Peng and Yongchang Ding},
keywords = {Knowledge Graph Question Answering (KGQA), Large language models (LLMs), Logical programming (Prolog), Named entity recognition (NER), Multi-hop reasoning, Transformer, BERT},
abstract = {Efficiently answering questions over complex, domain-specific knowledge graphs remain a substantial challenge, as large language models (LLMs) often lack the logical reasoning abilities and particular knowledge required for such tasks. This paper presents a novel framework integrating LLMs with logical programming languages like Prolog for Logic-Infused Knowledge Graph Question Answering (KGQA) in specialized domains. The proposed methodology uses a transformer-based encoder–decoder architecture. An encoder reads the question, and a named entity recognition (NER) module connects entities to the knowledge graph. The extracted entities are fed into a grammar-guided decoder, producing a logical form (Prolog query) that captures the semantic constraints and relationships. The Prolog query is executed over the knowledge graph to perform symbolic reasoning and retrieve relevant answer entities. Comprehensive experiments on the MetaQA benchmark dataset demonstrate the superior performance of this logic-infused method in accurately identifying correct answer entities from the knowledge graph. Even when trained on a limited subset of annotated data, it outperforms state-of-the-art baselines, achieving 89.60 % and F1-scores of up to 89.61 %, showcasing its effectiveness in enhancing large language models with symbolic reasoning capabilities for specialized question-answering tasks. The seamless integration of LLMs and logical programming enables the proposed framework to reason effectively over complex, domain-specific knowledge graphs, overcoming a key limitation of existing KGQA systems. In specialized domains, the interpretability provided by representing questions such as Prologue queries is a valuable asset.}
}
@article{YANG2025102868,
title = {GS-KGC: A generative subgraph-based framework for knowledge graph completion with large language models},
journal = {Information Fusion},
volume = {117},
pages = {102868},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102868},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524006468},
author = {Rui Yang and Jiahao Zhu and Jianping Man and Hongze Liu and Li Fang and Yi Zhou},
keywords = {Knowledge graph, Knowledge graph completion, Large language models, Question answer},
abstract = {Knowledge graph completion (KGC) focuses on identifying missing triples in a knowledge graph (KG) , which is crucial for many downstream applications. Given the rapid development of large language models (LLMs), some LLM-based methods are proposed for KGC task. However, most of them focus on prompt engineering while overlooking the fact that finer-grained subgraph information can aid LLMs in generating more accurate answers. In this paper, we propose a novel completion framework called Generative Subgraph-based KGC (GS-KGC), which utilizes subgraph information as contextual reasoning and employs a QA approach to achieve the KGC task. This framework primarily includes a subgraph partitioning algorithm designed to generate negatives and neighbors. Specifically, negatives can encourage LLMs to generate a broader range of answers, while neighbors provide additional contextual insights for LLM reasoning. Furthermore, we found that GS-KGC can discover potential triples within the KGs and new facts beyond the KGs. Experiments conducted on four common KGC datasets highlight the advantages of the proposed GS-KGC, e.g., it shows a 5.6% increase in Hits@3 compared to the LLM-based model CP-KGC on the FB15k-237N, and a 9.3% increase over the LLM-based model TECHS on the ICEWS14.}
}
@article{XU2025348,
title = {Design of Question-Answering and Reasoning System Combining Large Language Models and Knowledge Graphs},
journal = {Procedia Computer Science},
volume = {262},
pages = {348-357},
year = {2025},
note = {The 5th International Conference on Multi-modal Information Analytics (MMIA)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.05.062},
url = {https://www.sciencedirect.com/science/article/pii/S187705092501909X},
author = {Yanru Xu},
keywords = {LLM, LLaMa, KG, question-answering, reasoning, system design, command fine-tuning},
abstract = {Problem reasoning uses natural language processing technology to analyze natural language questions input by users, and finally generate accurate and suitable answers. At present, the performance of Q&A model has been significantly improved, and the deep semantic understanding of text can be obtained through accurate knowledge reasoning. Based on LLaMa model, this paper systematically studies Transformer architecture, normalization technology, rotary position coding and packet query attention, creatively studies the combination of LLM and KG, and constructs a mathematical model of instruction fine-tuning algorithm. It not only understands the surface meaning of text, but also uses background knowledge such as entity relationships in KG to execute instructions more accurately and intelligently. The LLM studied in this paper is combined with KG to build a question-and-answer reasoning system, which can overcome the "illusion" problem of large models. KG significantly improves the accuracy by constrains the generated results of large models with structured knowledge.}
}
@article{XIE2024119115,
title = {Intelligent maritime question-answering and recommendation system based on maritime vessel activity knowledge graph},
journal = {Ocean Engineering},
volume = {312},
pages = {119115},
year = {2024},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2024.119115},
url = {https://www.sciencedirect.com/science/article/pii/S0029801824024533},
author = {Cunxiang Xie and Zhaogen Zhong and Limin Zhang},
keywords = {Maritime traffic management, Knowledge graph, Question answering system, Recommendation system, Graph neural networks},
abstract = {Traditional maritime traffic management typically relies on positioning data for data mining without incorporating other multi-source data to analyze the maritime vessel activity, which cannot conduct comprehensive maritime knowledge mining. Thus, this study integrates multi-source data, such as trajectory, maritime accident text, and geographic data, to create a maritime vessel activity knowledge graph. On this basis, a question-answering model is developed based on a bidirectional question-answering attention graph neural network, and a personalized recommendation model is developed based on an attention-enhanced joint knowledge propagation and a user preference graph neural network. The former assists users in extracting valuable information from the maritime vessel activity knowledge graph, while the latter predicts the users' potential interests and automatically recommends vessel entities based on their historical query information. Experimental results show that the proposed question-answering model improved the F1-score by 2.31%–10.09% compared to state-of-the-art baseline models on the MVA question-answering dataset. Similarly, the proposed personalized recommendation model improved the click-through rate prediction accuracy by 2.46%–7.05% compared to state-of-the-art baseline models on the MVA personalized recommendation dataset.}
}
@article{ZHAO2025198,
title = {LLM-powered database migration: A framework for knowledge graph system evolution},
journal = {Alexandria Engineering Journal},
volume = {130},
pages = {198-207},
year = {2025},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2025.08.014},
url = {https://www.sciencedirect.com/science/article/pii/S1110016825008841},
author = {Shangqing Zhao and Qifan Zhang and Man Lan},
keywords = {Knowledge graph, Question answering, Large Language Model, In-context learning, Database migration},
abstract = {Database migration, particularly the translation of query languages, remains a significant barrier to modernizing data infrastructure. This challenge is especially acute as organizations adopt advanced knowledge graph (KG) technologies to support demanding applications in domains like smart cities and eHealth. This paper introduces a novel, LLM-powered framework for automated query translation, demonstrated through KG migration from RDF/SPARQL to LPG/Cypher. Our method leverages in-context learning with strategic exemplar selection and iterative refinement, achieving up to 89.6% translation accuracy and a 97.3% executable rate without requiring large parallel corpora or manual rule creation. Experiments on both the KQA Pro and enterprise-scale DBLP-QuAD datasets validate the approach’s effectiveness and scalability. With migration costs under $1.50 for thousands of queries, our framework offers an economically viable solution that reduces migration costs and accelerates the adoption of modern database technologies for next-generation applications.}
}
@article{LIANG2025883,
title = {A survey of large language model-augmented knowledge graphs for advanced complex product design},
journal = {Journal of Manufacturing Systems},
volume = {80},
pages = {883-901},
year = {2025},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2025.04.016},
url = {https://www.sciencedirect.com/science/article/pii/S0278612525001050},
author = {Xinxin Liang and Zuoxu Wang and Jihong Liu},
keywords = {Knowledge Graph, Large Language Model, Complex Product Design, Intelligent Manufacturing},
abstract = {In the Human-AI collaboration rapid development era, the design and development of knowledge-intensive complex products should enable the design process with the help of advanced AI technology, and enhance the reasoning and application of design domain knowledge. Extracting and reusing domain knowledge would greatly facilitate the success of complex product design. Knowledge graphs (KGs), a powerful knowledge representation and storage technology, have been widely deployed in advanced complex product design because of their advantages in mining and applying large-scale, complex, and specialized domain knowledge. But merely KG and its related reasoning approaches still cannot fully support the ill-defined product design tasks. In the future complex product design, Human-AI collaboration will become a mainstream prevention trend. Large language models (LLMs) have outstanding performance in natural language understanding and generation, showing promising potential to collaborate with KGs in complex product design and development. Till 2024/03/04, only a few studies have systematically reviewed the current status of LLM and KG applications in the engineering field, not to mention a further detailed review in the complex product design field, leaving many issues not covered or fully examined. To fill this gap, 100 articles published in the last 4 years (i.e., 2021–2024) were screened and surveyed. This study provides a statistical analysis of the screened research articles, mainstream techniques of LLM & KG, and LLM & KG applications were analyze. To understand how KG and LLM could support complex product design, a framework of LLMs-augmented KG in advanced complex product design was proposed, which contains data layer, KG & LLM collaboration layer, enhanced design capability layer, and design task layer. Furthermore, we also discussed the challenges and future research directions of the LLM-KG-collaborated complex product design paradigm. As an exploratory review paper, it provides insightful ideas for implementing more specialized domain KGs in product design field.}
}
@article{CHOI2025114033,
title = {INQUIRER: Harnessing internal knowledge graphs for video question generation},
journal = {Knowledge-Based Systems},
volume = {326},
pages = {114033},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.114033},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125010780},
author = {Woo Suk Choi and Youwon Jang and Minsu Lee and Byoung-Tak Zhang},
keywords = {Video question generation, Internal knowledge graph, Video understanding, Video question answering},
abstract = {Video question generation (VideoQG) aims to generate questions about video content to facilitate and assess video understanding. Existing works which primarily condition question generation on answer-related information such as the answer itself or its attributes. However, these methods are primarily designed as data augmentation techniques and thus struggle to produce semantically diverse questions. We propose INQUIRER, a novel VideoQG framework that leverages internal knowledge graphs derived from video information to generate meaningful and diverse questions. INQUIRER consists of three key steps: KCon, which constructs an internal knowledge graph to represent a video similarly to human knowledge structures, QGen which generates questions based on the video and the knowledge graph, and QCur which refines the generated questions to ensure quality and contextual relevance. Each generated question is accompanied by a correct answer and plausible distractors to support downstream QA evaluation. To comprehensively evaluate the generated QAs and utility of INQUIRER from multiple perspectives, we utilize widely used video question answering (VideoQA) benchmarks, including DramaQA, TVQA, How2QA, and STAR. Experiment results demonstrate that INQUIRER not only generates high-quality question–answer pairs but also significantly enhances VideoQA performance, validating its effectiveness as a robust framework for video question generation.}
}
@article{DENG2025104871,
title = {MedKA: A knowledge graph-augmented approach to improve factuality in medical Large Language Models},
journal = {Journal of Biomedical Informatics},
volume = {168},
pages = {104871},
year = {2025},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2025.104871},
url = {https://www.sciencedirect.com/science/article/pii/S1532046425001005},
author = {Yiyan Deng and Shen Zhao and Yongming Miao and Junjie Zhu and Jin Li},
keywords = {Knowledge graph, Large Language Models, Factuality evaluation, Natural Language Processing},
abstract = {Large language models (LLMs) have demonstrated remarkable potential in medical applications. However, they still face critical challenges such as hallucinations, knowledge inconsistency, and insufficient integration of domain-specific medical expertise. To address these limitations, we introduce MedKA, a novel knowledge graph-augmented approach for fine-tuning and evaluating medical LLMs. Our approach systematically transforms structured knowledge from a medical knowledge graph into a high-quality QA corpus, cMKGQA, by clustering multiple fields around clinically meaningful scenarios (e.g., diagnosis, treatment planning). This grouping strategy enables comprehensive and use-case-specific data construction and supports one-stage training of the LLM, ensuring better alignment with structured medical knowledge. This transformation process ensures the comprehensive integration of domain-specific knowledge while maintaining factual consistency. To evaluate the factuality of LLM-generated response, we further propose the Knowledge Graph-based Auxiliary Evaluation Metrics (KG-AEMs)—a novel benchmarking framework that compares LLM outputs with fine-grained, attribute-level ground truth from knowledge graph. Experimental results demonstrate that MedKA achieves state-of-the-art performance, significantly outperforming existing models, including LLaMA-3.1-8B-Chinese-Chat, HuatuoGPT2-7B, and Apollo2-7B. On the cMKGQA dataset, MedKA achieves 44.63 BLEU-1 and 17.62 BLEU-4 scores, with particularly strong performance in areas such as medication recommendations and diagnostic tests as measured by KG-AEMs. Our approach highlights the potential of integrating knowledge graphs into LLM fine-tuning to improve the accuracy and reliability of medical AI systems. It advances factual accuracy in medical dialogue systems and provides a comprehensive framework for evaluating the integration of medical knowledge into LLMs. This work is publicly available on Github: https://github.com/Yai017/MedKA.}
}
@article{CHEN2025103124,
title = {Knowledge Graphs for Multi-modal Learning: Survey and Perspective},
journal = {Information Fusion},
volume = {121},
pages = {103124},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2025.103124},
url = {https://www.sciencedirect.com/science/article/pii/S1566253525001976},
author = {Zhuo Chen and Yichi Zhang and Yin Fang and Yuxia Geng and Lingbing Guo and Jiaoyan Chen and Xiaoze Liu and Jeff Z. Pan and Ningyu Zhang and Huajun Chen and Wen Zhang},
keywords = {Knowledge Graphs, Multi-modal Learning, Knowledge-based Information Fusion, Visual Question Answering, Large Language Model, Literature review},
abstract = {Integrated with multi-modal learning, knowledge graphs (KGs) as structured knowledge repositories, can enhance AI for processing and understanding complex, real-world data. This paper provides a comprehensive survey of cutting-edge research on KG-aware multi-modal learning. For these core areas, we provide task definitions, evaluation benchmarks, and comprehensive insights into key breakthroughs, offering detailed explanations critical for conducting related research. Furthermore, we also discuss current challenges, highlighting emerging trends and future research directions. The repository for this paper can be found at https://github.com/zjukg/KG-MM-Survey.}
}
@article{CUI2025127430,
title = {KLLMs4Rec: Knowledge graph-enhanced LLMs sentiment extraction for personalized recommendations},
journal = {Expert Systems with Applications},
volume = {282},
pages = {127430},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127430},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425010528},
author = {Yachao Cui and Kaiguang Wang and Hongli Yu and Xiaoxu Guo and Han Cao},
keywords = {Large Language Models, Text sentiment analysis, Knowledge graphs, Personalized recommendation},
abstract = {Recommendation algorithms typically leverage auxiliary information such as user reviews and knowledge graphs to enhance algorithm performance, thereby alleviating data sparsity and cold start issues. Recently, researchers have increasingly employed large language models, which boast powerful natural language understanding capabilities, to further improve recommendation systems. However, these models often suffer from hallucination problems. Moreover, integrating heterogeneous information, such as reviews and knowledge graphs, can introduce new noise, potentially impairing recommendation performance. Knowledge graphs, as tightly organized structured knowledge bases, can assist in addressing the hallucination problem and heterogeneous information fusion problem of LLMs. To effectively address the aforementioned issues, we propose the Knowledge Graph-Enhanced Large Language Model Sentiment Extraction for the Personalized Recommendation Model (KLLMs4Rec). It aims to solve the LLMs hallucination problem and the noise problem caused by the fusion of heterogeneous information in recommender systems, and provide users with more accurate, diverse and novel personalized recommendations. To address the hallucination problem when extracting user sentiments from reviews with LLMs, we designed a knowledge graph-enhanced prompt template. It is worth noting that this scheme also solves the noise issue of heterogeneous information fusion. Additionally, to further expand user preferences extracted from reviews, this paper proposes a new hierarchical sentiment attention graph convolutional network, which utilizes three sentiment weight schemes to propagate user personalized preferences on the knowledge graph. Extensive experiments on the Movielens-20 m, Amazon-book, and Yelp datasets demonstrate that our model surpasses current leading methods while effectively addressing the hallucination problem of LLMs and the noise problem of heterogeneous information fusion.}
}
@article{SINGH2020100594,
title = {No one is perfect: Analysing the performance of question answering components over the DBpedia knowledge graph},
journal = {Journal of Web Semantics},
volume = {65},
pages = {100594},
year = {2020},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2020.100594},
url = {https://www.sciencedirect.com/science/article/pii/S1570826820300342},
author = {Kuldeep Singh and Ioanna Lytra and Arun Sethupat Radhakrishna and Saeedeh Shekarpour and Maria-Esther Vidal and Jens Lehmann},
keywords = {Question answering, Knowledge graph, Entity linking, Relation extraction, Relation linking, Experiment and analysis},
abstract = {Question answering (QA) over knowledge graphs has gained significant momentum over the past five years due to the increasing availability of large knowledge graphs and the rising importance of Question Answering for user interaction. Existing QA systems have been extensively evaluated as black boxes and their performance has been characterised in terms of average results over all the questions of benchmarking datasets (i.e. macro evaluation). Albeit informative, macro evaluation studies do not provide evidence about QA components’ strengths and concrete weaknesses. Therefore, the objective of this article is to analyse and micro evaluate available QA components in order to comprehend which question characteristics impact on their performance. For this, we measure at question level and with respect to different question features the accuracy of 29 components reused in QA frameworks for the DBpedia knowledge graph using state-of-the-art benchmarks. As a result, we provide a perspective on collective failure cases, study the similarities and synergies among QA components for different component types and suggest their characteristics preventing them from effectively solving the corresponding QA tasks. Finally, based on these extensive results, we present conclusive insights for future challenges and research directions in the field of Question Answering over knowledge graphs.}
}
@article{YANG2026104280,
title = {Enhancing knowledge graph interactions: A comprehensive Text-to-Cypher pipeline with large language models},
journal = {Information Processing & Management},
volume = {63},
number = {1},
pages = {104280},
year = {2026},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2025.104280},
url = {https://www.sciencedirect.com/science/article/pii/S0306457325002213},
author = {Chao Yang and Changyi Li and Xiaodu Hu and Hao Yu and Jinzhi Lu},
keywords = {Knowledge graph, Large language model, Preference learning, Synthetic data generation, Text-to-Cypher},
abstract = {Knowledge Graphs (KGs) store structured information but typically require specialized query languages, such as Cypher for Neo4j, creating accessibility challenges for users unfamiliar with graph syntax. Large Language Models (LLMs) offer a solution by translating natural language into Cypher queries. However, existing models—including large-scale LLMs (e.g., ChatGPT) and smaller open-source models (e.g., Llama-7B, 8B) often struggle with accurately generating domain-specific queries due to inadequate alignment with KG schemas and limited domain-specific training data. To address these limitations, we propose a training pipeline tailored specifically for domain-aligned Cypher query generation, emphasizing usability for smaller-scale models. Our method integrates template-based synthetic data generation for diverse, high-quality training samples. We combine supervised fine-tuning with preference learning to enhance domain knowledge and Cypher syntax understanding. Additionally, our approach includes a context-aware retrieval mechanism that dynamically incorporates relevant schema elements at inference, improving alignment with domain-specific knowledge. We evaluated our method on the Hetionet biomedical KG using a benchmark dataset of 240 queries across three complexity levels. Our results show that our context-aware prompting achieves a substantial improvement, increasing component matching accuracy by 23.6% for ChatGPT-4o over the vanilla prompt baseline. When applying our full training pipeline to smaller-scale models, CodeLlama-13B* achieves an execution accuracy of 69.2%, nearly matching ChatGPT-4o’s 72.1%. Importantly, our approach significantly narrows the performance gap, enabling smaller models to effectively manage complex, domain-specific tasks previously dominated by larger models. These findings demonstrate that our method is scalable, computationally efficient, and robust for practical Cypher query generation applications.}
}
@article{HAN2025131027,
title = {SCR: A completion-then-reasoning framework for multi-hop question answering over incomplete knowledge graph},
journal = {Neurocomputing},
volume = {651},
pages = {131027},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.131027},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225016996},
author = {Ridong Han and Jia Liu and Haijia Bi and Tao Peng and Lu Liu},
keywords = {Knowledge graph question answering, Multi-hop, Reinforcement learning, Subgraph completion, Semantic rewards},
abstract = {Reinforcement learning has become the widely adopted technique for multi-hop knowledge graph question answering task thanks to its excellent interpretability in reasoning process. However, it is severely affected by the incompleteness of knowledge graphs and the sparse rewards caused by weak supervision. In this paper, we propose a completion-then-reasoning framework, called SCR, to address these two issues. For the incompleteness of knowledge graphs, we first extract a subgraph from the given knowledge graph for a given question, and use the knowledge graph embedding model to predict and complete missing triples, followed by reinforcement learning for answer reasoning on the completed subgraph. To alleviate the sparse rewards in reinforcement learning, we introduce a semantic reward based on the semantic similarity between original question and full relational path, enabling the model to receive partial rewards for partially correct paths instead of a zero reward. Detailed experiments on PQ, PQL, MetaQA, and WebQSP datasets demonstrate that our SCR model effectively improves the performance of multi-hop knowledge graph question answering task. Particularly, under sparse KG setting, SCR model outperforms baselines by a large margin, highlighting the effectiveness of completion-then-reasoning framework in mitigating the incompleteness of knowledge graphs.}
}
@article{GAJO2025104195,
title = {Natural vs programming language in LLM knowledge graph construction},
journal = {Information Processing & Management},
volume = {62},
number = {5},
pages = {104195},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2025.104195},
url = {https://www.sciencedirect.com/science/article/pii/S0306457325001360},
author = {Paolo Gajo and Alberto Barrón-Cedeño},
keywords = {Knowledge graph construction, large language models, Code language models, Information extraction},
abstract = {Research on knowledge graph construction (KGC) has recently shown great promise also thanks to the adoption of large language models (LLM) for the automatic extraction of structured information from raw text. However, most works rely on commercial, closed-source LLMs, hindering reproducibility and accessibility. We explore KGC with smaller, open-weight LLMs and investigate whether they can be used to improve upon the results obtained by systems leveraging bigger, closed-source models. Specifically, we focus on CodeKGC, a prompting framework based on GPT-3.5. We choose a variety of models either pre-trained primarily on natural language or on code and fine-tune them on three datasets used for information extraction. We fine-tune with prompts formatted either in natural language or as Python-like scripts. In addition, we optionally train the models with prompts including chain-of-thought sections. After fine-tuning, the choice of coding vs natural language prompts has a limited impact on performance, while chain-of-thought training mostly leads to a performance decrease. Moreover, we show that a LLM can be outperformed by much smaller versions on this task, after undergoing the same amount of training. We find that in general the selected lightweight LLMs outperform the much larger CodeKGC by as much as 15–20 absolute F1 points after fine-tuning. The results show that state-of-the-art KGC systems can be developed using smaller and open-weight models, enhancing research transparency, lowering compute requirements, and decreasing third-party API reliance. Code: https://github.com/TinfFoil/natcode-llm-kgc}
}
@article{MORIOKA2025762,
title = {Automatic construction of asset knowledge graph with large language model},
journal = {Procedia CIRP},
volume = {135},
pages = {762-767},
year = {2025},
note = {32nd CIRP Conference on Life Cycle Engineering (LCE2025)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2025.01.097},
url = {https://www.sciencedirect.com/science/article/pii/S2212827125004469},
author = {Tomoaki Morioka and Toshiaki Kono and Takehisa Nishida},
keywords = {Maintenance, Knowledge graph construction, Large language model, Reliability centered maintenance},
abstract = {Lifecycle engineering is a critical concept for fostering environmentally sustainable practices within the manufacturing sector. An essential component of lifecycle management for achieving sustainability is reliability-centered maintenance, which enhances various key performance indicators (KPIs), including machine availability and environmental impact. Effective and reliable maintenance necessitates expert knowledge of the equipment. For instance, determining which components and failure modes should be addressed through condition-based maintenance requires insights derived from failure mode and effect analysis (FMEA). However, constructing expert knowledge is labor-intensive, and ensuring its quality presents significant challenges. This study proposes a method for the automated construction of expert knowledge related to maintenance, along with a corresponding tool designed to reduce construction costs and enhance knowledge quality. The proposed method leverages a large language model (LLM) to automatically generate asset knowledge graphs based on FMEA. By combining general knowledge about equipment derived from the pre-trained LLM with specialized information extracted from technical documents, the tool creates knowledge structures such as component trees and failure modes. Subject matter experts can then iteratively refine and validate this knowledge. To evaluate the proposed approach, we assessed the accuracy and coverage of the knowledge generated by the tool in two case studies involving specific types of equipment. The results indicated that the LLM-generated output contained 4.98 times more items than those manually created, with precision ranging from 0.490 to 0.662 and recall ranging from 0.481 to 0.810.}
}
@article{YANG2025,
title = {Large Language Model–Driven Knowledge Graph Construction in Sepsis Care Using Multicenter Clinical Databases: Development and Usability Study},
journal = {Journal of Medical Internet Research},
volume = {27},
year = {2025},
issn = {1438-8871},
doi = {https://doi.org/10.2196/65537},
url = {https://www.sciencedirect.com/science/article/pii/S1438887125004534},
author = {Hao Yang and Jiaxi Li and Chi Zhang and Alejandro Pazos Sierra and Bairong Shen},
keywords = {sepsis, knowledge graph, large language models, prompt engineering, real-world, GPT-4.0},
abstract = {Background
Sepsis is a complex, life-threatening condition characterized by significant heterogeneity and vast amounts of unstructured data, posing substantial challenges for traditional knowledge graph construction methods. The integration of large language models (LLMs) with real-world data offers a promising avenue to address these challenges and enhance the understanding and management of sepsis.
Objective
This study aims to develop a comprehensive sepsis knowledge graph by leveraging the capabilities of LLMs, specifically GPT-4.0, in conjunction with multicenter clinical databases. The goal is to improve the understanding of sepsis and provide actionable insights for clinical decision-making. We also established a multicenter sepsis database (MSD) to support this effort.
Methods
We collected clinical guidelines, public databases, and real-world data from 3 major hospitals in Western China, encompassing 10,544 patients diagnosed with sepsis. Using GPT-4.0, we used advanced prompt engineering techniques for entity recognition and relationship extraction, which facilitated the construction of a nuanced sepsis knowledge graph.
Results
We established a sepsis database with 10,544 patient records, including 8497 from West China Hospital, 690 from Shangjin Hospital, and 357 from Tianfu Hospital. The sepsis knowledge graph comprises of 1894 nodes and 2021 distinct relationships, encompassing nine entity concepts (diseases, symptoms, biomarkers, imaging examinations, etc) and 8 semantic relationships (complications, recommended medications, laboratory tests, etc). GPT-4.0 demonstrated superior performance in entity recognition and relationship extraction, achieving an F1-score of 76.76 on a sepsis-specific dataset, outperforming other models such as Qwen2 (43.77) and Llama3 (48.39). On the CMeEE dataset, GPT-4.0 achieved an F1-score of 65.42 using few-shot learning, surpassing traditional models such as BERT-CRF (62.11) and Med-BERT (60.66). Building upon this, we compiled a comprehensive sepsis knowledge graph, comprising of 1894 nodes and 2021 distinct relationships.
Conclusions
This study represents a pioneering effort in using LLMs, particularly GPT-4.0, to construct a comprehensive sepsis knowledge graph. The innovative application of prompt engineering, combined with the integration of multicenter real-world data, has significantly enhanced the efficiency and accuracy of knowledge graph construction. The resulting knowledge graph provides a robust framework for understanding sepsis, supporting clinical decision-making, and facilitating further research. The success of this approach underscores the potential of LLMs in medical research and sets a new benchmark for future studies in sepsis and other complex medical conditions.}
}
@article{ZHANG2025293,
title = {A disambiguation method for potential ambiguities in Chinese based on knowledge graphs and large language model},
journal = {Alexandria Engineering Journal},
volume = {126},
pages = {293-302},
year = {2025},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2025.04.089},
url = {https://www.sciencedirect.com/science/article/pii/S1110016825005861},
author = {Dan Zhang and Delong Jia},
keywords = {Chinese ambiguity, Disambiguation model, Knowledge graph, Large language model, Natural language processing},
abstract = {Traditional disambiguation methods struggle to effectively balance and integrate a wide range of contextual information and world knowledge when dealing with potential ambiguities in Chinese. To address this issue, this paper proposes a disambiguation model that integrates knowledge graphs and large language models (LLMs) to tackle lexical ambiguity in Chinese texts. This article uses an attention based disambiguation model, which is fine-tuned using multiple hyperparameter configurations. It optimizes network layers and knowledge graph embedding dimensions to enhance performance. Visualization of the attention mechanism reveals the model's focus on target words, context, and knowledge graph entities. Experiments conducted on a dataset comprising 200,000 sentences demonstrate significant improvements in accuracy and F1 scores, reaching 92.4 % and 91.9 %, respectively, compared to traditional statistical and deep learning models. Visualization of the attention mechanism reveals the model's focus on target words, context, and knowledge graph entities. The findings suggest that integrating knowledge graphs with LLMs offers an innovative approach to complex language tasks. In practical applications such as machine translation and chatbots, this model is expected to enhance both performance and interpretability.}
}
@article{TAN2023100381,
title = {MLPQ: A Dataset for Path Question Answering over Multilingual Knowledge Graphs},
journal = {Big Data Research},
volume = {32},
pages = {100381},
year = {2023},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2023.100381},
url = {https://www.sciencedirect.com/science/article/pii/S221457962300014X},
author = {Yiming Tan and Yongrui Chen and Guilin Qi and Weizhuo Li and Meng Wang},
keywords = {Question answering, Cross-lingual knowledge graphs, Multi-hop reasoning, Entity alignment},
abstract = {Knowledge Graph-based Multilingual Question Answering (KG-MLQA), as one of the essential subtasks in Knowledge Graph-based Question Answering (KGQA), emphasizes that questions on the KGQA task can be expressed in different languages to solve the lexical gap between questions and knowledge graph(s). However, the existing KG-MLQA works mainly focus on the semantic parsing of multilingual questions but ignore the questions that require integrating information from cross-lingual knowledge graphs (CLKG). This paper extends KG-MLQA to Cross-lingual KG-based multilingual Question Answering (CLKGQA) and constructs the first CLKGQA dataset over multilingual DBpedia named MLPQ, which contains 300K questions in English, Chinese, and French. We further propose a novel KG sampling algorithm for KG construction, making the MLPQ support the research of different types of methods. To evaluate the dataset, we put forward a general question answering workflow whose core idea is to transform CLKGQA into KG-MLQA. We first use the Entity Alignment (EA) model to merge CLKG into a single KG and get the answer to the question by the Multi-hop QA model combined with the Multilingual pre-training model. By instantiating the above QA workflow, we establish two baseline models for MLPQ, one of which uses Google translation to obtain alignment entities, and the other adopts the recent EA model. Experiments show that the baseline models are insufficient to obtain the ideal performances on CLKGQA. Moreover, the availability of our benchmark contributes to the community of question answering and entity alignment.}
}
@article{LEE2025114390,
title = {Target-guided dialog generation with dynamic knowledge path by commonsense knowledge graph and relation prediction},
journal = {Knowledge-Based Systems},
volume = {329},
pages = {114390},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.114390},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125014297},
author = {Hayoung Lee and Soyeop Yoo and Woong-Kee Loh and Ok-Ran Jeong},
keywords = {Target-guided dialog system, Knowledge graph, Relation prediction, Commonsense reasoning, Global planning},
abstract = {Conversational AI has been rapidly advancing with the development of large language models and has shown excellent performance. However, one of its limitations is a passive system that cannot ask or guide users back to ambiguous questions. To overcome this, we have implemented an active dialog system that can smoothly transition from previous conversations. Our system is a target-guided system, which means it can guide the conversation by asking the user to provide a desired response or target. This approach is knowledge-rich and challenging, as it requires achieving the target while maintaining contextual consistency. To generate responses, we dynamically construct knowledge paths through knowledge graphs and relation predictors. These play an essential role in generating diverse and logically connected responses. To achieve this, we follow a global planning method that systematically conducts conversations with a target, and constructs knowledge paths based on common sense. We perform multi-hop reasoning and bi-directional search simultaneously to increase diversity and logical connectivity. We have overcome the limitations of existing works that rely solely on knowledge graphs by reflecting the results of relation predictors along with each object’s WIKI data in the path. Therefore, the consideration of the knowledge graph and the performance of the relation predictor, compared to the existing system, in completing the dynamic knowledge path and generating transition responses allowed conversations to transition more naturally. We have verified the proposed model through experiments.}
}
@article{YE2026129199,
title = {SuKE: Structural Knowledge Extractor enhances large language model for knowledge graph completion},
journal = {Expert Systems with Applications},
volume = {297},
pages = {129199},
year = {2026},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.129199},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425028155},
author = {Yunhai Ye and Shuo Wang},
keywords = {Knowledge graph, Knowledge graph completion, Large language model, Graph neural network, Cross-modality transferability},
abstract = {Current large language model (LLM)-based knowledge graph completion (KGC) methods fail to fully leverage structural information of the knowledge graph (KG), resulting in suboptimal performance. They typically incorporate KG information into LLMs through either direct fine-tuning or employing soft prompts derived from conventional embedding-based KGC models. However, these methodologies exhibit fundamental limitations in their capacity to comprehensively encode and exploit the intricate structural information contained within KGs. To address these issues, we first develop KG-infused in-context learning and KG-infused instruction tuning by extending existing LLM paradigms to inject structural information through textual representations. Then we propose a Structural Knowledge Extractor (SuKE), a trainable encoder-decoder architecture designed to extract both structural and semantic information from KGs and incorporate it into LLMs, with the overarching goal of enhancing the structural reasoning capabilities of LLMs for KGC tasks. Specifically, in encoder, we propose a Group-wise Graph Attention Network with relation-augmented message function for entity embeddings generation and a dynamic relation representation module using subgraph relational path to produce relation embeddings. Subsequently, a global embedding mechanism is introduced to mitigate the over-smoothing issue of entity and relation embeddings. In decoder, a cross-modal projection mechanism transforms the embeddings of triples into virtual tokens within the textual space, which are then pretended as prefixes to the prompt tokens and injected into the LLM. Experimental performance outperforms the state-of-the-art methods, demonstrating that SuKE significantly boosts the performance of LLMs on KGC tasks and revealing the effectiveness of structural information.}
}
@article{HUANG2025113648,
title = {Prompting large language models with knowledge graphs for question answering involving long-tail facts},
journal = {Knowledge-Based Systems},
volume = {324},
pages = {113648},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113648},
url = {https://www.sciencedirect.com/science/article/pii/S095070512500694X},
author = {Wenyu Huang and Guancheng Zhou and Mirella Lapata and Pavlos Vougiouklis and Sebastien Montella and Jeff Z. Pan},
keywords = {Large language models, Knowledge graphs, Retrieval-augmented generation, Evaluation},
abstract = {Although Large Language Models (LLMs) are effective in performing various NLP tasks, they still struggle to handle tasks that require extensive, real-world knowledge, especially when dealing with long-tail facts (facts related to long-tail entities). This limitation highlights the need to supplement LLMs with non-parametric knowledge. To address this issue, we analysed the effects of different types of non-parametric knowledge, including textual passage and knowledge graphs (KGs). Since LLMs have probably seen the majority of factual question-answering datasets already, to facilitate our analysis, we proposed a fully automatic pipeline for creating a benchmark that requires knowledge of long-tail facts for answering the involved questions. Using this pipeline, we introduce the LTGen benchmark. We evaluate state-of-the-art LLMs in different knowledge settings using the proposed benchmark. Our experiments show that LLMs alone struggle with answering these questions, especially when the long-tail level is high or rich knowledge is required. Nonetheless, the performance of the same models improved significantly when they were prompted with non-parametric knowledge. We observed that, in most cases, prompting LLMs with KG triples surpasses passage-based prompting using a state-of-the-art retriever. In addition, while prompting LLMs with both KG triples and documents does not consistently improve knowledge coverage, it can dramatically reduce hallucinations in the generated content.}
}
@article{WU2025129999,
title = {MKGF: A multi-modal knowledge graph based RAG framework to enhance LVLMs for Medical visual question answering},
journal = {Neurocomputing},
volume = {635},
pages = {129999},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.129999},
url = {https://www.sciencedirect.com/science/article/pii/S092523122500671X},
author = {Yinan Wu and Yuming Lu and Yan Zhou and Yifan Ding and Jingping Liu and Tong Ruan},
keywords = {Multi-modal, Knowledge graph, Large language model},
abstract = {Medical visual question answering (MedVQA) is a challenging task that requires models to understand medical images and return accurate responses for the given questions. Most recent methods focus on transferring general-domain large vision–language models (LVLMs) to the medical domain by constructing medical instruction datasets and in-context learning. However, the performance of these methods are limited due to the hallucination issue of LVLMs. In addition, fine-tuning the abundant parameters of LVLMs on medical instruction datasets is high time and economic cost. Hence, we propose a MKGF framework that leverages a multi-modal medical knowledge graph (MMKG) to relieve the hallucination issue without fine-tuning the abundant parameters of LVLMs. Firstly, we employ a pre-trained text retriever to build question–knowledge relations on training set. Secondly, we train a multi-modal retriever with these relations. Finally, we use it to retrieve question-relevant knowledge and enhance the performance of LVLMs on the test set. To evaluate the effectiveness of MKGF, we conduct extensive experiments on two public datasets Slake and VQA-RAD. Our method improves the pre-trained SOTA LVLMs by 10.15% and 9.32%, respectively. The source codes are available at https://github.com/ehnal/MKGF.}
}