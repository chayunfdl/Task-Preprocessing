@article{BEHR20245699,
title = {Generating knowledge graphs through text mining of catalysis research related literature††Electronic supplementary information (ESI) available: https://github.com/AleSteB/CatalysisIE_Knowledge_Graph_Generator. See DOI: https://doi.org/10.1039/d4cy00369a},
journal = {Catalysis Science & Technology},
volume = {14},
number = {19},
pages = {5699-5713},
year = {2024},
issn = {2044-4753},
doi = {https://doi.org/10.1039/d4cy00369a},
url = {https://www.sciencedirect.com/science/article/pii/S2044475324004696},
author = {Alexander S. Behr and Diana Chernenko and Dominik Koßmann and Arjun Neyyathala and Schirin Hanf and Stephan A. Schunk and Norbert Kockmann},
abstract = {Structured research data management in catalysis is crucial, especially for large amounts of data, and should be guided by FAIR principles for easy access and compatibility of data. Ontologies help to organize knowledge in a structured and FAIR way. The increasing numbers of scientific publications call for automated methods to preselect and access the desired knowledge while minimizing the effort to search for relevant publications. While ontology learning can be used to create structured knowledge graphs, named entity recognition allows detection and categorization of important information in text. This work combines ontology learning and named entity recognition for automated extraction of key data from publications and organization of the implicit knowledge in a machine- and user-readable knowledge graph and data. CatalysisIE is a pre-trained model for such information extraction for catalysis research. This model is used and extended in this work based on a new data set, increasing the precision and recall of the model with regard to the data set. Validation of the presented workflow is presented on two datasets regarding catalysis research. Preformulated SPARQL-queries are provided to show the usability and applicability of the resulting knowledge graph for researchers.}
}
@article{PAN2025106710,
title = {A bottom-up approach of knowledge graph modelling for urban underground public spaces: Insights into public cognition},
journal = {Tunnelling and Underground Space Technology},
volume = {163},
pages = {106710},
year = {2025},
issn = {0886-7798},
doi = {https://doi.org/10.1016/j.tust.2025.106710},
url = {https://www.sciencedirect.com/science/article/pii/S0886779825003487},
author = {Qi Pan and Shiu-Tong Thomas Ng and Fang-Le Peng and Yun-Hao Dong},
keywords = {Urban underground public space, Public cognition, Knowledge graph, Complex network, Natural language processing},
abstract = {Urban underground public spaces are increasingly recognised as crucial for enhancing urban functionality and liveability. However, effectively integrating public cognition into underground public space planning remains challenging. This study addresses this gap by proposing a novel approach integrating natural language processing, knowledge graphs and complex network theory to systematically mine public cognition from WeChat official account articles and Web of Science abstracts. Integrating the principles of linguistics and semiotics, we developed a novel approach for knowledge elements extraction and vectorised knowledge graph construction. This yielded 15,335 Chinese and 10,589 English effective knowledge elements, which were used to construct and compare knowledge graphs representing the cognitive structures of social and academic communities. Findings reveal distinct priorities, with the social community emphasising experiential aspects while the academic community focuses on theoretical concepts. Network analysis underscores the scale-free nature of both graphs, with higher centrality in the social network. These insights offer valuable implications for developing tailored public participation strategies in underground space planning, promoting more inclusive and human-centred urban development.}
}
@article{LI20243832,
title = {Automated compliance checking for BIM models based on Chinese-NLP and knowledge graph: an integrative conceptual framework},
journal = {Engineering, Construction and Architectural Management},
volume = {32},
number = {6},
pages = {3832-3856},
year = {2024},
issn = {0969-9988},
doi = {https://doi.org/10.1108/ECAM-10-2023-1037},
url = {https://www.sciencedirect.com/science/article/pii/S0969998824000833},
author = {Sihao Li and Jiali Wang and Zhao Xu},
keywords = {Building information modeling, Knowledge graph, Natural language processing, Deep learning, Automatic checking},
abstract = {Purpose
The compliance checking of Building Information Modeling (BIM) models is crucial throughout the lifecycle of construction. The increasing amount and complexity of information carried by BIM models have made compliance checking more challenging, and manual methods are prone to errors. Therefore, this study aims to propose an integrative conceptual framework for automated compliance checking of BIM models, allowing for the identification of errors within BIM models.
Design/methodology/approach
This study first analyzed the typical building standards in the field of architecture and fire protection, and then the ontology of these elements is developed. Based on this, a building standard corpus is built, and deep learning models are trained to automatically label the building standard texts. The Neo4j is utilized for knowledge graph construction and storage, and a data extraction method based on the Dynamo is designed to obtain checking data files. After that, a matching algorithm is devised to express the logical rules of knowledge graph triples, resulting in automated compliance checking for BIM models.
Findings
Case validation results showed that this theoretical framework can achieve the automatic construction of domain knowledge graphs and automatic checking of BIM model compliance. Compared with traditional methods, this method has a higher degree of automation and portability.
Originality/value
This study introduces knowledge graphs and natural language processing technology into the field of BIM model checking and completes the automated process of constructing domain knowledge graphs and checking BIM model data. The validation of its functionality and usability through two case studies on a self-developed BIM checking platform.}
}
@article{JOSHI20252343,
title = {Developing Natural Language Processing Algorithms to Fact-Check Speech or Text},
journal = {Procedia Computer Science},
volume = {258},
pages = {2343-2351},
year = {2025},
note = {International Conference on Machine Learning and Data Engineering},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.04.497},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925016011},
author = {Himanshu Sanjay Joshi and Hamed Taherdoost},
keywords = {Fact-Checking, Natural Language Processing (NLP), Question-Answering Systems, Knowledge Graphs, Misinformation},
abstract = {Natural Language Processing (NLP) is pivotal in the fight against misinformation, offering tools to process and analyze both text and speech. This paper presents the development of NLP algorithms specifically designed for fact-checking, leveraging the capabilities of Question-Answering (QA) systems and knowledge graphs. We conduct a comprehensive analysis of existing QA systems, propose enhancements for fact-checking, and validate our approach through comparative studies. Our findings contribute to the ongoing efforts to improve the reliability of information in the digital age.}
}
@article{KUMAR2025169072,
title = {Yeast Knowledge Graphs Database for Exploring Saccharomyces Cerevisiae and Schizosaccharomyces Pombe},
journal = {Journal of Molecular Biology},
volume = {437},
number = {10},
pages = {169072},
year = {2025},
issn = {0022-2836},
doi = {https://doi.org/10.1016/j.jmb.2025.169072},
url = {https://www.sciencedirect.com/science/article/pii/S002228362500138X},
author = {Mani R. Kumar and Karthick Raja Arulprakasam and An-Nikol Kutevska and Marek Mutwil and Guillaume Thibault},
keywords = {yeast, knowledge graph, bioinformatics, , },
abstract = {Biomedical literature contains an extensive wealth of information on gene and protein function across various biological processes and diseases. However, navigating this vast and often restricted-access data can be challenging, making it difficult to extract specific insights efficiently. In this study, we introduce a high-throughput pipeline that leverages OpenAI’s Generative Pre-Trained Transformer Model (GPT) to automate the extraction and analysis of gene function information. We applied this approach to 84,427 publications on Saccharomyces cerevisiae and 6,452 publications on Schizosaccharomyces pombe, identifying 3,432,749 relationships for budding yeast and 421,198 relationships for S. pombe. This resulted in a comprehensive, searchable online Knowledge Graph database, available at yeast.connectome.tools and spombe.connectome.tools, which offers users extensive access to various interactions and pathways. Our analysis underscores the power of integrating artificial intelligence with bioinformatics, as demonstrated through key insights into important nodes like Hsp104 and Atg8 proteins. This work not only facilitates efficient data extraction in yeast research but also presents a scalable model for similar studies in other biological systems.}
}
@article{LAVRINOVICS2025100844,
title = {Knowledge Graphs, Large Language Models, and Hallucinations: An NLP Perspective},
journal = {Journal of Web Semantics},
volume = {85},
pages = {100844},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100844},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000301},
author = {Ernests Lavrinovics and Russa Biswas and Johannes Bjerva and Katja Hose},
keywords = {LLM, Factuality, Knowledge Graphs, Hallucinations},
abstract = {Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) based applications including automated text generation, question answering, chatbots, and others. However, they face a significant challenge: hallucinations, where models produce plausible-sounding but factually incorrect responses. This undermines trust and limits the applicability of LLMs in different domains. Knowledge Graphs (KGs), on the other hand, provide a structured collection of interconnected facts represented as entities (nodes) and their relationships (edges). In recent research, KGs have been leveraged to provide context that can fill gaps in an LLM’s understanding of certain topics offering a promising approach to mitigate hallucinations in LLMs, enhancing their reliability and accuracy while benefiting from their wide applicability. Nonetheless, it is still a very active area of research with various unresolved open problems. In this paper, we discuss these open challenges covering state-of-the-art datasets and benchmarks as well as methods for knowledge integration and evaluating hallucinations. In our discussion, we consider the current use of KGs in LLM systems and identify future directions within each of these challenges.}
}
@article{KONDINSKI20242070,
title = {Knowledge graph representation of zeolitic crystalline materials††Electronic supplementary information (ESI) available. See DOI: https://doi.org/10.1039/d4dd00166d},
journal = {Digital Discovery},
volume = {3},
number = {10},
pages = {2070-2084},
year = {2024},
issn = {2635-098X},
doi = {https://doi.org/10.1039/d4dd00166d},
url = {https://www.sciencedirect.com/science/article/pii/S2635098X24001669},
author = {Aleksandar Kondinski and Pavlo Rutkevych and Laura Pascazio and Dan N. Tran and Feroz Farazi and Srishti Ganguly and Markus Kraft},
abstract = {Zeolites are complex and porous crystalline inorganic materials that serve as hosts for a variety of molecular, ionic and cluster species. Formal, machine-actionable representation of this chemistry presents a challenge as a variety of concepts need to be semantically interlinked. This work demonstrates the potential of knowledge engineering in overcoming this challenge. We develop ontologies OntoCrystal and OntoZeolite, enabling the representation and instantiation of crystalline zeolite information into a dynamic, interoperable knowledge graph called The World Avatar (TWA). In TWA, crystalline zeolite instances are semantically interconnected with chemical species that act as guests in these materials. Information can be obtained via custom or templated SPARQL queries administered through a user-friendly web interface. Unstructured exploration is facilitated through natural language processing using the Marie System, showcasing promise for the blended large language model – knowledge graph approach in providing accurate responses on zeolite chemistry in natural language.}
}
@article{SHIM2025103001,
title = {OmEGa(Ω): Ontology-based information extraction framework for constructing task-centric knowledge graph from manufacturing documents with large language model},
journal = {Advanced Engineering Informatics},
volume = {64},
pages = {103001},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.103001},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624006529},
author = {Midan Shim and Hyojun Choi and Heeyeon Koo and Kaehyun Um and Kyong-Ho Lee and Sanghyun Lee},
keywords = {Ontology modeling, Manufacturing and maintenance process, Information extraction, Knowledge graph, Document understanding, Large language model},
abstract = {Manufacturing industry relies heavily on technical documents that encapsulate specialized knowledge essential for optimizing production and maintenance processes. However, extracting meaningful insights from these documents is challenging due to their complex structure, domain-specific terminology, and multimodal content, which includes text, images, and tables. Furthermore, there is a contextual gap between the generic training data of pre-trained language models (PLMs) and the specialized knowledge required for manufacturing documents. To address these issues, a Task-Centric Ontology (TCO) is designed to describe fundamental manufacturing tasks, and develop OmEGa, an Ontology-based Information Extraction Framework for Task-Centric Knowledge Graphs. OmEGa leverages large language models (LLMs) to perform instance recognition and relation classification on multimodal documents. By utilizing spatial embedding and modality linking, OmEGa addresses structural challenges, while TCO-driven reasoning mitigates contextual challenges. Experimental results demonstrate the effectiveness of OmEGa, achieving strong performance on both proprietary and open-source datasets. Additionally, a Knowledge Graph Question Answering (KGQA) system built on the extracted task-centric knowledge shows promise in enhancing communication among domain experts in the manufacturing sector.}
}
@article{HARUNA2025103578,
title = {AddManBERT: A combinatorial triples extraction and classification task for establishing a knowledge graph to facilitate design for additive manufacturing},
journal = {Advanced Engineering Informatics},
volume = {67},
pages = {103578},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103578},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625004719},
author = {Auwal Haruna and Khandaker Noman and Yongbo Li and Xin Wang and Md Junayed Hasan and Ahmad Bala Alhassan},
keywords = {Knowledge graph, BERT model, Textual Data, Additive manufacturing, Triples extraction and classification},
abstract = {In recent years, triple extraction and classification have received attention in the context of Additive Manufacturing (AM). However, the lack of a formalized process to extract and classify triple from textual data poses challenges for the effective embedding learning techniques in utilizing AM’s product innovation and manufacturing capabilities. Hence, the AM field’s manual cognitive process hinders the broader adoption of Design for AM (DFAM) in manufacturing. Aiming to solve these challenging problems, this research proposes a Natural Language Processing (NLP) and Knowledge Graph (KG) methodology for triple extraction and classification from textual data to provide an embedding learning approach. Initially, multi-source textual data for triple extraction and classification is developed. Then, AM Bidirectional Encoder Representation from the Transformers (AddManBERT) is used for triple extraction and classification. The AddManBERT utilizes dependency parsing to determine the semantic relations between the entities for triple extraction and classification. Consequently, the AddManBERT transformed each extracted piece of knowledge from the textual data into a 768-dimensional vector structure by analyzing the projected probability of the output within the center word based on the token embedding surrounding the input. The triples extracted and classified are then saved in the Neo4j database and displayed as graph nodes. An experiment and an application case study verify the proposed method’s efficacy. The experiment results indicate that the proposed method outperforms the traditional centralized approaches in responsiveness, classification accuracy, and prediction efficiency.}
}
@article{GALLEGO2025113211,
title = {Enhancing cross-encoders using knowledge graph hierarchy for medical entity linking in zero- and few-shot scenarios},
journal = {Knowledge-Based Systems},
volume = {314},
pages = {113211},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113211},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125002588},
author = {Fernando Gallego and Pedro Ruas and Francisco M. Couto and Francisco J. Veredas},
keywords = {Knowledge enrichment, Medical entity linking, Contrastive-learning, Knowledge bases, Candidate reranking, Zero-few shot},
abstract = {Medical Entity Linking (MEL) is a common task in natural language processing, focusing on the normalization of recognized entities from clinical texts using large knowledge bases (KBs). This task presents significant challenges, especially when working with electronic health records that often lack annotated clinical notes, even in languages like English. The difficulty increases in few-shot or zero-shot scenarios, where models must operate with minimal or no training data, a common issue when dealing with less-documented languages such as Spanish. Existing solutions that combine contrastive learning with external sources, like the Unified Medical Language System (UMLS), have shown competitive results. However, most of these methods focus on individual concepts from the KBs, ignoring relationships such as synonymy or hierarchical links between concepts. In this paper, we propose leveraging these relationships to enrich the training triplets used for contrastive learning, improving performance in MEL tasks. Specifically, we fine-tune several BERT-based cross-encoders using enriched triplets on three clinical corpora in Spanish : DisTEMIST, MedProcNER, and SympTEMIST. Our approach addresses the complexity of real-world data, where unseen mentions and concepts are frequent. The results show a notable improvement in lower top-k accuracies, surpassing the state-of-the-art by up to 5.5 percentage points for unseen mentions and by up to 5.9 points for unseen concepts. This improvement reduces the number of candidate concepts required for cross-encoders, enabling more efficient semi-automatic annotation and decreasing human effort. Additionally, our findings underscore the importance of leveraging not only the concept-level information in KBs but also the relationships between those concepts.}
}
@article{KUMAR2025109011,
title = {An evolutionary study on technologies for polyethylene terephthalate waste recycling using natural language processing},
journal = {Computers & Chemical Engineering},
volume = {195},
pages = {109011},
year = {2025},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2025.109011},
url = {https://www.sciencedirect.com/science/article/pii/S0098135425000158},
author = {Avan Kumar and Harshitha Chandra Jami and Bhavik R. Bakshi and Manojkumar Ramteke and Hariprasad Kodamana},
keywords = {PET waste, Recycle technologies, NLP technique, Knowledge graph, Topic modeling, Popularity index},
abstract = {Polyethylene terephthalate (PET) is valued for its durability, tensile strength, low moisture absorption, and cost-effectiveness. However, its non-biodegradability poses an environmental threat, and plastic recycling is the sole remedy. This study proposes an NLP framework for concisely extracting and summarizing key information on recycling technologies and alternatives from relevant scientific literature. This NLP framework comprises three approaches: time-series knowledge graphs, dynamic transformer-based topic modeling, and estimating popularity indices for technologies. The framework aims to streamline the extraction of qualitative and quantitative insights for sustainable and economical PET waste recycling pathways. Key findings of the study show that there is a 406% rise in pyrolysis technology use, a 278% increase in chemical conversion, and a 1353% surge in waste PET utilization for electronic device-making. It is worth noting that some of the identified recycling pathways corroborate well with the actual implementation in the industries.}
}
@article{NIE2025100866,
title = {What can knowledge graph do for few-shot named entity recognition},
journal = {Journal of Web Semantics},
volume = {86},
pages = {100866},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2025.100866},
url = {https://www.sciencedirect.com/science/article/pii/S157082682500006X},
author = {Binling Nie and Yiming Shao and Yigang Wang},
keywords = {Few-shot NER, Knowledge graph, Knowledge fusion},
abstract = {Due to its extensive applicability in various downstream domains, few-shot named entity recognition (NER) has attracted increasing attention, particularly in areas where acquiring sufficient labeled data poses a significant challenge. Recent studies have highlighted the potential of knowledge graphs (KGs) in enhancing natural language processing (NLP) tasks. However, a comprehensive understanding of whether and how KGs can effectively improve the NER performance under low-resource conditions remains elusive. In this paper, for the first time, we quantitatively investigate the effects of different kinds of extra KG features for few-shot NER. We enable our analysis by aggregating extra KG features into an NER framework. Through extensive experiments, we find that incorporating class features yields the best performance. To fully explore the potential of class features from KGs, we propose a novel network architecture, named KGen, to jointly leverage KG-based knowledge from both the input sentence side and the label semantic side for few-shot NER.The efficacy of our proposed method is validated through extensive experiments on five challenging datasets.}
}
@article{ELLOUZE2025100308,
title = {Management of psychological emergency cases on social media: A hybrid approach combining knowledge graphs and graph neural networks},
journal = {Online Social Networks and Media},
volume = {46},
pages = {100308},
year = {2025},
issn = {2468-6964},
doi = {https://doi.org/10.1016/j.osnem.2025.100308},
url = {https://www.sciencedirect.com/science/article/pii/S2468696425000096},
author = {Mourad Ellouze and Sonda Rekik and Lamia {Hadrich Belguith}},
keywords = {Psychological emergency, Personality disorder, Social media, Natural language processing, Ontology, Graph neural networks},
abstract = {The effects of psychological crises are evolving at an astounding rate nowadays, presenting a significant challenge for everyone involved in tracking these disorders. Therefore, we propose in this paper a hybrid approach based on linguistic processing and numerical techniques allowing to: (i) identify the presence of psychological emergencies among social network users by analyzing their textual production, (ii) determine the specific type of emergency case, (iii) elaborate a graph for each type of emergency, reflecting the different dimensions linked to the psychological emergency, allowing for a better diagnosis of the situation and providing an overall view of the crisis type, (iv) combine the separate graphs for each emergency to address the various semantic aspects. The work was accomplished using advanced language model techniques, knowledge graphs and neural network graphs. The combination of these techniques ensures that their advantages are leveraged while overcoming their limitations in terms of result generalization. The evaluation of different parts related to detecting the presence of psychological problems, predicting specific type of emergency cases, and detecting links between knowledge graphs was measured using the F-measure metric. The values derived from this measure, corresponding to the evaluation of these three tasks, are, respectively, 83%, 87% and 80%. For the evaluation of the elaboration of each graph related to specific type of emergency cases, this was accomplished using qualitative metric standards. The results obtained can be considered encouraging given the significant scale of our approach.}
}
@article{GAN2025104761,
title = {ARCH: Large-scale knowledge graph via aggregated narrative codified health records analysis},
journal = {Journal of Biomedical Informatics},
volume = {162},
pages = {104761},
year = {2025},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104761},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424001795},
author = {Ziming Gan and Doudou Zhou and Everett Rush and Vidul A. Panickan and Yuk-Lam Ho and George Ostrouchovm and Zhiwei Xu and Shuting Shen and Xin Xiong and Kimberly F. Greco and Chuan Hong and Clara-Lea Bonzel and Jun Wen and Lauren Costa and Tianrun Cai and Edmon Begoli and Zongqi Xia and J. Michael Gaziano and Katherine P. Liao and Kelly Cho and Tianxi Cai and Junwei Lu},
keywords = {Electronic health records, Natural language processing, Representation learning, Knowledge graph},
abstract = {Objective:
Electronic health record (EHR) systems contain a wealth of clinical data stored as both codified data and free-text narrative notes (NLP). The complexity of EHR presents challenges in feature representation, information extraction, and uncertainty quantification. To address these challenges, we proposed an efficient Aggregated naRrative Codified Health (ARCH) records analysis to generate a large-scale knowledge graph (KG) for a comprehensive set of EHR codified and narrative features.
Methods:
Using data from 12.5 million Veterans Affairs patients, ARCH first derives embedding vectors and generates similarities along with associated p-values to measure the strength of relatedness between clinical features with statistical certainty quantification. Next, ARCH performs a sparse embedding regression to remove indirect linkage between features to build a sparse KG. Finally, ARCH was validated on various clinical tasks, including detecting known relationships between entity pairs, predicting drug side effects, disease phenotyping, as well as sub-typing Alzheimer’s disease patients.
Results:
ARCH produces high-quality clinical embeddings and KG for over 60,000 codified and narrative EHR concepts. The KG and embeddings are visualized in the R-shiny powered web-API.33https://celehs.hms.harvard.edu/ARCH/, https://phenomics.va.ornl.gov/web/cipher/vistools. ARCH achieved high accuracy in detecting EHR concept relationships, with AUCs of 0.926 (codified) and 0.861 (NLP) for similar EHR concepts, and 0.810 (codified) and 0.843 (NLP) for related pairs. It detected drug side effects with a 0.723 AUC, which improved to 0.826 after fine-tuning. Using both codified and NLP features, the detection power increased significantly. Compared to other methods, ARCH has superior accuracy and enhances weakly supervised phenotyping algorithms’ performance. Notably, it successfully categorized Alzheimer’s patients into two subgroups with varying mortality rates.
Conclusion:
The proposed ARCH algorithm generates large-scale high-quality semantic representations and knowledge graph for both codified and NLP EHR features, useful for a wide range of predictive modeling tasks.}
}
@article{THEUNER20251125,
title = {Weaving Knowledge Graphs and Large Language Models (LLMs): Leveraging Semantics for Contextualized Design Knowledge Retrieval},
journal = {Procedia CIRP},
volume = {134},
pages = {1125-1130},
year = {2025},
note = {58th CIRP Conference on Manufacturing Systems 2025},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2025.03.073},
url = {https://www.sciencedirect.com/science/article/pii/S2212827125006389},
author = {Katharina Theuner and Tomas Mikael Elmgren and Axel Götling and Marvin Carl May and Haluk Akay},
keywords = {Knowledge Engineering, Large Language Models, Design, Knowledge Graph},
abstract = {Demographic change in Europe challenges companies as retiring employees take valuable expertise with them. To address this, knowledge graphs (KGs) are emerging as tools for structured knowledge representation. Simultaneously, large language models (LLMs) are increasingly being used as innovative solutions for information retrieval. However, LLMs generally process only public knowledge, and recent approaches integrating Retrieval Augmented Generation (RAG) for private knowledge retrieval often lack contextual relevance. To enhance trustworthiness and overcome these limitations, a method is proposed for embedding latent problem-solving structures within design processes into LLM-driven information retrieval systems. Using a case study in energy infrastructure, a KG of design problems was constructed by extracting functional requirements from semi-structured documentation via LLMs. This KG is further utilized by an LLM to answer queries, with results visualized through an interactive interface. Validation through field studies with engineers underscores the approach’s effectiveness in enhancing contextual and trustworthy knowledge dissemination.}
}
@article{ZHENG2025106179,
title = {Automating construction contract review using knowledge graph-enhanced large language models},
journal = {Automation in Construction},
volume = {175},
pages = {106179},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106179},
url = {https://www.sciencedirect.com/science/article/pii/S0926580525002195},
author = {Chunmo Zheng and Saika Wong and Xing Su and Yinqiu Tang and Ahsan Nawaz and Mohamad Kassem},
keywords = {Construction contract, Risk identification, Knowledge graph, Large language model},
abstract = {An effective and efficient review of construction contracts is essential for minimizing construction projects losses, but current methods are time-consuming and error-prone. Studies using methods based on Natural Language Processing (NLP) exist, but their scope is often limited to text classification or segmented label prediction. This paper investigates whether integrating Large Language Models (LLMs) and Knowledge Graphs (KGs) can enhance the accuracy and interpretability of automated contract risk identification. A tuning-free approach is proposed that integrates LLMs with a Nested Contract Knowledge Graph (NCKG) using a Graph Retrieval-Augmented Generation (GraphRAG) framework for contract knowledge retrieval and reasoning. Tested on international EPC contracts, the method achieves more accurate risk evaluation and interpretable risk summaries than baseline models. These findings demonstrate the potential of combining LLMs and KGs for reliable reasoning in tasks that are knowledge-intensive and specialized, such as contract review.}
}
@article{YACOUBIAYADI2024100484,
title = {A unified approach to publish semantic annotations of agricultural documents as knowledge graphs},
journal = {Smart Agricultural Technology},
volume = {8},
pages = {100484},
year = {2024},
issn = {2772-3755},
doi = {https://doi.org/10.1016/j.atech.2024.100484},
url = {https://www.sciencedirect.com/science/article/pii/S2772375524000893},
author = {Nadia {Yacoubi Ayadi} and Stephan Bernard and Robert Bossy and Marine Courtin and Bill Gates {Happi Happi} and Pierre Larmande and Franck Michel and Claire Nédellec and Catherine Roussey and Catherine Faron},
keywords = {Agriculture, Knowledge graphs, Semantic modelling, RDF transformation, Natural language processing, Annotations, Semantic resources, Named entity recognition and linking},
abstract = {The research results presented in this paper were obtained as part of the D2KAB project (Data to Knowledge in Agriculture and Biodiversity) which aims to develop semantic web-based tools to describe and make agronomical data actionable and accessible following the FAIR principles. We focus on constructing domain-specific Knowledge Graphs (KGs) from textual data sources, using Natural Language Processing (NLP) techniques to extract and structure relevant entities. Our approach is based on the formalization of a semantic data model using common linked open vocabularies such as the Web Annotation Ontology (OA) and the Provenance Ontology (PROV). The model was developed by formulating motivating scenarios and competency questions from domain experts. This model has been used to construct three different KGs from three distinct corpora: PubMed scientific publications on wheat and rice genetics and phenotyping, and French agricultural alert bulletins. The named entities to be recognized include genes, phenotypes, traits, genetic markers, taxa and phenological stages normalized using semantic resources such as the Wheat Trait and Phenotype Ontology (WTO), the French Crop Usage (FCU) thesaurus and the Plant Phenological Description Ontology (PPDO). Named entities were extracted using different NLP approaches and tools. The relevance of the semantic model was validated by implementing experts questions as SPARQL queries to be answered on the constructed RDF knowledge graphs. Our work demonstrates how domain-specific vocabularies and systematic querying of KGs can reveal hidden interactions and support agronomists in navigating vast amounts of data. The resources and transformation pipelines developed are publicly available in Git repositories.}
}
@article{KOHL202487,
title = {A Knowledge Graph-based Learning Assistance Systems for Industrial Maintenance},
journal = {Procedia CIRP},
volume = {126},
pages = {87-92},
year = {2024},
note = {17th CIRP Conference on Intelligent Computation in Manufacturing Engineering (CIRP ICME ‘23)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.08.305},
url = {https://www.sciencedirect.com/science/article/pii/S221282712400876X},
author = {Linus Kohl and Fazel Ansari},
keywords = {Artificial Intelligence, Maintenance, Knowledge Graph, Natural Language Processing, Learning Assistance System},
abstract = {Manufacturing industry is constantly evolving technologically. To ensure that the workforce comply with pace of changing requirements, Learning Assistance Systems (LAS) can provide competency-based support for daily tasks. LAS provide personalized support to blue and white collars through modular and contextualized workflows, information and learning materials. This paper presents a LAS designed and implemented for maintenance personnel in semiconductor manufacturing. The implemented LAS employs a novel statistical learning algorithm to determine the required competencies per task, interlinking them with information from knowledge bases and Manufacturing Execution Systems. This leads to a reduced mean time to repair of 18% in the illustrated use case.}
}
@article{DESSI2022109945,
title = {SCICERO: A deep learning and NLP approach for generating scientific knowledge graphs in the computer science domain},
journal = {Knowledge-Based Systems},
volume = {258},
pages = {109945},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.109945},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122010383},
author = {Danilo Dessí and Francesco Osborne and Diego {Reforgiato Recupero} and Davide Buscaldi and Enrico Motta},
keywords = {Knowledge graph, Scholarly domain, Scientific facts, Artificial intelligence},
abstract = {Science communication has a number of bottlenecks that include the rising number of published research papers and its non-machine-accessible and document-based paradigm, which makes the exploration, reading, and reuse of research outcomes rather inefficient. Recently, Knowledge Graphs (KG), i.e., semantic interlinked networks of entities, have been proposed as a new core technology to describe and curate scholarly information with the goal to make it machine readable and understandable. However, the main drawback of the use of such a technology is that researchers are asked to manually annotate their research papers and add their contributions within the KGs. To address this problem, in this paper we propose SCICERO, a novel KG generation approach that takes in input text from research articles and generates a KG of research entities. SCICERO uses Natural Language Processing techniques to parse the content of scientific papers to discover entities and relationships, exploits state-of-the-art Deep Learning Transformer models to make sense and validate extracted information, and uses Semantic Web best practices to formally represent the extracted entities and relationships, making the written content of research papers machine-actionable. SCICERO has been tested on a dataset of 6.7M papers about Computer Science generating a KG of about 10M entities. It has been evaluated on a manually generated gold standard of 3,600 triples that cover three Computer Science subdomains (Information Retrieval, Natural Language Processing, and Machine Learning) obtaining remarkable results.}
}
@article{ZHU2024107971,
title = {Event-centric hierarchical hyperbolic graph for multi-hop question answering over knowledge graphs},
journal = {Engineering Applications of Artificial Intelligence},
volume = {133},
pages = {107971},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.107971},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624001295},
author = {Xun Zhu and Wang Gao and Tianyu Li and Wenguang Yao and Hongtao Deng},
keywords = {Question answering, Knowledge graphs, Graph attentive network, Hyperbolic geometry, Contrastive learning},
abstract = {Question Answering over Knowledge Graphs (KGQA) blends natural language processing with structured knowledge representation. While much attention of existing research has been given to entity-centric representations, the significance of events has not been fully explored. This paper introduces a novel Event-centric Hierarchical Hyperbolic Graph system for KGQA that effectively integrates entity and event information from knowledge graphs. Utilizing hyperbolic geometry, our model captures hierarchical structures, offering a refined representation of questions and related knowledge. Additionally, our integration of a Hierarchical Graph Attentive Network (HGAT) with Contrastive Representation Learning enables our model to effectively extract deep semantics and align them with knowledge graph structures. Empirical evaluations on the EventQA dataset demonstrate our approach’s effectiveness, significantly surpassing current leading models by 3% F1 and accuracy. This work not only extends the scope of KGQA but also highlights the importance of event-centric representations in knowledge-based tasks.}
}
@article{CHEN2019100959,
title = {An automatic literature knowledge graph and reasoning network modeling framework based on ontology and natural language processing},
journal = {Advanced Engineering Informatics},
volume = {42},
pages = {100959},
year = {2019},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2019.100959},
url = {https://www.sciencedirect.com/science/article/pii/S1474034619302642},
author = {Hainan Chen and Xiaowei Luo},
keywords = {Representation ontology, Natural language processing, Knowledge graph, Knowledge reasoning},
abstract = {With the advancement of scientific and engineering research, a huge number of academic literature are accumulated. Manually reviewing the existing literature is the main way to explore embedded knowledge, and the process is quite time-consuming and labor intensive. As the quantity of literature is increasing exponentially, it would be more difficult to cover all aspects of the literature using the traditional manual review approach. To overcome this drawback, bibliometric analysis is used to analyze the current situation and trend of a specific research field. In the bibliometric analysis, only a few key phrases (e.g., authors, publishers, journals, and citations) are usually used as the inputs for analysis. Information other than those phrases is not extracted for analysis, while that neglected information (e.g., abstract) might provide more detailed knowledge in the article. To tackle with this problem, this study proposed an automatic literature knowledge graph and reasoning network modeling framework based on ontology and Natural Language Processing (NLP), to facilitate the efficient knowledge exploration from literature abstract. In this framework, a representation ontology is proposed to characterize the literature abstract data into four knowledge elements (background, objectives, solutions, and findings), and NLP technology is used to extract the ontology instances from the abstract automatically. Based on the representation ontology, a four-space integrated knowledge graph is built using NLP technology. Then, reasoning network is generated according to the reasoning mechanism defined in the proposed ontology model. To validate the proposed framework, a case study is conducted to analyze the literature in the field of construction management. The case study proves that the proposed ontology model can be used to represent the knowledge embedded in the literatures’ abstracts, and the ontology elements can be automatically extracted by NLP models. The proposed framework can be an enhancement for the bibliometric analysis to explore more knowledge from the literature.}
}
@article{SHAMSHIRI2024105200,
title = {Text mining and natural language processing in construction},
journal = {Automation in Construction},
volume = {158},
pages = {105200},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.105200},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523004600},
author = {Alireza Shamshiri and Kyeong Rok Ryu and June Young Park},
keywords = {Text mining, Natural language processing, Machine learning, Computational linguistics, Language models, Construction, Project management},
abstract = {Text mining (TM) and natural language processing (NLP) have stirred interest within the construction field, as they offer enhanced capabilities for managing and analyzing text-based information. This highlights the need for a systematic review to identify the status quo, gaps, and future directions from the perspective of construction management. A review was conducted by aligning the objectives of 205 publications with the specific domains, areas, tasks, and processes outlined in construction management practices. This review reveals multiple facets of the construction sector empowered by TM/NLP approaches and highlights essential voids demanding consideration for automation possibilities and minimizing manual tasks. Ultimately, following identified obstacles, the review results indicate potential research opportunities: (1) strengthening overlooked construction aspects, (2) coupling diverse data formats, and (3) leveraging pre-trained language models and reinforcement learning. The findings will provide vital insights, fostering further progress in TM/NLP research and its applications in academia and industry.}
}
@article{SINGH2025102414,
title = {Evaluating diabetes dataset for knowledge graph embedding based link prediction},
journal = {Data & Knowledge Engineering},
volume = {157},
pages = {102414},
year = {2025},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2025.102414},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X25000096},
author = {Sushmita Singh and Manvi Siwach},
keywords = {Link prediction, Knowledge graphs, Knowledge graph embeddings, Knowledge graph completion, Translational embeddings, Diabetes},
abstract = {For doing any accurate analysis or prediction on data, a complete and well-populated dataset is required. Medical based data for any disease like diabetes is highly coupled and heterogeneous in nature, with numerous interconnections. This inherently complex data cannot be analysed by simple relational databases making knowledge graphs an ideal tool for its representation which can efficiently handle intricate relationships. Thus, knowledge graphs can be leveraged to analyse diabetes data, enhancing both the accuracy and efficiency of data-driven decision-making processes. Although substantial data exists on diabetes in various formats, the availability of organized and complete datasets is limited, highlighting the critical need for creation of a well-populated knowledge graph. Moreover while developing the knowledge graph, an inevitable problem of incompleteness is present due to missing links or relationships, necessitating the use of knowledge graph completion tasks to fill in this absent information which involves predicting missing data with various Link Prediction (LP) techniques. Among various link prediction methods, approaches based on knowledge graph embeddings have demonstrated superior performance and effectiveness. These knowledge graphs can support in-depth analysis and enhance the prediction of diabetes-associated risks in this field. This paper introduces a dataset specifically designed for performing link prediction on a diabetes knowledge graph, so that it can be used to fill the information gaps further contributing in the domain of risk analysis in diabetes. The accuracy of the dataset is assessed through validation with state-of-the-art embedding-based link prediction methods.}
}
@article{CORDEIRO2024105714,
title = {Petro NLP: Resources for natural language processing and information extraction for the oil and gas industry},
journal = {Computers & Geosciences},
volume = {193},
pages = {105714},
year = {2024},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2024.105714},
url = {https://www.sciencedirect.com/science/article/pii/S0098300424001973},
author = {Fábio Corrêa Cordeiro and Patrícia Ferreira {da Silva} and Alexandre Tessarollo and Cláudia Freitas and Elvis {de Souza} and Diogo {da Silva Magalhaes Gomes} and Renato Rocha Souza and Flávio Codeço Coelho},
keywords = {Natural language processing, Information extraction, Ontology, Knowledge graphs, Linguistic corpora},
abstract = {Most companies struggle to find and extract relevant information from their technical documents. In particular, the Oil and Gas (O&G) industry faces the challenge of dealing with large amounts of data hidden within old and new geoscientific reports collected over decades of operation. Making this information available in a structured format can unlock valuable information among these mountains of data, which is crucial to support a wide range of industrial and academic applications. However, most natural language processing resources were built from general domain corpora extracted from the Internet and primarily written in English. This paper presents Petro NLP, a comprehensive set of natural language processing and information extraction resources for the oil and gas industry in Portuguese. We connected an interdisciplinary team of geoscientists, linguists, computer scientists, petroleum engineers, librarians, and ontologists to build a knowledge graph and several annotated corpora. The Petro NLP resources comprise: (i) Petro KGraph– a knowledge graph populated with entities and relations commonly found on technical reports; and (ii) Petrolês, PetroGold, PetroNER, and PetroRE– sets of corpora containing raw text and documents annotated with morphosyntactic labels, named entities, and relations. These resources are fundamental infrastructure for future research in natural language processing and information extraction in the oil industry. Our ongoing research uses these datasets to train and enhance pre-trained machine learning models that automatically extract information from geoscientific technical documents.}
}
@article{BABAIHA2024100095,
title = {Rationalism in the face of GPT hypes: Benchmarking the output of large language models against human expert-curated biomedical knowledge graphs},
journal = {Artificial Intelligence in the Life Sciences},
volume = {5},
pages = {100095},
year = {2024},
issn = {2667-3185},
doi = {https://doi.org/10.1016/j.ailsci.2024.100095},
url = {https://www.sciencedirect.com/science/article/pii/S2667318524000023},
author = {Negin Sadat Babaiha and Sathvik Guru Rao and Jürgen Klein and Bruce Schultz and Marc Jacobs and Martin Hofmann-Apitius},
keywords = {Large language models (LLMs), Natural language processing (NLP), Biomedical text mining, Biomedical knowledge graphs, Biological expression language (BEL)},
abstract = {Biomedical knowledge graphs (KGs) hold valuable information regarding biomedical entities such as genes, diseases, biological processes, and drugs. KGs have been successfully employed in challenging biomedical areas such as the identification of pathophysiology mechanisms or drug repurposing. The creation of high-quality KGs typically requires labor-intensive multi-database integration or substantial human expert curation, both of which take time and contribute to the workload of data processing and annotation. Therefore, the use of automatic systems for KG building and maintenance is a prerequisite for the wide uptake and utilization of KGs. Technologies supporting the automated generation and updating of KGs typically make use of Natural Language Processing (NLP), which is optimized for extracting implicit triples described in relevant biomedical text sources. At the core of this challenge is how to improve the accuracy and coverage of the information extraction module by utilizing different models and tools. The emergence of pre-trained large language models (LLMs), such as ChatGPT which has grown in popularity dramatically, has revolutionized the field of NLP, making them a potential candidate to be used in text-based graph creation as well. So far, no previous work has investigated the power of LLMs on the generation of cause-and-effect networks and KGs encoded in Biological Expression Language (BEL). In this paper, we present initial studies towards one-shot BEL relation extraction using two different versions of the Generative Pre-trained Transformer (GPT) models and evaluate its performance by comparing the extracted results to a highly accurate, manually curated BEL KG curated by domain experts.}
}
@article{TAO2025127758,
title = {Speaker identification combining role knowledge graph correction and contextual block attention},
journal = {Expert Systems with Applications},
volume = {284},
pages = {127758},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127758},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425013806},
author = {Ye Tao and Fei Wang and Yanchang Cai and Wei Li},
keywords = {End-to-end speaker recognition, Knowledge graph, Contextual chunk attention},
abstract = {Character dialogue play a crucial role in novels, serving as a key element for understanding both the plot and character relationships. With advancements in artificial intelligence and natural language processing, dialogue speaker recognition has seen significant progress. In this paper, we integrate a character role knowledge graph with a self-training mechanism to perform the speaker recognition on Chinese novel quotations using large-scale novel corpora. Quotes in novels can generally be classified as explicit or implicit. Implicit quotes require identifying speakers from extensive contextual information, which poses challenges for existing models in handling long and detail-rich contexts. In addition, existing end-to-end speaker recognition methods are ineffective because they do not fully consider the relationship between context and quotes. In this paper, we first propose a Narrative Unit-based Context Selection (NUCS) algorithm for determining the context to which quotes belong. Secondly, a speaker recognition method based on Role Knowledge Graph Correction (RKGC) and Contextual Block Attention (CBA) is proposed. The proposed role knowledge graph correction algorithm improves speaker attribution by deeply analyzing role entities, their relationships, and relevant trigger words within quotations. It then refines candidate speaker probabilities obtained from the previous module. Additionally, the algorithm captures character relationships within the context using role mappings from existing novels. The CBA method introduces a block attention mechanism to capture character relationships within the context. It effectively computes the probability of each character being the speaker and determines the start and end indices of the most probable speaker segments. This allows the model to focus more precisely on speaker-related content, leading to more accurate speaker predictions. Experimental results demonstrate that our approach achieves competitive performance: on our self-constructed CNSI dataset, it attains 91.9% EM and 93.3% F1 scores. Although slightly lower than the SOTA SPC method (EM 92.3%, F1 93.6%), our approach demonstrates significant advantages in contextual reasoning capabilities, particularly for complex multi-character dialogue scenarios.}
}
@article{SU2024332,
title = {Chinese Nested Named Entity Recognition Algorithm Based on Knowledge Graph},
journal = {Procedia Computer Science},
volume = {243},
pages = {332-339},
year = {2024},
note = {The 4th International Conference on Machine Learning and Big Data Analytics for IoT Security and Privacy},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.041},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924020489},
author = {Qianqian Su and Zhenyu Yan and He Wang},
keywords = {Chinese Nested, Named Entity Recognition, Knowledge Graph, Information Extraction, Recognition Accuracy},
abstract = {Nested named entity recognition has been widely used in natural language processing, information extraction and other fields. However, the multiple boundaries of nested named entities make the recognition of single entity face great challenges. This article used the information in the knowledge graph to extract and annotate entities in the Chinese nested named entity recognition algorithm, and built a new named entity recognition model based on this. Experimental results showed that this article applied the knowledge graph to the Chinese nested named entity recognition task, and its accuracy could reach up to 95.3%, which has obvious advantages in complex structure processing, relationship extraction and entity links.}
}
@article{QIU2023105739,
title = {Information extraction and knowledge linkage of geological profiles and related contextual texts from mineral exploration reports for geological knowledge graphs construction},
journal = {Ore Geology Reviews},
volume = {163},
pages = {105739},
year = {2023},
issn = {0169-1368},
doi = {https://doi.org/10.1016/j.oregeorev.2023.105739},
url = {https://www.sciencedirect.com/science/article/pii/S0169136823004559},
author = {Qinjun Qiu and Yuxi Duan and Kai Ma and Liufeng Tao and Zhong Xie},
keywords = {Geological profiles, Natural language processing, Knowledge graph, Semantic relation},
abstract = {Geological profiles serve as invaluable repositories of data for knowledge extraction and the identification of geological entities, constituting a fundamental resource in scientific investigation. By encapsulating the stratigraphic arrangement and rock characteristics, these profiles offer a visually compelling and efficacious representation of stratigraphy along the vertical axis. Within geological profiles, intricate semantic associations frequently emerge, connecting various objects such as strata, rocks, and minerals across two or more profiles. These relationships often unveil inherent correlations or evolutionary attributes among geological entities. Understanding the general semantic relations between geological objects by manually scrutinizing vast quantities of profiles proves arduous, if not impracticable. In response, this study proposes an innovative framework capable of automated comprehension of extensive geological profiles and their associated contextual textual information. Built upon a natural language processing model and employing image vectorization techniques, this framework enables extraction of semantic information encompassing geological profiles, semantic relationships, and properties. In this article, we propose the comprehensive architecture of the framework, delineating its individual modules and subsequently applying it within a case study. Moreover, we conduct exploratory visualizations utilizing a knowledge graph to depict the extracted semantic information concerning diverse geological objects. The proposed framework presents a promising approach for supporting large-scale content analysis in the realm of stratigraphic research, facilitating advancements in our understanding of the evolution of stratigraphy.}
}
@article{BAHR2025100807,
title = {Knowledge graph enhanced retrieval-augmented generation for failure mode and effects analysis},
journal = {Journal of Industrial Information Integration},
volume = {45},
pages = {100807},
year = {2025},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2025.100807},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X25000317},
author = {Lukas Bahr and Christoph Wehner and Judith Wewerka and José Bittencourt and Ute Schmid and Rüdiger Daub},
keywords = {FMEA, Risk assessment, Knowledge graph, Retrieval-augmented generation, Large language models},
abstract = {Failure mode and effects analysis (FMEA) is an essential tool for mitigating potential failures, particularly during the ramp-up phases of new products. However, its effectiveness is often limited by the reasoning capabilities of the FMEA tools, which are usually tabular structured. Meanwhile, large language models (LLMs) offer novel prospects for advanced natural language processing tasks. However, LLMs face challenges in tasks that require factual knowledge, a gap that retrieval-augmented generation (RAG) approaches aim to fill. RAG retrieves information from a non-parametric data store and uses a language model to generate responses. Building on this concept, we propose to enhance the non-parametric data store with a knowledge graph (KG). By integrating a KG into the RAG framework, we aim to leverage analytical and semantic question-answering capabilities for FMEA data. This paper contributes by presenting set-theoretic standardization and a schema for FMEA data, an algorithm for creating vector embeddings from the FMEA-KG, and a KG-enhanced RAG framework. Our approach is validated through a user experience design study, and we measure the precision and performance of the context retrieval recall.}
}
@article{MELLUSO2022103676,
title = {Enhancing Industry 4.0 standards interoperability via knowledge graphs with natural language processing},
journal = {Computers in Industry},
volume = {140},
pages = {103676},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2022.103676},
url = {https://www.sciencedirect.com/science/article/pii/S0166361522000732},
author = {Nicola Melluso and Irlan Grangel-González and Gualtiero Fantoni},
keywords = {Standards, Interoperability, Natural language processing, Knowledge graphs, Industry 4.0},
abstract = {Industry 4.0 (I4.0) has brought several challenges related to the need to acquire and integrate large amounts of data from multiple sources in order to integrate these elements into an automated manufacturing system. Establishing interoperability is crucial to meet these challenges, and standards development and adoptions play a key role in achieving this. Therefore, academics and industrial stakeholders must join their forces in order to develop methods to enhance interoperability and to mitigate possible conflicts between standards. The aim of this paper is to propose an approach that enhances interoperability between standards through the combined use of Natural Language Processing (NLP) and Knowledge Graphs (KG). In particular, the proposed method is based on the measurement of semantic similarity among the textual content of standards documents belonging to different standardization frameworks. The present study contributes to the research and practice in three ways. First, it fills research gaps concerning the synergy of NLP, KGs and I4.0. Second, it provides an automatic method that improves the process of creating, curating and enriching a KG. Third, it provides qualitative and quantitative evidence of Semantic Interoperability Conflicts (SICs). The experimental results of the application of the proposed method to the I4.0 Standards Knowledge Graph (I40KG) show that different standards are still struggling to use a shared language and that there exists a strong different in the view of I4.0 proposed by the two main standardization frameworks (RAMI and IIRA). By automatically enriching the I40KG with a solid experimental approach, we are paving the way for actionable knowledge which has been extracted from the PDFs and made available in the I40KG.}
}
@article{ZHANG2025103468,
title = {A knowledge graph-enhanced large language model for question answering of hydraulic structure safety management},
journal = {Advanced Engineering Informatics},
volume = {66},
pages = {103468},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103468},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625003611},
author = {Dongliang Zhang and Gang Ma and Tongming Qu and Xudong Wang and Wei Zhou and Xiaomao Wang},
keywords = {Hydraulic structure safety management, Domain knowledge QA, Intent parsing, Knowledge graph, Large language model},
abstract = {Early detection and mitigation of hazards in hydraulic structures are crucial for effectively reducing economic and life losses. However, traditional hydraulic structure safety management methods rely on error-prone individual experience and emergency manuals, which are insufficient for making timely, scientifically informed decisions during crises. To address this challenge, this study presents an AI-driven framework for hydraulic structure safety management based on knowledge-based question answering. First, an ontology model was developed through a detailed analysis of safety management texts. Next, a partition fusion Kolmogorov-arnold network (PFKAN) enhanced with attention mechanisms was designed to jointly extract entities and relational knowledge. A safety management knowledge graph (KG) was then constructed from this knowledge. Subsequently, a large language model (LLM) was employed with a voting strategy to interpret query intent and extract relevant domain-specific knowledge from the KG. Finally, domain knowledge was integrated into the LLM to generate professional responses. Experimental results show that the F1 scores for entity and relation extraction with PFKAN reached 0.91 and 0.90, respectively, and the F1 score for query intent parsing with the voting strategy was 0.95, demonstrating competitive performance. The KG-enhanced LLM significantly improves decision-making in hydraulic structure safety management, providing an accurate and scalable tool for engineering safety managers.}
}
@article{ONG2025126648,
title = {Dynamic link prediction: Using language models and graph structures for temporal knowledge graph completion with emerging entities and relations},
journal = {Expert Systems with Applications},
volume = {272},
pages = {126648},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.126648},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425002702},
author = {Ryan Ong and Jiahao Sun and Yi-Ke Guo and Ovidiu Serban},
keywords = {Dynamic knowledge graphs, Language models, Link prediction},
abstract = {Knowledge graphs (KGs) represent real-world facts through entities and relations. However, static KGs fail to capture continuously emerging entities and relations over time. Temporal knowledge graphs address this by incorporating time information or providing multiple sequential snapshots of a static knowledge graph. Most existing work focuses on static KGs with fixed sets of entities and relations, meaning existing methods still struggle to encode emerging entities and relations. Therefore, we propose a novel methodology of combining language models and graph structure to enable the encoding of unseen entities and relations for temporal KG completion. Specifically, we encode relations with RoBERTa and entities using neighbouring relations alongside the entity’s relation type to provide contextual information. We evaluate our methodology on three datasets with emerging entities and relations over temporal snapshots: LKGE-Hybrid, FB-MBE, and the mergers and acquisitions domain TKGQA dataset. Our experiments show that our model achieves new state-of-the-art results on FB-MBE and LKGE-Hybrid while providing strong benchmark results for the TKGQA dataset. Our ablation studies show us that graph structure information is only beneficial if there is sufficient connectivity with the knowledge graph since sparser knowledge graphs can lead to noisy signals. We also explore the performance of Llama v2 on temporal link prediction, and the results show that current LLMs struggle with domain-specific temporal link prediction. Overall, our work provides an essential advance around effectively encoding continuously emerging entities and relations for temporal link prediction across evolving knowledge graphs over time.}
}
@article{TASKIRAN2025109318,
title = {A knowledge-graph-based pharmaceutical engineering chatbot for drug discovery},
journal = {Computers & Chemical Engineering},
volume = {203},
pages = {109318},
year = {2025},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2025.109318},
url = {https://www.sciencedirect.com/science/article/pii/S0098135425003205},
author = {Naz Pinar Taskiran and Chia-En Jacklyn Tsai and Shuxin Huang and Arijit Chakraborty and Venkat Venkatasubramanian},
keywords = {Hybrid AI, Pharmaceutical engineering, Ontology, Knowledge graph, Neo4j, LLMs},
abstract = {Despite their success in day-to-day applications, ChatGPT and other large language models (LLMs) have not covered as much ground in scientific and engineering domains. One key challenge is the abundance of domain-specific terminology, which an LLM is not trained to extract in accordance with the underlying physical laws. Such black-box models can also lead to unreliable results or hallucinations. Hybrid AI, which combines data-driven and symbolic methods, leverages domain knowledge to add explainability and reliability to answers. Our group has previously developed a domain-informed ontology-based information extraction tool called SUSIE, which extracts key terms and their context to present them to the user as knowledge graphs (KGs). Although KGs are used to visualize relationships between different entities, they are not easily accessible for user questions. However, they serve as a structured input for LLMs. Thus, KGs can efficiently query a corpus of pharmaceutical documents, streamlining drug discovery and manufacturing processes. In this work, we propose methods to improve the information extraction capabilities of SUSIE by expanding its knowledge base and improving its ability to understand scientific material through a sentence-restructuring module. Additionally, we present a customized question-and-answer module that enables the user to query from generated KGs and get an answer in natural language. Unlike black-box models such as those purely powered by OpenAI’s models and the LangChain GraphQA packages, combining our KGs with Neo4j limits hallucinations and provides reliable and traceable answers in a user-friendly chatbot interface.}
}
@article{FENG2025332,
title = {Temporal Knowledge Graph Embedding with Pre-trained Language Model},
journal = {Procedia Computer Science},
volume = {264},
pages = {332-345},
year = {2025},
note = {International Neural Network Society Workshop on Deep Learning Innovations and Applications 2025},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.07.144},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925021945},
author = {Wenying Feng and Jianming Li and Haiyan Wang and Zhaoquan Gu},
keywords = {Knowledge graph, knowledge graph representation, temporal knowledge graph, pre-trained language model},
abstract = {Large language models (LLMs) have demonstrated exceptional performance in natural language processing. This also leads to extensive research on knowledge extraction, knowledge fusion, knowledge representation, and knowledge completion using pre-trained language models (PLMs). Most of the existing works focus on static multi-relational knowledge graphs (KGs). In contrast, temporal knowledge graphs (TKGs) incorporate temporal information, whereas lack of research utilizing PLMs or LLMs. In this paper, we introduce PT2KGC, a temporal knowledge graph embedding model which employs the pre-trained language model for TKG completion and extrapolation. We present three modeling approaches of PT2KGC to model temporal knowledge: original knowledge embedding, explicit time modeling, and implicit time modeling. PT2KGC(Org.) relies solely on static knowledge; PT2KGC(Exp.) explicitly incorporates timestamps into quadruples; and PT2KGC(Imp.) models time implicitly through dataset reconstruction. We conduct experiments on two public TKG datasets. The results demonstrate the effectiveness of pre-trained language models for TKG embedding. Experiment results on three types of tasks show that all three modeling methods of PT2KGC outperform existing models. Additionally, we compare the performance of PT2KGC under different time modeling approaches.}
}
@article{ZHANG2024119434,
title = {A knowledge graph-based inspection items recommendation method for port state control inspection of LNG carriers},
journal = {Ocean Engineering},
volume = {313},
pages = {119434},
year = {2024},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2024.119434},
url = {https://www.sciencedirect.com/science/article/pii/S0029801824027720},
author = {Xiyu Zhang and Chengyong Liu and Yi Xu and Beiyan Ye and Langxiong Gan and  {Yaqing Shu}},
keywords = {Port state control inspection, Liquefied natural gas carriers, Knowledge graph, Knowledge graph embedding, Knowledge graph-based recommendation model},
abstract = {The safe navigation and operation of Liquified natural gas (LNG) carriers is crucial in maritime transportation. The Port State Control (PSC) inspection is the primary method for identifying deficiencies in vessels. Different from other vessels, the PSC inspection of LNG carriers requires knowledge of specialized transport equipment. Therefore, effective management of inspection knowledge and precise targeting of inspection items are particularly crucial. Knowledge graph is an efficient way to manage knowledge in the context of artificial intelligence. In this study, an LNG carrier PSC inspection knowledge graph is constructed for fusing multisource knowledge data from PSC inspection. Additionally, a knowledge graph-based recommendation model, namely the improved knowledge graph convolutional network combined with pretrained translation embedding (PT-KGCN), is developed to recommend inspection items and knowledge. The PT-KGCN model first carries out knowledge graph embedding. It then predicts possible deficiencies on the basis of historical PSC inspection data. Finally, it recommends inspection items by correlating the predicted deficiencies with knowledge from the knowledge graph. The results show that more than 87% of defective items in historical data are correctly predicted. The research findings can provide ideas for the practical application of knowledge data in the inspection fields.}
}
@article{MA2025112018,
title = {A knowledge graph dataset for broiler farming automatically constructed based on a large language model},
journal = {Data in Brief},
volume = {62},
pages = {112018},
year = {2025},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2025.112018},
url = {https://www.sciencedirect.com/science/article/pii/S2352340925007401},
author = {Nan Ma and Fantao Kong and Jifang Liu and Chenyang Zhang and Chenxv Zhao and Shanshan Cao and Wei Sun},
keywords = {Broiler farming, Large language model, Extraction of knowledge, Knowledge graph},
abstract = {With the rapid advancement of artificial intelligence, intelligent farming has become a key trend in modern agriculture. In particular, the application of intelligent systems in broiler farming is essential for enhancing production efficiency and optimizing management practices. Broiler farming is a complex process involving multiple interrelated components. However, existing knowledge graphs primarily focus on disease and prevention, making it difficult to capture the intricate interdependencies within the farming process. This limits the effectiveness of knowledge-based support in decision-making. To develop a high-quality broiler farming knowledge system, this study adopts large language modeling technology to integrate a Chinese corpus and construct a comprehensive knowledge graph dataset covering four core dimensions: broiler breeds, farming environment, feeding management, and disease prevention. The construction of the dataset involved three key stages. First, text scanning was used to extract information from farming-related literature, while web crawlers collected data from authoritative online sources. The data were then cleaned and manually validated to ensure accuracy and consistency. Second, the DeepKE knowledge extraction framework is used to automatically extract triples related to broiler farming from the text. These are then used as prompts to guide large-scale pre-trained language models (LLMs) to complete and optimize the knowledge, ultimately constructing a relatively complete knowledge graph of broiler farming. Finally, the structured knowledge was stored in a Neo4j graph database to support efficient querying and reasoning. The dataset not only provides researchers and farms with multidimensional knowledge of the broiler farming domain, but also supports visual management and analysis, enables data-driven inference through large models, and offers new approaches to optimize farming strategies and enhance production efficiency.}
}
@article{CHEN2025131230,
title = {Knowledge graph and large language model integration with focus on educational applications: A survey},
journal = {Neurocomputing},
volume = {654},
pages = {131230},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.131230},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225019022},
author = {Guanyu Chen and Tao Song and Quanyu Wang and Zheng Ma and Jun Hu and Qi Li and Chunming Wu},
keywords = {Knowledge graph, Large language model, Educational application, Retrieval augmented generation, Pre-training},
abstract = {In recent years, artificial intelligence (AI) technology has made significant advancements, particularly in the areas of large language models (LLMs) and knowledge graphs (KGs). KGs excel at structured knowledge representation and reasoning, offering interpretability; however, they are costly to construct, have limited coverage, and lack natural language processing capabilities. Conversely, LLMs possess powerful language understanding and generation abilities, but they rely heavily on vast amounts of data, are prone to “hallucinations," and lack interpretability. The integration of these two approaches is an inevitable trend for achieving stronger and more reliable AI applications, and has become a hot topic of research. Simultaneously, the combination of LLMs and KGs perfectly aligns with the pressing needs of the education field for precise reasoning and personalized services, addressing the shortcomings of traditional teaching methods and providing support for intelligent education. In light of this, this paper undertakes work in the following three key areas. Firstly, the concepts and technologies of both LLMs and KGs, along with their applications in education, are introduced. On this basis, the paper then delves into a discussion of the methods for integrating LLMs and KGs, and reviews related research progress. Finally, the paper focuses on specific educational scenarios, such as intelligent tutoring systems, intelligent learning companions, and intelligent evaluation systems, to explore the collaborative application of LLMs and KGs. The aim of this paper is to provide researchers in the field with a systematic understanding of LLMs and KGs, and to offer valuable references for future AI-driven educational innovation.}
}
@article{BABAIHA2023100078,
title = {A natural language processing system for the efficient updating of highly curated pathophysiology mechanism knowledge graphs},
journal = {Artificial Intelligence in the Life Sciences},
volume = {4},
pages = {100078},
year = {2023},
issn = {2667-3185},
doi = {https://doi.org/10.1016/j.ailsci.2023.100078},
url = {https://www.sciencedirect.com/science/article/pii/S2667318523000223},
author = {Negin Sadat Babaiha and Hassan Elsayed and Bide Zhang and Abish Kaladharan and Priya Sethumadhavan and Bruce Schultz and Jürgen Klein and Bruno Freudensprung and Vanessa Lage-Rupprecht and Alpha Tom Kodamullil and Marc Jacobs and Stefan Geissler and Sumit Madan and Martin Hofmann-Apitius},
keywords = {Knowledge graphs, Relation extraction, Natural language processing, Biomedical text mining, Biological expression language (BEL), Human brain pharmacome (HBP)},
abstract = {Background
Biomedical knowledge graphs (KG) have become crucial for describing biological findings in a structured manner. To keep up with the constantly changing flow of knowledge, their embedded information must be regularly updated with the latest findings. Natural language processing (NLP) has created new possibilities for automating this upkeep by facilitating information extraction from free text. However, due to annotated and labeled biomedical data limitations, the development of completely autonomous information extraction systems remains a substantial scientific and technological hurdle. This study aims to explore methodologies best suited to support the automatic extraction of causal relationships from biomedical literature with the aim of regular and rapid updating of disease-specific pathophysiology mechanism KGs.
Methods
Our proposed approach first searches and retrieves PubMed abstracts using the desired terms and keywords. The extension corpora are then passed through the NLP pipeline for automatic information extraction. We then identify triples representing cause-and-effect relationships and encode this content using the Biological Expression Language (BEL). Finally, domain experts perform an analysis of the completeness, relevance, accuracy, and novelty of the extracted triples.
Results
In our test scenario, which is focused on the KG regarding the phosphorylation of the Tau protein, our pipeline successfully contributed novel data, which was then subsequently used to update the KG leading to the identification of six additional upstream regulators of Tau phosphorylation.
Conclusion
Here, it is demonstrated that the NLP-based workflow we created is capable of rapidly updating pathophysiology mechanism graphs. As a result, production-scale, semi-automated updating of pre-existing, curated mechanism graphs is enabled.}
}
@article{ASGARIBIDHENDI2021100638,
title = {FarsBase-KBP: A knowledge base population system for the Persian Knowledge Graph},
journal = {Journal of Web Semantics},
volume = {68},
pages = {100638},
year = {2021},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2021.100638},
url = {https://www.sciencedirect.com/science/article/pii/S1570826821000135},
author = {Majid Asgari-Bidhendi and Behrooz Janfada and Behrouz Minaei-Bidgoli},
keywords = {Knowledge extraction, Knowledge Graph, Canonicalization, Natural Language Processing, Persian language},
abstract = {While most of the knowledge bases already support the English language, there is only one knowledge base for the Persian language, known as FarsBase, which is automatically created via semi-structured web information. Unlike English knowledge bases such as Wikidata, which have tremendous community support, the population of a knowledge base like FarsBase must rely on automatically extracted knowledge. Knowledge base population can let FarsBase keep growing in size, as the system continues working. In this paper, we present a knowledge base population system for the Persian language, which extracts knowledge from unlabelled raw text, crawled from the Web. The proposed system consists of a set of state-of-the-art modules such as an entity linking module as well as information and relation extraction modules designed for FarsBase. Moreover, a canonicalization system is introduced to link extracted relations to FarsBase properties. Then, the system uses knowledge fusion techniques with minimal intervention of human experts to integrate and filter the proper knowledge instances, extracted by each module. To evaluate the performance of the presented knowledge base population system, we present the first gold dataset for benchmarking knowledge base population in the Persian language, which consisting of 22015 FarsBase triples and verified by human experts. The evaluation results demonstrate the efficiency of the proposed system.}
}
@incollection{GYRARD2022143,
title = {Chapter 8 - A naturopathy knowledge graph and recommendation system to boost the immune system: KISS: Knowledge-based Immune System Suggestion},
editor = {Sanju Tiwari and Fernando {Ortiz Rodriguez} and M.A. Jabbar},
booktitle = {Semantic Models in IoT and eHealth Applications},
publisher = {Academic Press},
pages = {143-169},
year = {2022},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-323-91773-5},
doi = {https://doi.org/10.1016/B978-0-32-391773-5.00014-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323917735000145},
author = {Amelie Gyrard and Karima Boudaoud},
keywords = {Food knowledge graph food ontology, Naturopathy, Healthy diet, Nutrition, Immune system, Recommender system, Preventive health, Well-being, Wellness, Rule-based reasoning, Inference engine, FAIR principles},
abstract = {Because of COVID-19 worldwide pandemic, there is a need for any complementary solutions to boost the immune system. Nowadays, healthy lifestyle, fitness, and diet habits have become central applications in our daily life. We designed a Naturopathy Knowledge Graph for a recommender system to boost the immune system (KISS: Knowledge-based Immune System Suggestion). The Naturopathy Knowledge Graph is built from more than 50 ontology-based food projects, also released as the LOV4IoT-Food ontology catalog. The naturopathy data set is referenced on the Linked Open Data (LOD) cloud. The LOV4IoT-Food ontology catalog encourages researchers to follow FAIR principles and share their reproducible experiments by publishing online their ontologies, data sets, rules, etc. The set of the ontology code shared online can be semiautomatically processed, if not available, the scientific publications describing the food ontologies are semiautomatically processed with Natural Language Processing (NLP) techniques. We build the naturopathy recommender system that will suggest food to boost the immune system. The recommender system can be extended to address other advice such as aromatherapy and take into consideration medical devices to monitor patients' vital signals.}
}
@article{ASCIONE2024102320,
title = {Leveraging NLP and web knowledge graphs to harmonize locations: A case study on US patent transactions},
journal = {World Patent Information},
volume = {79},
pages = {102320},
year = {2024},
issn = {0172-2190},
doi = {https://doi.org/10.1016/j.wpi.2024.102320},
url = {https://www.sciencedirect.com/science/article/pii/S0172219024000607},
author = {Grazia Sveva Ascione and Andrea Vezzulli},
keywords = {Patent transactions, Patent assignee, Patent data harmonization, Natural language processing, Knowledge graph},
abstract = {In the present study, we introduce a novel methodology for the harmonization and standardization of locations associated with patent transactions recorded at the USPTO from 2005 to 2022. Using natural language processing (NLP) techniques in conjunction with search engine-based web knowledge graphs, our method comprises four phases: data pre-processing, semantic clustering, exploitation of web-knowledge graphs, and API-driven harmonization. Initiating our analysis with a dataset of 63,838 unique locations, our methodology effectively reduces this number by more than 50 %. This approach exhibits an accuracy rate of approximately 92 %. The resulting geolocated dataset of companies’ patent transactions offers a valuable resource for fine-grained geographical analyses of the markets for technology; in particular, we provide examples of relevant economic insights which can be learned from looking at the geographical patterns of those transactions.}
}
@article{DIAZ2025179,
title = {Knowledge graphs in heterogeneous catalysis: Recent advances and future opportunities},
journal = {Chinese Journal of Chemical Engineering},
volume = {84},
pages = {179-189},
year = {2025},
issn = {1004-9541},
doi = {https://doi.org/10.1016/j.cjche.2025.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S1004954125002393},
author = {Raúl Díaz and Hongliang Xin},
keywords = {Heterogeneous catalysis, Knowledge graph, Ontology, Large language models, Deep learning},
abstract = {Knowledge graphs (KGs) offer a structured, machine-readable format for organizing complex information. In heterogeneous catalysis, where data on catalytic materials, reaction conditions, mechanisms, and synthesis routes are dispersed across diverse sources, KGs provide a semantic framework that supports data integration under the FAIR (Findable, Accessible, Interoperable, and Reusable) principles. This review aims to survey recent developments in catalysis KGs, describe the main techniques for graph construction, and highlight how artificial intelligence, particularly large language models (LLMs), enhances graph generation and query. We conducted a systematic analysis of the literature, focusing on ontology-guided text mining pipelines, graph population methods, and maintenance strategies. Our review identifies key trends: ontology-based approaches enable the automated extraction of domain knowledge, LLM-driven retrieval-augmented generation supports natural-language queries, and scalable graph architectures range from a few thousand to over a million triples. We discuss state-of-the-art applications, such as catalyst recommendation systems and reaction mechanism discovery tools, and examine the major challenges, including data heterogeneity, ontology alignment, and long-term graph curation. We conclude that KGs, when combined with AI methods, hold significant promise for accelerating catalyst discovery and knowledge management, but progress depends on establishing community standards for ontology development and maintenance. This review provides a roadmap for researchers seeking to leverage KGs to advance heterogeneous catalysis research.}
}
@article{CHAUDHARY2025127612,
title = {Fact retrieval from knowledge graphs through semantic and contextual attention},
journal = {Expert Systems with Applications},
volume = {282},
pages = {127612},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127612},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425012345},
author = {Akhil Chaudhary and Enayat Rajabi and Somayeh Kafaie and Evangelos Milios},
keywords = {Querying, Knowledge graphs, Fact retrieval, Information extraction, BERT, Zero shot},
abstract = {Knowledge Graphs (KGs), such as DBpedia and ConceptNet, enhance Natural Language Processing (NLP) applications by providing structured information. However, extracting accurate data from KGs is challenging due to issues in entity detection, disambiguation, and relation classification, which often lead to errors and inefficiencies. We introduce Attention2Query (A2Q), an attention-driven approach that directly ranks and selects the most relevant facts, thus minimizing error propagation. A2Q centres on three key contributions: (1) Focused Node Selection, which streamlines graph traversal; (2) Global Attention Alignment, improving retrieval by comparing facts against the query text; and (3) Contextual Re-ranking, enabling on-the-fly adjustments of fact importance based on evolving query context. Experimental results across multiple tasks and datasets show that A2Q substantially outperforms baseline methods, including those in zero-shot settings, achieving higher retrieval accuracy with reduced computational overhead.}
}
@article{HAO2025105669,
title = {Uncovering compound urban crises with large language model-assisted knowledge graph construction},
journal = {International Journal of Disaster Risk Reduction},
volume = {127},
pages = {105669},
year = {2025},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2025.105669},
url = {https://www.sciencedirect.com/science/article/pii/S2212420925004935},
author = {Haiyan Hao and Xiaorui Chen and Yudi Chen and Nan Li},
abstract = {Urban areas frequently face an array of crises, including natural hazards, public health emergencies, and technological disruptions, which are becoming more interconnected due to the projected growing trend of climate change and rapid urbanization. While prior studies have investigated compound events, much of the focus has been on weather- and climate-related events, leaving a gap in understanding compound urban crises that integrate diverse types of crises at the city level. To address this, we propose a novel method for mining location-specific knowledge of compound urban crises using large language models (LLMs), enhanced with techniques such as Retrieval-Augmented Generation (RAG), iterative self-refinement, and prompt engineering. By applying the method to 1941 news articles collected from Shenzhen, China, we constructed knowledge graphs (KGs) that reveal the interconnections and compounding patterns among 13 distinct types of urban crises. The results demonstrate the effectiveness of the proposed method in identifying and mapping compound urban crises at the city level. The findings offer practical implications for urban crisis management, equipping cities with tailored knowledge to better anticipate and respond to complex, interrelated crisis scenarios.}
}
@article{RUEHLE20251534,
title = {Natural language processing for automated workflow and knowledge graph generation in self-driving labs††Electronic supplementary information (ESI) available: Links to further resources, detailed description of dataset creation steps, and further figures and tables. See DOI: https://doi.org/10.1039/d5dd00063g},
journal = {Digital Discovery},
volume = {4},
number = {6},
pages = {1534-1543},
year = {2025},
issn = {2635-098X},
doi = {https://doi.org/10.1039/d5dd00063g},
url = {https://www.sciencedirect.com/science/article/pii/S2635098X25000786},
author = {Bastian Ruehle},
abstract = {Natural language processing with the help of large language models such as ChatGPT has become ubiquitous in many software applications and allows users to interact even with complex hardware or software in an intuitive way. The recent concepts of Self-Driving Labs and Material Acceleration Platforms stand to benefit greatly from making them more accessible to a broader scientific community through enhanced user-friendliness or even completely automated ways of generating experimental workflows that can be run on the complex hardware of the platform from user input or previously published procedures. Here, two new datasets with over 1.5 million experimental procedures and their (semi)automatic annotations as action graphs, i.e., structured output, were created and used for training two different transformer-based large language models. These models strike a balance between performance, generality, and fitness for purpose and can be hosted and run on standard consumer-grade hardware. Furthermore, the generation of node graphs from these action graphs as a user-friendly and intuitive way of visualizing and modifying synthesis workflows that can be run on the hardware of a Self-Driving Lab or Material Acceleration Platform is explored. Lastly, it is discussed how knowledge graphs – following an ontology imposed by the underlying node setup and software architecture – can be generated from the node graphs. All resources, including the datasets, the fully trained large language models, the node editor, and scripts for querying and visualizing the knowledge graphs are made publicly available.}
}
@article{SHAO2024105636,
title = {Integrated natural language processing method for text mining and visualization of underground engineering text reports},
journal = {Automation in Construction},
volume = {166},
pages = {105636},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105636},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524003728},
author = {Ruiqi Shao and Peng Lin and Zhenhao Xu},
keywords = {Text mining, Natural language processing, Information extraction, Visualization, Underground engineering},
abstract = {Efficient extraction of information from underground engineering text reports can significantly enhance project efficiency and quality. Traditional manual analysis is often time-consuming and subjective, resulting in low utilization of critical information due to the absence of advanced text mining techniques. This paper presents a comprehensive method for mining and visualizing data from text reports. A BERT-BiLSTM-CRF model is employed for Chinese text segmentation. To improve keyword and summary interpretability, the BK-TextRank algorithm is proposed. Key term distribution is visually represented using word clouds, while semantic similarity analysis assesses relationships between word embeddings through network visualization. Latent Dirichlet Allocation topic modeling is integrated to explore and visualize underlying themes. Additionally, a web-based platform was developed to facilitate rapid extraction and visualization of report data. The method's effectiveness in analyzing reports and supporting decision-making processes in underground engineering projects has been validated.}
}
@article{SHAN2025103655,
title = {Large language Models-empowered automatic knowledge graph development based on multi-modal data for building health resilience},
journal = {Advanced Engineering Informatics},
volume = {68},
pages = {103655},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103655},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625005488},
author = {Tianlong Shan and Fan Zhang and Albert P.C. Chan and Shiyao Zhu and Kaijian Li},
keywords = {Building health resilience, Knowledge graph, Large language models, Multi-modal data, Rainstorm},
abstract = {Improving the health resilience of building (BHR) helps keep stable health status of both the building and its occupants under disasters. As BHR is an emerging concept, there is no structured knowledge graph to understand the whole process of BHR under disasters. Therefore, this study aims to build a structured BHR knowledge graph based on multi-modal data, providing sufficient structured knowledge for BHR enhancement. An automated knowledge graph construction approach is proposed to empower the ontology design and triple extraction by large language models (LLMs), and validation processes based on In-context Learning (ICL) prompts. A case study is conducted to construct the knowledge graph of BHR under rainstorms in Hong Kong. The performance of the proposed LLMs-empowered knowledge extraction is also validated based on natural language processing metrics and LLMs-based Evaluation (LLMs-Eval). BHR knowledge graph indicates the potential relations between disasters, factors, response actions, and the health status of the building and occupants, and provides insight to guide the BHR enhancement. The superiority of the proposed LLMs-empowered automated knowledge graph construction approach is proven, implying LLMs have great potential in knowledge graph construction, not only for BHR but also for other concepts that require structured knowledge for further explorations and analyses.}
}
@article{WANG2025100268,
title = {A lightweight knowledge graph-driven question answering system for field-based mineral resource survey},
journal = {Applied Computing and Geosciences},
volume = {27},
pages = {100268},
year = {2025},
issn = {2590-1974},
doi = {https://doi.org/10.1016/j.acags.2025.100268},
url = {https://www.sciencedirect.com/science/article/pii/S2590197425000503},
author = {Mingguo Wang and Chengbin Wang and Jianguo Chen and Bo Wang and Wei Wang and Xiaogang Ma and Jiangtao Ren and Zichen Li and Yicai Ye and Jiakai Zhang and Yue Wang},
keywords = {Knowledge graph-driven question answering, Question answering, Sentence transformer, Mineral resource survey, Intelligent service},
abstract = {Geoscience data associated with mineral resource surveys have become essential digital assets for governments and mining companies. The rapid increase in the volume of geoscience data makes it challenging to acquire knowledge quickly. In this study, we proposed and built a workflow that employs knowledge graph techniques, deep learning, question templates, and matching algorithms to provide a lightweight question-answering service for field-based geologists involved in mineral resource surveys. Initially, we utilized deep-learning-based geological entities and their semantic relation recognition, along with relational data mapping, to construct the mineral resource survey knowledge graph based on the ontology model. We then employed question template matching, a geological entity recognition model, and a sentence transformer to determine the optimal question template and generate a query statement for knowledge acquisition from a knowledge graph based on the Cypher language. Subsequently, we utilized a subgraph and a short abstract to express the results. The comparison with large language models and retrieval-augmented generation indicates that our solution is suitable for field-based mineral source surveys in a poor network environment with low-performance devices, data privacy concerns, and narrowly focused topics. The results also suggest that further studies on geoscience pre-trained models, an informative library of question templates, and multimodal knowledge graphs are necessary to improve the performance of the knowledge graph-driven question-answering system.}
}
@article{LI2024100164,
title = {A hybrid knowledge graph for efficient exploration of lithostratigraphic information in open text data},
journal = {Applied Computing and Geosciences},
volume = {22},
pages = {100164},
year = {2024},
issn = {2590-1974},
doi = {https://doi.org/10.1016/j.acags.2024.100164},
url = {https://www.sciencedirect.com/science/article/pii/S2590197424000119},
author = {Wenjia Li and Xiaogang Ma and Xinqing Wang and Liang Wu and Sanaz Salati and Zhong Xie},
keywords = {Knowledge graph, Stratigraphy, Natural language processing, Relationship extraction, Bidirectional encoder representation from transformers (BERT), Data mining},
abstract = {Rocks formed during different geologic time record the diverse evolution of the geosphere and biosphere. In the past decades, substantial geoscience data have been made open access, providing invaluable resources for studying the stratigraphy in different regions and at different scales. However, many open datasets have information recorded in natural language with heterogeneous terminologies, short of efficient approaches to analyze them. In this research, we constructed a hybrid Stratigraphic Knowledge Graph (StraKG) to help address this challenge. StraKG has two layers, a simple schema layer and a rich instance layer. For the schemas, we used a short but functional list of classes and relationships, and then incorporated community-recognized terminologies from geological dictionaries. For the instances, we used natural language processing techniques to analyze open text data and obtained massive records, such as rocks and spatial locations. The nodes in the two layers were associated to establish a consistent structure of stratigraphic knowledge. To verify the functionality of StraKG, we applied it to the Baidu encyclopedia, the largest online Chinese encyclopedia. Three experiments were implemented on the topics of stratigraphic correlation, spatial distribution of ophiolite in China, and spatio-temporal distribution of open lithostratigraphic data. The results show that StraKG can provide strong knowledge reference for stratigraphic studies. Used together with data exploration and data mining methods, StraKG illustrates a new approach to analyze the open and big text data in geoscience.}
}
@article{WEN2023e20390,
title = {A systematic knowledge graph-based smart management method for operations: A case study of standardized management},
journal = {Heliyon},
volume = {9},
number = {10},
pages = {e20390},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e20390},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023075989},
author = {Peihan Wen and Yiming Zhao and Jin Liu},
keywords = {Knowledge graph, Operation management, Quality management system, Knowledge management},
abstract = {Standardized routine operation management (SROM) has been widely accepted and applied by kinds of enterprises and played a key supporting role. With full use of the emerging knowledge-based smart management technology, SROM will further increase comprehensive efficiency and save human resources greatly at the same time, especially for small and medium enterprises (SMEs). Hence, we propose a systematic knowledge-based smart management method to transfer SROM activities from human operations to automatic response by means of knowledge explicitation, organization, sharing and reusing, which can be further achieved by employing knowledge graph. We took a typical SROM instance, ISO 9000 implementation management, as an example to validate the transformation from human activities to knowledge graph-based automatic operation. We firstly analyzed characteristics of domain knowledge and constructed an ontology model according to the knowledge stability. Secondly, a hybrid knowledge graph construction and dynamic updating framework together with related algorithms were designed by deliberately integrating semantic similarity calculation and natural language processing. Thirdly, we developed a question-answering mechanism and reasoning system based on the ISO 9000 implementation knowledge graph to support automatic decision and feedback for ISO 9000 routine operation management including knowledge learning and processes auditing. Finally, the practicability and effectiveness of SROM knowledge graph has been validated in a SME in China, realizing the application of question-answering, job responsibility recommendation, conflict detection, semantic detection, multidimensional statistical analysis. The proposed method can also be generalized to support auxiliary optimization decision, vertical risk control, operation mode analysis, optimization model improvement experience and so on.}
}
@article{ZHANG202599,
title = {Osteosarcoma knowledge graph question answering system: deep learning-based knowledge graph and large language model fusion},
journal = {Intelligent Medicine},
volume = {5},
number = {2},
pages = {99-110},
year = {2025},
issn = {2667-1026},
doi = {https://doi.org/10.1016/j.imed.2024.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S2667102625000269},
author = {Lulu Zhang and Weisong Zhao and Zhiwei Cheng and Yafei Jiang and Kai Tian and Jia Shi and Zhenyu Jiang and Yingqi Hua},
keywords = {Osteosarcoma, Knowledge graph, Large language model, Text mining},
abstract = {Objective
Osteosarcoma is a prevalent primary malignant bone tumor in children and adolescents, accounting for approximately 5 % of childhood malignancies. Because of its rarity and biological complexity, treatment breakthroughs for osteosarcoma have been limited. To advance research in this field, we aimed to construct the first comprehensive osteosarcoma knowledge graph (OSKG) using the PubMed database.
Methods
A systematic search of PubMed (2003–2023) using the keyword “osteosarcoma” yielded 25,415 abstracts. Leveraging BioBERT, pretrained on biomedical corpora and fine-tuned with osteosarcoma-specific manual annotations, we identified 16 entity types and 17 biological relationships. The extracted elements were synthesized to create the OSKG, resulting in a deep learning-based knowledge base to explore osteosarcoma pathogenesis and molecular mechanisms. We then developed a specialized question-answering system (knowledge graph question answering (KGQA)) powered by ChatGLM3. This system employs advanced natural language processing and incorporates the OSKG to ensure optimal response quality and accuracy.
Results
The pretrained BioBERT averaged > 92 % accuracy in entity and relationship training. Evaluation using 100 pairs of gold-standard quizzes showed that the final quiz system outperformed other large language models in accuracy and robustness.
Conclusion
The system is designed to provide accurate disease-related queries and answers, effectively facilitating knowledge acquisition and reasoning in medical research and clinical practice. This project offers a robust tool for osteosarcoma research and promotes the deep integration of knowledge graphs and artificial intelligence technologies in the medical field.}
}
@article{MELLINAANDREU2025114507,
title = {Data-driven interpretation of dimensions in an embedding language model based on a reference knowledge graph},
journal = {Knowledge-Based Systems},
pages = {114507},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.114507},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125015461},
author = {Jose L. Mellina-Andreu and Alejandro Cisterna-García and Juan A. Botía},
keywords = {NLP, Explainable AI, Ontologies, LLMs},
abstract = {As language models are used in more applications, a key problem has become clear: their numerical embeddings are hard to interpret because it is unclear how each part of the vector relates to real-world meanings in specific fields. The prevailing embedding methods are inadequate in their current state, as they are unable to effectively bridge the gap between mathematical representations and human-understandable knowledge structures. The present study proposes a novel framework that explicitly links ontology classes to specific embedding dimensions through a dual-component architecture combining a text encoder that produces the target embedding dimensions with domain knowledge graphs. The Area Under the Interpretability Curve (AUIC) metric is introduced as a means to systematically evaluate model-alignment with ontological concepts. The analysis reveals that targeted dimensional mapping enables direct interpretation of individual vector components through ontological terms. The practical applications of this framework are illustrated through case studies in biomedical contexts, demonstrating enhanced model transparency without compromising performance. This approach establishes a measurable pathway for reconciling statistical language representations with structured domain knowledge, particularly benefiting fields requiring precise concept alignment like biomedicine. The implementation is publicly available at: https://github.com/Mellandd/DEIBO.}
}
@article{LU2025105129,
title = {Using social media data to construct and analyze knowledge graph for "7.20" Henan rainstorm flood event},
journal = {International Journal of Disaster Risk Reduction},
volume = {116},
pages = {105129},
year = {2025},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2024.105129},
url = {https://www.sciencedirect.com/science/article/pii/S2212420924008914},
author = {Haipeng Lu and Shuliang Zhang and Yu Gao and Hengxu Jin and Pengcheng Zhao and Yixuan Gao and Yating Li and Wenxuan Wang and Yixuan Zhang},
keywords = {Social media, Text classification, Natural language processing (NLP), Knowledge graph, Spatio-temporal evolution analysis, "7.20″ Henan rainstorm flood event},
abstract = {Understanding the spatio-temporal evolution of rainstorm flood events is crucial for disaster prevention and mitigation. Social media data are highly beneficial for spatio-temporal analysis and timely perception of rainstorm flood events. However, effectively organizing and managing social media data remains an urgent problem, for which a knowledge graph is considered a feasible solution. This study proposes a framework for constructing a knowledge graph for rainstorm flood events using social media data, and analyzes the spatio-temporal evolution of these events based on it. Using the "7.20″ Henan rainstorm flood event as an example, we first conduct the cognition and representation of the rainstorm flood event, constructing an event knowledge representation model. The ERNIE-SVM model is then used for event classification, and knowledge is extracted using the BERT-BiLSTM-CRF hybrid model and the DRCC-DPA method. Finally, we integrate event knowledge and complete the construction of the knowledge graph, which is then used to analyze the spatio-temporal evolution of the events. The constructed knowledge graph achieves structure and content completeness scores of 0.79 and 0.81, respectively. The main purpose of this study is to combine the timeliness of social media data with the organizational benefits of knowledge graphs, enabling rapid spatio-temporal analysis of rainstorm flood events.}
}
@article{GAO2025,
title = {Leveraging Medical Knowledge Graphs Into Large Language Models for Diagnosis Prediction: Design and Application Study},
journal = {JMIR AI},
volume = {4},
year = {2025},
issn = {2817-1705},
doi = {https://doi.org/10.2196/58670},
url = {https://www.sciencedirect.com/science/article/pii/S2817170525000092},
author = {Yanjun Gao and Ruizhe Li and Emma Croxford and John Caskey and Brian W Patterson and Matthew Churpek and Timothy Miller and Dmitriy Dligach and Majid Afshar},
keywords = {knowledge graph, natural language processing, machine learning, electronic health record, large language model, diagnosis prediction, graph model, artificial intelligence},
abstract = {Background
Electronic health records (EHRs) and routine documentation practices play a vital role in patients’ daily care, providing a holistic record of health, diagnoses, and treatment. However, complex and verbose EHR narratives can overwhelm health care providers, increasing the risk of diagnostic inaccuracies. While large language models (LLMs) have showcased their potential in diverse language tasks, their application in health care must prioritize the minimization of diagnostic errors and the prevention of patient harm. Integrating knowledge graphs (KGs) into LLMs offers a promising approach because structured knowledge from KGs could enhance LLMs’ diagnostic reasoning by providing contextually relevant medical information.
Objective
This study introduces DR.KNOWS (Diagnostic Reasoning Knowledge Graph System), a model that integrates Unified Medical Language System–based KGs with LLMs to improve diagnostic predictions from EHR data by retrieving contextually relevant paths aligned with patient-specific information.
Methods
DR.KNOWS combines a stack graph isomorphism network for node embedding with an attention-based path ranker to identify and rank knowledge paths relevant to a patient’s clinical context. We evaluated DR.KNOWS on 2 real-world EHR datasets from different geographic locations, comparing its performance to baseline models, including QuickUMLS and standard LLMs (Text-to-Text Transfer Transformer and ChatGPT). To assess diagnostic reasoning quality, we designed and implemented a human evaluation framework grounded in clinical safety metrics.
Results
DR.KNOWS demonstrated notable improvements over baseline models, showing higher accuracy in extracting diagnostic concepts and enhanced diagnostic prediction metrics. Prompt-based fine-tuning of Text-to-Text Transfer Transformer with DR.KNOWS knowledge paths achieved the highest ROUGE-L (Recall-Oriented Understudy for Gisting Evaluation–Longest Common Subsequence) and concept unique identifier F1-scores, highlighting the benefits of KG integration. Human evaluators found the diagnostic rationales of DR.KNOWS to be aligned strongly with correct clinical reasoning, indicating improved abstraction and reasoning. Recognized limitations include potential biases within the KG data, which we addressed by emphasizing case-specific path selection and proposing future bias-mitigation strategies.
Conclusions
DR.KNOWS offers a robust approach for enhancing diagnostic accuracy and reasoning by integrating structured KG knowledge into LLM-based clinical workflows. Although further work is required to address KG biases and extend generalizability, DR.KNOWS represents progress toward trustworthy artificial intelligence–driven clinical decision support, with a human evaluation framework focused on diagnostic safety and alignment with clinical standards.}
}
@article{SONG2025647,
title = {Construction of Question Answering System Based on English Pre-Trained Language Model Enhanced by Knowledge Graph},
journal = {Procedia Computer Science},
volume = {261},
pages = {647-655},
year = {2025},
note = {The 5th International Conference on Machine Learning and Big Data Analytics for IoT Security and Privacy (SPIoT2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.04.256},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925013584},
author = {Pei Song},
keywords = {NLP technology, knowledge graph enhancement, BERT model, English pre-training, language model},
abstract = {With the continuous development of technology, question answering systems based on pre trained language models have become an important component of intelligent applications. However, traditional question-answering systems often face challenges in terms of understanding accuracy and insufficient knowledge coverage when dealing with open-domain questions. To this end, this paper uses a method for building a question-answering system based on a knowledge graph-enhanced BERT pre-trained language model. First, this paper trains the basic model based on a large-scale BERT pre-trained language model. Then, this paper adopts knowledge graph technology to introduce structured knowledge information into the model by integrating domain-specific knowledge bases. Finally, in order to effectively integrate knowledge graph information, this paper uses graph neural network (GNN) to model graph data, and combines the self-attention mechanism in the BERT model to optimize the weighted fusion process of knowledge graph information. Experimental results show that the BERT model enhanced with knowledge graph performs well in multiple question-answering tasks. On the simple question of the first experiment, the average accuracy of the enhanced model increased from 84.6% of the standard BERT model to 89.8%, and the F1 score increased from 0.86 to 0.91. In complex reasoning tasks, the BERT+knowledge graph model demonstrates stronger reasoning ability and higher knowledge coverage. In the experimental conclusion, the introduction of the knowledge graph significantly improves the model’s reasoning ability and knowledge coverage, especially in professional field problems and multi-step reasoning tasks, the enhanced model shows stronger capabilities. This research provides a new construction method for question-answering systems, demonstrates the great potential of knowledge graphs in natural language processing, and has broad application prospects.}
}
@article{ZHOU2025108009,
title = {Integrating machine learning and a large language model to construct a domain knowledge graph for reducing the risk of fall-from-height accidents},
journal = {Accident Analysis & Prevention},
volume = {215},
pages = {108009},
year = {2025},
issn = {0001-4575},
doi = {https://doi.org/10.1016/j.aap.2025.108009},
url = {https://www.sciencedirect.com/science/article/pii/S0001457525000958},
author = {Zhipeng Zhou and Xinhui Yu and Joseph Jonathan Magoua and Jianqiang Cui and Haiying Luan and Dong Lin},
keywords = {Fall-from-height, Domain knowledge graph, Knowledge extraction, Natural language process, Named entity recognition, Large language model},
abstract = {Fall-from-height (FFH) accidents remain a major source of workplace injuries and fatalities. Fall protection systems (FPS) are critical for preventing falls in the work-at-height (WAH) environment. However, challenges in designing and selecting effective FPS persist across various industries, and existing tools often lack practical references. This study aims to develop an FFH-specific knowledge graph (FFH-KG) to support FPS design. By structuring accident data, the FFH-KG provides empirical insights to help designers improve FPS frameworks, aiding safety planning and decision-making. It serves as a decision support system for FPS designers and safety professionals, guiding the selection and design of appropriate protection solutions for diverse WAH scenarios. The FFH-KG was constructed using a hybrid natural language processing approach, combining manual extraction, entity recognition, text segmentation, and rule-based relation extraction. It was grounded in a schema layer (i.e., ontology) established by experts. A text-mining approach, integrating machine learning with a large language model, facilitated the categorization of fall types, refinement of WAH scenarios, and identification of fall causes, enhancing the content and applicability of knowledge graph. A total of 2,200 entities and 4,820 relationships were created based on fall protection equipment standard documents and fall-from-height accident investigation reports, forming a foundation for developing countermeasures. The retrieval performance of FFH-KG was validated through three case studies. This research has also made significant progress in intelligent safety engineering and management across industries.}
}
@article{GONG2025110396,
title = {The application progress and research trends of knowledge graphs and large language models in agriculture},
journal = {Computers and Electronics in Agriculture},
volume = {235},
pages = {110396},
year = {2025},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2025.110396},
url = {https://www.sciencedirect.com/science/article/pii/S0168169925005022},
author = {Ruizi Gong and Xinxing Li},
keywords = {Knowledge graphs, Large language models, Agricultural knowledge intelligent services},
abstract = {Enhancing agricultural productivity remains a crucial issue in agriculture, wherein agricultural knowledge intelligent services can improve the scientific and intelligent level of agricultural production through technologies such as knowledge coupling and inference decision-making. The application of knowledge graphs (KGs) in agriculture effectively supports the structured representation of agricultural data and helps manage agricultural data. On the other hand, the recent emergence of large language models (LLMs), with their strong language understanding and generation capabilities, provides new methods and ideas for knowledge services in agriculture. Though KGs and LLMs each have different strengths and limitations, their integration is believed to complement each other and has great potential to promote the development of agricultural knowledge intelligent services. In this paper, we review the current status of the application of KGs and LLMs in agriculture. We also discuss their complementary fusion as well as the prospect of their agricultural application, hoping to provide some references for the future development of agricultural knowledge intelligent services.}
}
@article{LIU2025113288,
title = {Knowledge-based natural answer generation via effective graph learning},
journal = {Knowledge-Based Systems},
volume = {316},
pages = {113288},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113288},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125003351},
author = {Zedong Liu and Jianxin Li and Yongle Huang and Ningning Cui and Lili Pei},
keywords = {Natural answer generation, Adaptive multi-hop retrieval, Graph-based Mamba, Prompt optimization},
abstract = {Objectives:
Natural Answer Generation (NAG) aims to generate natural and fluent answers to user questions. Existing NAG methods typically employ fixed-hop retrieval to construct knowledge graphs and utilize attention-based networks for answer generation. However, these approaches lack interpretability, struggle to filter out redundant information in the graph, and are computationally intensive.
Methods:
To address these issues, this paper introduces an innovative approach AdaptQA model. Initially, AdaptQA constructs a knowledge graph from the knowledge base (KB) using an adaptive multi-hop retrieval algorithm. Subsequently, it generates answers through the Graph-based Mamba module (GBM), effectively filtering out redundant information. Finally, the answers are optimized using a pre-trained large language model to enhance their fluency and accuracy.
Novelty:
The proposed AdaptQA model introduces a new approach to NAG by improving the completeness of the knowledge graph and optimizing question answers. This method overcomes the limitations of existing NAG techniques by reducing the complexity of model inference.
Findings:
Through extensive experiments on two benchmark datasets, HotpotQA and WikiHop, AdaptQA demonstrates superior performance, significantly outperforming existing NAG methods. Specifically, AdaptQA achieves an accuracy of 94.47% on the HotpotQA dataset and 91.38% on the WikiHop dataset.}
}
@article{XU2025103244,
title = {A large language model-enabled machining process knowledge graph construction method for intelligent process planning},
journal = {Advanced Engineering Informatics},
volume = {65},
pages = {103244},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103244},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625001375},
author = {Qingfeng Xu and Fei Qiu and Guanghui Zhou and Chao Zhang and Kai Ding and Fengtian Chang and Fengyi Lu and Yongrui Yu and Dongxu Ma and Jiancong Liu},
keywords = {Large language models, Knowledge graphs, Machining process, Intelligent process planning},
abstract = {As a pivotal step in translating design into production, process planning significantly influences product quality, cost, production efficiency, and market competitiveness. The process knowledge base, a fundamental element of process planning, determines the intelligence level of product manufacturing. Methods that construct process knowledge bases using Knowledge Graphs (KGs) have increasingly become critical technologies for supporting intelligent process planning. However, traditional deep learning-based named entity recognition methods for constructing KGs require extensive manual effort in domain-specific data annotation, resulting in inefficiencies, prolonged construction cycles, and high costs. To address these challenges, this paper introduces a Large Language Model-enabled method for constructing Machining Process KGs (LLM-MPKG). Initially, Large Language Models (LLMs) are employed to pre-annotate machining process text datasets. A verifier is then developed to assess and filter the pre-annotated datasets, with domain experts re-annotating deficient data to create a high-quality annotated machining process dataset. Subsequently, using this dataset and a fine-tuned LLM, a machining process knowledge extraction model, MPKE-GPT, is constructed. MPKE-GPT is then applied to extract knowledge from process planning case data for 50 parts within an enterprise, leading to the creation of the MPKG. A prototype system was also developed to support intelligent process planning. Compared to traditional deep learning methods, the proposed method reduces construction time by 48.58%, lowers costs by 46.44%, and enhances performance by 1.96%.}
}
@article{DESSI2021253,
title = {Generating knowledge graphs by employing Natural Language Processing and Machine Learning techniques within the scholarly domain},
journal = {Future Generation Computer Systems},
volume = {116},
pages = {253-264},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.10.026},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X2033003X},
author = {Danilo Dessì and Francesco Osborne and Diego {Reforgiato Recupero} and Davide Buscaldi and Enrico Motta},
abstract = {The continuous growth of scientific literature brings innovations and, at the same time, raises new challenges. One of them is related to the fact that its analysis has become difficult due to the high volume of published papers for which manual effort for annotations and management is required. Novel technological infrastructures are needed to help researchers, research policy makers, and companies to time-efficiently browse, analyse, and forecast scientific research. Knowledge graphs i.e., large networks of entities and relationships, have proved to be effective solution in this space. Scientific knowledge graphs focus on the scholarly domain and typically contain metadata describing research publications such as authors, venues, organizations, research topics, and citations. However, the current generation of knowledge graphs lacks of an explicit representation of the knowledge presented in the research papers. As such, in this paper, we present a new architecture that takes advantage of Natural Language Processing and Machine Learning methods for extracting entities and relationships from research publications and integrates them in a large-scale knowledge graph. Within this research work, we (i) tackle the challenge of knowledge extraction by employing several state-of-the-art Natural Language Processing and Text Mining tools, (ii) describe an approach for integrating entities and relationships generated by these tools, (iii) show the advantage of such an hybrid system over alternative approaches, and (vi) as a chosen use case, we generated a scientific knowledge graph including 109,105 triples, extracted from 26,827 abstracts of papers within the Semantic Web domain. As our approach is general and can be applied to any domain, we expect that it can facilitate the management, analysis, dissemination, and processing of scientific knowledge.}
}
@article{GUO2022102222,
title = {An automatic method for constructing machining process knowledge base from knowledge graph},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {73},
pages = {102222},
year = {2022},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2021.102222},
url = {https://www.sciencedirect.com/science/article/pii/S0736584521001058},
author = {Liang Guo and Fu Yan and Tian Li and Tao Yang and Yuqian Lu},
keywords = {Process Knowledge Base, Knowledge Graph, Fuzzy Evaluation, NLP},
abstract = {The process knowledge base is the key module in intelligent process design, it determines the intelligence degree of the design system and affects the quality of product design. However, traditional process knowledge base construction is non-automated, time consuming and requires much manual work, which is not sufficient to meet the demands of the modern manufacturing mode. Moreover, the knowledge base often adopts a single knowledge representation, and this may lead to ambiguity in the meaning of some knowledge, which will affect the quality of the process knowledge base. To overcome the above problems, an automatic construction framework for the process knowledge base in the field of machining based on knowledge graph (KG) is introduced. First, the knowledge is classified and annotated based on the function-behavior-states (FBS) design method. Second, a knowledge extraction framework based on BERT-BiLSTM-CRF is established to perform the automatic knowledge extraction of process text. Third, a knowledge representation method based on fuzzy comprehensive evaluation is established, forming three types of knowledge representation with the KG as the main, production rules and two-dimensional data linked list as a supplement. In addition, to overcome the redundancy in the knowledge fusion stage, a hybrid algorithm based on an improved edit distance and attribute weighting is built. Finally, a prototype system is developed, and quality analysis is carried out. Compared with the F values of BiLSTM-CRF and CNN-BiLSTM-CRF, that of the proposed extraction method in the machining domain is increased by 7.35% and 3.87%, respectively.}
}
@article{CAO2024,
title = {An Automatic and End-to-End System for Rare Disease Knowledge Graph Construction Based on Ontology-Enhanced Large Language Models: Development Study},
journal = {JMIR Medical Informatics},
volume = {12},
year = {2024},
issn = {2291-9694},
doi = {https://doi.org/10.2196/60665},
url = {https://www.sciencedirect.com/science/article/pii/S2291969424001881},
author = {Lang Cao and Jimeng Sun and Adam Cross},
keywords = {rare disease, clinical informatics, LLM, natural language processing, machine learning, artificial intelligence, large language models, data extraction, ontologies, knowledge graphs, text mining},
abstract = {Background
Rare diseases affect millions worldwide but sometimes face limited research focus individually due to low prevalence. Many rare diseases do not have specific International Classification of Diseases, Ninth Edition (ICD-9) and Tenth Edition (ICD-10), codes and therefore cannot be reliably extracted from granular fields like “Diagnosis” and “Problem List” entries, which complicates tasks that require identification of patients with these conditions, including clinical trial recruitment and research efforts. Recent advancements in large language models (LLMs) have shown promise in automating the extraction of medical information, offering the potential to improve medical research, diagnosis, and management. However, most LLMs lack professional medical knowledge, especially concerning specific rare diseases, and cannot effectively manage rare disease data in its various ontological forms, making it unsuitable for these tasks.
Objective
Our aim is to create an end-to-end system called automated rare disease mining (AutoRD), which automates the extraction of rare disease–related information from medical text, focusing on entities and their relations to other medical concepts, such as signs and symptoms. AutoRD integrates up-to-date ontologies with other structured knowledge and demonstrates superior performance in rare disease extraction tasks. We conducted various experiments to evaluate AutoRD’s performance, aiming to surpass common LLMs and traditional methods.
Methods
AutoRD is a pipeline system that involves data preprocessing, entity extraction, relation extraction, entity calibration, and knowledge graph construction. We implemented this system using GPT-4 and medical knowledge graphs developed from the open-source Human Phenotype and Orphanet ontologies, using techniques such as chain-of-thought reasoning and prompt engineering. We quantitatively evaluated our system’s performance in entity extraction, relation extraction, and knowledge graph construction. The experiment used the well-curated dataset RareDis2023, which contains medical literature focused on rare disease entities and their relations, making it an ideal dataset for training and testing our methodology.
Results
On the RareDis2023 dataset, AutoRD achieved an overall entity extraction F1-score of 56.1% and a relation extraction F1-score of 38.6%, marking a 14.4% improvement over the baseline LLM. Notably, the F1-score for rare disease entity extraction reached 83.5%, indicating high precision and recall in identifying rare disease mentions. These results demonstrate the effectiveness of integrating LLMs with medical ontologies in extracting complex rare disease information.
Conclusions
AutoRD is an automated end-to-end system for extracting rare disease information from text to build knowledge graphs, addressing critical limitations of existing LLMs by improving identification of these diseases and connecting them to related clinical features. This work underscores the significant potential of LLMs in transforming health care, particularly in the rare disease domain. By leveraging ontology-enhanced LLMs, AutoRD constructs a robust medical knowledge base that incorporates up-to-date rare disease information, facilitating improved identification of patients and resulting in more inclusive research and trial candidacy efforts.}
}
@article{LIU2025111625,
title = {Knowledge graph reasoning: Mainstream methods, applications and prospects},
journal = {Engineering Applications of Artificial Intelligence},
volume = {159},
pages = {111625},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.111625},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625016276},
author = {Han Liu and Shaojie Yang and Guokai Shi and Zongcheng Miao},
keywords = {Knowledge graph, Knowledge graph reasoning, Deep learning, Reinforcement learning},
abstract = {Knowledge graphs have emerged as a leading paradigm for knowledge representation due to their robust capacity to mine, organize, and manage massive datasets. Their versatility has led to extensive research and application in various advanced fields. Knowledge graph reasoning plays a critical role in this context by reasoning new facts from existing ones, thereby completing and refining the knowledge base. In this paper, we provide a comprehensive review of knowledge graph reasoning by researching its definition, foundational concepts, and methodological approaches. We systematically categorize reasoning methods into five groups: reasoning based on ontology, reasoning based on rules, neural rule reasoning based on distributed representations, neural rule reasoning based on deep learning, and hybrid reasoning. Furthermore, we analyze the applications of knowledge graph reasoning, discuss their current limitations, and outline promising directions for future research to enhance both performance and scalability in this rapidly evolving field.}
}
@article{SHIMIZU2025100862,
title = {Accelerating knowledge graph and ontology engineering with large language models},
journal = {Journal of Web Semantics},
volume = {85},
pages = {100862},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2025.100862},
url = {https://www.sciencedirect.com/science/article/pii/S1570826825000022},
author = {Cogan Shimizu and Pascal Hitzler},
keywords = {Knowledge graph engineering, Ontology engineering, Large language models, Modular ontologies, Ontology modeling, Ontology population, Ontology alignment, Entity disambiguation},
abstract = {Large Language Models bear the promise of significant acceleration of key Knowledge Graph and Ontology Engineering tasks, including ontology modeling, extension, modification, population, alignment, as well as entity disambiguation. We lay out LLM-based Knowledge Graph and Ontology Engineering as a new and coming area of research, and argue that modular approaches to ontologies will be of central importance.}
}
@article{CHEN2023100634,
title = {A management knowledge graph approach for critical infrastructure protection: Ontology design, information extraction and relation prediction},
journal = {International Journal of Critical Infrastructure Protection},
volume = {43},
pages = {100634},
year = {2023},
issn = {1874-5482},
doi = {https://doi.org/10.1016/j.ijcip.2023.100634},
url = {https://www.sciencedirect.com/science/article/pii/S1874548223000471},
author = {Jiarui Chen and Yiqin Lu and Yang Zhang and Fang Huang and Jiancheng Qin},
keywords = {Knowledge graph, Critical infrastructure, Ontology, Named entity recognition, Relation prediction},
abstract = {Critical Infrastructures (CI) underpin the basic functioning of society and the economy. Proper governance of CI security management remains a crucial challenge. This study aims to construct a knowledge graph for modeling CI protection. While the previous research has focused on threat intelligence modeling and open knowledge bases, they miss considering the defense side. Accordingly, we propose a knowledge graph for critical infrastructure protection, CIPKG, that extends the management ontology to include the defense side. It addresses the cross-industry and cross-time information gaps that occur in the process of CI protection management, making it more comprehensive in structure than the existing knowledge graph. We employ simplified Structured Threat Information Expression as attack ontology and design a new ontology for the defense side, which could combine with the existing threat ontology to form the CI protection knowledge graph. To dynamically extract information from emerging knowledge, we employ a Bi-directional Long Short-Term Memory and Conditional Random Field model with pre-trained cybersecurity domain-specific Bidirectional Encoder Representations from Transformers to recognize the named entities from CI regulations and standards. To associate the threat part with the management portion of the knowledge graph, we adopt the Knowledge Graph Bidirectional Encoder Representations from Transformer model to capture the semantic information and predict the relationship between threat and management. After information extraction and relation prediction, we build a knowledge graph with 529,360 nodes and about 3,335,000 edges.}
}
@article{YANG2025,
title = {The Financial Institution Text Data Mining and Value Analysis Model Based on Big Data and Natural Language Processing},
journal = {Journal of Organizational and End User Computing},
volume = {37},
number = {1},
year = {2025},
issn = {1546-2234},
doi = {https://doi.org/10.4018/JOEUC.374213},
url = {https://www.sciencedirect.com/science/article/pii/S1546223425000267},
author = {Juan Yang and Yu Bai and Jie Gong and Menghui Han},
keywords = {Financial Text Mining, Multi-task Learning, Temporal Graph Convolutional Networks, Knowledge Graph, Value Prediction},
abstract = {ABSTRACT
Financial markets are inherently complex and influenced by a variety of factors, making it challenging to predict trends and detect key events. Traditional models often struggle to integrate both structured, or numerical, and unstructured, or textual, data; additionally, they fail to capture temporal dependencies or the dynamic relationships between financial entities. To address this, the multidimensional integrated model for financial text mining and value analysis (MI-FinText), was proposed. MI-FinText integrated multi-task learning, temporal graph convolutional networks and dynamic knowledge graph construction. MI-FinText simultaneously performed sentiment analysis, event detection, and value prediction by learning shared representations across tasks and modeling time-dependent relationships between financial events. MI-FinText continuously updated a dynamic knowledge graph to reflect the evolving financial landscape, enabling real-time insights.}
}
@article{WANG2024102820,
title = {Knowledge graph of agricultural engineering technology based on large language model},
journal = {Displays},
volume = {85},
pages = {102820},
year = {2024},
issn = {0141-9382},
doi = {https://doi.org/10.1016/j.displa.2024.102820},
url = {https://www.sciencedirect.com/science/article/pii/S0141938224001847},
author = {Haowen Wang and Ruixue Zhao},
keywords = {LLM, Knowledge graph},
abstract = {Agriculture is an industry that has evolved alongside human evolution and has faithfully fulfilled its core mission of food supply. With the reduction of rural labor, the progress of artificial intelligence and the development of Internet of Things technology, it is hoped that the efficiency and productivity of the agricultural industry can be improved. Recently, with the development of information and intelligent technology, agricultural production and management have been significantly enhanced. However, there is still a considerable challenge in effectively integrating the vast amount of fragmented information for downstream applications. An agricultural knowledge graph (AGKG) will serve as the foundation for achieving these goals. Knowledge graphs can be general or domain-specific, and are the basis for many applications, such as search engines, online question-and-answer services, and knowledge inference. Therefore, there are many knowledge graphs, including Wikidata and DBpedia, for accessing structured knowledge. Although some general knowledge graphs contain some entities and relationships related to agriculture, there are no domain-specific knowledge graphs specifically for agricultural applications. Therefore, this paper proposes an agricultural knowledge graph (AGKG) for automatically integrating large amounts of agricultural data from the Internet. By applying natural language processing and deep learning technologies, AGKG can automatically identify agricultural entities from unstructured text and connect them to form a knowledge graph. In addition, we have described the typical scenarios of our AGKG and validated it through real-world applications such as agricultural entity retrieval and agricultural question-answering.}
}
@article{LING2024102725,
title = {Hybrid NLP-based extraction method to develop a knowledge graph for rock tunnel support design},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102725},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102725},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624003732},
author = {Jiaxin Ling and Xiaojun Li and Haijiang Li and Yi An and Yi Rui and Yi Shen and Hehua Zhu},
keywords = {Tunnel support design, Knowledge graph, Natural language processing, Knowledge extraction, Named entity recognition (NER), Ontology},
abstract = {In the realm of drill and blast (D&B) tunneling, the design of tunnel support emerges as a pivotal concern. As a typical knowledge-intensive task, the practical application of tunnel support design often relies extensively on experienced engineers or experts who formulate designs based on their expertise. Compared with data-driven approaches, knowledge-driven tunnel support design allows for a nuanced understanding of the intricate geological and engineering factors influencing support design, and yield results that are more transparent and explainable. However, in practice, the substantial knowledge sources and complex interrelationships between factors which have direct or indirect influences on support design hinder the implementation of knowledge-based support design methods. To solve such knowledge gap, this study proposed a hybrid natural language processing (NLP)-based method to develop a knowledge graph for rock tunnel design. Specifically, rule-based methods are used to extract entities and classification relationships from standards and specifications, and statistical and artificial intelligence (AI)-based methods are used for extracting entities and non-classification relationships from a large amount of scientific literature, academic papers, and support design schemes. A total of 947 entities, 3 kinds of classification relationships and 11 kinds of non-classification relationships were extracted. On the basis of extracted entities, relations, and attributes, a knowledge graph was developed using ontology-based method, providing functionalities such as knowledge element retrieval, semantic retrieval, and query expansion. The findings of this study are expected to provide practical recommendations for the design of the tunnel support and advance the existing knowledge about the tunnel design from a knowledge-driven perspective.}
}
@article{ZHANG2025103705,
title = {A knowledge graphs construction method enhanced by multimodal large language model for industrial equipment operation and maintenance},
journal = {Advanced Engineering Informatics},
volume = {68},
pages = {103705},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103705},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625005981},
author = {Zhengping Zhang and Junyuan Yu and Bo Yang and Kaze Du and Shilong Wang and Xing Qi},
keywords = {Equipment operation and maintenance, Multi-modal knowledge graph, Multimodal large language modals, Attention mechanism},
abstract = {The industrial equipment Operation and Maintenance (O&M) is a core component in ensuring production safety and efficiency, urgently requiring the support of intelligent technologies. Knowledge graphs, which represent equipment and faults in graph structures, are widely utilized to enable efficient association and rapid retrieval of maintenance knowledge, thereby being extensively applied in intelligent decision-making for the equipment O&M. Traditional knowledge graph construction methods, which rely on a single textual modality, are confronted with challenges such as the scarcity of annotated samples, difficulties in dynamically associating old and new equipment, and insufficient parsing of complex equipment relationships. As a result, issues like missing graph entities and broken causal chains are often encountered, thereby negatively impacting the quality of maintenance decision-making. Therefore, a dual-attention model enhanced by multimodal large language models (MllmDA-KGC) is proposed in this paper. Multimodal large language model(MLLM) is introduced to fully utilize multi-modal knowledge from the O&M domain, thereby enabling a more effective understanding and modeling of complex O&M knowledge. As a result, the quality of knowledge graph construction is significantly improved. In the MllmDA-KGC framework, first, QWEN2-VL is introduced into a dual-stream Transformer architecture to achieve dynamic alignment between images and text while supplementing semantics. As a result, the precision of identifying relationships between parts and problem entities in the O&M domain is significantly enhanced; second, the MT-Transformer module is proposed, which integrates causal convolution, dilated convolution, and Memory_Bank mechanisms to achieve cross-modal temporal embedding fusion. As a result, the continuity of associations between new and old parts, as well as the precision of causal chain embeddings, is significantly improved; third, a multimodal dynamic weight attention-guided module is designed, in which weighted key-value guided attention mechanisms are introduced to focus on critical aspects. Schematic diagrams and textual features are fused to enhance the precision of entity relationship modeling between parts and faults; finally, to fully leverage the multimodal understanding capabilities of the MLLM, the image embeddings and positional embeddings generated and marked by MLLM are integrated into the feature embeddings of RoBERTa. Subsequently, CRF and Softmax are combined to accomplish the MNER (Multimodal Named Entity Recognition) and MRE (Multimodal Relation Extraction) tasks, thereby enabling the construction of a multimodal equipment O&M knowledge graph. In this paper, the vehicle O&M dataset from an automobile company was utilized to validate the proposed method. The experimental results showed that the F1 scores of the model in the MNER and MRE tasks reached 88.40% and 93.79%, respectively, demonstrating its effectiveness in constructing a multimodal knowledge graph for equipment O&M. Furthermore, during the process of graph inference, the performance of multimodal graph inference was significantly better than that of the unimodal approach, further confirming the superiority of the multimodal knowledge graph.}
}
@article{PROBIERZ20234324,
title = {Knowledge graphs to an analysis and visualization of texts from scientific articles},
journal = {Procedia Computer Science},
volume = {225},
pages = {4324-4333},
year = {2023},
note = {27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.429},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923015879},
author = {Barbara Probierz and Jan Kozak},
keywords = {knowledge graphs, natural language processing, analysis of texts, scientific articles},
abstract = {Reviewing the literature is one of the key elements of scientific research that allows you to identify existing solutions and research niches. However, it can be difficult for researchers to find relevant scientific articles related to the research topic. A limited number of available sources of information, diverse ways of describing them, and a multitude of scientific publications mean that scientists often have to spend a lot of time and effort to find the information they need. For this reason, we propose a solution for text analysis and its presentation in the form of a graph visualization. In our research, we used Natural Language Processing (NLP) methods and word weighting measures such as TF and TF-IDF. In addition, knowledge graphs were used to present the results visually. The conducted research was based on the analysis of the content of scientific articles, which allowed to draw important conclusions related to the presentation of texts in graphic form. Experimental results also identified potential methods and suggestions for literature reviews in specific fields of science. The analysis of the methods used and the results obtained allowed for a better understanding of the potential of natural language processing and graph text representation in the analysis of scientific articles.}
}
@article{ZHANG2026129050,
title = {Natural language processing and text mining in transportation: Current status, challenges, and future roadmap},
journal = {Expert Systems with Applications},
volume = {296},
pages = {129050},
year = {2026},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.129050},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425026673},
author = {Xiaocai Zhang and Ruobin Gao and Zhe Xiao and Ke Wang and Tao Liu and Maohan Liang and Jianjia Zhang},
keywords = {Natural language processing (NLP), Text mining, Transportation, Large language models (LLMs)},
abstract = {The transportation sector is generating and accumulating an increasing amount of unstructured data from a variety of sources. As a result, Natural Language Processing (NLP) and text mining are becoming critical for their capability to automatically process and interpret human language in transportation research. However, there is a lack of a thorough review of existing research in this field, as well as a detailed guide on how to use these techniques in transportation studies. This paper offers an updated review of NLP and text mining techniques, including the latest developments in Large Language Models (LLMs), tailored for comprehensive transportation modeling across land, maritime, and aviation sectors. It highlights the data sources and methodologies used in previous studies, provides an analysis of word representation, sentiment analysis, external NLP toolkits, language diversity, and performance evaluation, and offers insights into performance analysis for transportation research. The paper concludes by outlining the current challenges and future directions for research in this area.}
}
@incollection{DHAMENIYA2025227,
title = {12 - Knowledge graph-based question answering (KG-QA) using natural language processing},
editor = {Rajesh Kumar Dhanaraj and M. Nalini and Malathy Sathyamoorthy and Manar Mohaisen},
booktitle = {Knowledge Graph-Based Methods for Automated Driving},
publisher = {Elsevier},
pages = {227-249},
year = {2025},
isbn = {978-0-443-30040-0},
doi = {https://doi.org/10.1016/B978-0-443-30040-0.00012-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780443300400000126},
author = {Priyanshu Dhameniya and Rashmi Yadav},
keywords = {Natural language processing, Knowledge graphsand question answering, Information retrieval, Unstructured, Entity-resolution models, Semantic-parsing, Semantic annotations, Recurrent neural networks, Entity recognition, RDF-based knowledge graphs, SPARQL},
abstract = {This chapter begins by providing an overview of knowledge graphs, their construction, and their role in representing relationships between entities. It then delves into the challenges posed by question-answering tasks and how knowledge graphs can enhance traditional QA systems by offering a structured and context-aware information base. The core of this chapter focuses on the integration of NLP techniques in KG-QA systems. It discusses the processing of natural language queries, entity recognition, and relationship extraction to bridge the gap between user queries and the structured information encapsulated in knowledge graphs. Special attention is given to semantic parsing and understanding, enabling the extraction of nuanced information from unstructured text.}
}
@article{LI20252790,
title = {DNMKG: A method for constructing domain of nonferrous metals knowledge graph based on multiple corpus},
journal = {Transactions of Nonferrous Metals Society of China},
volume = {35},
number = {8},
pages = {2790-2802},
year = {2025},
issn = {1003-6326},
doi = {https://doi.org/10.1016/S1003-6326(25)66848-8},
url = {https://www.sciencedirect.com/science/article/pii/S1003632625668488},
author = {Hai-liang LI and Hai-dong WANG},
keywords = {knowledge graph, nonferrous metals, thesaurus, word vector model, multi-source heterogeneous corpus},
abstract = {To address the underutilization of Chinese research materials in nonferrous metals, a method for constructing a domain of nonferrous metals knowledge graph (DNMKG) was established. Starting from a domain thesaurus, entities and relationships were mapped as resource description framework (RDF) triples to form the graph’s framework. Properties and related entities were extracted from open knowledge bases, enriching the graph. A large-scale, multi- source heterogeneous corpus of over 1×109 words was compiled from recent literature to further expand DNMKG. Using the knowledge graph as prior knowledge, natural language processing techniques were applied to the corpus, generating word vectors. A novel entity evaluation algorithm was used to identify and extract real domain entities, which were added to DNMKG. A prototype system was developed to visualize the knowledge graph and support human−computer interaction. Results demonstrate that DNMKG can enhance knowledge discovery and improve research efficiency in the nonferrous metals field.}
}
@article{YANG2025121963,
title = {Construction of a multimodal knowledge graph for LNG carrier port state control inspections based on improved visual prompt tuning},
journal = {Ocean Engineering},
volume = {339},
pages = {121963},
year = {2025},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2025.121963},
url = {https://www.sciencedirect.com/science/article/pii/S0029801825016695},
author = {Qihao Yang and Langxiong Gan and Qiongyao Mao and Yi Xu and Yaqing Shu and Chengyong Liu},
keywords = {Port state control inspection, Multimodal knowledge graph, Visual prompt tuning, Vision transformer, Natural language processing},
abstract = {Because of transitions in global energy, “intelligent” port state control (PSC) inspections for liquefied natural gas (LNG) carrier transportation safety are urgently needed. Traditional PSC inspections rely on practical experience, suffer from low efficiency and strong subjectivity, and struggle to handle multisource heterogeneous data. This study comprehensively applies data processing techniques of multiple modalities to construct a multimodal knowledge graph and is committed to improving the accuracy and efficiency of PSC inspections for LNG carriers. First, a multimodal database containing text, images, audio, and video is established. Data standardization and annotation are achieved through two-dimensional image classification and text preprocessing. To address equipment image recognition, an efficient visual prompt tuning (EVPT) model for channel attention is proposed, which integrates the channelwise convolutional attention module and visual prompt tuning (VPT). This model significantly improves equipment classification accuracy at low computational costs, achieving accuracies of 88.69 % and 85.47 % on the LNG-E and LNG-A image datasets, respectively. The RoBERTa-BiLSTM-CRF model is used for text entity recognition to extract key entities such as inspection points, with an F1 score of 87.53 %. Through manual cross-modal data alignment, multisource information is subsequently integrated to construct a multimodal knowledge graph containing 7401 valid triples. Case studies demonstrate that the graph helps inspectors locate defect evidence, understand operational specifications, and provide multidimensional knowledge support. This research innovates multimodal data fusion and knowledge management in the maritime field, promoting intelligent LNG carrier safety supervision.}
}
@article{ZHANG2025104527,
title = {More intelligent knowledge graph: A large language model-driven method for knowledge representation in geospatial digital twins},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {139},
pages = {104527},
year = {2025},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2025.104527},
url = {https://www.sciencedirect.com/science/article/pii/S1569843225001748},
author = {Jinbin Zhang and Jun Zhu and Zhihao Guo and Jianlin Wu and Yukun Guo and Jianbo Lai and Weilian Li},
keywords = {Geospatial digital twins, Knowledge graph, Large language model, Knowledge extraction, Dynamic update},
abstract = {Knowledge graphs (KGs) can describe the nature and relationships of geographic entities and are an essential knowledge base for realizing geospatial digital twins (GDTs). However, existing KGs make it challenging to describe dynamic geographic entities under geographic spatiotemporal evolution accurately. Furthermore, they are constrained by the professional backgrounds of their users, which hinders updates and communication. Therefore, the research constructed an “event-object-state” three-domain associated GDT-oriented KG, proposed a large language model (LLM) −driven KG dynamic update algorithm, and established a KG intelligent Q&A method integrating LLM. We developed a prototype system and selected an earthquake disaster as a typical geographic event for experimental analysis. The results showed that the proposed method can reflect the space, time, state, evolution process, and interrelationships of geographic entities in a more comprehensive way, support users to build, update, and query KGs using natural language, with an updating efficiency of less than 1 min, and an updating quality comparable to that of manual updating by experts. Compared with the traditional KGs, our method can represent virtual geographic entities and has significant advantages in intelligence and automation, which effectively breaks down professional barriers and supports the construction of GDTs with the need for rapid updating of knowledge.}
}
@article{OYELADE2025127455,
title = {SMAR + NIE IdeaGen: A knowledge graph based node importance estimation with analogical reasoning on large language model for idea generation},
journal = {Expert Systems with Applications},
volume = {279},
pages = {127455},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127455},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425010772},
author = {Olaide N. Oyelade and Hui Wang and Karen Rafferty},
keywords = {Knowledge graphs (KGs), Large language model (LLMs), Idea generation, Novelty, Analogical reasoning, Node importance estimation, Natural language processing (NLP), Isomorphic subgraphs},
abstract = {Idea generation describes a creative process involving reasoning over some knowledge to derive new information. Traditional approaches such as mind-map and brainstorming are limited and often fail due to lack of quality ideas and ineffective methods. The reasoning capability of large language models (LLMs) have been investigated for ideation tasks and have reported interesting performance. However, these models suffer from limited logical reasoning capability which hinders the use of structural and factual real-world knowledge in discovery of latent insight and predict possible outcome when applied to ideation. In addition, the possibility of LLMs regurgitating knowledge learnt from datasets might adversely impact the degree of novel ideas the models can generate. In this paper, a two-stage logical reasoning approach is applied to initiate the search for candidate idea pathways based on the knowledge graphs (KGs) to address the problem of reasoning, domain-specificity and novelty. The divergence stage this reasoning explores utilizes a new node importance estimation (NIE) technique over KGs to discover latent connections supporting idea generation. In the convergence stage of this reasoning, subgraph matching using analogical reasoning (SMAR) is applied to find matching patterns to describe a new idea. The use of SMAR + NIE and KGs helps to achieve an improvement in reasoning over KGs before transferring such reasoning to LLMs for translation of idea into natural language. To evaluate the degree of novelty of ideas generated, a relevance-to-novelty scoring metrics is proposed based on multiple premise entailment (MPE). We combined this metric with other popular metrics to evaluate the performance of SMAR + NIE on benchmark datasets, and as well on the quality of ideas generated. Findings from the study showed that this approach demonstrates competitive performance with mainstream LLMs in idea generation tasks.}
}
@article{BASHIR2025102406,
title = {Logic-infused knowledge graph QA: Enhancing large language models for specialized domains through Prolog integration},
journal = {Data & Knowledge Engineering},
volume = {157},
pages = {102406},
year = {2025},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2025.102406},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X25000011},
author = {Aneesa Bashir and Rong Peng and Yongchang Ding},
keywords = {Knowledge Graph Question Answering (KGQA), Large language models (LLMs), Logical programming (Prolog), Named entity recognition (NER), Multi-hop reasoning, Transformer, BERT},
abstract = {Efficiently answering questions over complex, domain-specific knowledge graphs remain a substantial challenge, as large language models (LLMs) often lack the logical reasoning abilities and particular knowledge required for such tasks. This paper presents a novel framework integrating LLMs with logical programming languages like Prolog for Logic-Infused Knowledge Graph Question Answering (KGQA) in specialized domains. The proposed methodology uses a transformer-based encoder–decoder architecture. An encoder reads the question, and a named entity recognition (NER) module connects entities to the knowledge graph. The extracted entities are fed into a grammar-guided decoder, producing a logical form (Prolog query) that captures the semantic constraints and relationships. The Prolog query is executed over the knowledge graph to perform symbolic reasoning and retrieve relevant answer entities. Comprehensive experiments on the MetaQA benchmark dataset demonstrate the superior performance of this logic-infused method in accurately identifying correct answer entities from the knowledge graph. Even when trained on a limited subset of annotated data, it outperforms state-of-the-art baselines, achieving 89.60 % and F1-scores of up to 89.61 %, showcasing its effectiveness in enhancing large language models with symbolic reasoning capabilities for specialized question-answering tasks. The seamless integration of LLMs and logical programming enables the proposed framework to reason effectively over complex, domain-specific knowledge graphs, overcoming a key limitation of existing KGQA systems. In specialized domains, the interpretability provided by representing questions such as Prologue queries is a valuable asset.}
}
@article{NIE2023,
title = {Research on the Construction and Application of Knowledge Graph in the Ceramic Field Based on Natural Language Processing},
journal = {International Journal on Semantic Web and Information Systems},
volume = {19},
number = {1},
year = {2023},
issn = {1552-6283},
doi = {https://doi.org/10.4018/IJSWIS.327352},
url = {https://www.sciencedirect.com/science/article/pii/S1552628323000340},
author = {Yu Nie and Na Huang and Junjie Peng and Guanghua Song and Yilai Zhang and Yongkang Peng and Chenglin Ni},
keywords = {Ceramics Field, Entity Recognition, Knowledge Graph, Natural Language Processing, Relationship Recognition},
abstract = {ABSTRACT
There are problems of knowledge deficiency and effective unified expression of knowledge in the process of relevant knowledge data acquired by workers in the ceramic domain. In this study, the authors designed relevant experiments to construct ceramic field knowledge graphs to solve these problems. In the experiments of named entity recognition and relationship recognition, the authors compared the performance of several models in OwnThink and ceramics field datasets. The experimental results showed that the BiLSTM-CRF model is the best for named entity recognition and the TextCNN model is the best for relationship recognition in ceramics field datasets. Therefore, the first used the BiLSTM-CRF model to complete the naming entity recognition and then combined with the TextCNN model to complete the relationship recognition to construct the ceramic field knowledge graph. Then, they applied the constructed graph to the ceramic knowledge Q&A service to provide accurate data retrieval service for ceramic domain workers.}
}
@article{XU2025348,
title = {Design of Question-Answering and Reasoning System Combining Large Language Models and Knowledge Graphs},
journal = {Procedia Computer Science},
volume = {262},
pages = {348-357},
year = {2025},
note = {The 5th International Conference on Multi-modal Information Analytics (MMIA)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.05.062},
url = {https://www.sciencedirect.com/science/article/pii/S187705092501909X},
author = {Yanru Xu},
keywords = {LLM, LLaMa, KG, question-answering, reasoning, system design, command fine-tuning},
abstract = {Problem reasoning uses natural language processing technology to analyze natural language questions input by users, and finally generate accurate and suitable answers. At present, the performance of Q&A model has been significantly improved, and the deep semantic understanding of text can be obtained through accurate knowledge reasoning. Based on LLaMa model, this paper systematically studies Transformer architecture, normalization technology, rotary position coding and packet query attention, creatively studies the combination of LLM and KG, and constructs a mathematical model of instruction fine-tuning algorithm. It not only understands the surface meaning of text, but also uses background knowledge such as entity relationships in KG to execute instructions more accurately and intelligently. The LLM studied in this paper is combined with KG to build a question-and-answer reasoning system, which can overcome the "illusion" problem of large models. KG significantly improves the accuracy by constrains the generated results of large models with structured knowledge.}
}
@article{LIANG2025883,
title = {A survey of large language model-augmented knowledge graphs for advanced complex product design},
journal = {Journal of Manufacturing Systems},
volume = {80},
pages = {883-901},
year = {2025},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2025.04.016},
url = {https://www.sciencedirect.com/science/article/pii/S0278612525001050},
author = {Xinxin Liang and Zuoxu Wang and Jihong Liu},
keywords = {Knowledge Graph, Large Language Model, Complex Product Design, Intelligent Manufacturing},
abstract = {In the Human-AI collaboration rapid development era, the design and development of knowledge-intensive complex products should enable the design process with the help of advanced AI technology, and enhance the reasoning and application of design domain knowledge. Extracting and reusing domain knowledge would greatly facilitate the success of complex product design. Knowledge graphs (KGs), a powerful knowledge representation and storage technology, have been widely deployed in advanced complex product design because of their advantages in mining and applying large-scale, complex, and specialized domain knowledge. But merely KG and its related reasoning approaches still cannot fully support the ill-defined product design tasks. In the future complex product design, Human-AI collaboration will become a mainstream prevention trend. Large language models (LLMs) have outstanding performance in natural language understanding and generation, showing promising potential to collaborate with KGs in complex product design and development. Till 2024/03/04, only a few studies have systematically reviewed the current status of LLM and KG applications in the engineering field, not to mention a further detailed review in the complex product design field, leaving many issues not covered or fully examined. To fill this gap, 100 articles published in the last 4 years (i.e., 2021–2024) were screened and surveyed. This study provides a statistical analysis of the screened research articles, mainstream techniques of LLM & KG, and LLM & KG applications were analyze. To understand how KG and LLM could support complex product design, a framework of LLMs-augmented KG in advanced complex product design was proposed, which contains data layer, KG & LLM collaboration layer, enhanced design capability layer, and design task layer. Furthermore, we also discussed the challenges and future research directions of the LLM-KG-collaborated complex product design paradigm. As an exploratory review paper, it provides insightful ideas for implementing more specialized domain KGs in product design field.}
}
@article{FENG2025103217,
title = {Crafting user-centric prompts for UI generations based on Kansei engineering and knowledge graph},
journal = {Advanced Engineering Informatics},
volume = {65},
pages = {103217},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103217},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625001107},
author = {Xuejing Feng and Huifang Du and Jun Ma and Haofen Wang and Lijuan Zhou and Meng Wang},
keywords = {Human-centered AI, User interface generations, Prompt engineering, Knowledge graph, Kansei engineering},
abstract = {Text-to-image (T2I) models are emerging as a powerful tool for designers to create user interface (UI) prototypes from natural language inputs (i.e., prompts). However, the discrepancy between designer inputs and model-preferred prompts makes it challenging for designers to consistently deliver effective results to end users. To bridge this gap, we introduce a novel hybrid method that assists designers in crafting user-centric prompts for T2I models, ensuring that the generated UIs align with end-user expectations. First, this method merges text mining and Kansei Engineering (KE) to analyze online user reviews and construct a Knowledge Graph (KG), mapping the intricate relationships between diverse affective requirements of users, design features, and corresponding text prompts for UI generation. Then, our approach automatically transforms designer inputs into model-preferred prompts through entity mention recognition and entity linking during the human-AI collaborative design process. Finally, we validate the proposed approach with a case study on automotive human–machine interface design. Experimental results demonstrate that our approach achieves high performance in perceived efficiency, satisfaction, and expectation disconfirmation. Overall, this study represents a step forward in integrating human and AI contributions in design and innovation within engineering disciplines, enabling AI to inspire, develop, and reinforce human creativity from a human factors perspective.}
}
@article{DENG2025104871,
title = {MedKA: A knowledge graph-augmented approach to improve factuality in medical Large Language Models},
journal = {Journal of Biomedical Informatics},
volume = {168},
pages = {104871},
year = {2025},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2025.104871},
url = {https://www.sciencedirect.com/science/article/pii/S1532046425001005},
author = {Yiyan Deng and Shen Zhao and Yongming Miao and Junjie Zhu and Jin Li},
keywords = {Knowledge graph, Large Language Models, Factuality evaluation, Natural Language Processing},
abstract = {Large language models (LLMs) have demonstrated remarkable potential in medical applications. However, they still face critical challenges such as hallucinations, knowledge inconsistency, and insufficient integration of domain-specific medical expertise. To address these limitations, we introduce MedKA, a novel knowledge graph-augmented approach for fine-tuning and evaluating medical LLMs. Our approach systematically transforms structured knowledge from a medical knowledge graph into a high-quality QA corpus, cMKGQA, by clustering multiple fields around clinically meaningful scenarios (e.g., diagnosis, treatment planning). This grouping strategy enables comprehensive and use-case-specific data construction and supports one-stage training of the LLM, ensuring better alignment with structured medical knowledge. This transformation process ensures the comprehensive integration of domain-specific knowledge while maintaining factual consistency. To evaluate the factuality of LLM-generated response, we further propose the Knowledge Graph-based Auxiliary Evaluation Metrics (KG-AEMs)—a novel benchmarking framework that compares LLM outputs with fine-grained, attribute-level ground truth from knowledge graph. Experimental results demonstrate that MedKA achieves state-of-the-art performance, significantly outperforming existing models, including LLaMA-3.1-8B-Chinese-Chat, HuatuoGPT2-7B, and Apollo2-7B. On the cMKGQA dataset, MedKA achieves 44.63 BLEU-1 and 17.62 BLEU-4 scores, with particularly strong performance in areas such as medication recommendations and diagnostic tests as measured by KG-AEMs. Our approach highlights the potential of integrating knowledge graphs into LLM fine-tuning to improve the accuracy and reliability of medical AI systems. It advances factual accuracy in medical dialogue systems and provides a comprehensive framework for evaluating the integration of medical knowledge into LLMs. This work is publicly available on Github: https://github.com/Yai017/MedKA.}
}
@article{COSTELLO2023,
title = {Leveraging Knowledge Graphs and Natural Language Processing for Automated Web Resource Labeling and Knowledge Mobilization in Neurodevelopmental Disorders: Development and Usability Study},
journal = {Journal of Medical Internet Research},
volume = {25},
year = {2023},
issn = {1438-8871},
doi = {https://doi.org/10.2196/45268},
url = {https://www.sciencedirect.com/science/article/pii/S1438887123002388},
author = {Jeremy Costello and Manpreet Kaur and Marek Z Reformat and Francois V Bolduc},
keywords = {knowledge graph, natural language processing, neurodevelopmental disorders, autism spectrum disorder, intellectual disability, attention deficit hyperactivity disorder, named entity recognition, topic modeling, aggregation operator},
abstract = {Background
Patients and families need to be provided with trusted information more than ever with the abundance of online information. Several organizations aim to build databases that can be searched based on the needs of target groups. One such group is individuals with neurodevelopmental disorders (NDDs) and their families. NDDs affect up to 18% of the population and have major social and economic impacts. The current limitations in communicating information for individuals with NDDs include the absence of shared terminology and the lack of efficient labeling processes for web resources. Because of these limitations, health professionals, support groups, and families are unable to share, combine, and access resources.
Objective
We aimed to develop a natural language–based pipeline to label resources by leveraging standard and free-text vocabularies obtained through text analysis, and then represent those resources as a weighted knowledge graph.
Methods
Using a combination of experts and service/organization databases, we created a data set of web resources for NDDs. Text from these websites was scraped and collected into a corpus of textual data on NDDs. This corpus was used to construct a knowledge graph suitable for use by both experts and nonexperts. Named entity recognition, topic modeling, document classification, and location detection were used to extract knowledge from the corpus.
Results
We developed a resource annotation pipeline using diverse natural language processing algorithms to annotate web resources and stored them in a structured knowledge graph. The graph contained 78,181 annotations obtained from the combination of standard terminologies and a free-text vocabulary obtained using topic modeling. An application of the constructed knowledge graph is a resource search interface using the ordered weighted averaging operator to rank resources based on a user query.
Conclusions
We developed an automated labeling pipeline for web resources on NDDs. This work showcases how artificial intelligence–based methods, such as natural language processing and knowledge graphs for information representation, can enhance knowledge extraction and mobilization, and could be used in other fields of medicine.}
}
@article{CHEN2025103124,
title = {Knowledge Graphs for Multi-modal Learning: Survey and Perspective},
journal = {Information Fusion},
volume = {121},
pages = {103124},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2025.103124},
url = {https://www.sciencedirect.com/science/article/pii/S1566253525001976},
author = {Zhuo Chen and Yichi Zhang and Yin Fang and Yuxia Geng and Lingbing Guo and Jiaoyan Chen and Xiaoze Liu and Jeff Z. Pan and Ningyu Zhang and Huajun Chen and Wen Zhang},
keywords = {Knowledge Graphs, Multi-modal Learning, Knowledge-based Information Fusion, Visual Question Answering, Large Language Model, Literature review},
abstract = {Integrated with multi-modal learning, knowledge graphs (KGs) as structured knowledge repositories, can enhance AI for processing and understanding complex, real-world data. This paper provides a comprehensive survey of cutting-edge research on KG-aware multi-modal learning. For these core areas, we provide task definitions, evaluation benchmarks, and comprehensive insights into key breakthroughs, offering detailed explanations critical for conducting related research. Furthermore, we also discuss current challenges, highlighting emerging trends and future research directions. The repository for this paper can be found at https://github.com/zjukg/KG-MM-Survey.}
}
@article{CUI2025127430,
title = {KLLMs4Rec: Knowledge graph-enhanced LLMs sentiment extraction for personalized recommendations},
journal = {Expert Systems with Applications},
volume = {282},
pages = {127430},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127430},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425010528},
author = {Yachao Cui and Kaiguang Wang and Hongli Yu and Xiaoxu Guo and Han Cao},
keywords = {Large Language Models, Text sentiment analysis, Knowledge graphs, Personalized recommendation},
abstract = {Recommendation algorithms typically leverage auxiliary information such as user reviews and knowledge graphs to enhance algorithm performance, thereby alleviating data sparsity and cold start issues. Recently, researchers have increasingly employed large language models, which boast powerful natural language understanding capabilities, to further improve recommendation systems. However, these models often suffer from hallucination problems. Moreover, integrating heterogeneous information, such as reviews and knowledge graphs, can introduce new noise, potentially impairing recommendation performance. Knowledge graphs, as tightly organized structured knowledge bases, can assist in addressing the hallucination problem and heterogeneous information fusion problem of LLMs. To effectively address the aforementioned issues, we propose the Knowledge Graph-Enhanced Large Language Model Sentiment Extraction for the Personalized Recommendation Model (KLLMs4Rec). It aims to solve the LLMs hallucination problem and the noise problem caused by the fusion of heterogeneous information in recommender systems, and provide users with more accurate, diverse and novel personalized recommendations. To address the hallucination problem when extracting user sentiments from reviews with LLMs, we designed a knowledge graph-enhanced prompt template. It is worth noting that this scheme also solves the noise issue of heterogeneous information fusion. Additionally, to further expand user preferences extracted from reviews, this paper proposes a new hierarchical sentiment attention graph convolutional network, which utilizes three sentiment weight schemes to propagate user personalized preferences on the knowledge graph. Extensive experiments on the Movielens-20 m, Amazon-book, and Yelp datasets demonstrate that our model surpasses current leading methods while effectively addressing the hallucination problem of LLMs and the noise problem of heterogeneous information fusion.}
}
@article{GAJO2025104195,
title = {Natural vs programming language in LLM knowledge graph construction},
journal = {Information Processing & Management},
volume = {62},
number = {5},
pages = {104195},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2025.104195},
url = {https://www.sciencedirect.com/science/article/pii/S0306457325001360},
author = {Paolo Gajo and Alberto Barrón-Cedeño},
keywords = {Knowledge graph construction, large language models, Code language models, Information extraction},
abstract = {Research on knowledge graph construction (KGC) has recently shown great promise also thanks to the adoption of large language models (LLM) for the automatic extraction of structured information from raw text. However, most works rely on commercial, closed-source LLMs, hindering reproducibility and accessibility. We explore KGC with smaller, open-weight LLMs and investigate whether they can be used to improve upon the results obtained by systems leveraging bigger, closed-source models. Specifically, we focus on CodeKGC, a prompting framework based on GPT-3.5. We choose a variety of models either pre-trained primarily on natural language or on code and fine-tune them on three datasets used for information extraction. We fine-tune with prompts formatted either in natural language or as Python-like scripts. In addition, we optionally train the models with prompts including chain-of-thought sections. After fine-tuning, the choice of coding vs natural language prompts has a limited impact on performance, while chain-of-thought training mostly leads to a performance decrease. Moreover, we show that a LLM can be outperformed by much smaller versions on this task, after undergoing the same amount of training. We find that in general the selected lightweight LLMs outperform the much larger CodeKGC by as much as 15–20 absolute F1 points after fine-tuning. The results show that state-of-the-art KGC systems can be developed using smaller and open-weight models, enhancing research transparency, lowering compute requirements, and decreasing third-party API reliance. Code: https://github.com/TinfFoil/natcode-llm-kgc}
}
@article{HAN2025131027,
title = {SCR: A completion-then-reasoning framework for multi-hop question answering over incomplete knowledge graph},
journal = {Neurocomputing},
volume = {651},
pages = {131027},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.131027},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225016996},
author = {Ridong Han and Jia Liu and Haijia Bi and Tao Peng and Lu Liu},
keywords = {Knowledge graph question answering, Multi-hop, Reinforcement learning, Subgraph completion, Semantic rewards},
abstract = {Reinforcement learning has become the widely adopted technique for multi-hop knowledge graph question answering task thanks to its excellent interpretability in reasoning process. However, it is severely affected by the incompleteness of knowledge graphs and the sparse rewards caused by weak supervision. In this paper, we propose a completion-then-reasoning framework, called SCR, to address these two issues. For the incompleteness of knowledge graphs, we first extract a subgraph from the given knowledge graph for a given question, and use the knowledge graph embedding model to predict and complete missing triples, followed by reinforcement learning for answer reasoning on the completed subgraph. To alleviate the sparse rewards in reinforcement learning, we introduce a semantic reward based on the semantic similarity between original question and full relational path, enabling the model to receive partial rewards for partially correct paths instead of a zero reward. Detailed experiments on PQ, PQL, MetaQA, and WebQSP datasets demonstrate that our SCR model effectively improves the performance of multi-hop knowledge graph question answering task. Particularly, under sparse KG setting, SCR model outperforms baselines by a large margin, highlighting the effectiveness of completion-then-reasoning framework in mitigating the incompleteness of knowledge graphs.}
}
@article{MORIOKA2025762,
title = {Automatic construction of asset knowledge graph with large language model},
journal = {Procedia CIRP},
volume = {135},
pages = {762-767},
year = {2025},
note = {32nd CIRP Conference on Life Cycle Engineering (LCE2025)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2025.01.097},
url = {https://www.sciencedirect.com/science/article/pii/S2212827125004469},
author = {Tomoaki Morioka and Toshiaki Kono and Takehisa Nishida},
keywords = {Maintenance, Knowledge graph construction, Large language model, Reliability centered maintenance},
abstract = {Lifecycle engineering is a critical concept for fostering environmentally sustainable practices within the manufacturing sector. An essential component of lifecycle management for achieving sustainability is reliability-centered maintenance, which enhances various key performance indicators (KPIs), including machine availability and environmental impact. Effective and reliable maintenance necessitates expert knowledge of the equipment. For instance, determining which components and failure modes should be addressed through condition-based maintenance requires insights derived from failure mode and effect analysis (FMEA). However, constructing expert knowledge is labor-intensive, and ensuring its quality presents significant challenges. This study proposes a method for the automated construction of expert knowledge related to maintenance, along with a corresponding tool designed to reduce construction costs and enhance knowledge quality. The proposed method leverages a large language model (LLM) to automatically generate asset knowledge graphs based on FMEA. By combining general knowledge about equipment derived from the pre-trained LLM with specialized information extracted from technical documents, the tool creates knowledge structures such as component trees and failure modes. Subject matter experts can then iteratively refine and validate this knowledge. To evaluate the proposed approach, we assessed the accuracy and coverage of the knowledge generated by the tool in two case studies involving specific types of equipment. The results indicated that the LLM-generated output contained 4.98 times more items than those manually created, with precision ranging from 0.490 to 0.662 and recall ranging from 0.481 to 0.810.}
}
@article{DOU201819,
title = {Knowledge graph based on domain ontology and natural language processing technology for Chinese intangible cultural heritage},
journal = {Journal of Visual Languages & Computing},
volume = {48},
pages = {19-28},
year = {2018},
issn = {1045-926X},
doi = {https://doi.org/10.1016/j.jvlc.2018.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S1045926X18300041},
author = {Jinhua Dou and Jingyan Qin and Zanxia Jin and Zhuang Li},
keywords = {Intangible cultural heritage, The 24 solar terms, Domain ontology, Knowledge graph, Natural language processing, Deep learning},
abstract = {Intangible cultural heritage (ICH) is a precious historical and cultural resource of a country. Protection and inheritance of ICH is important to the sustainable development of national culture. There are many different intangible cultural heritage items in China. With the development of information technology, ICH database resources were built by government departments or public cultural services institutions, but most databases were widely dispersed. Certain traditional database systems are disadvantageous to storage, management and analysis of massive data. At the same time, a large quantity of data has been produced, accompanied by digital intangible cultural heritage development. The public is unable to grasp key knowledge quickly because of the massive and fragmented nature of the data. To solve these problems, we proposed the intangible cultural heritage knowledge graph to assist knowledge management and provide a service to the public. ICH domain ontology was defined with the help of intangible cultural heritage experts and knowledge engineers to regulate the concept, attribute and relationship of ICH knowledge. In this study, massive ICH data were obtained, and domain knowledge was extracted from ICH text data using the Natural Language Processing (NLP) technology. A knowledge base based on domain ontology and instances for Chinese intangible cultural heritage was constructed, and the knowledge graph was developed. The pattern and characteristics behind the intangible cultural heritage were presented based on the ICH knowledge graph. The knowledge graph for ICH could foster support for organization, management and protection of the intangible cultural heritage knowledge. The public can also obtain the ICH knowledge quickly and discover the linked knowledge. The knowledge graph is helpful for the protection and inheritance of intangible cultural heritage.}
}
@article{LI2025113500,
title = {KERMIT: Knowledge graph completion of enhanced relation modeling with inverse transformation},
journal = {Knowledge-Based Systems},
volume = {324},
pages = {113500},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113500},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125005465},
author = {Haotian Li and Bin Yu and Yuliang Wei and Kai Wang and Richard Yi Da Xu and Bailing Wang},
keywords = {Knowledge graph completion (KGC), Large language models (LLMs), Supervised contrastive learning},
abstract = {Knowledge graph completion (KGC) revolves around populating missing triples in a knowledge graph using available information. Text-based methods, which depend on textual descriptions of triples, often encounter difficulties when these descriptions lack sufficient information for accurate prediction, an issue inherent to the datasets and not easily resolved through modeling alone. To address this and ensure data consistency, we first use large language models (LLMs) to generate coherent descriptions, bridging the semantic gap between queries and answers. Secondly, we utilize inverse relations to create a symmetric graph, thereby providing augmented training samples for KGC. Additionally, we employ the label information inherent in knowledge graphs (KGs) to enhance the existing contrastive framework, making it fully supervised. These efforts have led to significant performance improvements on the WN18RR, FB15k-237 and UMLS datasets. According to standard evaluation metrics, our approach achieves a 3.0% improvement in Hit@1 on WN18RR and a 12.1% improvement in Hit@3 on UMLS, demonstrating superior performance.}
}
@article{YANG2025,
title = {Large Language Model–Driven Knowledge Graph Construction in Sepsis Care Using Multicenter Clinical Databases: Development and Usability Study},
journal = {Journal of Medical Internet Research},
volume = {27},
year = {2025},
issn = {1438-8871},
doi = {https://doi.org/10.2196/65537},
url = {https://www.sciencedirect.com/science/article/pii/S1438887125004534},
author = {Hao Yang and Jiaxi Li and Chi Zhang and Alejandro Pazos Sierra and Bairong Shen},
keywords = {sepsis, knowledge graph, large language models, prompt engineering, real-world, GPT-4.0},
abstract = {Background
Sepsis is a complex, life-threatening condition characterized by significant heterogeneity and vast amounts of unstructured data, posing substantial challenges for traditional knowledge graph construction methods. The integration of large language models (LLMs) with real-world data offers a promising avenue to address these challenges and enhance the understanding and management of sepsis.
Objective
This study aims to develop a comprehensive sepsis knowledge graph by leveraging the capabilities of LLMs, specifically GPT-4.0, in conjunction with multicenter clinical databases. The goal is to improve the understanding of sepsis and provide actionable insights for clinical decision-making. We also established a multicenter sepsis database (MSD) to support this effort.
Methods
We collected clinical guidelines, public databases, and real-world data from 3 major hospitals in Western China, encompassing 10,544 patients diagnosed with sepsis. Using GPT-4.0, we used advanced prompt engineering techniques for entity recognition and relationship extraction, which facilitated the construction of a nuanced sepsis knowledge graph.
Results
We established a sepsis database with 10,544 patient records, including 8497 from West China Hospital, 690 from Shangjin Hospital, and 357 from Tianfu Hospital. The sepsis knowledge graph comprises of 1894 nodes and 2021 distinct relationships, encompassing nine entity concepts (diseases, symptoms, biomarkers, imaging examinations, etc) and 8 semantic relationships (complications, recommended medications, laboratory tests, etc). GPT-4.0 demonstrated superior performance in entity recognition and relationship extraction, achieving an F1-score of 76.76 on a sepsis-specific dataset, outperforming other models such as Qwen2 (43.77) and Llama3 (48.39). On the CMeEE dataset, GPT-4.0 achieved an F1-score of 65.42 using few-shot learning, surpassing traditional models such as BERT-CRF (62.11) and Med-BERT (60.66). Building upon this, we compiled a comprehensive sepsis knowledge graph, comprising of 1894 nodes and 2021 distinct relationships.
Conclusions
This study represents a pioneering effort in using LLMs, particularly GPT-4.0, to construct a comprehensive sepsis knowledge graph. The innovative application of prompt engineering, combined with the integration of multicenter real-world data, has significantly enhanced the efficiency and accuracy of knowledge graph construction. The resulting knowledge graph provides a robust framework for understanding sepsis, supporting clinical decision-making, and facilitating further research. The success of this approach underscores the potential of LLMs in medical research and sets a new benchmark for future studies in sepsis and other complex medical conditions.}
}
@article{ZHANG2025293,
title = {A disambiguation method for potential ambiguities in Chinese based on knowledge graphs and large language model},
journal = {Alexandria Engineering Journal},
volume = {126},
pages = {293-302},
year = {2025},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2025.04.089},
url = {https://www.sciencedirect.com/science/article/pii/S1110016825005861},
author = {Dan Zhang and Delong Jia},
keywords = {Chinese ambiguity, Disambiguation model, Knowledge graph, Large language model, Natural language processing},
abstract = {Traditional disambiguation methods struggle to effectively balance and integrate a wide range of contextual information and world knowledge when dealing with potential ambiguities in Chinese. To address this issue, this paper proposes a disambiguation model that integrates knowledge graphs and large language models (LLMs) to tackle lexical ambiguity in Chinese texts. This article uses an attention based disambiguation model, which is fine-tuned using multiple hyperparameter configurations. It optimizes network layers and knowledge graph embedding dimensions to enhance performance. Visualization of the attention mechanism reveals the model's focus on target words, context, and knowledge graph entities. Experiments conducted on a dataset comprising 200,000 sentences demonstrate significant improvements in accuracy and F1 scores, reaching 92.4 % and 91.9 %, respectively, compared to traditional statistical and deep learning models. Visualization of the attention mechanism reveals the model's focus on target words, context, and knowledge graph entities. The findings suggest that integrating knowledge graphs with LLMs offers an innovative approach to complex language tasks. In practical applications such as machine translation and chatbots, this model is expected to enhance both performance and interpretability.}
}
@article{LIU2025106500,
title = {Natural language-extracted and BIM-referenced knowledge base for construction quality inspection via augmented reality},
journal = {Automation in Construction},
volume = {179},
pages = {106500},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106500},
url = {https://www.sciencedirect.com/science/article/pii/S0926580525005400},
author = {Han Liu and Donghai Liu and Junjie Chen},
keywords = {Construction quality inspection, Building information modeling, Knowledge mining, Knowledge database, Smart construction, Augmented reality},
abstract = {Construction quality is of upmost importance for delivering well-performed civil structures. Inspection offers a critical means to ensure construction quality. However, its effective implementation relies on an excess of domain knowledge that usually takes years to accumulate, making inspection expensive and challenging to conduct. This paper introduces a construction quality inspection knowledge base that empowers inspectors with easily accessible and intuitive construction requirement information. Natural language processing (NLP) is applied to automatically extract knowledge from construction documents such as specification and regulatory files. The extracted knowledge is linked to the building information model (BIM) using proposed association methods and semantic similarity matching. The natural language-extracted and BIM-referenced knowledge base (NLBIM-KB) is integrated into an augmented reality (AR) interface, which provides a freehand tool to assist inspectors' decision-making via on-demand construction knowledge extraction.}
}
@article{KAUR2022,
title = {Deciphering the Diversity of Mental Models in Neurodevelopmental Disorders: Knowledge Graph Representation of Public Data Using Natural Language Processing},
journal = {Journal of Medical Internet Research},
volume = {24},
number = {8},
year = {2022},
issn = {1438-8871},
doi = {https://doi.org/10.2196/39888},
url = {https://www.sciencedirect.com/science/article/pii/S143888712200512X},
author = {Manpreet Kaur and Jeremy Costello and Elyse Willis and Karen Kelm and Marek Z Reformat and Francois V Bolduc},
keywords = {concept map, neurodevelopmental disorder, knowledge graph, text analysis, semantic relatedness, PubMed, forums, mental model},
abstract = {Background
Understanding how individuals think about a topic, known as the mental model, can significantly improve communication, especially in the medical domain where emotions and implications are high. Neurodevelopmental disorders (NDDs) represent a group of diagnoses, affecting up to 18% of the global population, involving differences in the development of cognitive or social functions. In this study, we focus on 2 NDDs, attention deficit hyperactivity disorder (ADHD) and autism spectrum disorder (ASD), which involve multiple symptoms and interventions requiring interactions between 2 important stakeholders: parents and health professionals. There is a gap in our understanding of differences between mental models for each stakeholder, making communication between stakeholders more difficult than it could be.
Objective
We aim to build knowledge graphs (KGs) from web-based information relevant to each stakeholder as proxies of mental models. These KGs will accelerate the identification of shared and divergent concerns between stakeholders. The developed KGs can help improve knowledge mobilization, communication, and care for individuals with ADHD and ASD.
Methods
We created 2 data sets by collecting the posts from web-based forums and PubMed abstracts related to ADHD and ASD. We utilized the Unified Medical Language System (UMLS) to detect biomedical concepts and applied Positive Pointwise Mutual Information followed by truncated Singular Value Decomposition to obtain corpus-based concept embeddings for each data set. Each data set is represented as a KG using a property graph model. Semantic relatedness between concepts is calculated to rank the relation strength of concepts and stored in the KG as relation weights. UMLS disorder-relevant semantic types are used to provide additional categorical information about each concept’s domain.
Results
The developed KGs contain concepts from both data sets, with node sizes representing the co-occurrence frequency of concepts and edge sizes representing relevance between concepts. ADHD- and ASD-related concepts from different semantic types shows diverse areas of concerns and complex needs of the conditions. KG identifies converging and diverging concepts between health professionals literature (PubMed) and parental concerns (web-based forums), which may correspond to the differences between mental models for each stakeholder.
Conclusions
We show for the first time that generating KGs from web-based data can capture the complex needs of families dealing with ADHD or ASD. Moreover, we showed points of convergence between families and health professionals’ KGs. Natural language processing–based KG provides access to a large sample size, which is often a limiting factor for traditional in-person mental model mapping. Our work offers a high throughput access to mental model maps, which could be used for further in-person validation, knowledge mobilization projects, and basis for communication about potential blind spots from stakeholders in interactions about NDDs. Future research will be needed to identify how concepts could interact together differently for each stakeholder.}
}
@article{CHEN2025104795,
title = {Automatic generation of monitoring report based on large language model and knowledge graph inference},
journal = {Results in Engineering},
volume = {26},
pages = {104795},
year = {2025},
issn = {2590-1230},
doi = {https://doi.org/10.1016/j.rineng.2025.104795},
url = {https://www.sciencedirect.com/science/article/pii/S2590123025008722},
author = {Zengxiong Chen and Yanfeng Qiu and Longlong Yang and Baijian Liao and Defa Cao},
keywords = {Large language model, Knowledge graph inference, Monitoring reports, Automatic generation, Stream processing technology, Ant colony algorithm, Semantic similarity},
abstract = {To optimize traditional data collection, sorting, analysis, and report preparation processes, this paper introduces an automatic monitoring report generation method grounded in large language models and knowledge graph inference. Leveraging stream processing technology for data loading, transformation, and extraction, and the ant colony algorithm for querying and integrating environmental, equipment, and virtual monitoring data, this method enhances feature extraction and data using Mel-frequency Cepstral Coefficients (MFCC) for speech signals and Generative Pre-trained Transformer (GPT) series models. Text-based semantic similarity evaluation and a replication mechanism in time knowledge graph inference further refine the process. The experimental results show significant improvement: for 300,000 data points, the extraction time of the design method is 39.2 s, and the efficiency is improved by 86.3 %, 87.6 %, 88.5 % and 85.5 %, respectively, compared with the methods in [[3], [4], [5], [6]]. For 10 data points, the extraction time was only 4.29 μ s. With 180 nodes, the packet loss rate is a mere 0.09 %, and the knowledge graph coverage exceeds 90 %. This method's unique contribution lies in its seamless integration of advanced technologies, ensuring high efficiency and accuracy in data processing and analysis.}
}
@article{GAN2023106660,
title = {Knowledge graph construction based on ship collision accident reports to improve maritime traffic safety},
journal = {Ocean & Coastal Management},
volume = {240},
pages = {106660},
year = {2023},
issn = {0964-5691},
doi = {https://doi.org/10.1016/j.ocecoaman.2023.106660},
url = {https://www.sciencedirect.com/science/article/pii/S0964569123001850},
author = {Langxiong Gan and Beiyan Ye and Zhiqiu Huang and Yi Xu and Qiaohong Chen and Yaqing Shu},
keywords = {Ship collision accident knowledge graph (SCAKG), Ontology module, Ship accident reports, Maritime traffic safety, Natural language processing},
abstract = {As an important data source, marine accident investigation reports are frequently used for accident analysis. However, it is hard to extract effective information since key knowledge is normally hidden in large blocks of text. In most cases, the collection of accident-related data is done manually. In this paper, a new knowledge graph construction approach to explore ship collision accidents is proposed, aiming to show the correlation among important factors of the accidents. In this research, 241 investigation reports on ship collision accidents from 2018 to 2021 published on the official website of the China Maritime Safety Administration (CMSA) were collected and analyzed. Then, the ship collision accident ontology module is constructed. According to the ontology information in the accident reports, entities were divided into context-based metadata and content-based metadata, which were used for describing different types of data. To extract the information for the accident report with semi-structured data, an information extraction module based on ontology was proposed. In this process, natural language processing (NLP) was used to obtain text information about the ontology. On this basis, the Ship Collision Accident Knowledge Graph (SCAKG) including 910 entity nodes and 1920 relation edges was constructed and stored in the graph database Neo4j. Finally, two case retrievals were conducted using the SCAKG to show the potential utilization of the method. The results show the effectiveness of the proposed approach in terms of discovering the internal relationship of the accident and could be used to expedite the judicial process, which simplifies the process of marine accident investigation.}
}
@article{ZHANG2025129919,
title = {LGKGR: A knowledge graph reasoning model using LLMs augmented GNNs},
journal = {Neurocomputing},
volume = {635},
pages = {129919},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.129919},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225005910},
author = {Yuanming Zhang and Wenbo Zheng and Jiacheng Huang and Gang Xiao},
keywords = {Knowledge graph, Knowledge graph reasoning, Graph neural networks, Large language models},
abstract = {Knowledge graph reasoning (KGR) aims to infer new factual knowledge based on existing structured factual data, and plays a vital role in various applications. Graph neural networks (GNNs)-based methods have garnered attention due to their exceptional capabilities in learning graph structures. However, they cannot effectively leverage rich text semantics within KG for reasoning. Given the remarkable semantic understanding capabilities of large language models (LLMs), this paper proposes a novel KGR model using LLMs augmented GNNs (LGKGR), which aims to utilize LLMs to enhance the graph structure learning of GNNs. Each round of reasoning includes three stages: path search, path pruning, and path decision. The first stage adopts an incremental path search strategy to identify adjacent entities of current query entity and extract features. The second stage adopts GNNs as a pruning tool to filter out semantically irrelevant reasoning paths. The third stage exploits LLMs for semantic analysis of candidate reasoning paths, and then selects the most possible reasoning paths. In the end, LLMs are further exploited to analyze the semantic information of reasoning paths and generate final reasoning results. Experimental results on public datasets demonstrate that the proposed method achieves an average improvement of 2.1% in the MRR metric and 2.68% in the Hits@1 metric compared to existing SOTA methods. Explainable reasoning justifications are also generated during the reasoning process.}
}
@article{LEE2025114390,
title = {Target-guided dialog generation with dynamic knowledge path by commonsense knowledge graph and relation prediction},
journal = {Knowledge-Based Systems},
volume = {329},
pages = {114390},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.114390},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125014297},
author = {Hayoung Lee and Soyeop Yoo and Woong-Kee Loh and Ok-Ran Jeong},
keywords = {Target-guided dialog system, Knowledge graph, Relation prediction, Commonsense reasoning, Global planning},
abstract = {Conversational AI has been rapidly advancing with the development of large language models and has shown excellent performance. However, one of its limitations is a passive system that cannot ask or guide users back to ambiguous questions. To overcome this, we have implemented an active dialog system that can smoothly transition from previous conversations. Our system is a target-guided system, which means it can guide the conversation by asking the user to provide a desired response or target. This approach is knowledge-rich and challenging, as it requires achieving the target while maintaining contextual consistency. To generate responses, we dynamically construct knowledge paths through knowledge graphs and relation predictors. These play an essential role in generating diverse and logically connected responses. To achieve this, we follow a global planning method that systematically conducts conversations with a target, and constructs knowledge paths based on common sense. We perform multi-hop reasoning and bi-directional search simultaneously to increase diversity and logical connectivity. We have overcome the limitations of existing works that rely solely on knowledge graphs by reflecting the results of relation predictors along with each object’s WIKI data in the path. Therefore, the consideration of the knowledge graph and the performance of the relation predictor, compared to the existing system, in completing the dynamic knowledge path and generating transition responses allowed conversations to transition more naturally. We have verified the proposed model through experiments.}
}
@article{YE2026129199,
title = {SuKE: Structural Knowledge Extractor enhances large language model for knowledge graph completion},
journal = {Expert Systems with Applications},
volume = {297},
pages = {129199},
year = {2026},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.129199},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425028155},
author = {Yunhai Ye and Shuo Wang},
keywords = {Knowledge graph, Knowledge graph completion, Large language model, Graph neural network, Cross-modality transferability},
abstract = {Current large language model (LLM)-based knowledge graph completion (KGC) methods fail to fully leverage structural information of the knowledge graph (KG), resulting in suboptimal performance. They typically incorporate KG information into LLMs through either direct fine-tuning or employing soft prompts derived from conventional embedding-based KGC models. However, these methodologies exhibit fundamental limitations in their capacity to comprehensively encode and exploit the intricate structural information contained within KGs. To address these issues, we first develop KG-infused in-context learning and KG-infused instruction tuning by extending existing LLM paradigms to inject structural information through textual representations. Then we propose a Structural Knowledge Extractor (SuKE), a trainable encoder-decoder architecture designed to extract both structural and semantic information from KGs and incorporate it into LLMs, with the overarching goal of enhancing the structural reasoning capabilities of LLMs for KGC tasks. Specifically, in encoder, we propose a Group-wise Graph Attention Network with relation-augmented message function for entity embeddings generation and a dynamic relation representation module using subgraph relational path to produce relation embeddings. Subsequently, a global embedding mechanism is introduced to mitigate the over-smoothing issue of entity and relation embeddings. In decoder, a cross-modal projection mechanism transforms the embeddings of triples into virtual tokens within the textual space, which are then pretended as prefixes to the prompt tokens and injected into the LLM. Experimental performance outperforms the state-of-the-art methods, demonstrating that SuKE significantly boosts the performance of LLMs on KGC tasks and revealing the effectiveness of structural information.}
}
@article{HUANG2025113648,
title = {Prompting large language models with knowledge graphs for question answering involving long-tail facts},
journal = {Knowledge-Based Systems},
volume = {324},
pages = {113648},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113648},
url = {https://www.sciencedirect.com/science/article/pii/S095070512500694X},
author = {Wenyu Huang and Guancheng Zhou and Mirella Lapata and Pavlos Vougiouklis and Sebastien Montella and Jeff Z. Pan},
keywords = {Large language models, Knowledge graphs, Retrieval-augmented generation, Evaluation},
abstract = {Although Large Language Models (LLMs) are effective in performing various NLP tasks, they still struggle to handle tasks that require extensive, real-world knowledge, especially when dealing with long-tail facts (facts related to long-tail entities). This limitation highlights the need to supplement LLMs with non-parametric knowledge. To address this issue, we analysed the effects of different types of non-parametric knowledge, including textual passage and knowledge graphs (KGs). Since LLMs have probably seen the majority of factual question-answering datasets already, to facilitate our analysis, we proposed a fully automatic pipeline for creating a benchmark that requires knowledge of long-tail facts for answering the involved questions. Using this pipeline, we introduce the LTGen benchmark. We evaluate state-of-the-art LLMs in different knowledge settings using the proposed benchmark. Our experiments show that LLMs alone struggle with answering these questions, especially when the long-tail level is high or rich knowledge is required. Nonetheless, the performance of the same models improved significantly when they were prompted with non-parametric knowledge. We observed that, in most cases, prompting LLMs with KG triples surpasses passage-based prompting using a state-of-the-art retriever. In addition, while prompting LLMs with both KG triples and documents does not consistently improve knowledge coverage, it can dramatically reduce hallucinations in the generated content.}
}