@article{DAS2022117575,
title = {A multi-stage multimodal framework for sentiment analysis of Assamese in low resource setting},
journal = {Expert Systems with Applications},
volume = {204},
pages = {117575},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.117575},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422008879},
author = {Ringki Das and Thoudam Doren Singh},
keywords = {Multimodal sentiment analysis, Low resource, Assamese language, Fusion, Caption generation, Deep learning},
abstract = {Multimodality has shown to be helpful in several natural language processing tasks. Thus, adding multiple modalities to the traditional sentiment analysis has also proven to be useful. However, multimodality in a low resource setting for sentiment analysis is yet to be explored for several resource-constrained languages. Assamese is a low-resource language spoken mainly in the state of Assam in India. This paper presents an Assamese multimodal dataset comprising of 16,000 articles from the news domain as a benchmark resource. Secondly, we present a multi-stage multimodal sentiment analysis framework that concurrently exploits textual and visual cues to determine the sentiment. The proposed architecture encodes the news content collaboratively. The text branch encodes semantic content information by considering the semantic information of the news. At the same time, the visual branch encodes the visual appearance information from the news image. Then, an intermediate fusion-based multimodal framework is proposed to exploit the internal correlation between textual and visual features for joint sentiment classification. Finally, a decision-level fusion mechanism is employed on the three models to integrate cross-modal information effectively for final sentiment prediction. Experimental results conducted on the Assamese dataset built in-house demonstrate that the contextual integration of multimodal features delivers better performance (89.3%) than the best unimodal features (85.6%).}
}
@article{LAI2023102563,
title = {Multimodal sentiment analysis: A survey},
journal = {Displays},
volume = {80},
pages = {102563},
year = {2023},
issn = {0141-9382},
doi = {https://doi.org/10.1016/j.displa.2023.102563},
url = {https://www.sciencedirect.com/science/article/pii/S0141938223001968},
author = {Songning Lai and Xifeng Hu and Haoxuan Xu and Zhaoxia Ren and Zhi Liu},
keywords = {Multimodal sentiment analysis, Multimodal fusion, Affective computing, Computer vision},
abstract = {Multimodal sentiment analysis has emerged as a prominent research field within artificial intelligence, benefiting immensely from recent advancements in deep learning. This technology has unlocked unprecedented possibilities for application and research, rendering it a highly sought-after area of study. In this review, we aim to present a comprehensive overview of multimodal sentiment analysis by delving into its definition, historical context, and evolutionary trajectory. Furthermore, we explore recent datasets and state-of-the-art models, with a particular focus on the challenges encountered and the future prospects that lie ahead. By offering constructive suggestions for promising research directions and the development of more effective multimodal sentiment analysis models, this review intends to provide valuable guidance to researchers in this dynamic field.}
}
@article{PANDEY2024111206,
title = {Progress, achievements, and challenges in multimodal sentiment analysis using deep learning: A survey},
journal = {Applied Soft Computing},
volume = {152},
pages = {111206},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2023.111206},
url = {https://www.sciencedirect.com/science/article/pii/S1568494623012243},
author = {Ananya Pandey and Dinesh Kumar Vishwakarma},
keywords = {Multimodal sentiment analysis (MSA), Sentiment analysis (SA), Multiple modalities, Fusion methods, Deep architectures, Challenges, Benchmark datasets},
abstract = {Sentiment analysis is a computational technique that analyses the subjective information conveyed within a given expression. This encompasses appraisals, opinions, attitudes or emotions towards a particular subject, individual, or entity. Conventional sentiment analysis solely considers the text modality and derives sentiment by identifying the semantic relationship between words within a sentence. Despite this, certain expressions, such as exaggeration, sarcasm and humor, pose a challenge for automated detection when conveyed only through text. Multimodal sentiment analysis incorporates various forms of data, such as visual and acoustic cues, in addition to text. By utilizing fusion analysis, this approach can more precisely determine the implied sentiment polarity, which includes positive, neutral, and negative sentiments. Thus, the recent advancements in deep learning have boosted the domain of multimodal sentiment analysis to new heights. The research community has also shown significant interest in this topic due to its potential for both practical application and educational research. In light of this fact, this paper aims to present a thorough analysis of recent ground-breaking research studies conducted in multimodal sentiment analysis, which employs deep learning models across various modalities such as text, audio, image, and video. Furthermore, the article dives into a discussion of the multiple categories of multimodal data, diverse domains in which multimodal sentiment analysis can be applied, a range of operations that are integral to multimodal sentiment analysis, deep learning architectures, a variety of fusion methods, challenges associated with multimodal sentiment analysis, and the benchmark datasets in addition to the state-of-the-art approaches. The ultimate goal of this survey is to indicate the success of deep learning architectures in tackling the complexities associated with multimodal sentiment analysis.}
}
@article{WICKRAMASEKARA2025301859,
title = {Exploring the potential of large language models for improving digital forensic investigation efficiency},
journal = {Forensic Science International: Digital Investigation},
volume = {52},
pages = {301859},
year = {2025},
issn = {2666-2817},
doi = {https://doi.org/10.1016/j.fsidi.2024.301859},
url = {https://www.sciencedirect.com/science/article/pii/S2666281724001860},
author = {Akila Wickramasekara and Frank Breitinger and Mark Scanlon},
keywords = {Digital forensics, Large language models, LLM, Investigative process, Challenges},
abstract = {The ever-increasing workload of digital forensic labs raises concerns about law enforcement's ability to conduct both cyber-related and non-cyber-related investigations promptly. Consequently, this article explores the potential and usefulness of integrating Large Language Models (LLMs) into digital forensic investigations to address challenges such as bias, explainability, censorship, resource-intensive infrastructure, and ethical and legal considerations. A comprehensive literature review is carried out, encompassing existing digital forensic models, tools, LLMs, deep learning techniques, and the use of LLMs in investigations. The review identifies current challenges within existing digital forensic processes and explores both the obstacles and the possibilities of incorporating LLMs. In conclusion, the study states that the adoption of LLMs in digital forensics, with appropriate constraints, has the potential to improve investigation efficiency, improve traceability, and alleviate the technical and judicial barriers faced by law enforcement entities.}
}
@article{PENG2023119721,
title = {A fine-grained modal label-based multi-stage network for multimodal sentiment analysis},
journal = {Expert Systems with Applications},
volume = {221},
pages = {119721},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.119721},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423002221},
author = {Junjie Peng and Ting Wu and Wenqiang Zhang and Feng Cheng and Shuhua Tan and Fen Yi and Yansong Huang},
keywords = {Sentiment analysis, Multimodality, Multi-task learning, Multimodal fusion},
abstract = {Sentiment analysis is a challenging but valuable research topic in affective computing. It can improve the quality of various real-world applications, including financial market prediction, disease analysis even politics. As sentiment may be expressed by text, image, audio, video, etc., multimodal sentiment analysis has emerged to capture information in multiple ways. Take video as an example, the analysis process may be difficult since the modalities in the video are heterogeneous and may express different sentiments. To deal with such issues, a Fine-grained modal label-based Multi-Stage Network (FmlMSN) is proposed. Utilizing seven sentiment labels in unimodal, bimodal and trimodal, the model focuses on information at different granularities from text, audio, image and the combinations of them. Meanwhile, inspired by the idea of stacking ensemble learning which is still limited in sentiment analysis, multi-stage training is performed for base learners of acoustic-visual, visual-textual and acoustic-textual. In each stage, the singleton modality and pair-wise modalities are interconnected by hard parameter sharing multi-task learning. Subsequently, the hidden bimodal features are used to train the meta-learner for the final sentiment prediction. Extensive experiments on three public datasets, including one in Chinese and two in English indicate that our model outperforms the existing state-of-the-art methods. Furthermore, empirical analysis suggests that the model is flexible and can reduce training time and calculation to some extent.}
}
@article{GHORBANALI2022102929,
title = {Ensemble transfer learning-based multimodal sentiment analysis using weighted convolutional neural networks},
journal = {Information Processing & Management},
volume = {59},
number = {3},
pages = {102929},
year = {2022},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2022.102929},
url = {https://www.sciencedirect.com/science/article/pii/S030645732200053X},
author = {Alireza Ghorbanali and Mohammad Karim Sohrabi and Farzin Yaghmaee},
keywords = {Multimodal sentiment analysis, Transfer learning, Ensemble learning, Deep learning, Dempster-Shafer},
abstract = {Huge amounts of multimodal content and comments in a mixture form of text, image, and emoji are continuously shared by users on various social networks. Most of the comments of the users in these networks have emotional aspects, which make the multimodal sentiment analysis (MSA) an important and attractive research topics in this area. In this paper, an ensemble transfer learning method is exploited to propose a hybrid MSA model based on weighted convolutional neural networks. The extended Dempster–Shafer (Yager) theory is also utilized in the proposed method of this paper to fuse the outputs of text and image classifiers to determine the final polarity at the decision level. The pre-trained VGG16 network is firstly used to extract visual features and fine-tune on the MVSA-Multiple and T4SA datasets for image sentiment classification. The Mask-RCNN model is then exploited to determine the objects in the images and convert them to text. The BERT model receives the output of this step along with the textual descriptions of the images for extracting the text features and embedding the words. The output of the BERT model is then imported into a weighted convolutional neural network ensemble (WCNNE). The texts are classified by several weak learners using the AdaBoost that is an ensemble learning technique in which, classifiers are trained sequentially. The combined use of several weak classifiers results in a strong classification. The WCNNE improves the performance and increases the accuracy of the results. As a fusing phase at the decision level, the outputs of the VGG16 and the WCNNE models will be finally merged using the extended Dempster-Shafer theory to obtain the correct sentiment label. The results of the experiments on the MVSA-Multiple and T4SA datasets show that the proposed model is better than the other compared methods and achieved an appropriate accuracy of 0.9348 on MVSA and 0.9689 on the T4SA datasets. Moreover, the proposed model reduces training time due to the use of transfer learning and the proposed AdaBoostCNN achieves better results compared to the single CNN.}
}
@article{HUANG2025102725,
title = {AtCAF: Attention-based causality-aware fusion network for multimodal sentiment analysis},
journal = {Information Fusion},
volume = {114},
pages = {102725},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102725},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524005037},
author = {Changqin Huang and Jili Chen and Qionghao Huang and Shijin Wang and Yaxin Tu and Xiaodi Huang},
keywords = {Multimodal sentiment analysis, Causal inference, Multimodal fusion},
abstract = {Multimodal sentiment analysis (MSA) involves interpreting sentiment using various sensory data modalities. Traditional MSA models often overlook causality between modalities, resulting in spurious correlations and ineffective cross-modal attention. To address these limitations, we propose the Attention-based Causality-Aware Fusion (AtCAF) network from a causal perspective. To capture a causality-aware representation of text, we introduce the Causality-Aware Text Debiasing Module (CATDM) utilizing the front-door adjustment. Furthermore, we employ the Counterfactual Cross-modal Attention (CCoAt) module integrate causal information in modal fusion, thereby enhancing the quality of aggregation by incorporating more causality-aware cues. AtCAF achieves state-of-the-art performance across three datasets, demonstrating significant improvements in both standard and Out-Of-Distribution (OOD) settings. Specifically, AtCAF outperforms existing models with a 1.5% improvement in ACC-2 on the CMU-MOSI dataset, a 0.95% increase in ACC-7 on the CMU-MOSEI dataset under normal conditions, and a 1.47% enhancement under OOD conditions. CATDM improves category cohesion in feature space, while CCoAt accurately classifies ambiguous samples through context filtering. Overall, AtCAF offers a robust solution for social media sentiment analysis, delivering reliable insights by effectively addressing data imbalance. The code is available at https://github.com/TheShy-Dream/AtCAF.}
}
@article{HOU2024140,
title = {A shared-private sentiment analysis approach based on cross-modal information interaction},
journal = {Pattern Recognition Letters},
volume = {183},
pages = {140-146},
year = {2024},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2024.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S0167865524001533},
author = {Yilin Hou and Xianjing Zhong and Hui Cao and Zheng Zhu and Yunfeng Zhou and Jie Zhang},
keywords = {Sentiment analysis, Multimodal data, Improved transformer, Self-attention mechanism, Multi-head attention},
abstract = {To explore the heterogeneous sentiment information in each modal feature and improve the accuracy of sentiment analysis, this paper proposes a Multimodal Sentiment Analysis based on Text-Centric Sharing-Private Affective Semantics (TCSP). First, the Deep Canonical Time Wrapping (DCTW) algorithm is employed to effectively align the timing deviations of Audio and Picture modalities. Then, a cross-modal shared mask matrix is designed, and a mutual attention mechanism is introduced to compute the shared affective semantic features of Audio-picture-to-text. Following this, the private affective semantic features within Audio and Picture modalities are derived via the self-attention mechanism with LSTM. Finally, the Transformer Encoder structure is improved, achieving deep interaction and feature fusion of cross-modal emotional information, and conducting emotional analysis. Experiments are conducted on the IEMOCAP and MELD datasets. By comparing with current state-of-the-art models, the accuracy of the TCSP model reaches 82.02%, fully validating the effectiveness. In addition, the rationality of the design of each structure within the model is verified through ablation experiments.}
}
@article{SINGH2025113995,
title = {Multimodal Multi-task deep learning framework for classification of sentiment, emotion, humor, sarcasm and toxicity from speech},
journal = {Knowledge-Based Systems},
volume = {326},
pages = {113995},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113995},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125010408},
author = {Harish Pratap Singh and Puneet Prashar and Gaddam Sai Bharath Chandra Reddy and Santosh Kumar Mishra},
keywords = {Multimodal, Multi-task, Emotion, Speech, Streaming cross-modal attention},
abstract = {This paper presents M2-S2ETH, a Multimodal Multi-task deep learning framework designed for recognizing Sentiment, Sarcasm, Emotion, Toxicity, and Humor from speech and text. M2-S2ETH overcomes the limitations of existing approaches, which mainly rely on datasets consisting of relatively short audio clips of 10–30 s. In contrast, we introduce MuSETHS, a new dataset consisting of videos sourced from social media platforms, featuring variable audio durations. This shift towards longer audios presents unique challenges in analyzing contextually rich content. The proposed architecture is specifically tailored to handle these longer audio segments, allowing for more complex interactions between different modes of communication. We leverage the attention mechanism to selectively focus on the most relevant parts of the audio, ensuring that the model can effectively capture the nuances in long audio data. In this work, three primary challenges are addressed: context dependency, multimodal fusion, and multi-task interaction. By addressing these issues, our model achieves significant improvements over state-of-the-art multimodal architectures in the tasks of emotion, sarcasm, humor, toxicity, and sentiment classification. This work not only broadens the scope of existing methodologies by considering long-form audio, but also demonstrates the efficacy of attention mechanisms like extracting meaningful insights from complex, real-world audio data.}
}
@incollection{GARZAULLOA202239,
title = {Chapter 2 - Introduction to Cognitive Science, Cognitive Computing, and Human Cognitive relation to help in the solution of Artificial Intelligence Biomedical Engineering problems},
editor = {Jorge Garza-Ulloa},
booktitle = {Applied Biomedical Engineering Using Artificial Intelligence and Cognitive Models},
publisher = {Academic Press},
pages = {39-111},
year = {2022},
isbn = {978-0-12-820718-5},
doi = {https://doi.org/10.1016/B978-0-12-820718-5.00007-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128207185000076},
author = {Jorge Garza-Ulloa},
keywords = {Biomedical, mathematical models, neurological diseases, MATLAB, IBM Cloud, IBM Watson, API, Natural Langue Processing (NLP), cognition},
abstract = {This chapter is an introduction to the analysis of body injuries, diseases, and neurological disorders, separated into “motor symptoms (related to movement disorders)” and “nonmotor symptoms (related to cognition and no related to movement disorders)”; “human cognitive development stages” and their relation to “neurons and neural pathways”; “cognition and its integration with multidiscipline sciences.” Also “Natural Language Processing applications,” are included with examples, and exercise as NLP Topics; Audio Labeler for Machine Learning; NLP Text to speech; NLP Speech to Text; NLP analysis for: Sentiment, Emotion, Keywords, Entities, Categories, Concept, and Semantic Roles with MATLAB® and API a set of functions and procedures allowing the creation of applications that access the features or data of an operating system, application, or other service through IBM Cloud services.}
}
@article{WANG2025100445,
title = {RAFT: Robust Adversarial Fusion Transformer for multimodal sentiment analysis},
journal = {Array},
volume = {27},
pages = {100445},
year = {2025},
issn = {2590-0056},
doi = {https://doi.org/10.1016/j.array.2025.100445},
url = {https://www.sciencedirect.com/science/article/pii/S2590005625000724},
author = {Rui Wang and Duyun Xu and Lucia Cascone and Yaoyang Wang and Hui Chen and Jianbo Zheng and Xianxun Zhu},
keywords = {Multimodal sentiment analysis, Robust feature fusion, Adversarial training, Cross-attention},
abstract = {Multimodal sentiment analysis (MSA) has emerged as a key technology for understanding human emotions by jointly processing text, audio, and visual cues. Despite significant progress, existing fusion models remain vulnerable to real-world challenges such as modality noise, missing channels, and weak inter-modal coupling. This paper addresses these limitations by introducing RAFT (Robust Adversarial Fusion Transformer), which integrates cross-modal and self-attention mechanisms with noise-imitation adversarial training to strengthen feature interactions and resilience under imperfect inputs. We first formalize the problem of noisy and incomplete data in MSA and demonstrate how adversarial noise simulation can bridge the gap between clean and corrupted modalities. RAFT is evaluated on two benchmark datasets, MOSI and MOSEI, where it achieves competitive binary classification accuracy (greater than 80%) and fine-grained sentiment performance (5-class accuracy 57%), while reducing mean absolute error and improving Pearson correlation by up to 2% over state-of-the-art baselines. Ablation studies confirm that both adversarial training and context-aware modules contribute substantially to robustness gains. Looking ahead, we plan to refine noise-generation strategies, explore more expressive fusion architectures, and extend RAFT to handle long-form dialogues and culturally diverse expressions. Our results suggest that RAFT lays a solid foundation for reliable, real-world sentiment analysis in noisy environments.}
}
@article{LIN2023102147,
title = {StyleBERT: Text-audio sentiment analysis with Bi-directional Style Enhancement},
journal = {Information Systems},
volume = {114},
pages = {102147},
year = {2023},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2022.102147},
url = {https://www.sciencedirect.com/science/article/pii/S0306437922001259},
author = {Fei Lin and Shengqiang Liu and Cong Zhang and Jin Fan and Zizhao Wu},
keywords = {Multimodal sentiment analysis, Modality style, Style enhancement},
abstract = {Recent multimodal sentiment analysis works focus on establishing sophisticated fusion strategies for better performance. However, a major limitation of these works is that they ignore effective modality representation learning before fusion. In this work, we propose a novel text-audio sentiment analysis framework, named StyleBERT, to enhance the emotional information of unimodal representations by learning distinct modality styles, such that the model already obtains an effective unimodal representation before fusion, which mitigates the reliance on fusion. In particular, we propose a Bi-directional Style Enhancement module, which learns one contextualized style representation and two differentiated style representations for each modality, where the relevant semantic information across modalities and the discriminative characteristics of each modality will be captured. Furthermore, to learn fine-grained acoustic representation, we only use the directly available Log-Mel spectrograms as audio modality inputs and encode it with a multi-head self-attention mechanism. Comprehensive experimental results on three widely-used benchmark datasets demonstrate that the proposed StyleBERT is an effective multimodal framework and significantly outperforms the state-of-the-art multimodal baselines. Our code is available at https://github.com/lsq960124/StyleBERT.}
}
@article{JIM2024100059,
title = {Recent advancements and challenges of NLP-based sentiment analysis: A state-of-the-art review},
journal = {Natural Language Processing Journal},
volume = {6},
pages = {100059},
year = {2024},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2024.100059},
url = {https://www.sciencedirect.com/science/article/pii/S2949719124000074},
author = {Jamin Rahman Jim and Md Apon Riaz Talukder and Partha Malakar and Md Mohsin Kabir and Kamruddin Nur and M.F. Mridha},
keywords = {Sentiment classification, Text classification, Natural language processing, Emotion detection, Sentiment analysis},
abstract = {Sentiment analysis is a method within natural language processing that evaluates and identifies the emotional tone or mood conveyed in textual data. Scrutinizing words and phrases categorizes them into positive, negative, or neutral sentiments. The significance of sentiment analysis lies in its capacity to derive valuable insights from extensive textual data, empowering businesses to grasp customer sentiments, make informed choices, and enhance their offerings. For the further advancement of sentiment analysis, gaining a deep understanding of its algorithms, applications, current performance, and challenges is imperative. Therefore, in this extensive survey, we began exploring the vast array of application domains for sentiment analysis, scrutinizing them within the context of existing research. We then delved into prevalent pre-processing techniques, datasets, and evaluation metrics to enhance comprehension. We also explored Machine Learning, Deep Learning, Large Language Models and Pre-trained models in sentiment analysis, providing insights into their advantages and drawbacks. Subsequently, we precisely reviewed the experimental results and limitations of recent state-of-the-art articles. Finally, we discussed the diverse challenges encountered in sentiment analysis and proposed future research directions to mitigate these concerns. This extensive review provides a complete understanding of sentiment analysis, covering its models, application domains, results analysis, challenges, and research directions.}
}
@article{WEI2025130361,
title = {FREE-Net: A dual-modality emotion recognition network for fusing raw and enhanced data},
journal = {Neurocomputing},
volume = {640},
pages = {130361},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.130361},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225010331},
author = {Wei Wei and Bingkun Zhang and Yibing Wang},
keywords = {Speech Emotion Recognition, FREE-Net, Dynamic interaction},
abstract = {Speech Emotion Recognition (SER) holds a significant position in the fields of natural language processing and affective computing. Although existing methods are able to recognize and classify emotions in speech to some extent, the captured speech information is often incomplete, and there are still limitations in feature fusion, temporal modeling, and data augmentation, which restricts recognition accuracy. This paper proposes a dual-modal emotion recognition network (FREE-Net) that achieves comprehensive modeling of speech-text dual-modal features by fusing raw and enhanced data. The Raw Modality Emotion Module (RMEM) and Enhanced Modality Emotion Module (EMEM) are designed to capture fine-grained emotional information, while dynamic interaction is achieved through the Dialogue-Driven Spectral Encoder (DDSE) and the Text-Driven Attention Encoder (TDAE) to improve recognition accuracy. Experimental results demonstrate that FREE-Net outperforms existing methods on the IEMOCAP and MELD datasets, effectively enhancing the robustness and accuracy of emotion recognition.}
}
@article{VAIANI2025113705,
title = {Cross-modal consistency types in multimodal social data},
journal = {Knowledge-Based Systems},
volume = {322},
pages = {113705},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113705},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125007518},
author = {Lorenzo Vaiani and Luca Cagliero and Paolo Garza and Jason Ravagli},
keywords = {Multimodal machine learning, Cross-modal consistency, Misogynous meme identification, Multimodal sentiment analysis},
abstract = {Social media content, such as internet memes or tweets, are nowadays largely or mainly multimodal. Machine learning models often need to jointly process images and text to solve complex tasks such as hate speech detection or sentiment analysis. For example, the misogyny of a meme cannot be accurately predicted while considering the visual and textual modalities separately. Similarly, sentiment annotations for tweets’ images and text can be discordant. Detecting the samples with inconsistent modality contributions is particularly relevant to analyze machine learning model performance and explain classification errors. In this paper, we formalize the types of cross-modal consistency by differentiating between consistent cases and not. Cross-modal consistency denotes whether all modalities agree on the label (i.e., full consistency) or not (i.e., inconsistency). When the visual and textual modalities are discordant, we distinguish the cases in which a joint analysis of multimodal features is sufficient to solve the issue from those requiring a human agreement (i.e., NOR consistency). We also propose a CLIP-based architecture to predict the cross-modal consistency types and identify the modalities causing the inconsistency. The results achieved on benchmark datasets show that cross-modal consistency annotation is cost-effective, i.e., it provides relevant insights into model predictions while requiring a limited extra human effort.}
}
@article{LALTHANGMAWII2025100181,
title = {Sentiment analysis of Mizo using lexical features in low resource based models},
journal = {Natural Language Processing Journal},
pages = {100181},
year = {2025},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2025.100181},
url = {https://www.sciencedirect.com/science/article/pii/S2949719125000573},
author = {Mercy Lalthangmawii and Thoudam Doren Singh},
keywords = {Sentiment analysis, Low-resource languages, Lexicon-based analysis, Machine learning, Transfer learning, XLM-roBERTa},
abstract = {Sentiment analysis is a vital area of natural language processing (NLP) for interpreting emotions in user-generated content. Although significant progress has been made for widely spoken languages, low-resource languages such as Mizo remain underexplored. This study addresses this gap by developing the first comprehensive sentiment analysis framework for Mizo language. We created a meticulously annotated data set that captures positive, negative, and neutral sentiments. Using classical machine learning models enhanced with lexicon features and transfer learning with XLM-RoBERTa, we demonstrate the feasibility of sentiment analysis in low-resource settings. Our approach achieves an accuracy of 82% with Logistic Regression and 78% with XLM-RoBERTa, which establishes a benchmark for future research in Mizo sentiment analysis.}
}
@article{ATMAJA202211,
title = {Survey on bimodal speech emotion recognition from acoustic and linguistic information fusion},
journal = {Speech Communication},
volume = {140},
pages = {11-28},
year = {2022},
issn = {0167-6393},
doi = {https://doi.org/10.1016/j.specom.2022.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167639322000413},
author = {Bagus Tris Atmaja and Akira Sasou and Masato Akagi},
keywords = {Speech emotion recognition, Affective computing, Audiotextual information, Bimodal fusion, Information fusion},
abstract = {Speech emotion recognition (SER) is traditionally performed using merely acoustic information. Acoustic features, commonly are extracted per frame, are mapped into emotion labels using classifiers such as support vector machines for machine learning or multi-layer perceptron for deep learning. Previous research has shown that acoustic-only SER suffers from many issues, mostly on low performances. On the other hand, not only acoustic information can be extracted from speech but also linguistic information. The linguistic features can be extracted from the transcribed text by an automatic speech recognition system. The fusion of acoustic and linguistic information could improve the SER performance. This paper presents a survey of the works on bimodal emotion recognition fusing acoustic and linguistic information. Five components of bimodal SER are reviewed: emotion models, datasets, features, classifiers, and fusion methods. Some major findings, including state-of-the-art results and their methods from the commonly used datasets, are also presented to give insights for the current research and to surpass these results. Finally, this survey proposes the remaining issues in the bimodal SER research for future research directions.}
}
@article{AZIZ2025129376,
title = {MMTF-DES: A fusion of multimodal transformer models for desire, emotion, and sentiment analysis of social media data},
journal = {Neurocomputing},
volume = {623},
pages = {129376},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.129376},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225000487},
author = {Abdul Aziz and Nihad Karim Chowdhury and Muhammad Ashad Kabir and Abu Nowshed Chy and Md. Jawad Siddique},
keywords = {Human desire understanding, Desire analysis, Sentiment analysis, Emotion analysis, Multimodal transformer, Vision-language models},
abstract = {Desires, emotions, and sentiments are pivotal in understanding and predicting human behavior, influencing various aspects of decision-making, communication, and social interactions. Their analysis, particularly in the context of multimodal data (such as images and texts) from social media, provides profound insights into cultural diversity, psychological well-being, and consumer behavior. Prior studies overlooked the use of image–text pairwise feature representation, which is crucial for the task of human desire understanding. In this research, we have proposed a unified multimodal-based framework with image–text pair settings to identify human desire, sentiment, and emotion. The core of our proposed method lies in the encoder module, which is built using two state-of-the-art multimodal vision-language models (VLMs). To effectively extract visual and contextualized embedding features from social media image and text pairs, we jointly fine-tune two pre-trained multimodal VLMs: Vision-and-Language Transformer (ViLT) and Vision-and-Augmented-Language Transformer (VAuLT). Subsequently, we use an early fusion strategy on these embedding features to obtain combined diverse feature representations. Moreover, we leverage a multi-sample dropout mechanism to enhance the generalization ability and expedite the training process of our proposed method. To evaluate our proposed approach, we used the multimodal dataset MSED for the human desire understanding task. Through our experimental evaluation, we demonstrate that our method excels in capturing both visual and contextual information, resulting in superior performance compared to other state-of-the-art techniques. Specifically, our method outperforms existing approaches by 3% for sentiment analysis, 2.2% for emotion analysis, and approximately 1% for desire analysis.}
}
@article{ALSAADAWI2024102731,
title = {TAC-Trimodal Affective Computing: Principles, integration process, affective detection, challenges, and solutions},
journal = {Displays},
volume = {83},
pages = {102731},
year = {2024},
issn = {0141-9382},
doi = {https://doi.org/10.1016/j.displa.2024.102731},
url = {https://www.sciencedirect.com/science/article/pii/S0141938224000957},
author = {Hussein Farooq Tayeb Alsaadawi and Bihter Das and Resul Das},
keywords = {Trimodal affective computing, Emotion recognition, Sentiment analysis, Multi-modal fusion},
abstract = {Affective computing, a field at the intersection of cognitive science, linguistics, and AI, seeks to enhance human–computer interactions. Recognizing the complexity of human emotions, which manifest across various channels, this paper advocates for a multi-modal approach to accurately recognize emotions. Such an approach enables the discernment of subtle emotional cues in multiple modalities, thus advancing the field of multi-modal affective computing. Focusing on a trimodal framework, this paper examines emotion recognition and sentiment analysis through text, voice, and visual data. It outlines key developments, current trends, and prominent datasets in trimodal emotional analysis. It also explores data fusion strategies across modalities and assesses various fusion techniques’ effectiveness. The paper presents detailed emotion models, recent advancements, and key trimodal databases, while thoroughly addressing challenges like data processing and the complexities of TAC. Finally, it highlights potential future directions, underscoring the importance of benchmark databases and practical applications to deepen our understanding of the nuanced spectrum of human emotions.}
}
@article{EZZAMELI2023101847,
title = {Emotion recognition from unimodal to multimodal analysis: A review},
journal = {Information Fusion},
volume = {99},
pages = {101847},
year = {2023},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.101847},
url = {https://www.sciencedirect.com/science/article/pii/S156625352300163X},
author = {K. Ezzameli and H. Mahersia},
keywords = {Affective computing, Deep learning, Emotion recognition, Fusion, Modality, Multimodality},
abstract = {The omnipresence of numerous information sources in our daily life brings up new alternatives for emotion recognition in several domains including e-health, e-learning, robotics, and e-commerce. Due to the variety of data, the research area of multimodal machine learning poses special problems for computer scientists; how did the field of emotion recognition progress in each modality and what are the most common strategies for recognizing emotions? What part does deep learning play in this? What is multimodality? How did it progress? What are the methods of information fusion? What are the most used datasets in each modality and in multimodal recognition? We can understand and compare the various methods by answering these questions.}
}
@article{LI2025113915,
title = {Improving speech emotion recognition using gated cross-modal attention and multimodal homogeneous feature discrepancy learning},
journal = {Applied Soft Computing},
pages = {113915},
year = {2025},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2025.113915},
url = {https://www.sciencedirect.com/science/article/pii/S1568494625012281},
author = {Feng Li and Jiusong Luo and Wanjun Xia},
keywords = {Speech emotion recognition, Gated cross-modal attention, Multimodal homogeneous feature, Wav2vec 2.0},
abstract = {Speech emotion recognition (SER) remains a significant and crucial challenge due to the complex and multifaceted nature of human emotions. To tackle this challenge, researchers strive to integrate information from diverse modalities through multimodal learning. However, existing multimodal fusion techniques often overlook the intricacies of interactions between different modalities, resulting in suboptimal feature representations. In this paper, we propose WavFusion, a multimodal framework designed for SER that tackles key research challenges, such as effective multimodal fusion, modality heterogeneity, and discriminative representation learning. By utilizing a gated cross-modal attention mechanism and multimodal homogeneous feature discrepancy learning, WavFusion outperforms existing state-of-the-art methods on benchmark datasets. Our research highlights the importance of capturing subtle cross-modal interactions and learning discriminative representations for accurate multimodal SER. Experimental results indicate that the proposed method is very competitive and better than most of the latest and state-of-the-art methods for SER. WavFusion achieves 0.78 % and 1.27 % improvement in accuracy and 0.74 % and 0.44 % improvement in weighted F1 score over the previous methods on the IEMOCAP and MELD datasets, respectively.}
}
@article{MA2025100552,
title = {Time-synchronized sentiment labeling via autonomous online comments data mining: A multimodal information fusion on large-scale multimedia data},
journal = {Big Data Research},
volume = {41},
pages = {100552},
year = {2025},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2025.100552},
url = {https://www.sciencedirect.com/science/article/pii/S2214579625000474},
author = {Jiachen Ma and Nazmus Sakib and Fahim Islam Anik and Sheikh Iqbal Ahamed},
keywords = {Time-sync comments, Sentiment labeling, Automated data mining, Multimedia analysis, YouTube comments, Big dat},
abstract = {While temporal sentiment labels prove invaluable for video tagging, segmentation, and labeling tasks in multimedia studies, large-scale manual annotation remains cost and time-prohibitive. Emerging Online Time-Sync Comment (TSC) datasets offer promising alternatives for generating sentiment maps. However, limitations in existing TSC scope and a lack of resource-constrained data creation guidelines hinder broader use. This study addresses these challenges by proposing a novel system for automated TSC generation utilizing recent YouTube comments as a readily accessible source of time-synchronized data. The efficacy of our multi-platform data mining system is evaluated through extensive long-term trials, leading to the development and analysis of two large-scale TSC datasets. Benchmarking against original temporal Automatic Speech Recognition (ASR) sentiment annotations validates the accuracy of our generated data. This work establishes a promising method for automatic TSC generation, laying the groundwork for further advancements in multimedia research and paving the way for novel sentiment analysis applications.}
}
@article{HUANG2025129566,
title = {Multimodal hypergraph network with contrastive learning for sentiment analysis},
journal = {Neurocomputing},
volume = {627},
pages = {129566},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.129566},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225002383},
author = {Jian Huang and Kun Jiang and Yuanyuan Pu and Zhengpeng Zhao and Qiuxia Yang and Jinjing Gu and Dan Xu},
keywords = {Multimodal sentiment analysis, Hypergraph structure optimization, Contrastive learning},
abstract = {Multimodal Sentiment Analysis (MSA) is the process of relying on multimodal information, such as text, audio, and visual, to determine a subject’s affective tendencies. While many recent studies have adopted graph-based techniques for MSA, they have yet to fully explore the sentimental interactions both within unimodal temporal steps and across individual modalities. To address the limitations arising from the isolation of unimodal hypergraphs in affective relationship mining, this paper proposes a multimodal hypergraph network based on contrastive learning. It constructs the hypergraph structure by utilizing the sequential time steps of all three modalities as a collection of nodes, aiming to explore the multidimensional affective relationships across uni-, bi-, and tri-modalities. Specifically, this paper first generates the initial hypergraph structure using a correlation-based hypergraph construction method to ensure the effectiveness of the constructed hypergraph. Then, both supervised and unsupervised contrastive learning methods are designed to optimize feature learning and the structure of the multimodal hypergraph, adaptively and simultaneously capturing the relationships among the time-series nodes of uni-, bi-, and tri-modalities. The proposed methods in this paper have demonstrated the advantages and effectiveness by conducting a large number of comparative and validation experiments on the English CMU-MOSI and CMU-MOSEI datasets as well as the Chinese CH-SIMS dataset.}
}
@article{WANG2025103278,
title = {Multimodal speech emotion recognition via modality constraint with hierarchical bottleneck feature fusion},
journal = {Speech Communication},
volume = {173},
pages = {103278},
year = {2025},
issn = {0167-6393},
doi = {https://doi.org/10.1016/j.specom.2025.103278},
url = {https://www.sciencedirect.com/science/article/pii/S0167639325000937},
author = {Ying Wang and Jianjun Lei and Xiangwei Zhu and Tao Zhang},
keywords = {Speech emotion recognition, Multimodal, Feature fusion, Representation constraint},
abstract = {Multimodal can combine different channels of information simultaneously to improve the modeling capabilities. Many recent studies focus on overcoming challenges arising from inter-modal conflicts and incomplete intra-modal learning for multimodal architectures. In this paper, we propose a scalable multimodal speech emotion recognition (SER) framework incorporating a hierarchical bottleneck feature (HBF) fusion approach. Furthermore, we design an intra-modal and inter-modal contrastive learning mechanism that enables self-supervised calibration of both modality-specific and cross-modal feature distributions. This approach achieves adaptive feature fusion and alignment while significantly reducing reliance on rigid feature alignment constraints. Meanwhile, by restricting the learning path of modality encoders, we design a modality representation constraint (MRC) method to mitigate conflicts between modalities. Furthermore, we present a modality bargaining (MB) strategy that facilitates learning within modalities through a mechanism of mutual bargaining and balance, which can avoid falling into suboptimal modal representations by allowing the learning of different modalities to perform alternating interchangeability. Our aggressive and disciplined training strategies enable our architecture to perform well on some multimodal emotion datasets such as CREMA-D, IEMOCAP, and MELD. Finally, we also conduct extensive experiments to demonstrate the effectiveness of our proposed architecture on various modal encoders and different modal combination methods.}
}
@article{QI2025128646,
title = {MFGCN: Multimodal fusion graph convolutional network for speech emotion recognition},
journal = {Neurocomputing},
volume = {611},
pages = {128646},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128646},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224014176},
author = {Xin Qi and Yujun Wen and Pengzhou Zhang and Heyan Huang},
keywords = {Speech emotion recognition, Graph convolutional networks, Multimodal learning, Representation learning},
abstract = {Speech emotion recognition (SER) is challenging owing to the complexity of emotional representation. Hence, this article focuses on multimodal speech emotion recognition that analyzes the speaker’s sentiment state via audio signals and textual content. Existing multimodal approaches utilize sequential networks to capture the temporal dependency in various feature sequences, ignoring the underlying relations in acoustic and textual modalities. Moreover, current feature-level and decision-level fusion methods have unresolved limitations. Therefore, this paper develops a novel multimodal fusion graph convolutional network that comprehensively executes information interactions within and between the two modalities. Specifically, we construct the intra-modal relations to excavate exclusive intrinsic characteristics in each modality. For the inter-modal fusion, a multi-perspective fusion mechanism is devised to integrate the complementary information between the two modalities. Substantial experiments on the IEMOCAP and RAVDESS datasets and experimental results demonstrate that our approach achieves superior performance.}
}
@article{FAN2025103198,
title = {Coordination Attention based Transformers with bidirectional contrastive loss for multimodal speech emotion recognition},
journal = {Speech Communication},
volume = {169},
pages = {103198},
year = {2025},
issn = {0167-6393},
doi = {https://doi.org/10.1016/j.specom.2025.103198},
url = {https://www.sciencedirect.com/science/article/pii/S0167639325000135},
author = {Weiquan Fan and Xiangmin Xu and Guohua Zhou and Xiaofang Deng and Xiaofen Xing},
keywords = {Multimodal emotion recognition, Coordination attention, Contrastive loss, Human–computer interaction},
abstract = {Emotion recognition is crucial to improve the human–computer interaction experience. Attention mechanisms have become a mainstream technique due to their excellent ability to capture emotion representations. Existing algorithms often employ self-attention and cross-attention for multimodal interactions, which artificially set specific attention patterns at specific layers of the model. However, it is uncertain which attention mechanism is more important in different layers of the model. In this paper, we propose a Coordination Attention based Transformers (CAT). Based on the dual attention paradigm, CAT dynamically infers the pass rates of self-attention and cross-attention layer by layer, coordinating the importance of intra-modal and inter-modal factors. Further, we propose a bidirectional contrastive loss to cluster the matching pairs between modalities and push the mismatching pairs farther apart. Experiments demonstrate the effectiveness of our method, and the state-of-the-art performance is achieved under the same experimental conditions.}
}
@article{SHAHFAHAD2021102951,
title = {A survey of speech emotion recognition in natural environment},
journal = {Digital Signal Processing},
volume = {110},
pages = {102951},
year = {2021},
issn = {1051-2004},
doi = {https://doi.org/10.1016/j.dsp.2020.102951},
url = {https://www.sciencedirect.com/science/article/pii/S1051200420302967},
author = {Md. {Shah Fahad} and Ashish Ranjan and Jainath Yadav and Akshay Deepak},
keywords = {Language-independent, Natural-environment, Noisy-environment, Speech emotion recognition (SER), Speaker-independent, Text-independent},
abstract = {While speech emotion recognition (SER) has been an active research field since the last three decades, the techniques that deal with the natural environment have only emerged in the last decade. These techniques have reduced the mismatch in the distribution of the training and testing data, which occurs due to the difference in speakers, texts, languages, and recording environments between the training and testing datasets. Although a few good surveys exist for SER, they either don't cover all aspects of SER in natural environments or don't discuss the specifics in detail. This survey focuses on SER in a natural environment, discussing SER techniques for natural environment along with their advantages and disadvantages in terms of speaker, text, language, and recording environments. In the recent past, the deep learning techniques have become very popular due to minimal speech processing and enhanced accuracy. Special attention has been given to deep-learning techniques and the related issues in this survey. Recent databases, features, and feature selection algorithms for SER, which have not been discussed in the existing surveys and can be promising for SER in a natural environment, have also been discussed in this paper.}
}
@article{RATHI2024103102,
title = {Analyzing the influence of different speech data corpora and speech features on speech emotion recognition: A review},
journal = {Speech Communication},
volume = {162},
pages = {103102},
year = {2024},
issn = {0167-6393},
doi = {https://doi.org/10.1016/j.specom.2024.103102},
url = {https://www.sciencedirect.com/science/article/pii/S0167639324000748},
author = {Tarun Rathi and Manoj Tripathy},
keywords = {Speech emotion recognition, Speech emotional data corpus, Speech features, Mel-frequency cepstral coefficients, Deep neural network, Convolutional neural network},
abstract = {Emotion recognition from speech has become crucial in human-computer interaction and affective computing applications. This review paper examines the complex relationship between two critical factors: the selection of speech data corpora and the extraction of speech features regarding speech emotion classification accuracy. Through an extensive analysis of literature from 2014 to 2023, publicly available speech datasets are explored and categorized based on their diversity, scale, linguistic attributes, and emotional classifications. The importance of various speech features, from basic spectral features to sophisticated prosodic cues, and their influence on emotion recognition accuracy is analyzed.. In the context of speech data corpora, this review paper unveils trends and insights from comparative studies exploring the repercussions of dataset choice on recognition efficacy. Various datasets such as IEMOCAP, EMODB, and MSP-IMPROV are scrutinized in terms of their influence on classifying the accuracy of the speech emotion recognition (SER) system. At the same time, potential challenges associated with dataset limitations are also examined. Notable features like Mel-frequency cepstral coefficients, pitch, intensity, and prosodic patterns are evaluated for their contributions to emotion recognition. Advanced feature extraction methods, too, are explored for their potential to capture intricate emotional dynamics. Moreover, this review paper offers insights into the methodological aspects of emotion recognition, shedding light on the diverse machine learning and deep learning approaches employed. Through a holistic synthesis of research findings, this review paper observes connections between the choice of speech data corpus, selection of speech features, and resulting emotion recognition accuracy. As the field continues to evolve, avenues for future research are proposed, ranging from enhanced feature extraction techniques to the development of standardized benchmark datasets. In essence, this review serves as a compass guiding researchers and practitioners through the intricate landscape of speech emotion recognition, offering a nuanced understanding of the factors shaping its recognition accuracy of speech emotion.}
}
@article{PARK2023100548,
title = {Visual language integration: A survey and open challenges},
journal = {Computer Science Review},
volume = {48},
pages = {100548},
year = {2023},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2023.100548},
url = {https://www.sciencedirect.com/science/article/pii/S1574013723000151},
author = {Sang-Min Park and Young-Gab Kim},
keywords = {Multimodal learning, Multi-task learning, End-to-end learning, Embodiment, Visual language interaction},
abstract = {With the recent development of deep learning technology comes the wide use of artificial intelligence (AI) models in various domains. AI shows good performance for definite-purpose tasks, such as image recognition and text classification. The recognition performance for every single task has become more accurate than feature engineering, enabling more work that could not be done before. In addition, with the development of generation technology (e.g., GPT-3), AI models are showing stable performances in each recognition and generation task. However, not many studies have focused on how to integrate these models efficiently to achieve comprehensive human interaction. Each model grows in size with improved performance, thereby consequently requiring more computing power and more complicated designs to train than before. This requirement increases the complexity of each model and requires more paired data, making model integration difficult. This study provides a survey on visual language integration with a hierarchical approach for reviewing the recent trends that have already been performed on AI models among research communities as the interaction component. We also compare herein the strengths of existing AI models and integration approaches and the limitations they face. Furthermore, we discuss the current related issues and which research is needed for visual language integration. More specifically, we identify four aspects of visual language integration models: multimodal learning, multi-task learning, end-to-end learning, and embodiment for embodied visual language interaction. Finally, we discuss some current open issues and challenges and conclude our survey by giving possible future directions.}
}
@article{HUANG2026103543,
title = {Text-Dominant Speech-Enhanced for Multimodal Aspect-Based Sentiment Analysis network},
journal = {Information Fusion},
volume = {126},
pages = {103543},
year = {2026},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2025.103543},
url = {https://www.sciencedirect.com/science/article/pii/S1566253525006153},
author = {Xiaoyong Huang and Heli Sun and Xuechun Liu and Jiaruo Wu and Liang He},
keywords = {Multimodal Aspect-Based Sentiment Analysis, Speech enhancement, Visual expression deficiency},
abstract = {Existing Multimodal Aspect-Based Sentiment Analysis techniques primarily focus on associating visual and textual content but often overlook the critical issue of visual expression deficiency, where images fail to provide complete aspect terms and sufficient sentiment signals. To address this limitation, we propose a Multimodal Aspect-Based Sentiment Analysis network that leverages Text-Dominant Speech Enhancement (TDSEN), aiming to alleviate the deficiency in visual expression by synthesizing speech and employing a text-dominant approach. Specifically, we introduce a Text-Driven Speech Enhancement Layer that generates speech with stable timbre to identify all aspect terms, compensate for the lacking parts of visual expression, and provide additional aspect term information and emotional cues. Meanwhile, we design a semantic distance mask matrix to enhance the capability of capturing key information from the textual modality. Furthermore, a text-driven multimodal feature fusion module is incorporated to strengthen the dominant role of text and facilitate multimodal feature interaction and integration for the extraction of the term of the aspect and sentiment recognition. Comprehensive evaluations on the Twitter-2015 and Twitter-2017 benchmarks demonstrate TDSEN’s superiority, achieving absolute improvements of 2.6% and 1.7% over state-of-the-art baselines, with ablation studies confirming the necessity of each component.}
}
@article{KIM2024108950,
title = {Comparison of AI with and without hand-crafted features to classify Alzheimer's disease in different languages},
journal = {Computers in Biology and Medicine},
volume = {180},
pages = {108950},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.108950},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524010357},
author = {Tong Min Kim and Junhyeok Son and Ji-Won Chun and Youngrong Lee and Dai-Jin Kim and In-Young Choi and Taehoon Ko and Seungjin Choi},
keywords = {Alzheimer's disease, Hand-crafted features, Non-explainable features, Deep learning, Language-agnostic},
abstract = {Background
Detecting and analyzing Alzheimer's disease (AD) in its early stages is a crucial and significant challenge. Speech data from AD patients can aid in diagnosing AD since the speech features have common patterns independent of race and spoken language. However, previous models for diagnosing AD from speech data have often focused on the characteristics of a single language, with no guarantee of scalability to other languages. In this study, we used the same method to extract acoustic features from two language datasets to diagnose AD.
Methods
Using the Korean and English speech datasets, we used ten models capable of real-time AD and healthy control classification, regardless of language type. Four machine learning models were based on hand-crafted features, while the remaining six deep learning models utilized non-explainable features.
Results
The highest accuracy achieved by the machine learning models was 0.73 and 0.69 for the Korean and English speech datasets, respectively. The deep learning models' maximum achievable accuracy reached 0.75 and 0.78, with their minimum classification time of 0.01s and 0.02s. These findings reveal the models’ robustness regardless of Korean and English and real-time diagnosis of AD through a 30-s voice sample.
Conclusion
Non-explainable deep learning models that directly acquire voice representations surpassed machine learning models utilizing hand-crafted features in AD diagnosis. In addition, these AI models could confirm the possibility of extending to a language-agnostic AD diagnosis.}
}
@article{ALSAADAWI2024124852,
title = {A systematic review of trimodal affective computing approaches: Text, audio, and visual integration in emotion recognition and sentiment analysis},
journal = {Expert Systems with Applications},
volume = {255},
pages = {124852},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124852},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424017196},
author = {Hussein Farooq Tayeb Al-Saadawi and Bihter Das and Resul Das},
keywords = {Multi-modal emotion recognition, Trimodal affective analysis, Multi-modal sentiment analysis, Multi-modal fusion},
abstract = {At the heart of affective computing lies the crucial task of decoding human emotions, a field that expertly intertwines emotion identification with the nuances of sentiment analysis. This dynamic discipline harnesses an array of data sources, from the intricacies of textual information to the subtleties of auditory signals and the dynamic realm of visual cues. One of its primary challenges is discerning emotions from physical cues like facial expressions and vocal tones, especially when these emotions are subtly concealed. The precise information yielded by physiological signals is invaluable, yet the complexity of their acquisition in real-world settings remains a formidable challenge. Our comprehensive systematic review marks a significant foray into trimodal affective computing, integrating text, audio, and visual data to provide a holistic understanding. We analyzed over 410 research articles from prominent conferences and journals spanning the last two decades. This extensive study categorizes and critically evaluates a spectrum of affect recognition methods, from unimodal to multimodal approaches, including bimodal and trimodal, offering profound insights into their structural composition and practical effectiveness. In concluding our exploration, we highlight the pivotal aspects of affective computing and chart a course for future groundbreaking research. This includes refining data integration techniques, overcoming challenges in emotion recognition, and addressing the critical ethical dimensions inherent in this field.}
}
@article{DEMIRIS2022104716,
title = {Examining spoken words and acoustic features of therapy sessions to understand family caregivers’ anxiety and quality of life},
journal = {International Journal of Medical Informatics},
volume = {160},
pages = {104716},
year = {2022},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2022.104716},
url = {https://www.sciencedirect.com/science/article/pii/S1386505622000302},
author = {George Demiris and Debra Parker Oliver and Karla T. Washington and Chad Chadwick and Jeffrey D. Voigt and Sam Brotherton and Mary D. Naylor},
keywords = {Machine learning, Quality of life, Caregiver, Chatbot, Communication},
abstract = {Background
Speech and language cues are considered significant data sources that can reveal insights into one’s behavior and well-being. The goal of this study is to evaluate how different machine learning (ML) classifiers trained both on the spoken word and acoustic features during live conversations between family caregivers and a therapist, correlate to anxiety and quality of life (QoL) as assessed by validated instruments.
Methods
The dataset comprised of 124 audio-recorded and professionally transcribed discussions between family caregivers of hospice patients and a therapist, of challenges they faced in their caregiving role, and standardized assessments of self-reported QoL and anxiety. We custom-built and trained an Automated Speech Recognition (ASR) system on older adult voices and created a logistic regression-based classifier that incorporated audio-based features. The classification process automated the QoL scoring and display of the score in real time, replacing hand-coding for self-reported assessments with a machine learning identified classifier.
Findings
Of the 124 audio files and their transcripts, 87 of these transcripts (70%) were selected to serve as the training set, holding the remaining 30% of the data for evaluation. For anxiety, the results of adding the dimension of sound and an automated speech-to-text transcription outperformed the prior classifier trained only on human-rendered transcriptions. Specifically, precision improved from 86% to 92%, accuracy from 81% to 89%, and recall from 78% to 88%.
Interpretation
Classifiers can be developed through ML techniques which can indicate improvements in QoL measures with a reasonable degree of accuracy. Examining the content, sound of the voice and context of the conversation provides insights into additional factors affecting anxiety and QoL that could be addressed in tailored therapy and the design of conversational agents serving as therapy chatbots.}
}
@article{LI2025126274,
title = {Learning fine-grained representation with token-level alignment for multimodal sentiment analysis},
journal = {Expert Systems with Applications},
volume = {269},
pages = {126274},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.126274},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424031415},
author = {Xiang Li and Haijun Zhang and Zhiqiang Dong and Xianfu Cheng and Yun Liu and Xiaoming Zhang},
keywords = {Sentiment analysis, Representation learning, Fine-grained representation, Token-level alignment, Multimodal fusion},
abstract = {Multimodal sentiment analysis (MSA) has gained significant attention recently due to its crucial applications in intelligent systems. The effective extraction and fusion of sentiment representations from heterogeneous modalities (visual, text, and audio) remain the key and challenge in improving MSA performance. However, most existing methods often directly integrate different modalities and fail to capture fine-grained multimodal representations, suffering from irrelevant information among heterogeneous modalities. Moreover, these methods overlook the explicit token-level alignment between different modalities, which hinders effective representation learning and multimodal fusion. To address these issues, we propose a Fine-grained Multimodal Fusion Network (FMFN) for MSA. Firstly, a Fine-grained Representation Learning (FRL) module is designed to extract critical sentiment information from original modality features by employing finite learnable denoising tokens. FRL enables sentiment-related fine-grained representations to interact with these denoising tokens, thereby mitigating noise propagation and information redundancy. Secondly, a Token-level Cross-modal Alignment (TCA) module is introduced, which aligns different modality representations through fine-grained contrastive learning. TCA refines each modality by filtering out extraneous representations while facilitating subsequent multimodal interactions. Finally, a Correlation-aware Multimodal Fusion (CMF) module is developed to capture latent cross-modal correlations, generating consistent and complementary multimodal representations that enhance sentiment prediction. Extensive experiments are conducted on two English datasets, CMU-MOSI and CMU-MOSEI (aligned and unaligned versions), and one Chinese dataset, CH-SIMS (unaligned). The proposed method outperforms state-of-the-art models with 1%–2% accuracy and F1 score improvements, particularly on unaligned datasets, demonstrating the effectiveness of learning the fine-grained representation and token-level alignment in enhancing MSA performance.}
}
@article{LI2021114683,
title = {Speech emotion recognition using recurrent neural networks with directional self-attention},
journal = {Expert Systems with Applications},
volume = {173},
pages = {114683},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.114683},
url = {https://www.sciencedirect.com/science/article/pii/S095741742100124X},
author = {Dongdong Li and Jinlin Liu and Zhuo Yang and Linyu Sun and Zhe Wang},
keywords = {Speech emotion recognition, Bi-directional long-short term memory with directional self-attention, Self-attention, Autocorrelation},
abstract = {As an important branch of affective computing, Speech Emotion Recognition (SER) plays a vital role in human–computer interaction. In order to mine the relevance of signals in audios an increase the diversity of information, Bi-directional Long-Short Term Memory with Directional Self-Attention (BLSTM-DSA) is proposed in this paper. Long Short-Term Memory (LSTM) can learn long-term dependencies from learned local features. Moreover, Bi-directional Long-Short Term Memory (BLSTM) can make the structure more robust by direction mechanism because that the directional analysis can better recognize the hidden emotions in sentence. At the same time, autocorrelation of speech frames can be used to deal with the lack of information, so that Self-Attention mechanism is introduced into SER. The attention weight of each frame is calculated with the output of the forward and backward LSTM respectively rather than calculated after adding them together. Thus, the algorithm can automatically annotate the weights of speech frames to correctly select frames with emotional information in temporal network. When evaluate it on the Interactive Emotional Dyadic Motion Capture (IEMOCAP) database and Berlin database of emotional speech (EMO-DB), the BLSTM-DSA demonstrates satisfactory performance on the task of speech emotion recognition. Especially in emotion recognizing of happiness and anger, BLSTM-DSA achieves the highest recognition accuracies.}
}
@article{KHAN2024122946,
title = {MSER: Multimodal speech emotion recognition using cross-attention with deep fusion},
journal = {Expert Systems with Applications},
volume = {245},
pages = {122946},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122946},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423034486},
author = {Mustaqeem Khan and Wail Gueaieb and Abdulmotaleb {El Saddik} and Soonil Kwon},
keywords = {Affective computing, Auto-encoders, Deep learning, Deep fusion, Multimodal speech emotion recognition, Speech and text processing},
abstract = {In human–computer interaction (HCI) and especially speech signal processing, emotion recognition is one of the most important and challenging tasks due to multi-modality and limited data availability. Nowadays, an intelligent system is required for real-world applications to efficiently process and understand the speaker's emotional state and to enhance the analytical abilities to assist communication by a human-machine interface (HMI). Designing a reliable and robust Multimodal Speech Emotion Recognition (MSER) to efficiently recognize emotions through multi-modality such as speech and text is necessary. This paper propose a novel MSER model with a deep feature fusion technique using a multi-headed cross-attention mechanism. The proposed model utilize audio and text cues to predict the emotion label accordingly. Our proposed model process the raw speech signal and text by CNN and feeds to corresponding encoders for discriminative and semantic feature extractions. The cross-attention mechanism is applied to both features to enhance the interaction between text and audio cues by crossway to extract the most relevant information for emotion recognition. Finally, combining the region-wise weights from both encoders enables interaction among different layers and paths by the proposed deep feature fusion scheme. The authors evaluate the proposed system using the IEMOCAP and MELD datasets and conduct extensive experiments that obtain state-of-the-art (SOTA) results and show a 4.5% improved recognition rate, respectively. Our model secured a significant improvement over SOTA methods, which shows the robustness and effectiveness of the proposed MSER model.}
}
@article{LIU2025128711,
title = {ESERNet: Learning spectrogram structure relationship for effective speech emotion recognition with swin transformer in classroom discourse analysis},
journal = {Neurocomputing},
volume = {612},
pages = {128711},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128711},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224014826},
author = {Tingting Liu and Minghong Wang and Bing Yang and Hai Liu and Shaoxin Yi},
keywords = {Speech emotion recognition, Intelligent education, Feature extraction, Swin Transformer, Classroom discourse analysis},
abstract = {Speech emotion recognition (SER) has received increased attention due to its extensive applications in many fields, especially in the analysis of teacher-student dialogue in classroom environment. It can help teachers to better learn about students’ emotions and thereby adjust teaching activities. However, SER has faced several challenges, such as the intrinsic ambiguity of emotions and the complex task of interpreting emotions from speech in noisy environments. These issues can result in reduced recognition accuracy due to a focus on less relevant or insignificant features. To address these challenges, this paper presents ESERNet, a Transformer-based model designed to effectively extract crucial clues from speech data by capturing both pivotal cues and long-range relationships in speech signal. The major contribution of our approach is a two-pathway SER framework. By leveraging the Transformer architecture, ESERNet captures long-range dependencies within speech mel-spectrograms, enabling a refined understanding of the emotional cues embedded in speech signals. Extensive experiments were conducted on the IEMOCAP and EmoDB datasets, the results show that ESERNet achieves state-of-the-art performance in SER and outperforms existing methods by effectively leveraging critical clues and capturing long-range dependencies in speech data. These results highlight the effectiveness of the model in addressing the complex challenges associated with SER tasks.}
}
@article{ATMAJA20219,
title = {Two-stage dimensional emotion recognition by fusing predictions of acoustic and text networks using SVM},
journal = {Speech Communication},
volume = {126},
pages = {9-21},
year = {2021},
issn = {0167-6393},
doi = {https://doi.org/10.1016/j.specom.2020.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167639320302946},
author = {Bagus Tris Atmaja and Masato Akagi},
keywords = {Automatic speech emotion recognition, Affective computing, Late fusion, Bimodal fusion, Dimensional emotion},
abstract = {Automatic speech emotion recognition (SER) by a computer is a critical component for more natural human-machine interaction. As in human-human interaction, the capability to perceive emotion correctly is essential to taking further steps in a particular situation. One issue in SER is whether it is necessary to combine acoustic features with other data such as facial expressions, text, and motion capture. This research proposes to combine acoustic and text information by applying a late-fusion approach consisting of two steps. First, acoustic and text features are trained separately in deep learning systems. Second, the prediction results from the deep learning systems are fed into a support vector machine (SVM) to predict the final regression score. Furthermore, the task in this research is dimensional emotion modeling, because it can enable deeper analysis of affective states. Experimental results show that this two-stage, late-fusion approach, obtains higher performance than that of any one-stage processing, with a linear correlation from one-stage to two-stage processing. This late-fusion approach improves previous early fusion result measured in concordance correlation coefficients score.}
}
@article{ZHANG2022108078,
title = {Multi-head attention fusion networks for multi-modal speech emotion recognition},
journal = {Computers & Industrial Engineering},
volume = {168},
pages = {108078},
year = {2022},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2022.108078},
url = {https://www.sciencedirect.com/science/article/pii/S0360835222001486},
author = {Junfeng Zhang and Lining Xing and Zhen Tan and Hongsen Wang and Kesheng Wang},
keywords = {Speech Emotion Recognition, Multimodal, Multi-Head Attention, Feature Fusion},
abstract = {Multi-modal speech emotion recognition is a study to predict emotion categories by combining speech data with other types of data, such as video, speech text transcription, body action, or facial expression when speaking, which will involve the fusion of multiple features. Most of the early studies, however, directly spliced multi-modal features in the fusion layer after single-modal modeling, resulting in ignoring the connection between speech and other modal features. As a result, we propose a novel multi-modal speech emotion recognition model based on multi-head attention fusion networks, which employs transcribed text and motion capture (MoCap) data involving facial expression, head rotation, and hand action to supplement speech data and perform emotion recognition. In unimodal, we use a two-layer Transformer’s encoder combination model to extract speech and text features separately, and MoCap is modeled using a deep residual shrinkage network. Simultaneously, We innovated by changing the input of the Transformer encoder to learn the similarities between speech and text, speech and MoCap, and then output text and MoCap features that are more similar to speech features, and finally, predict the emotion category using combined features. In the IEMOCAP dataset, our model outperformed earlier research with a recognition accuracy of 75.6%.}
}
@article{LI2024111826,
title = {MBCFNet: A Multimodal Brain–Computer Fusion Network for human intention recognition},
journal = {Knowledge-Based Systems},
volume = {296},
pages = {111826},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.111826},
url = {https://www.sciencedirect.com/science/article/pii/S095070512400460X},
author = {Zhongjie Li and Gaoyan Zhang and Shogo Okada and Longbiao Wang and Bin Zhao and Jianwu Dang},
keywords = {Brain–computer interface, Intention recognition, Emotion recognition, Human–computer interaction, Multitask learning},
abstract = {Accurate recognition of human intent is crucial for effective human–computer speech interaction. Numerous intent understanding studies were based on speech-to-text transcription, which often overlook the influence of paralinguistic cues (such as speaker’s emotion, attitude, etc.), leading to misunderstandings, especially when identical textual content conveys multiple intents by means of different paralinguistic information. Considering that interaction intent is produced from the human brain, we propose a novel Multimodal Brain–Computer Fusion Network (MBCFNet) to discriminate the different intents carried by the identical textual information, in which the acoustic-textual representation was adapted by brain functional information through a cross-modal transformer. In the output module, a joint multi-task learning method was used to optimize both the primary task of intent recognition and the auxiliary task of emotion recognition of the speaker. To evaluate the model performance, we constructed a multimodal dataset CMSLIU that consists of acoustic, textual, and electroencephalograph (EEG) data of the same Chinese texts with varying intents. Experimental results on the self-constructed dataset indicate that the proposed model achieves state-of-the-art (SOTA) performance compared with other competing models. Ablation experiment results reveal that the model performance declines without information from EEG or audio modality, and also when the auxiliary task of emotion recognition is removed. All these results suggest the effectiveness of the proposed information fusion strategy in spoken language intent recognition and the brain–computer information fusion idea can also be extended to other similar fields.}
}
@article{DONG2025131197,
title = {Unbiased multimodal intent recognition with auxiliary rationale generation},
journal = {Neurocomputing},
volume = {654},
pages = {131197},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.131197},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225018697},
author = {Qian Dong and Ruiting Dai and Guiduo Duan and Ke Qin and Yuyang Zhang and Tao He},
keywords = {Multimodal intent recognition, Modality bias, Self-critical learning},
abstract = {Multimodal Intent Recognition (MIR) is a crucial task in understanding user intentions from diverse input sources such as text, images, and audio. While recent advancements using Large Multimodal Models (LMMs) have yielded promising results in intent classification, these models often exhibit a strong modality bias—favoring textual inputs at the expense of visual and auditory cues. Our study empirically investigates this phenomenon across standard benchmark datasets, revealing a consistent over-reliance on textual information. We attribute this imbalance to the typically more explicit nature of intent cues in text, which can overshadow the more nuanced signals present in other modalities. To mitigate this bias, we propose a novel MIR framework that incorporates an auxiliary rationale generation task aimed at producing modality-grounded justifications for classification decisions. By jointly training intent classification and explanation generation through a reinforcement learning-based multi-task paradigm, our approach facilitates deeper multimodal integration. The classification output guides the explanation process via reward signals, while the generated rationales enhance the model’s interpretive capacity and provide complementary feedback for intent prediction. Experimental results on the MIR and EMIR datasets show that our method outperforms existing state-of-the-art approaches, achieving a relative improvement of approximately 3.24 % in classification accuracy over the leading CAGC model. Further ablation studies and qualitative analyses confirm the effectiveness of the auxiliary task in reducing modality bias and improving interpretability.}
}
@article{WANG2025128605,
title = {Bimodal speech emotion recognition via contrastive self-alignment learning},
journal = {Expert Systems with Applications},
volume = {293},
pages = {128605},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.128605},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425022249},
author = {Chang Wang and Guihua Wen and Pei Yang and Lianqi Liu},
keywords = {Bimodal speech emotion recognition, Contrastive learning, Feature alignment, Feature fusion},
abstract = {Speech emotion recognition (SER) aims to recognize speaker emotions through speech signals. In a speech, emotions can be expressed through changes in intonation and rhythm in voice, but emotion-related words play an important role in conveying emotions as well. Existing methods improve the accuracy of speech emotion recognition by fusing audio and text features. The limitation is that the existing fused methods are unable to maintain modality interaction effectively. Due to the heterogeneity between speech and text, these methods do not well align the timing of expressed emotions in speech and text. To this end, we propose a contrastive self-alignment learning (CSAL) method to align audio and text features for bimodal speech emotion recognition. It first adopts pre-trained large models to extract the high-dimensional audio and text features, and then maps them to the same temporal feature space. To tackle the challenging issues of heterogeneity and timing between speech and text, we design a novel audio-text contrastive loss to better align the expressed emotions between the heterogeneous speech and text spaces. It adopts a cross-modal contrastive loss to align audio and text features that are relevant to emotions in the temporal dimension. After that, a cross-attention mechanism is deployed to perform feature fusion, which achieves full interaction between the two modalities. The experimental results show that our method outperforms the state-of-the-art methods on speech emotion benchmarks.}
}
@article{VERMA2025111195,
title = {Navigating sentiment analysis through fusion, learning, utterance, and attention Methods: An extensive four-fold perspective survey},
journal = {Engineering Applications of Artificial Intelligence},
volume = {156},
pages = {111195},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.111195},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625011960},
author = {Bhavana Verma and Priyanka Meel and Dinesh Kumar Vishwakarma},
keywords = {Sentiment analysis (SA), Fusion techniques, Multimodal sentiment analysis (MSA), Deep learning, Hybrid learning, Attention mechanism},
abstract = {The growing need to automate user sentiment evaluation for goods and services, especially given the rising popularity of video content for online opinion expression, highlights the importance of sentiment analysis in artificial intelligence. This survey introduces a novel taxonomy and analyzes sentiment analysis approaches from four perspectives: Fusion-based, Learning-based, Utterance-based, and Attention-based methods. The study is organized around fundamental elements, significance, and applications of sentiment analysis. The proposed literature provides details about fusion techniques, which may be categorized based on levels, modalities, and procedures, including rule-based and classification-based approaches. This study seeks to provide a concise overview of several learning approaches, accompanied by a brief discussion of a benchmark dataset specific to each model. This work offers a concise analysis of utterance acquisition, focusing on the classification of sentiments according to language specificity and bilingual or multilingual proficiency, as well as the factors of linguistic basis, granularity, and multimodality. Additionally, a benchmark set derived from publicly accessible datasets is presented, accompanied by comprehensive insights. Critical analysis of sentiment analysis limitations provides valuable insights into challenges and potential solutions. The survey concludes by highlighting prospects in the dynamic landscape of sentiment analysis.}
}
@article{LI2024124236,
title = {Hierarchical denoising representation disentanglement and dual-channel cross-modal-context interaction for multimodal sentiment analysis},
journal = {Expert Systems with Applications},
volume = {252},
pages = {124236},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124236},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424011023},
author = {Zuhe Li and Zhenwei Huang and Yushan Pan and Jun Yu and Weihua Liu and Haoran Chen and Yiming Luo and Di Wu and Hao Wang},
keywords = {Multimodal sentiment analysis, Hierarchical disentanglement, Inter-modal enhancement, Cross-modal context interaction},
abstract = {Multimodal sentiment analysis aims to extract sentiment cues from various modalities, such as textual, acoustic, and visual data, and manipulate them to determine the inherent sentiment polarity in the data. Despite significant achievements in multimodal sentiment analysis, challenges persist in addressing noise features in modal representations, eliminating substantial gaps in sentiment information among modal representations, and exploring contextual information that expresses different sentiments between modalities. To tackle these challenges, our paper proposes a new Multimodal Sentiment Analysis (MSA) framework. Firstly, we introduce the Hierarchical Denoising Representation Disentanglement module (HDRD), which employs hierarchical disentanglement techniques. This ensures the extraction of both common and private sentiment information while eliminating interference noise from modal representations. Furthermore, to address the uneven distribution of sentiment information among modalities, our Inter-Modal Representation Enhancement module (IMRE) enhances non-textual representations by extracting sentiment information related to non-textual representations from textual representations. Next, we introduce a new interaction mechanism, the Dual-Channel Cross-Modal Context Interaction module (DCCMCI). This module not only mines correlated contextual sentiment information within modalities but also explores positive and negative correlation contextual sentiment information between modalities. We conducted extensive experiments on two benchmark datasets, MOSI and MOSEI, and the results indicate that our proposed method offers state-of-the-art approaches.}
}
@article{KAPLAN2025346,
title = {Can Large Language Models Generate Effective Datasets for Emotion Recognition in Conversations?},
journal = {Procedia Computer Science},
volume = {264},
pages = {346-355},
year = {2025},
note = {International Neural Network Society Workshop on Deep Learning Innovations and Applications 2025},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.07.145},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925021957},
author = {Burak Can Kaplan and Hugo Cesar {De Castro Carneiro} and Stefan Wermter},
keywords = {Large Language Models, Affective Computing, Data Generation},
abstract = {Emotion recognition in conversations (ERC) focuses on identifying emotion shifts within interactions, representing a significant step toward advancing machine intelligence. However, ERC data remains scarce, and existing datasets face numerous challenges due to their highly biased sources and the inherent subjectivity of soft labels. Even though Large Language Models (LLMs) have demonstrated their quality in many affective tasks, they are typically expensive to train, and their application to ERC tasks—particularly in data generation—remains limited. To address these challenges, we employ a small, resource-efficient, and general-purpose LLM to synthesize ERC datasets with diverse properties, supplementing the three most widely used ERC benchmarks. For each benchmark, we generated two datasets of similar characteristics, totaling six datasets. We evaluate the utility of these datasets to (1) supplement existing datasets for ERC classification, and (2) analyze the effects of label imbalance in ERC. Our experimental results indicate that ERC classifier models trained on the generated datasets exhibit strong robustness and consistently achieve statistically significant performance improvements on existing ERC benchmarks.}
}
@article{SINGH2022103712,
title = {Analysis of constant-Q filterbank based representations for speech emotion recognition},
journal = {Digital Signal Processing},
volume = {130},
pages = {103712},
year = {2022},
issn = {1051-2004},
doi = {https://doi.org/10.1016/j.dsp.2022.103712},
url = {https://www.sciencedirect.com/science/article/pii/S1051200422003293},
author = {Premjeet Singh and Shefali Waldekar and Md Sahidullah and Goutam Saha},
keywords = {Constant-Q filterbank, Constant-Q transform (CQT), Continuous wavelet transform (CWT), Time invariance, Speech emotion recognition (SER)},
abstract = {This work analyzes the constant-Q filterbank-based time-frequency representations for speech emotion recognition (SER). Constant-Q filterbank provides non-linear spectro-temporal representation with higher frequency resolution at low frequencies. Our investigation reveals how the increased low-frequency resolution benefits SER. The time-domain comparative analysis between short-term mel-frequency spectral coefficients (MFSCs) and constant-Q filterbank-based features, namely constant-Q transform (CQT) and continuous wavelet transform (CWT), reveals that constant-Q representations provide higher time-invariance at low-frequencies. This provides increased robustness against emotion irrelevant temporal variations in pitch, especially for low-arousal emotions. The corresponding frequency-domain analysis over different emotion classes shows better resolution of pitch harmonics in constant-Q-based time-frequency representations than MFSC. These advantages of constant-Q representations are further consolidated by SER performance in the extensive evaluation of features over four publicly available databases with six advanced deep neural network architectures as the back-end classifiers. Our inferences in this study hint toward the suitability and potentiality of constant-Q features for SER.}
}
@article{MAI2023101920,
title = {Learning from the global view: Supervised contrastive learning of multimodal representation},
journal = {Information Fusion},
volume = {100},
pages = {101920},
year = {2023},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.101920},
url = {https://www.sciencedirect.com/science/article/pii/S1566253523002361},
author = {Sijie Mai and Ying Zeng and Haifeng Hu},
keywords = {Multimodal sentiment analysis, Multimodal representation learning, Contrastive learning, Multimodal humor detection},
abstract = {The development of technology enables the availability of abundant multimodal data, which can be utilized in many representation learning tasks. However, most methods ignore the rich modality correlation information stored in each multimodal object and fail to fully exploit the potential of multimodal data. To address the aforementioned issue, cross-modal contrastive learning methods are proposed to learn the similarity score of each modality pair in a self-/weakly-supervised manner and improve the model robustness. Though effective, contrastive learning based on unimodal representations might be, in some cases, inaccurate as unimodal representations fail to reveal the global information of multimodal objects. To this end, we propose a contrastive learning pipeline based on multimodal representations to learn from the global view, and devise multiple techniques to generate negative and positive samples for each anchor. To generate positive samples, we apply the mix-up operation to mix two multimodal representations of different objects that have the maximal label similarity. Moreover, we devise a permutation-invariant fusion mechanism to define the positive samples by permuting the input order of modalities for fusion and sampling various contrastive fusion networks. In this way, we force the multimodal representation to be invariant regarding the order of modalities and the structures of fusion networks, so that the model can capture high-level semantic information of multimodal objects. To define negative samples, for each modality, we randomly replace the unimodal representation with that from another dissimilar object when synthesizing the multimodal representation. By this means, the model is led to capture the high-level concurrence information and correspondence relationship between modalities within each object. We also directly define the multimodal representation from another object as a negative sample, where the chosen object shares the minimal label similarity with the anchor. The label information is leveraged in the proposed framework to learn a more discriminative multimodal embedding space for downstream tasks. Extensive experiments demonstrate that our method outperforms previous state-of-the-art baselines on the tasks of multimodal sentiment analysis and humor detection.}
}
@article{AUGELLO2022510,
title = {Multimodal Mood Recognition for Assistive Scenarios},
journal = {Procedia Computer Science},
volume = {213},
pages = {510-517},
year = {2022},
note = {2022 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence: The 13th Annual Meeting of the BICA Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.11.098},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922017896},
author = {Agnese Augello and Giulia Di Bella and Ignazio Infantino and Giovanni Pilato and Gianpaolo Vitale},
keywords = {Emotion Detection, Multimodal Emotion Recognition, Mood, Social Assistive Robots},
abstract = {We illustrate a system performing multimodal human emotion detection from video input through the integration of audio emotional recognition, text emotional recognition, facial emotional recognition, and emotional recognition from a spectrogram. The outcomes of the four emotion recognition modalities are compared, and a final evaluation provides the most likely perceived emotion. The system has been designed to be easily implemented on cheap mini-computer based boards. It is conceived to be used as auxiliary tool in the field of telemedicine to remotely monitor the mood of patients and observe their healing process, which is closely related to their emotional condition.}
}
@incollection{QU2026251,
title = {12 - Quantum federated learning for speech emotion recognition: Quantum federated learning in 5G IoV},
editor = {Long Cheng and Nishant Saurabh and Ying Mao},
booktitle = {Quantum Computational AI},
publisher = {Morgan Kaufmann},
pages = {251-272},
year = {2026},
isbn = {978-0-443-30259-6},
doi = {https://doi.org/10.1016/B978-0-44-330259-6.00022-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443302596000220},
author = {Zhiguo Qu},
keywords = {Quantum federated learning, Speech emotion recognition, Quantum recurrent neural network, Quantum minimal gated unit},
abstract = {The technology of speech emotion recognition (SER) has found extensive application in human-computer interaction within the Internet of vehicles (IoV). The integration of emerging technologies, such as artificial intelligence and big data has significantly advanced SER capabilities. However, challenges remain, including limited computational resources, inefficiencies in data processing, and concerns regarding security and privacy. Recently, quantum machine learning has been introduced in intelligent transportation, showcasing advantages such as high prediction accuracy, robust noise resistance, and enhanced security. This study first integrates quantum federated learning (QFL) into 5G IoV using a quantum minimal gated unit (QMGU) recurrent neural network for local training. It then presents a novel QFL algorithm, QFSM, aimed at improving computational efficiency and privacy protection. Experimental results indicate that the QFSM algorithm outperforms existing methods using quantum long short-term memory networks or quantum gated recurrent units, achieving higher recognition accuracy and faster training convergence. Furthermore, it demonstrates superior privacy protection and noise robustness, enhancing its applicability and practicality.}
}
@article{MEHRISH2023101869,
title = {A review of deep learning techniques for speech processing},
journal = {Information Fusion},
volume = {99},
pages = {101869},
year = {2023},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.101869},
url = {https://www.sciencedirect.com/science/article/pii/S1566253523001859},
author = {Ambuj Mehrish and Navonil Majumder and Rishabh Bharadwaj and Rada Mihalcea and Soujanya Poria},
keywords = {Deep learning, Speech processing, Transformers, Survey, Trends},
abstract = {The field of speech processing has undergone a transformative shift with the advent of deep learning. The use of multiple processing layers has enabled the creation of models capable of extracting intricate features from speech data. This development has paved the way for unparalleled advancements in speech recognition, text-to-speech synthesis, automatic speech recognition, and emotion recognition, propelling the performance of these tasks to unprecedented heights. The power of deep learning techniques has opened up new avenues for research and innovation in the field of speech processing, with far-reaching implications for a range of industries and applications. This review paper provides a comprehensive overview of the key deep learning models and their applications in speech-processing tasks. We begin by tracing the evolution of speech processing research, from early approaches, such as MFCC and HMM, to more recent advances in deep learning architectures, such as CNNs, RNNs, transformers, conformers, and diffusion models. We categorize the approaches and compare their strengths and weaknesses for solving speech-processing tasks. Furthermore, we extensively cover various speech-processing tasks, datasets, and benchmarks used in the literature and describe how different deep-learning networks have been utilized to tackle these tasks. Additionally, we discuss the challenges and future directions of deep learning in speech processing, including the need for more parameter-efficient, interpretable models and the potential of deep learning for multimodal speech processing. By examining the field’s evolution, comparing and contrasting different approaches, and highlighting future directions and challenges, we hope to inspire further research in this exciting and rapidly advancing field.}
}
@article{LATIF2023109425,
title = {Generative emotional AI for speech emotion recognition: The case for synthetic emotional speech augmentation},
journal = {Applied Acoustics},
volume = {210},
pages = {109425},
year = {2023},
issn = {0003-682X},
doi = {https://doi.org/10.1016/j.apacoust.2023.109425},
url = {https://www.sciencedirect.com/science/article/pii/S0003682X23002232},
author = {Siddique Latif and Abdullah Shahid and Junaid Qadir},
keywords = {Tacotron, WaveRNN, Speech synthesis, Text-to-speech, Emotional speech synthesis, Speech emotion recognition},
abstract = {Despite advances in deep learning, current state-of-the-art speech emotion recognition (SER) systems still have poor performance due to a lack of speech emotion datasets. This paper proposes augmenting SER systems with synthetic emotional speech generated by an end-to-end text-to-speech (TTS) system based on an extended Tacotron 2 architecture. The proposed TTS system includes encoders for speaker and emotion embeddings, a sequence-to-sequence text generator for creating Mel-spectrograms, and a WaveRNN to generate audio from the Mel-spectrograms. Extensive experiments show that the quality of the generated emotional speech can significantly improve SER performance on multiple datasets, as demonstrated by a higher mean opinion score (MOS) compared to the baseline. The generated samples were also effective at augmenting SER performance.}
}
@article{ZENG2023119240,
title = {Heterogeneous graph convolution based on In-domain Self-supervision for Multimodal Sentiment Analysis},
journal = {Expert Systems with Applications},
volume = {213},
pages = {119240},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.119240},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422022588},
author = {Yufei Zeng and Zhixin Li and Zhenjun Tang and Zhenbin Chen and Huifang Ma},
keywords = {Multimodal sentiment analysis, Self-supervised learning, In-domain pretrained, Heterogeneous graph convolution, Multi-task learning},
abstract = {The inability to fully exploit domain-specific knowledge and the lack of an effective integration method have been the difficulties and focus of multimodal sentiment analysis. In this paper, we propose heterogeneous graph convolution with in-domain self-supervised multi-task learning for multimodal sentiment analysis (HIS-MSA) to solve these problems. Firstly, HIS-MSA carries out the second pre-trained with different self-supervised training strategies to fully mine the unique knowledge of the in-domain corpus, and give BERT the awareness of professional field. Secondly, HIS-MSA uses heterogeneous graph, which is good at integrating heterogeneous knowledge, to fuse feature from multiple sources. Finally, a unimodal label generation module is used to jointly guide multimodal tasks and unimodal tasks to balance independent and complementary information between the modalities. We conducted experiments on the datasets MOSI and MOSEI, which have 2199 and 23454 video segments respectively. The results show an average improvement of approximately 1.5 points in all metrics compared to the current state-of-the-art model.}
}
@article{PAN2024103856,
title = {Spanish MEACorpus 2023: A multimodal speech–text corpus for emotion analysis in Spanish from natural environments},
journal = {Computer Standards & Interfaces},
volume = {90},
pages = {103856},
year = {2024},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2024.103856},
url = {https://www.sciencedirect.com/science/article/pii/S0920548924000254},
author = {Ronghao Pan and José Antonio García-Díaz and Miguel Ángel Rodríguez-García and Rafel Valencia-García},
keywords = {Multimodal emotion analysis, Deep-learning, Speech emotion analysis, Transformers, Text classification, Natural language processing},
abstract = {In human–computer interaction, emotion recognition provides a deeper understanding of the user’s emotions, enabling empathetic and effective responses based on the user’s emotional state. While deep learning models have improved emotion recognition solutions, it is still an active area of research. One important limitation is that most emotion recognition systems use only text as input, ignoring features such as voice intonation. Another limitation is the limited number of datasets available for multimodal emotion recognition. In addition, most published datasets contain emotions that are simulated by professionals and produce limited results in real-world scenarios. In other languages, such as Spanish, hardly any datasets are available. Therefore, our contributions to emotion recognition are as follows. First, we compile and annotate a new corpus for multimodal emotion recognition in Spanish (Spanish MEACorpus 2023), which contains 13.16 h of speech divided into 5129 segments labeled by considering Ekman’s six basic emotions. The dataset is extracted from YouTube videos in natural environments. Second, we explore several deep learning models for emotion recognition using text- and audio-based features. Third, we evaluate different multimodal techniques to build a multimodal recognition system that improves the results of unimodal models, achieving a Macro F1-score of 87.745%, using late fusion with concatenation strategy approach.}
}
@article{LIN2024122254,
title = {Reinforcement learning and bandits for speech and language processing: Tutorial, review and outlook},
journal = {Expert Systems with Applications},
volume = {238},
pages = {122254},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122254},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423027562},
author = {Baihan Lin},
keywords = {Reinforcement learning, Bandits, Speech processing, Natural language processing, Speech recognition, Large language models, Survey, Perspective},
abstract = {In recent years, reinforcement learning and bandits have transformed a wide range of real-world applications including healthcare, finance, recommendation systems, robotics, and last but not least, the speech and natural language processing. While most speech and language applications of reinforcement learning algorithms are centered around improving the training of deep neural networks with its flexible optimization properties, there are still many grounds to explore to utilize the benefits of reinforcement learning, such as its reward-driven adaptability, state representations, temporal structures and generalizability. In this survey, we present an overview of recent advancements of reinforcement learning and bandits including those in the large language models, and discuss how they can be effectively employed to solve speech and natural language processing problems with models that are adaptive, interactive and scalable.}
}
@article{BAO2025111535,
title = {HIAN: A hybrid interactive attention network for multimodal sarcasm detection},
journal = {Pattern Recognition},
volume = {164},
pages = {111535},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2025.111535},
url = {https://www.sciencedirect.com/science/article/pii/S0031320325001955},
author = {Yongtang Bao and Xin Zhao and Peng Zhang and Yue Qi and Haojie Li},
keywords = {Multimodal sarcasm, Multimodal sentiment, Attention mechanism, Transformer, Deep learning},
abstract = {Multimodal sarcasm detection aims to use various modalities of data, such as text, images, etc., to identify whether they contain sarcastic meanings. Both images and texts contain rich sarcastic clues, but there are differences in dimension between them, and the quality of the sarcastic information they contain is very different. Therefore, seeking an appropriate feature fusion strategy to align modal features to maximize the utilization of inconsistent relationships between modalities is a significant challenge in this task. To this end, we introduce a novel sarcasm detection fusion model based on multimodal hybrid interactive attention (HIAN). We concatenate class words obtained from images with text and use the proposed bidirectional long short-term memory network with an interactive attention layer to enhance the extraction of text features. The text features obtained in this way can fully capture the contextual information of the text and the supplementary information in the image. To further enhance the feature fusion between modalities, we propose a multimodal interactive attention network and a fusion-enhanced transformer to promote the sharing of high-order complementary information, which represents the complementary non-linear semantic relationship between the three modalities and captures more inconsistencies between modalities. Extensive experiments conducted on publicly available multimodal sarcasm detection benchmark datasets show that our results surpass those of the baseline model and current state-of-the-art methods for the case of using the base BERT model.}
}
@article{ZHANG2024,
title = {Multimodal Sentiment Analysis Method Based on Hierarchical Adaptive Feature Fusion Network},
journal = {International Journal on Semantic Web and Information Systems},
volume = {20},
number = {1},
year = {2024},
issn = {1552-6283},
doi = {https://doi.org/10.4018/IJSWIS.335918},
url = {https://www.sciencedirect.com/science/article/pii/S1552628324000978},
author = {Huchao Zhang},
keywords = {Adaptive Gating Mechanism, Cross-Modal Feature Interaction, Hierarchy Adaptation, Multimodal Feature Fusion, Sentiment Analysis},
abstract = {ABSTRACT
The traditional multi-modal sentiment analysis (MSA) method usually considers the multi-modal characteristics to be equally important and ignores the contribution of different modes to the final MSA result. Therefore, an MSA method based on hierarchical adaptive feature fusion network is proposed. Firstly, RoBERTa, ResViT, and LibROSA are used to extract different modal features and construct a layered adaptive multi-modal fusion network. Then, the multi-modal feature extraction module and cross-modal feature interaction module are combined to realize the interactive fusion of information between modes. Finally, an adaptive gating mechanism is introduced to design a global multi-modal feature interaction module to learn the unique features of different modes. The experimental results on three public data sets show that the proposed method can make full use of multi-modal information, outperform other advanced comparison methods, improve the accuracy and robustness of sentiment analysis, and is expected to achieve better results in the field of sentiment analysis.}
}
@article{ARUNAGLADYS2023126693,
title = {Survey on multimodal approaches to emotion recognition},
journal = {Neurocomputing},
volume = {556},
pages = {126693},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.126693},
url = {https://www.sciencedirect.com/science/article/pii/S0925231223008160},
author = {A. {Aruna Gladys} and V. Vetriselvi},
keywords = {Emotion recognition, Affect sensing, Multimodal emotion recognition, Computer vision},
abstract = {Emotion is an instinctive state of mind created by the neurophysiological changes occurring in the human body as reactions to various internal or external stimuli. Emotions play a vital role in decision-making. The choices one makes in day-to-day life determine their behaviour and thus their character. Emotion and behaviour recognition are the key processes in ascertaining Emotional Intelligence (EQ) which is the inherent human potential to understand and manage one’s own emotions in positive ways. But the process requires high expertise in the field of psychology and is exhaustive and time-consuming. This has opened a new horizon for exploring the computational recognition of EQ. Emotion Recognition (ER) is one of its sub-processes that identifies various human emotional states. Emotions are detected from physiological signals and also through non-invasive, vision-based algorithms by exploiting video and audio modalities. With the emergence of big data and state-of-art deep learning architectures combined with the vast availability of emotion-rich video content from various streaming platforms, Multimodal Emotion Recognition (MER) which detects emotions through multiple and complementary input modalities from video has gathered momentum in recent years. This survey paper elaborately discusses the unimodal ER through visual, auditory, and linguistic modalities and reviews MER with combined features from these modalities. It also discusses the joint representations and fusion mechanisms used to acquire the intermodal correlations. Finally, we put forward the limitations and gaps identified in the literature along with a few suggestions for future work.}
}
@article{HASHEM2023102974,
title = {Speech emotion recognition approaches: A systematic review},
journal = {Speech Communication},
volume = {154},
pages = {102974},
year = {2023},
issn = {0167-6393},
doi = {https://doi.org/10.1016/j.specom.2023.102974},
url = {https://www.sciencedirect.com/science/article/pii/S0167639323001085},
author = {Ahlam Hashem and Muhammad Arif and Manal Alghamdi},
keywords = {Speech emotion recognition, Emotional speech database, Classification of emotion, Speech features, Systematic review},
abstract = {The speech emotion recognition (SER) field has been active since it became a crucial feature in advanced Human–Computer Interaction (HCI), and wide real-life applications use it. In recent years, numerous SER systems have been covered by researchers, including the availability of appropriate emotional databases, selecting robustness features, and applying suitable classifiers using Machine Learning (ML) and Deep Learning (DL). Deep models proved to perform more accurately for SER than conventional ML techniques. Nevertheless, SER is yet challenging for classification where to separate similar emotional patterns; it needs a highly discriminative feature representation. For this purpose, this survey aims to critically analyze what is being done in this field of research in light of previous studies that aim to recognize emotions using speech audio in different aspects and review the current state of SER using DL. Through a systematic literature review whereby searching selected keywords from 2012–2022, 96 papers were extracted and covered the most current findings and directions. Specifically, we covered the database (acted, evoked, and natural) and features (prosodic, spectral, voice quality, and teager energy operator), the necessary preprocessing steps. Furthermore, different DL models and their performance are examined in depth. Based on our review, we also suggested SER aspects that could be considered in the future.}
}
@article{ABIKANAAN2024100333,
title = {Combining a multi-feature neural network with multi-task learning for emergency calls severity prediction},
journal = {Array},
volume = {21},
pages = {100333},
year = {2024},
issn = {2590-0056},
doi = {https://doi.org/10.1016/j.array.2023.100333},
url = {https://www.sciencedirect.com/science/article/pii/S2590005623000589},
author = {Marianne {Abi Kanaan} and Jean-François Couchot and Christophe Guyeux and David Laiymani and Talar Atechian and Rony Darazi},
keywords = {Emergency calls, Natural language processing, Speech emotion recognition, Deep learning},
abstract = {In emergency call centers, operators are required to analyze and prioritize emergency situations prior to any intervention. This allows the team to deploy resources efficiently if needed, and thereby provide the optimal assistance to the victims. The automation of such an analysis remains challenging, given the unpredictable nature of the calls. Therefore, in this study, we describe our attempt in improving an emergency calls processing system’s accuracy in the classification of an emergency’s severity, based on transcriptions of the caller’s speech. Specifically, we first extend the baseline classifier to include additional feature extractors of different modalities of data. These features include detected emotions, time-based features, and the victim’s personal information. Second, we experiment with a multi-task learning approach, in which we attempt to detect the nature of the emergency on the one hand, and improve the severity classification score on the other hand. Additional improvements include the use of a larger dataset and an explainability study of the classifier’s decision-making process. Our best model was able to predict 833 emergency calls’ severity with a 71.27% accuracy, a 5.33% improvement over the baseline model. Moreover, we extended our tool with additional modules that can prove to be useful when handling emergency calls.}
}
@article{YU2025130640,
title = {Cross-modal Interaction and Multi-view Misalignment Network for multimodal sarcasm detection},
journal = {Neurocomputing},
volume = {649},
pages = {130640},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.130640},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225013128},
author = {Bengong Yu and Chenyue Li and Zhonghao Xi and Haoyu Wang and Shuping Zhao},
keywords = {Multimodal sarcasm detection, Cross-modal interaction, Graph Attention Network, Modality misalignment},
abstract = {Sarcasm often relies on semantic or emotional misalignment between text and images to convey deeper meaning. With the increasing variety of modalities on social media, multimodal sarcasm detection (MSD) has become a challenging task. However, existing multimodal methods still have limitations in heterogeneous modality fusion, text-image sentiment modeling, and modality misalignment detection, making it difficult to fully utilize multimodal information for cross-modal interaction. Therefore, this study proposes the Cross-modal Interaction and Multi-view Misalignment Network for Multimodal Sarcasm Detection (CIMMN) to enhance modality fusion capability and measure cross-modal inconsistencies. CIMMN employs a cross-modal co-attention mechanism with intermediary features to extract global correlation features between text and images, facilitating balanced modality interaction while reducing the computational complexity of large-scale processing. Additionally, a specially designed prompt and large language models (LLMs) are used to convert dispersed visual information into high-quality structured image captions, providing a uniform processing format for heterogeneous modalities. A graph attention network is further applied to extract deep sentiment features from text and images, addressing the shortcomings of existing research in image sentiment extraction. At the same time, a distance measurement method based on optimal transport theory is adopted to compute semantic deviations and emotional conflicts between modalities from three different perspectives, allowing for a comprehensive assessment of modality misalignment features. Finally, by integrating global correlation features, deep sentiment features, and misalignment features, CIMMN captures comprehensive multimodal sarcasm cues, enabling accurate sarcasm detection. Experimental comparisons on three public datasets demonstrate that CIMMN outperforms existing state-of-the-art methods, verifying its effectiveness in multimodal sarcasm detection tasks.}
}
@article{LLANESJURADO2024123261,
title = {Developing conversational Virtual Humans for social emotion elicitation based on large language models},
journal = {Expert Systems with Applications},
volume = {246},
pages = {123261},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.123261},
url = {https://www.sciencedirect.com/science/article/pii/S095741742400126X},
author = {Jose Llanes-Jurado and Lucía Gómez-Zaragozá and Maria Eleonora Minissi and Mariano Alcañiz and Javier Marín-Morales},
keywords = {Virtual Human, Conversational agent, Affective computing, Emotion recognition, Large language models, Statistical learning},
abstract = {Emotions play a critical role in numerous processes, including, but not limited to, social interactions. Consequently, the ability to evoke and recognize emotions is a challenging task with widespread implications, notably in the field of mental health assessment systems. However, up until now, emotional elicitation methods have not utilized simulated open social conversations. Our study introduces a comprehensive Virtual Human (VH), equipped with a realistic avatar and conversational abilities based on a Large Language Model. This architecture integrates psychological constructs – such as personality, mood, and attitudes – with emotional facial expressions, lip synchronization, and voice synthesis. All these features are embedded into a modular, cognitively-inspired framework, specifically designed for voice-based semi-guided emotional conversations in real time. The validation process involved an experiment with 64 participants interacting with six distinct VHs, each designed to provoke a different basic emotion. The system took an average of 4.44 s to generate the VH’s response. Participants assessed the naturalness and realism of the conversation, scoring averages of 4.61 and 4.44 out of 7, respectively. The VHs successfully generated the intended emotional valence in the users, while arousal was not evoked, though it could be recognized in the VHs. Our findings underscore the feasibility of employing VHs within affective computing to elicit emotions in socially and ecologically valid contexts. This development holds significant potential for application in sectors such as health, education, and marketing, among others.}
}
@article{ZHANG2025105023,
title = {Emotional boundaries and intensity aware model for incomplete multimodal sentiment analysis},
journal = {Digital Signal Processing},
volume = {160},
pages = {105023},
year = {2025},
issn = {1051-2004},
doi = {https://doi.org/10.1016/j.dsp.2025.105023},
url = {https://www.sciencedirect.com/science/article/pii/S1051200425000454},
author = {Yuqing Zhang and Dongliang Xie and Dawei Luo and Baosheng Sun},
keywords = {Multimodal sentiment analysis, Missing modality, Classification boundary fuzzy, Weak modality aware},
abstract = {Multimodal sentiment analysis aims to obtain comprehensive emotional features from multiple data sources when all modalities are accessible. However, in real-world scenarios, it is often impossible for all modalities to be available all the time. This issue leads to a significant degradation in the performance of multimodal sentiment analysis. There are two major challenges in this field: accurately identifying samples near the classification boundaries is difficult, and the recognition performance varies significantly among different modality combinations. In this article, we propose an Emotional Boundaries and Emotional Intensity Aware (EBIA) model to enhance the robustness of incomplete multimodal sentiment analysis. Specifically, we design a Boundary Fuzzy Aware (BFA) module to learn the intra-class and inter-class consistency of samples, transferring full modalities integrity information to the missing modality environment, and prompting the model to focus on samples near the class boundaries. Additionally, we introduce a Weak Modality Aware (WMA) module that calculates additional predictions for each modality, guiding the model to focus on weak modality combinations. Extensive experiments and analyses conducted on three popular benchmark datasets demonstrate the effectiveness of our proposed method compared with several baseline methods.}
}
@article{WANG2025100748,
title = {Empowering multimodal analysis with visualization: A survey},
journal = {Computer Science Review},
volume = {57},
pages = {100748},
year = {2025},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2025.100748},
url = {https://www.sciencedirect.com/science/article/pii/S1574013725000243},
author = {Jiachen Wang and Zikun Deng and Dazhen Deng and Xingbo Wang and Rui Sheng and Yi Cai and Huamin Qu},
keywords = {Visualization, Multimodal data, Machine learning},
abstract = {Multimodal data, which encompasses text, audio, image, and other modalities, is a popular research target in the field of visualization research. Existing visualization techniques for multimodal data are scattered and categorized by application domains, such as multimodal model analysis or online education. It lacks a comprehensive review from the perspective of data that summarizes the methodologies, research gaps, and future trends for researchers and practitioners. In this study, we delve into existing visualization research, identifying their data modalities, applications, strengths, and limitations. Furthermore, we shed light on the potential challenges and opportunities for further research in this domain to advance intelligent visualizations for multimodal data.}
}
@article{A2024111553,
title = {Sentiment analysis on a low-resource language dataset using multimodal representation learning and cross-lingual transfer learning},
journal = {Applied Soft Computing},
volume = {157},
pages = {111553},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.111553},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624003272},
author = {Aruna Gladys A. and Vetriselvi V.},
keywords = {Multimodal sentiment analysis, Representation learning, Cross-lingual transfer learning},
abstract = {Affect Sensing is a rapidly growing field with the potential to revolutionize human–computer interaction, healthcare, and many more applications. Multimodal Sentiment Analysis (MSA) is a recent research area that exploits the multimodal nature of video data for affect sensing. However, the success of a multimodal framework depends on addressing the challenges associated with integrating diverse modalities and selecting informative features. We propose a novel multimodal representation learning framework using multimodal autoencoders that learns a comprehensive representation of the underlying heterogeneous modalities. Affect Sensing is even more challenging in low-resource languages because annotated video datasets and language-specific models are limited. To address this concern, we introduce Multimodal Sentiment Analysis Corpus in Tamil (MSAT), a small-sized dataset in the Tamil language for MSA, and exhibit how a novel technique involving cross-lingual transfer learning in a multimodal setting, leverages the knowledge gained by training the model on a larger English MSA dataset to fine-tune a much smaller Tamil MSA dataset. Our transfer learning model achieves significant gain in the Tamil dataset by a large margin. Our experiments demonstrate that we can build efficient, generalized models for low-resource languages by using the existing MSA datasets.}
}
@article{CHEN2025111439,
title = {A survey of semantic extraction for speech semantic communications: Metrics, approaches, perspectives and challenges},
journal = {Engineering Applications of Artificial Intelligence},
volume = {158},
pages = {111439},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.111439},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625014411},
author = {Chong Chen and Linyu Huang},
keywords = {Semantic communication, Semantic extraction, Deep learning, Semantic metrics, Speech signals},
abstract = {With the rapid development of communication technology, traditional bit transmission can no longer meet the demands of emerging intelligent applications for real-time communication, low latency, and efficient transmission. As a new communication paradigm, Semantic communication (SemCom) focuses on understanding and transmitting task-relevant semantic information, making it an important direction for improving communication efficiency. Researchers have extensively studied SemCom for text and images, but the research on speech is relatively limited. Semantic extraction is a key component of SemCom, and it varies significantly across modalities. Machine learning and deep learning techniques have significantly advanced speech signal processing and semantic extraction tasks, including speech embedding and automatic speech recognition, thus enhancing end-to-end speech SemCom networks. This paper provides a survey of the research progress in semantic extraction for speech SemCom. First, the paper describes the fundamentals of SemCom and speech signal processing, including acoustic feature and speech preprocessing. Taking into account the differences in the evaluation metrics between SemCom and traditional communication, this paper then presents the semantic metrics proposed in existing studies. Next, this paper compares recent progress in speech embedding and two commonly used methods for semantic extraction from speech: one based on speech-to-text conversion and the other on direct extraction from speech signals. Assistive methods for pragmatic-level semantic extraction are also discussed. Subsequently, existing studies on speech SemCom are reviewed and compared. Finally, we discuss the challenges and future directions in SemCom, with the aim of providing references for future research.}
}
@article{GKOUMAS2021184,
title = {What makes the difference? An empirical comparison of fusion strategies for multimodal language analysis},
journal = {Information Fusion},
volume = {66},
pages = {184-197},
year = {2021},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2020.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S1566253520303675},
author = {Dimitris Gkoumas and Qiuchi Li and Christina Lioma and Yijun Yu and Dawei Song},
keywords = {Multimodal human language understanding, Video sentiment analysis, Emotion recognition, Reproducibility in multimodal machine learning},
abstract = {Multimodal video sentiment analysis is a rapidly growing area. It combines verbal (i.e., linguistic) and non-verbal modalities (i.e., visual, acoustic) to predict the sentiment of utterances. A recent trend has been geared towards different modality fusion models utilizing various attention, memory and recurrent components. However, there lacks a systematic investigation on how these different components contribute to solving the problem as well as their limitations. This paper aims to fill the gap, marking the following key innovations. We present the first large-scale and comprehensive empirical comparison of eleven state-of-the-art (SOTA) modality fusion approaches in two video sentiment analysis tasks, with three SOTA benchmark corpora. An in-depth analysis of the results shows that the attention mechanisms are the most effective for modelling crossmodal interactions, yet they are computationally expensive. Second, additional levels of crossmodal interaction decrease performance. Third, positive sentiment utterances are the most challenging cases for all approaches. Finally, integrating context and utilizing the linguistic modality as a pivot for non-verbal modalities improve performance. We expect that the findings would provide helpful insights and guidance to the development of more effective modality fusion models.}
}
@article{QI2025104213,
title = {Multimodal disentanglement implicit distillation for speech emotion recognition},
journal = {Information Processing & Management},
volume = {62},
number = {5},
pages = {104213},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2025.104213},
url = {https://www.sciencedirect.com/science/article/pii/S0306457325001542},
author = {Xin Qi and Yujun Wen and Junpeng Gong and Pengzhou Zhang and Yao Zheng},
keywords = {Speech emotion recognition, Disentanglement representation learning, Knowledge distillation, Deep mutual learning},
abstract = {Audio signals are generally utilized with textual data for speech emotion recognition. Nevertheless, cross-modal interactions suffer from distribution discrepancy and information redundancy, leading to an inaccurate multimodal representation. Hence, this paper proposes a multimodal disentanglement implicit distillation model (MDID) that excavates and exploits each modality’s sentiment and specific characteristics. Specifically, the pre-trained models extract high-level acoustic and textual features and align them via an attention mechanism. Then, each modality is disentangled into modality sentiment-specific features. Subsequently, feature-level and logit-level distillation distill the purified modality-specific feature into the modality-sentiment feature. Compared to the adaptive fusion feature, solely employing the refined modality-sentiment feature yields superior performance for emotion recognition. Comprehensive experiments on the IEMOCAP and RAVDESS datasets indicate that MDID outperforms state-of-the-art approaches.}
}
@article{SINGH2025170,
title = {Quantum neural networks for multimodal sentiment, emotion, and sarcasm analysis},
journal = {Alexandria Engineering Journal},
volume = {124},
pages = {170-187},
year = {2025},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2025.03.023},
url = {https://www.sciencedirect.com/science/article/pii/S1110016825003229},
author = {Jaiteg Singh and Kamalpreet Singh Bhangu and Abdulrhman Alkhanifer and Ahmad Ali AlZubi and Farman Ali},
keywords = {Multimodal dialogue, Emotion quantification, Quantum cognition, Variational Quantum Eigensolver (VQE), Quantum Neural Networks (QNN)},
abstract = {Sentiment, emotion, and sarcasm analysis in multimodal dialogues is crucial for understanding the underlying intentions and attitudes expressed by individuals. Traditional methods often struggle to capture the full intensity of these polarities, leading to less accurate results. To address this limitation, we propose a quantum-inspired approach leveraging Quantum Neural Networks (QNNs) for enhanced classification and intensity analysis. A key component of our method is the Variational Quantum Eigensolver (VQE), a hybrid quantum-classical algorithm that optimizes the parameters of the QNN by minimizing the eigenvalues of a Hamiltonian system. This optimization enables the network to learn complex relationships in multimodal data more effectively. Our approach surpasses state-of-the-art methods, achieving up to 7.5 % higher accuracy and 6.8 % greater precision. Experiments on benchmark datasets such as MUStARD, Memotion, CMU-MOSEI, and MELD demonstrate its effectiveness, with an F1-score of 87.3 % on CMU-MOSEI. This method is particularly beneficial in domains like social media, customer support, and entertainment, where both verbal and non-verbal cues play a critical role in accurate sentiment analysis.}
}
@article{JAYASINGHE2026112122,
title = {A systematic review of interpretability and explainability for speech emotion features in automatic speech emotion recognition},
journal = {Pattern Recognition},
volume = {171},
pages = {112122},
year = {2026},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2025.112122},
url = {https://www.sciencedirect.com/science/article/pii/S0031320325007824},
author = {Hiruni Maleesa Jayasinghe and Kok Wai Wong and Anupiya Nugaliyadde},
keywords = {Speech emotion features, Automatic speech emotion recognition, Interpretability, Explainability},
abstract = {Speech Emotion Recognition (SER) is a method of identifying emotional states from the human voice. Automatic SER (ASER) is a research domain where Machine Learning (ML) is used to extract and analyze speech features to predict emotional states. Using ML in a sensitive area like SER requires transparency and reliability of the models. For instance, ASER is crucial to understanding the underlying decision-making in real-world applications such as mental health monitoring systems. Researchers, therefore, have focused attention on advancing the interpretability and explainability of ASER models. Interpretability maximizes human understanding of complex processes by providing meaningful insights. Explainability presents the interpretable insights in a clear and human-understandable manner. Some standard interpretability methods include feature importance, feature selection methods, and attention models. Explainability methods include SHapley Additive exPlanations (SHAP), visualizations using embedding plots, saliency maps, etc., and feature importance analysis. The current systematic review explores the different interpretability and explainability methods for speech emotion features. The current review paper aims to identify the progress in the area, identify potential research gaps, and motivate future research.}
}
@article{GRAGEDA2025101666,
title = {Speech emotion recognition in real static and dynamic human-robot interaction scenarios},
journal = {Computer Speech & Language},
volume = {89},
pages = {101666},
year = {2025},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2024.101666},
url = {https://www.sciencedirect.com/science/article/pii/S0885230824000494},
author = {Nicolás Grágeda and Carlos Busso and Eduardo Alvarado and Ricardo García and Rodrigo Mahu and Fernando Huenupan and Néstor Becerra Yoma},
keywords = {Speech emotion recognition, Human-robot interaction, Static and dynamic conditions, Acoustic modelling, Beamforming responses},
abstract = {The use of speech-based solutions is an appealing alternative to communicate in human-robot interaction (HRI). An important challenge in this area is processing distant speech which is often noisy, and affected by reverberation and time-varying acoustic channels. It is important to investigate effective speech solutions, especially in dynamic environments where the robots and the users move, changing the distance and orientation between a speaker and the microphone. This paper addresses this problem in the context of speech emotion recognition (SER), which is an important task to understand the intention of the message and the underlying mental state of the user. We propose a novel setup with a PR2 robot that moves as target speech and ambient noise are simultaneously recorded. Our study not only analyzes the detrimental effect of distance speech in this dynamic robot-user setting for speech emotion recognition but also provides solutions to attenuate its effect. We evaluate the use of two beamforming schemes to spatially filter the speech signal using either delay-and-sum (D&S) or minimum variance distortionless response (MVDR). We consider the original training speech recorded in controlled situations, and simulated conditions where the training utterances are processed to simulate the target acoustic environment. We consider the case where the robot is moving (dynamic case) and not moving (static case). For speech emotion recognition, we explore two state-of-the-art classifiers using hand-crafted features implemented with the ladder network strategy and learned features implemented with the wav2vec 2.0 feature representation. MVDR led to a signal-to-noise ratio higher than the basic D&S method. However, both approaches provided very similar average concordance correlation coefficient (CCC) improvements equal to 116 % with the HRI subsets using the ladder network trained with the original MSP-Podcast training utterances. For the wav2vec 2.0-based model, only D&S led to improvements. Surprisingly, the static and dynamic HRI testing subsets resulted in a similar average concordance correlation coefficient. Finally, simulating the acoustic environment in the training dataset provided the highest average concordance correlation coefficient scores with the HRI subsets that are just 29 % and 22 % lower than those obtained with the original training/testing utterances, with ladder network and wav2vec 2.0, respectively.}
}
@article{FAN2025127669,
title = {Multimodal speech emotion recognition via dynamic multilevel contrastive loss under local enhancement network},
journal = {Expert Systems with Applications},
volume = {281},
pages = {127669},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127669},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425012916},
author = {Weiquan Fan and Xiangmin Xu and Fang Liu and Xiaofen Xing},
keywords = {Affective computing, Multimodal emotion recognition, Speech and text processing, Dynamic multilevel contrastive loss, Local enhancement attention},
abstract = {Multimodal speech emotion recognition is crucial for advancing human–computer interaction technology. Contrastive learning, due to its powerful ability of representation, is increasingly being applied to emotion recognition. Existing algorithms usually only consider samples of the same emotion as positive matching pairs, but ignore that the distances of different positive pairs are often different. For this issue, this paper designs a novel dynamic multilevel contrastive loss (DMCL), which achieves adaptive distance constraint by dynamic multilevel similarity. It generalizes positive matching pairs in different cases, assigns them different distances, and dynamically adjusts the corresponding labels while modeling. Building upon the DMCL, this paper further proposes a local enhancement attention mechanism (LEA) that enhances local information token-by-token on a global basis, which can enhance the robustness of the model to emotional mutations. By integrating the advantages of LEA and DMCL, this paper constructs an end-to-end multimodal speech emotion recognition network (LEDMCN). Finally, experimental results on the IEMOCAP and LSSED datasets validate the effectiveness of the proposed method, achieving state-of-the-art performance.}
}
@article{PARRAGALLEGO2025104820,
title = {Multimodal evaluation of customer satisfaction from voicemails using speech and language representations},
journal = {Digital Signal Processing},
volume = {156},
pages = {104820},
year = {2025},
issn = {1051-2004},
doi = {https://doi.org/10.1016/j.dsp.2024.104820},
url = {https://www.sciencedirect.com/science/article/pii/S1051200424004457},
author = {Luis Felipe Parra-Gallego and Tomás Arias-Vergara and Juan Rafael Orozco-Arroyave},
keywords = {Group gated fusion, Customer satisfaction, Speech processing, Speech emotion recognition, Multimodal acoustic analysis},
abstract = {Customer satisfaction (CS) evaluation in call centers is essential for assessing service quality but commonly relies on human evaluations. Automatic evaluation systems can be used to perform CS analyses, enabling the evaluation of larger datasets. This research paper focuses on CS analysis through a multimodal approach that employs speech and language representations derived from the real-world voicemails. Additionally, given the similarity between the evaluation of a provided service (which may elicit different emotions in customers) and the automatic classification of emotions in speech, we also explore the topic of emotion recognition with the well-known corpus IEMOCAP which comprises 4-classes corresponding to different emotional states. We incorporated a language representation with word embeddings based on a CNN-LSTM model, and three different self-supervised learning (SSL) speech encoders, namely Wav2Vec2.0, HuBERT, and WavLM. A bidirectional alignment network based on attention mechanisms is employed for synchronizing speech and language representations. Three different fusion strategies are also explored in the paper. According to our results, the GGF model outperformed both, unimodal and other multimodal methods in the 4-class emotion recognition task on the IEMOCAP dataset and the binary CS classification task on the KONECTADB dataset. The study also demonstrated superior performance of our methodology compared to previous works on KONECTADB in both unimodal and multimodal approaches.}
}
@article{RZADECZKA2025,
title = {The Efficacy of Conversational AI in Rectifying the Theory-of-Mind and Autonomy Biases: Comparative Analysis},
journal = {JMIR Mental Health},
volume = {12},
year = {2025},
issn = {2368-7959},
doi = {https://doi.org/10.2196/64396},
url = {https://www.sciencedirect.com/science/article/pii/S2368795925000174},
author = {Marcin Rządeczka and Anna Sterna and Julia Stolińska and Paulina Kaczyńska and Marcin Moskalewicz},
keywords = {cognitive bias, conversational artificial intelligence, artificial intelligence, AI, chatbots, digital mental health, bias rectification, affect recognition},
abstract = {Background
The increasing deployment of conversational artificial intelligence (AI) in mental health interventions necessitates an evaluation of their efficacy in rectifying cognitive biases and recognizing affect in human-AI interactions. These biases are particularly relevant in mental health contexts as they can exacerbate conditions such as depression and anxiety by reinforcing maladaptive thought patterns or unrealistic expectations in human-AI interactions.
Objective
This study aimed to assess the effectiveness of therapeutic chatbots (Wysa and Youper) versus general-purpose language models (GPT-3.5, GPT-4, and Gemini Pro) in identifying and rectifying cognitive biases and recognizing affect in user interactions.
Methods
This study used constructed case scenarios simulating typical user-bot interactions to examine how effectively chatbots address selected cognitive biases. The cognitive biases assessed included theory-of-mind biases (anthropomorphism, overtrust, and attribution) and autonomy biases (illusion of control, fundamental attribution error, and just-world hypothesis). Each chatbot response was evaluated based on accuracy, therapeutic quality, and adherence to cognitive behavioral therapy principles using an ordinal scale to ensure consistency in scoring. To enhance reliability, responses underwent a double review process by 2 cognitive scientists, followed by a secondary review by a clinical psychologist specializing in cognitive behavioral therapy, ensuring a robust assessment across interdisciplinary perspectives.
Results
This study revealed that general-purpose chatbots outperformed therapeutic chatbots in rectifying cognitive biases, particularly in overtrust bias, fundamental attribution error, and just-world hypothesis. GPT-4 achieved the highest scores across all biases, whereas the therapeutic bot Wysa scored the lowest. Notably, general-purpose bots showed more consistent accuracy and adaptability in recognizing and addressing bias-related cues across different contexts, suggesting a broader flexibility in handling complex cognitive patterns. In addition, in affect recognition tasks, general-purpose chatbots not only excelled but also demonstrated quicker adaptation to subtle emotional nuances, outperforming therapeutic bots in 67% (4/6) of the tested biases.
Conclusions
This study shows that, while therapeutic chatbots hold promise for mental health support and cognitive bias intervention, their current capabilities are limited. Addressing cognitive biases in AI-human interactions requires systems that can both rectify and analyze biases as integral to human cognition, promoting precision and simulating empathy. The findings reveal the need for improved simulated emotional intelligence in chatbot design to provide adaptive, personalized responses that reduce overreliance and encourage independent coping skills. Future research should focus on enhancing affective response mechanisms and addressing ethical concerns such as bias mitigation and data privacy to ensure safe, effective AI-based mental health support.}
}
@article{CHEN2023103193,
title = {Joint multimodal sentiment analysis based on information relevance},
journal = {Information Processing & Management},
volume = {60},
number = {2},
pages = {103193},
year = {2023},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2022.103193},
url = {https://www.sciencedirect.com/science/article/pii/S0306457322002941},
author = {Danlei Chen and Wang Su and Peng Wu and Bolin Hua},
keywords = {Network public opinion, Sentiment classification, Image-text relevance, Multimodal fusion, Multimodal deep learning},
abstract = {Social media users are increasingly turning to express opinions with both images and text, while the visual content and text description may cover some conflicting information diverse from each other. Information relevance refers to the matching degree between cross-modal features at the emotional semantic level, which is not systematically studied. In order to exploit the discriminative features and the internal correlation among different modalities, the mid-level representation extracted by a visual sentiment concept classifier is used to determine information relevance, with the integration of other features, including attended textual and visual features. Then grid search is applied to tune weighting coefficients of the decision fusion scheme, followed by a multimodal adaptive method for joint sentiment analysis based on image-text relevance. The superiority of our architecture approach is demonstrated experimentally by comparing it with several state-of-the-art baselines, such as the vision-aware language modeling approach and contrastive learning-based model. The results indicate that fused multiple features lead to more precise classification than unimodal ones, while the contributions of each single modality differ obviously in emotional expression. Besides, the performance of every involved model varies markedly per dataset of different content correlation, which proves that it is of extensive theoretical significance and application prospect to introduce the image-text relevance classifier into a multimodal task.}
}
@article{GAN2023126623,
title = {Speech emotion recognition via multiple fusion under spatial–temporal parallel network},
journal = {Neurocomputing},
volume = {555},
pages = {126623},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.126623},
url = {https://www.sciencedirect.com/science/article/pii/S0925231223007464},
author = {Chenquan Gan and Kexin Wang and Qingyi Zhu and Yong Xiang and Deepak Kumar Jain and Salvador García},
keywords = {Speech emotion recognition, Speech spectrum, Spatial–temporal parallel network, Multiple fusion},
abstract = {Speech, as a necessary way to express emotions, plays a vital role in human communication. With the continuous deepening of research on emotion recognition in human–computer interaction, speech emotion recognition (SER) has become an essential task to improve the human–computer interaction experience. When performing emotion feature extraction of speech, the method of cutting the speech spectrum will destroy the continuity of speech. Besides, the method of using the cascaded structure without cutting the speech spectrum cannot simultaneously extract speech spectrum information from both temporal and spatial domains. To this end, we propose a spatial–temporal parallel network for speech emotion recognition without cutting the speech spectrum. To further mix the temporal and spatial features, we design a novel fusion method (called multiple fusion) that combines the concatenate fusion and ensemble strategy. Finally, the experimental results on five datasets demonstrate that the proposed method outperforms state-of-the-art methods.}
}
@article{DO2024112279,
title = {Multimodal sentiment analysis using deep learning and fuzzy logic: A comprehensive survey},
journal = {Applied Soft Computing},
volume = {167},
pages = {112279},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.112279},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624010536},
author = {Hoang Nam Do and Huyen Trang Phan and Ngoc Thanh Nguyen},
keywords = {Multimodal sentiment analysis, Convolutional neural network, Fuzzy logic, Deep learning, MSA-based fuzzy deep learning},
abstract = {Multimodal sentiment analysis (MSA) is the process of identifying sentiment polarities that users may simultaneously display in text, audio, and video data. Sentiment analysis methods based on a single data type are becoming increasingly unsuitable. Therefore, MSA has emerged, developed, and become increasingly popular today. MSA plays an important role in many practical applications, particularly in decision-making, recommendation systems, and fake news detection systems. Therefore, new proposals and performance improvements in multimodal sentiment-analysis methods are of great interest to scientists. Many multimodal sentiment-analysis methods have been proposed and developed, including those based on deep learning (DL) models, which have achieved significant prospects. This study comprehensively surveys the aspects related to MSA methods based on deep learning (MSA-based on DL) techniques. MSA-based on DL is the process of identifying sentiment polarities using DL, such as CNN, RNN, LSTM, GAN, and DBN, to create hidden layers for the automation of time-consuming stages, such as feature selection, feature extraction, parameter optimization, feature vector processing, and prediction generation on various data types simultaneously. In this paper, we propose a new taxonomy for MSA methods based on DL, and evaluate and compare method groups using commonly used data types. This article presents the advantages, disadvantages, and challenges that must be addressed in the future. Unlike previous survey works, this study proposes a particular classification by adding a group of multimodal sentiment-analysis methods based on a combination of DL techniques and fuzzy logic. The results of this study are expected to provide essential guidance to beginners, practitioners, and researchers regarding building and improving the performance of multimodal sentiment-analysis methods.}
}
@article{SINGH2021107316,
title = {A multimodal hierarchical approach to speech emotion recognition from audio and text},
journal = {Knowledge-Based Systems},
volume = {229},
pages = {107316},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107316},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121005785},
author = {Prabhav Singh and Ridam Srivastava and K.P.S. Rana and Vineet Kumar},
keywords = {Speech emotion recognition, Hierarchical approach, Multimodal, Deep learning, Lexical features},
abstract = {Speech emotion recognition (SER) plays a crucial role in improving the quality of man–machine interfaces in various fields like distance learning, medical science, virtual assistants, and automated customer services. A deep learning-based hierarchical approach is proposed for both unimodal and multimodal SER systems in this work. Of these, the audio-based unimodal system proposes using a combination of 33 features, which include prosody, spectral, and voice quality-based audio features. Further, for the multimodal system, both the above-mentioned audio features and additional textual features are used. Embeddings from Language Models v2 (ELMo v2) is implemented to extract word and character embeddings which helped to capture the context-dependent aspects of emotion in text. The proposed models’ performances are evaluated on two audio-only unimodal datasets – SAVEE and RAVDESS, and one audio-text multimodal dataset – IEMOCAP. The proposed hierarchical models offered SER accuracies of 81.2%, 81.7%, and 74.5% on the RAVDESS, SAVEE, and IEMOCAP datasets, respectively. Further, these results are also benchmarked against recently reported techniques, and the reported performances are found to be superior. Therefore, based on the presented investigations, it is concluded that the application of a deep learning-based network in a hierarchical manner significantly improves SER over generic unimodal and multimodal systems.}
}
@article{DONG2021246,
title = {Affect-salient event sequence modelling for continuous speech emotion recognition},
journal = {Neurocomputing},
volume = {458},
pages = {246-258},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.06.036},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221009401},
author = {Yizhuo Dong and Xinyu Yang},
keywords = {Continuous speech emotion recognition, Affect-salient events, Affect responses, Connectionist temporal classification, Event probability vector decoding},
abstract = {Continuous speech emotion recognition, which faces the problems of delay caused by annotators’ reaction time and noise caused by non-emotional segments, is a challenging subject in the field of affective computing. To solve these problems, we propose a new affect-salient event sequence modelling (ASESM) method based on connectionist temporal classification (CTC). This method treats a sentence’s label as a chain of affect-salient event (ASE) and non-affect-salient event Null states rather than continuous emotional value. With this representation, a CTC-based convolutional neural network (CNN) is built to automatically label the sentence’s emotional segments with ASE and non-emotional segments with Null, so as to reduce the impact of noise caused by non-emotional segments. Furthermore, we propose an event probability vector decoding (EPVD) algorithm to search the optimal ASE sequence from the CTC loss matrix and mark the occurrence time of each event within this sequence. Then, the arousal and valence ground-truth annotations of each ASE are used to represent the continuous emotional value of a segment which is predicted as the ASE. Since the ground-truth annotations of each ASE have contained different time-delays, taking events as the target can avoid the additional reaction delay compensation. We test our method on the RECOLA and AVEC 2014 benchmark databases. The experimental results demonstrate that the proposed event-based method can improve the performance of continuous emotion recognition and the improvement is more obvious when the selected ASE has high annotation consistency.}
}
@article{RASTOGI2023,
title = {Emotion Detection via Voice and Speech Recognition},
journal = {International Journal of Cyber Behavior, Psychology and Learning},
volume = {13},
number = {1},
year = {2023},
issn = {2155-7136},
doi = {https://doi.org/10.4018/IJCBPL.333473},
url = {https://www.sciencedirect.com/science/article/pii/S2155713623000176},
author = {Rohit Rastogi and Tushar Anand and Shubham Kumar Sharma and Sarthak Panwar},
keywords = {Deep Learning, Emotion Recognition, Librosa, MFCC (Mel-Frequency Cepstral Coefficients), MLP Classifier, Numpy, RAVDESS Dataset, SKlearn, Speech Features},
abstract = {ABSTRACT
Emotion detection from voice signals is needed for human-computer interaction (HCI), which is a difficult challenge. In the literature on speech emotion recognition, various well known speech analysis and classification methods have been used to extract emotions from signals. Deep learning strategies have recently been proposed as a workable alternative to conventional methods and discussed. Several recent studies have employed these methods to identify speech-based emotions. The review examines the databases used, the emotions collected, and the contributions to speech emotion recognition. The Speech Emotion Recognition Project was created by the research team. It recognizes human speech emotions. The research team developed the project using Python 3.6. RAVDEESS dataset was also used since it contained eight distinct emotions expressed by all speakers. The RAVDESS dataset, Python programming languages, and Pycharm as an IDE were all used by the author team.}
}
@article{HAN202389,
title = {A survey of transformer-based multimodal pre-trained modals},
journal = {Neurocomputing},
volume = {515},
pages = {89-106},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.09.136},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222012346},
author = {Xue Han and Yi-Tong Wang and Jun-Lan Feng and Chao Deng and Zhan-Heng Chen and Yu-An Huang and Hui Su and Lun Hu and Peng-Wei Hu},
keywords = {Transformer, Pre-trained model, Multimodal, Documet Layout},
abstract = {With the broad industrialization of Artificial Intelligence(AI), we observe a large fraction of real-world AI applications are multimodal in nature in terms of relevant data and ways of interaction. Pre-trained big models have been proven as the most effective framework for joint modeling of multi-modality data. This paper provides a thorough account of the opportunities and challenges of Transformer-based multimodal pre-trained model (PTM) in various domains. We begin by reviewing the representative tasks of multimodal AI applications, ranging from vision-text and audio-text fusion to more complex tasks such as document layout understanding. We particularly address the new multi-modal research domain of document layout understanding. We further analyze and compare the state-of-the-art Transformer-based multimodal PTMs from multiple aspects, including downstream applications, datasets, input feature embedding, and model architectures. In conclusion, we summarize the key challenges of this field and suggest several future research directions.}
}
@article{WOLK202262,
title = {Survey on dialogue systems including slavic languages},
journal = {Neurocomputing},
volume = {477},
pages = {62-84},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.11.076},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221017549},
author = {Krzysztof Wołk and Agnieszka Wołk and Dominika Wnuk and Tomasz Grześ and Ida Skubis},
keywords = {Slavic languages, Task-oriented dialogue systems, Non-task-oriented dialogue systems, Chatbots, Machine learning, Artificial intelligence},
abstract = {Slavic languages pose a challenge to the researchers in the domain of dialogue technology. A relatively free word order with a large degree of inflection, such as conjugation of verbs, and declension of adjectives, pronouns, and nouns are exhibited by the Slavic languages, which has a significant impact on the size of lexical inventories that significantly complicate the design of dialogue systems. This article conducts an empirical study on the state-of-the-art dialogue systems within Slavic languages. Moreover, we review the existing models in recent dialogue systems, pinpoint the current main challenges and identify potential research directions of practical and intelligent systems within low-resourced languages.}
}
@article{SUPRIYONO2025112079,
title = {Dataset of Transcribed Indonesian Stand-Up Comedy Videos with Audience Laughter Annotations from Kompas TV’s YouTube Channel},
journal = {Data in Brief},
pages = {112079},
year = {2025},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2025.112079},
url = {https://www.sciencedirect.com/science/article/pii/S2352340925008017},
author = { Supriyono and Aji Prasetya Wibawa and  Suyono and Fachrul Kurniawan and Andri Pranolo and Nahdi Saubari and Kunfeng Wang},
keywords = {Indonesian stand-up comedy, Laughter annotation dataset, YouTube transcription corpus, Humor detection NLP, Spoken language resources},
abstract = {This dataset presents a large-scale compilation of Indonesian stand-up comedy video transcripts collected from Kompas TV’s official YouTube channel. A total of 3,934 videos were processed, capturing over 2.8 million words, 6,124 sentences, and 17,394 annotated audience laughter events. Each entry includes the video title, URL, the number of laughter instances, the original transcript, and a cleaned version suitable for downstream natural language processing (NLP) tasks. Data collection employed Python-based web scraping, followed by pre-processing routines such as timestamp and tag removal, whitespace normalization, and character cleaning. The dataset supports research in humor detection, speech emotion recognition, and cultural studies of performative discourse in Indonesian. It is particularly valuable for low-resource language NLP development and training models on informal spoken content. Researchers may utilize the dataset for sentiment analysis, summarization, laughter prediction, and sociolinguistic exploration. This openly accessible resource is hosted on Mendeley Data and adheres to ethical standards, with no personal identifiers and full compliance with platform redistribution policies. The dataset fills a notable gap in Indonesian language corpora, particularly in the entertainment and humor domain, providing a foundation for both academic and applied research in computational linguistics and human-cantered AI.}
}
@article{TODD2025104322,
title = {A multimodal sentiment classifier for financial decision making},
journal = {International Review of Financial Analysis},
volume = {105},
pages = {104322},
year = {2025},
issn = {1057-5219},
doi = {https://doi.org/10.1016/j.irfa.2025.104322},
url = {https://www.sciencedirect.com/science/article/pii/S1057521925004090},
author = {Andrew Todd and James Bowden and Mark Cummins and Yang Su},
keywords = {Sentiment analysis, Multimodal analysis, Transformer model, Deep learning, Earnings call},
abstract = {This study pioneers a multimodal approach to financial sentiment analysis through the integration of audio and textual data to enhance predictive accuracy. Motivated by the underutilisation of paralinguistic features and deep learning techniques in financial sentiment analysis, we introduce a novel deep learning-enabled multimodal classifier trained on corporate earnings calls using a subset of S&P 100 constituents. Our methodology incorporates FinBERT, a financial variant of Bidirectional Encoder Representation Transformations (BERT), alongside paralinguistic features and a deep learning classifier. Comparative analysis against established sentiment analysis methods, including dictionary approaches and machine learning models, suggests that our multimodal classifier achieves improved out-of-sample accuracy. Specifically, the inclusion of paralinguistic characteristics improves sentiment detection accuracy. Our research provides nuanced insights into sentiment analysis detection of different speakers (managers and analysts) during both the management discussion and Q&A sections of corporate earnings calls. Combined, our results suggest that multimodal sentiment analysis classification possesses the ability to deepen our understanding of the interplay between sentiment and market characteristics.}
}
@article{WANG2024127181,
title = {Multimodal transformer with adaptive modality weighting for multimodal sentiment analysis},
journal = {Neurocomputing},
volume = {572},
pages = {127181},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.127181},
url = {https://www.sciencedirect.com/science/article/pii/S0925231223013048},
author = {Yifeng Wang and Jiahao He and Di Wang and Quan Wang and Bo Wan and Xuemei Luo},
keywords = {Multimodal sentiment analysis, Transformer, Multimodal fusion},
abstract = {Multimodal Sentiment Analysis (MSA) constitutes a pivotal technology in the realm of multimedia research. The efficacy of MSA models largely hinges on the quality of multimodal fusion. Notably, when conveying information pertinent to specific tasks or applications, not all modalities hold equal importance. Previous research, however, has either disregarded the importance of modalities altogether or solely focused on the importance of linguistic and non-linguistic modalities while neglecting the importance between non-linguistic modalities. To facilitate effective multimodal information fusion based on the relative importance of modalities, a novel multimodal fusion mode named Multimodal Transformer with Adaptive Modality Weighting (MTAMW) is proposed in this paper. Specifically, we introduce a multimodal adaptive weight matrix that allocates appropriate weights to each modality based on its contribution to sentiment analysis. Furthermore, a multimodal attention mechanism is introduced, utilizing multiple Softmax functions to compute attention weights, thereby efficiently fusion multimodal information via a single-stream Transformer. By meticulously considering the relative importance of each modality during the fusion process, more effective multimodal information fusion is achievable. Extensive experiments on benchmark datasets show that it is superior to or comparable to state-of-the-art methods on MSA tasks. The codes for our experiments are available at https://github.com/Vamos66/MTAMW.}
}
@article{CHEN2025126855,
title = {MTLSER: Multi-task learning enhanced speech emotion recognition with pre-trained acoustic model},
journal = {Expert Systems with Applications},
volume = {273},
pages = {126855},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.126855},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425004774},
author = {Zengzhao Chen and Chuan Liu and Zhifeng Wang and Chuanxu Zhao and Mengting Lin and Qiuyu Zheng},
keywords = {Multi-task learning, Speech emotion recognition, Speaker identification, Automatic speech recognition, Speech representation learning},
abstract = {This study proposes a novel Speech Emotion Recognition (SER) approach employing a Multi-Task Learning framework (MTLSER), designed to boost recognition accuracy by training multiple related tasks simultaneously and sharing information via a joint loss function. This framework integrates SER as the primary task, with Automatic Speech Recognition (ASR) and speaker identification serving as auxiliary tasks. Feature extraction is conducted using the pre-trained wav2vec2.0 model, which acts as a shared layer within our multi-task learning (MTL) framework. Extracted features are then processed in parallel by the three tasks. The contributions of auxiliary tasks are adjusted through hyperparameters, and their loss functions are amalgamated into a singular joint loss function for effective backpropagation. This optimization refines the model’s internal parameters. Our method’s efficacy is tested during the inference stage, where the model concurrently outputs the emotion, textual content, and speaker identity from the input audio. We conducted ablation studies and a sensitivity analysis on the hyperparameters to determine the optimal settings for emotion recognition. The performance of our proposed MTLSER model is evaluated using the public IEMOCAP dataset. Results from extensive testing show a significant improvement over traditional methods, achieving a Weighted Accuracy (WA) of 82.63% and an Unweighted Accuracy (UA) of 82.19%. These findings affirm the effectiveness and robustness of our approach. Our code is publicly available at https://github.com/CCNU-nercel-lc/MTL-SER.}
}
@article{BRADLEY2024115893,
title = {Quantifying abnormal emotion processing: A novel computational assessment method and application in schizophrenia},
journal = {Psychiatry Research},
volume = {336},
pages = {115893},
year = {2024},
issn = {0165-1781},
doi = {https://doi.org/10.1016/j.psychres.2024.115893},
url = {https://www.sciencedirect.com/science/article/pii/S0165178124001781},
author = {Ellen R. Bradley and Jake Portanova and Josh D. Woolley and Benjamin Buck and Ian S. Painter and Michael Hankin and Weizhe Xu and Trevor Cohen},
keywords = {Computational linguistics, Speech, Language, Emotion processing, Oxytocin, Schizophrenia spectrum disorders},
abstract = {Abnormal emotion processing is a core feature of schizophrenia spectrum disorders (SSDs) that encompasses multiple operations. While deficits in some areas have been well-characterized, we understand less about abnormalities in the emotion processing that happens through language, which is highly relevant for social life. Here, we introduce a novel method using deep learning to estimate emotion processing rapidly from spoken language, testing this approach in male-identified patients with SSDs (n = 37) and healthy controls (n = 51). Using free responses to evocative stimuli, we derived a measure of appropriateness, or “emotional alignment” (EA). We examined psychometric characteristics of EA and its sensitivity to a single-dose challenge of oxytocin, a neuropeptide shown to enhance the salience of socioemotional information in SSDs. Patients showed impaired EA relative to controls, and impairment correlated with poorer social cognitive skill and more severe motivation and pleasure deficits. Adding EA to a logistic regression model with language-based measures of formal thought disorder (FTD) improved classification of patients versus controls. Lastly, oxytocin administration improved EA but not FTD among patients. While additional validation work is needed, these initial results suggest that an automated assay using spoken language may be a promising approach to assess emotion processing in SSDs.}
}
@article{CHEN2021104277,
title = {A novel dual attention-based BLSTM with hybrid features in speech emotion recognition},
journal = {Engineering Applications of Artificial Intelligence},
volume = {102},
pages = {104277},
year = {2021},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2021.104277},
url = {https://www.sciencedirect.com/science/article/pii/S095219762100124X},
author = {Qiupu Chen and Guimin Huang},
keywords = {Speech emotion recognition, Deep learning, Hybrid features, Dual-level model},
abstract = {Though the emotional state does not alter the content of language, it is a major determinant in human communication, because it provides much more positive feedback. The purpose of the speech emotion recognition is to automatically identify emotional or physiological state of a human being from their voice. In this paper, we propose a novel dual-level architecture, called dual attention-based bidirectional long short-term memory networks (dual attention-BLSTM) to recognize speech emotion. We also confirm that the recognition performance is better with different features as input than with only identical features in the dual-layer structure. Experiments on the IEMOCAP databases show the advantage of our proposed approach. The average recognition accuracy of our method is 70.29% in unweighted accuracy (UA) and the corresponding performance improvements are 2.89 compared to the best baseline methods. The results show that the architecture of our designed can better learn to distinguish features of the emotional information.}
}
@article{ABDUSALOMOV20232915,
title = {Improved Speech Emotion Recognition Focusing on High-Level Data Representations and Swift Feature Extraction Calculation},
journal = {Computers, Materials and Continua},
volume = {77},
number = {3},
pages = {2915-2933},
year = {2023},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2023.044466},
url = {https://www.sciencedirect.com/science/article/pii/S1546221823007336},
author = {Akmalbek Abdusalomov and Alpamis Kutlimuratov and Rashid Nasimov and Taeg Keun Whangbo},
keywords = {Feature extraction, MFCC, ResNet, speech emotion recognition},
abstract = {The performance of a speech emotion recognition (SER) system is heavily influenced by the efficacy of its feature extraction techniques. The study was designed to advance the field of SER by optimizing feature extraction techniques, specifically through the incorporation of high-resolution Mel-spectrograms and the expedited calculation of Mel Frequency Cepstral Coefficients (MFCC). This initiative aimed to refine the system’s accuracy by identifying and mitigating the shortcomings commonly found in current approaches. Ultimately, the primary objective was to elevate both the intricacy and effectiveness of our SER model, with a focus on augmenting its proficiency in the accurate identification of emotions in spoken language. The research employed a dual-strategy approach for feature extraction. Firstly, a rapid computation technique for MFCC was implemented and integrated with a Bi-LSTM layer to optimize the encoding of MFCC features. Secondly, a pretrained ResNet model was utilized in conjunction with feature Stats pooling and dense layers for the effective encoding of Mel-spectrogram attributes. These two sets of features underwent separate processing before being combined in a Convolutional Neural Network (CNN) outfitted with a dense layer, with the aim of enhancing their representational richness. The model was rigorously evaluated using two prominent databases: CMU-MOSEI and RAVDESS. Notable findings include an accuracy rate of 93.2% on the CMU-MOSEI database and 95.3% on the RAVDESS database. Such exceptional performance underscores the efficacy of this innovative approach, which not only meets but also exceeds the accuracy benchmarks established by traditional models in the field of speech emotion recognition.}
}
@article{ZHOU2025113665,
title = {SAGE-Net: Single-layer augmented gated encoder network for efficient multimodal sentiment analysis},
journal = {Applied Soft Computing},
volume = {184},
pages = {113665},
year = {2025},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2025.113665},
url = {https://www.sciencedirect.com/science/article/pii/S1568494625009767},
author = {Jiazheng Zhou and Xin Kang and Weiping Ding and Linhuang Wang and Fei Ding and Kazuyuki Matsumoto and Chenmeng Zhang and Huiwen Chi},
keywords = {Multimodal sentiment analysis, Lightweight model, Cross-attention mechanism, Data augmentation strategies},
abstract = {Multimodal sentiment analysis has achieved remarkable progress. However, the increasing computational complexity of existing models poses significant challenges in resource-constrained scenarios. To address these challenges, this study introduces a single-layer augmented gated encoder network (SAGE-Net), a novel lightweight multimodal sentiment analysis architecture. In contrast to conventional multilayer, deeply stacked architectures, SAGE-Net employs only a single-layer encoder to preserve multimodal feature comprehension, significantly reducing computational complexity. To enable effective inter-modal interaction, a single-layer cross-attention mechanism is integrated. We extensively evaluate multiple feature fusion strategies and data augmentation strategies to enhance model effectiveness. Experimental results on the CMU-MOSI and CMU-MOSEI, and CH-SIMS datasets demonstrate that SAGE-Net achieves competitive performance and significantly lowers model size and tuning costs. These results validate SAGE-Net as a viable lightweight solution for multimodal sentiment analysis in resource-constrained scenarios.}
}
@article{GANDHI2023424,
title = {Multimodal sentiment analysis: A systematic review of history, datasets, multimodal fusion methods, applications, challenges and future directions},
journal = {Information Fusion},
volume = {91},
pages = {424-444},
year = {2023},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2022.09.025},
url = {https://www.sciencedirect.com/science/article/pii/S1566253522001634},
author = {Ankita Gandhi and Kinjal Adhvaryu and Soujanya Poria and Erik Cambria and Amir Hussain},
keywords = {Affective computing, Sentiment analysis, Multimodal fusion, Fusion techniques},
abstract = {Sentiment analysis (SA) has gained much traction In the field of artificial intelligence (AI) and natural language processing (NLP). There is growing demand to automate analysis of user sentiment towards products or services. Opinions are increasingly being shared online in the form of videos rather than text alone. This has led to SA using multiple modalities, termed Multimodal Sentiment Analysis (MSA), becoming an important research area. MSA utilises latest advancements in machine learning and deep learning at various stages including for multimodal feature extraction and fusion and sentiment polarity detection, with aims to minimize error rate and improve performance. This survey paper examines primary taxonomy and newly released multimodal fusion architectures. Recent developments in MSA architectures are divided into ten categories, namely early fusion, late fusion, hybrid fusion, model-level fusion, tensor fusion, hierarchical fusion, bi-modal fusion, attention-based fusion, quantum-based fusion and word-level fusion. A comparison of several architectural evolutions in terms of MSA fusion categories and their relative strengths and limitations are presented. Finally, a number of interdisciplinary applications and future research directions are proposed.}
}
@article{ZHAO2025,
title = {Design of an Enterprise Public Opinion Monitoring System Based on Natural Language Processing:},
journal = {Journal of Global Information Management},
volume = {33},
number = {1},
year = {2025},
issn = {1062-7375},
doi = {https://doi.org/10.4018/JGIM.381306},
url = {https://www.sciencedirect.com/science/article/pii/S1062737525000599},
author = {Yue Zhao and Jin Zhang and Yixu Tong and Zheng Li and Xue Yu and Sangbing Tsai},
keywords = {Multimodal Sentiment Detection, Public Opinion Monitoring, Sarcasm Detection, Sentiment Analysis, User Profiling},
abstract = {ABSTRACT
The widespread influence of social media and online discourse has made enterprise public opinion monitoring essential, yet existing sentiment analysis models struggle with sarcasm detection, multimodal sentiment integration, and high-risk user identification. Additionally, traditional text-based models face cross-domain generalization issues, limiting their effectiveness in real-world sentiment trend forecasting. To address these challenges, we propose HDLU-POMS (Hybrid Deep Learning and User Profiling-Enhanced Public Opinion Monitoring System), a novel framework that integrates Convolutional Neural Network-Long Short-Term Memory-Attention (CNN-LSTM-Attention) sentiment modeling, multimodal sarcasm detection, user profiling, and feature fusion. By incorporating audio-visual cues for sarcasm interpretation, behavioral metadata for user profiling, and feature fusion techniques for adaptability, HDLU-POMS enhances sentiment classification and trend forecasting.}
}
@article{LATIF2025100768,
title = {Transformers in speech processing: Overcoming challenges and paving the future},
journal = {Computer Science Review},
volume = {58},
pages = {100768},
year = {2025},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2025.100768},
url = {https://www.sciencedirect.com/science/article/pii/S1574013725000449},
author = {Siddique Latif and Syed Aun Muhammad Zaidi and Heriberto Cuayáhuitl and Fahad Shamshad and Moazzam Shoukat and Muhammad Usama and Junaid Qadir},
keywords = {Transformer, Speech processing, Automatic speech recognition, Deep learning},
abstract = {The remarkable success of transformers in the field of natural language processing has sparked interest in their potential for mod- elling long-range dependencies within speech sequences. Transformers have gained prominence across various speech-related do- mains, including automatic speech recognition, speech synthesis, speech translation, speech para-linguistics, speech enhancement, spoken dialogue systems, and numerous multimodal applications. However, the integration of transformers in speech processing comes with significant challenges such as managing the high computational costs, handling the complexity of speech variability, and addressing the data scarcity for certain speech tasks. In this paper, we present a comprehensive survey that aims to bridge research studies from diverse subfields within speech technology. By consolidating findings from across the speech technology landscape, we provide a valuable resource for researchers interested in harnessing the power of transformers to advance the field. We identify the challenges encountered by transformers in speech processing while also offering insights into potential solutions to address these issues.}
}
@article{KHALED2025103682,
title = {UniTextFusion: A low-resource framework for Arabic multimodal sentiment analysis using early fusion and LoRA-tuned language models},
journal = {Ain Shams Engineering Journal},
volume = {16},
number = {11},
pages = {103682},
year = {2025},
issn = {2090-4479},
doi = {https://doi.org/10.1016/j.asej.2025.103682},
url = {https://www.sciencedirect.com/science/article/pii/S209044792500423X},
author = {Salma Khaled and Walaa Medhat and Ensaf Hussein Mohamed},
keywords = {Arabic sentiment analysis, Multimodal, Early fusion, LLMs, LLama, SILMA, LoRA},
abstract = {Multimodal Sentiment Analysis (MuSA) seeks to interpret human emotions by combining textual, auditory, and visual cues. While this field has advanced significantly in English, Arabic MuSA remains underdeveloped due to limited large language models (LLMs), scarce annotated datasets, dialectal variation, and the complexity of fusing multiple modalities. Cultural elements such as sarcasm and emotional nuance are particularly difficult to capture without multimodal context. An early fusion approach, UniTextFusion, is introduced as a means of overcoming these challenges. This fusion strategy transforms audio and visual inputs into descriptive text, allowing seamless integration with Arabic-compatible LLMs. We apply parameter-efficient Low-Rank Adaption (LoRA) fine-tuning to two generative models—LLaMA 3.1-8B Instruct and SILMA AI 9B. Experiments on our Arabic MuSA dataset show that UniTextFusion improves sentiment classification performance by up to 34% in F1-score over strong unimodal and multimodal baselines, reaching 68% with LLaMA and 71% with SILMA. These results validate our hypothesis that modality textualization combined with lightweight fine-tuning is effective for Arabic MuSA and offers a scalable solution for sentiment analysis in low-resource settings.}
}
@article{XI2025127020,
title = {Multimodal sarcasm detection based on sentiment-clue inconsistency global detection fusion network},
journal = {Expert Systems with Applications},
volume = {275},
pages = {127020},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127020},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425006426},
author = {Zhonghao Xi and Bengong Yu and Haoyu Wang},
keywords = {Sentiment inconsistency, Dynamic global detection, Sarcasm detection, Information fusion, Sentiment analysis},
abstract = {Sarcasm, characterized by its intense emotional contrasts, often conveys meanings that differ significantly from their literal interpretations. This complexity is particularly evident in online communication, where textual and visual cues intermingle, resulting in the proliferation of sarcastic content across social media platforms. Recognizing the challenges and limitations of existing approaches, which primarily focus on local inconsistencies within multimodal data, this study introduces a novel framework for multimodal sarcasm detection. We propose a dynamic global detection network designed to address the sentiment-clue inconsistency present in image–text data. This network consists of two key components: a dynamic global detection module and a sentiment-clue inconsistency module. The former adopts a global perspective to select optimal image–text pairs, utilizing dynamic routing probability matrices and image–text weight ratios to adaptively capture their differences. The latter delves deeper into semantic layers, providing word-level interpretability and constructing a sentiment-dependency network to identify sentiment inconsistencies between textual and visual data. Additionally, we integrate image captioning techniques to enhance the emotional and semantic connections between images and texts. Comparative analysis on open-access datasets demonstrates the superior effectiveness of our model compared to existing methods, addressing the need for more sophisticated tools in sarcasm detection within digital communication.}
}
@article{HAZMOUNE2024108339,
title = {Using transformers for multimodal emotion recognition: Taxonomies and state of the art review},
journal = {Engineering Applications of Artificial Intelligence},
volume = {133},
pages = {108339},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.108339},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624004974},
author = {Samira Hazmoune and Fateh Bougamouza},
keywords = {Transformers, Multimodal emotion recognition, Modality fusion, Cross-attention, Survey, Taxonomy},
abstract = {Emotion recognition is an aspect of human-computer interaction, affective computing, and social robotics. Conventional unimodal approaches for emotion recognition, depending on single data sources such as facial expressions or speech signals often fall short in capturing the complexity and context-dependent nature of emotions. Multimodal Emotion Recognition (MER), which integrates information from multiple modalities, has emerged as a promising solution to overcome these limitations. In recent years, Transformers-based approaches have gathered significant attention in the fields of natural language processing and computer vision, highlighting their ability to capture long-range dependencies and semantic representations. These models have rapidly achieved the MER state-of-the-art. However, current survey papers that cover MER lack a specific focus on Transformer-based techniques. To bridge this research gap, this review paper provides a comprehensive investigation of Transformers-based approaches for MER. It explores various Transformer architectures and proposes several scenarios for using Transformers at different stages of MER process. In addition, it examines datasets suitable for MER, discusses fusion mechanisms, and introduces novel taxonomies in both MER and Transformer technologies. The review also addresses challenges and future research directions. Through this review, we aim to provide researchers with an inclusive understanding of the current state-of-the-art in Transformers-based approaches for MER, paving the way for further advancements in this rapidly developing field.}
}
@article{OZER2021102502,
title = {Pseudo-colored rate map representation for speech emotion recognition},
journal = {Biomedical Signal Processing and Control},
volume = {66},
pages = {102502},
year = {2021},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2021.102502},
url = {https://www.sciencedirect.com/science/article/pii/S1746809421000999},
author = {Ilyas OZER},
keywords = {Speech emotion recognition, Rate map, Convolutional neural network, Pseudo-coloration},
abstract = {Speech emotion recognition (SER) is an exciting topic in the field of human-machine interaction. Several handcrafted features are used for SER. However, determining these features is both a difficult and time-consuming process. Instead, the use of features generated by convolutional neural networks (CNNs) with spectrograms and Mel-spectrograms has gained momentum in recent years. These CNNs are widely employed in image applications. Therefore, the audio signals must be represented in the best way as images. The spectrogram presents evenly spaced frequency components. However, spectral energy when mostly at low frequencies is not desirable. The Mel-filter provides benefits, but several studies have shown that its performance is inferior to biologically inspired models. In addition, the high variance between features negatively affects its classification performance. In this study, log-power rate map features are suggested as an auditory model for the SER task. In addition, we have proposed the use of a threshold function to focus on regions with high spectral energy. A rate map provides better resolution in the low-frequency region. In addition, smoothing reduces the variance between features, and focusing on spectral peaks reduces the effect of user-dependent features. The proposed approach was tested, independent of subject and gender, on the EMO-DB and EMOVO datasets, which are widely used in the literature. In the EMO-DB dataset, an increase of 2.42 % was achieved, with a classification performance of 91.32 %. In the EMOVO dataset, an increase of 4.95 % was achieved, with a classification performance of 68.93 %.}
}
@article{WANG2025127631,
title = {MGC: A modal mapping coupling and gate-driven contrastive learning approach for multimodal intent recognition},
journal = {Expert Systems with Applications},
volume = {281},
pages = {127631},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127631},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425012539},
author = {Mengsheng Wang and Lun Xie and Chiqin Li and Xinheng Wang and Minglong Sun and Ziyang Liu},
keywords = {Multimodal intent recognition, Multimodal fusion, Attention mechanism, Gated neural network, Contrastive learning},
abstract = {Multimodal intent recognition seeks to comprehensively analyze and interpret a speaker’s viewpoint, attitude, and emotional inclination by integrating information from diverse modalities. However, conventional methods often fail to adequately address the issue of modality frequency inconsistency arising from data desynchronization during the feature fusion process, which subsequently hampers the accuracy of the recognition task. To mitigate this challenge, the present study proposes a novel multimodal intent recognition framework (MGC) grounded in attention mechanisms. Initially, the framework employs a modality mapping coupling module to achieve dynamic alignment of multimodal data, thereby alleviating modality inconsistencies. Subsequently, a gated feature fusion module, enhanced by an adaptive mechanism, dynamically adjusts the fusion weights of each modality’s features. Finally, a contrastive learning module strengthens the semantic similarity between textual and hybrid features, thereby augmenting the model’s discriminative capability. Experimental results demonstrate that, when compared to existing baseline models, the proposed method yields substantial performance improvements across two widely recognized benchmark datasets, validating its effectiveness and superiority in addressing the multimodal intent recognition task.}
}
@article{LIU2024,
title = {Semantic-Driven Crossmodal Fusion for Multimodal Sentiment Analysis},
journal = {International Journal on Semantic Web and Information Systems},
volume = {20},
number = {1},
year = {2024},
issn = {1552-6283},
doi = {https://doi.org/10.4018/IJSWIS.359985},
url = {https://www.sciencedirect.com/science/article/pii/S1552628324000905},
author = {Pingshan Liu and Zhaoyang Wang and Fu Huang},
keywords = {Text-Driven, Crossmodal Attention, Feature Fusion, Mutual Information, Multimodal Sentiment Analysis},
abstract = {ABSTRACT
In multimodal sentiment analysis (MSA), the fusion strategies of multimodal features significantly influence the performance of MSA models. Previous works frequently face challenges in integrating heterogeneous data without fully leveraging the rich semantic content of text, resulting in poor information association. This paper propose an MSA model based on Text-Driven Crossmodal Fusion and Mutual Information Estimation, called TeD-MI, which comprises a Stacked Text-Driven Crossmodal Fusion (STDC) module, which efficiently fusions the three modalities driven by the text modality to optimize fusion feature representation and enhance semantic understanding. Furthermore, TeD-MI designed a mutual information estimation module to achieve the best balance between preserving task-related information and filtering out irrelevant noise information as much as possible. Comprehensive experiments conducted on the CMU-MOSI and CMU-MOSEI datasets demonstrate our proposed model achieves varying degrees of improvement across most evaluation metrics.}
}
@article{CHAVESVILLOTA2026101873,
title = {Deep feature representations and fusion strategies for speech emotion recognition from acoustic and linguistic modalities: A systematic review},
journal = {Computer Speech & Language},
volume = {96},
pages = {101873},
year = {2026},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2025.101873},
url = {https://www.sciencedirect.com/science/article/pii/S0885230825000981},
author = {Andrea Chaves-Villota and Ana Jimenez-Martín and Mario Jojoa-Acosta and Alfonso Bahillo and Juan Jesús García-Domínguez},
keywords = {Emotion recognition, Speech, Linguistic, Acoustic, Fusion, Deep learning, Machine learning, Low and high-level features},
abstract = {Emotion Recognition (ER) has gained significant attention due to its importance in advanced human-machine interaction and its widespread real-world applications. In recent years, research on ER systems has focused on multiple key aspects, including the development of high-quality emotional databases, the selection of robust feature representations, and the implementation of advanced classifiers leveraging AI-based techniques. Despite this progress in research, ER still faces significant challenges and gaps that must be addressed to develop accurate and reliable systems. To systematically assess these critical aspects, particularly those centered on AI-based techniques, we employed the PRISMA methodology. Thus, we include journal and conference papers that provide essential insights into key parameters required for dataset development, involving emotion modeling (categorical or dimensional), the type of speech data (natural, acted, or elicited), the most common modalities integrated with acoustic and linguistic data from speech and the technologies used. Similarly, following this methodology, we identified the key representative features that serve as critical emotional information sources in both modalities. For acoustic, this included those extracted from the time and frequency domains, while for linguistic, earlier embeddings and the most common transformer models were considered. In addition, Deep Learning (DL) and attention-based methods were analyzed for both. Given the importance of effectively combining these diverse features for improving ER, we then explore fusion techniques based on the level of abstraction. Specifically, we focus on traditional approaches, including feature-, decision-, DL-, and attention-based fusion methods. Next, we provide a comparative analysis to assess the performance of the approaches included in our study. Our findings indicate that for the most commonly used datasets in the literature: IEMOCAP and MELD, the integration of acoustic and linguistic features reached a weighted accuracy (WA) of 85.71% and 63.80%, respectively. Finally, we discuss the main challenges and propose future guidelines that could enhance the performance of ER systems using acoustic and linguistic features from speech.}
}
@article{ULGENSONMEZ2024200351,
title = {In-depth investigation of speech emotion recognition studies from past to present –The importance of emotion recognition from speech signal for AI–},
journal = {Intelligent Systems with Applications},
volume = {22},
pages = {200351},
year = {2024},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2024.200351},
url = {https://www.sciencedirect.com/science/article/pii/S2667305324000279},
author = {Yeşim {ÜLGEN SÖNMEZ} and Asaf VAROL},
keywords = {Artificial intelligence, Machine-learning methods, Speech emotion recognition},
abstract = {In the super smart society (Society 5.0), new and rapid methods are needed for speech recognition, emotion recognition, and speech emotion recognition areas to maximize human-machine or human-computer interaction and collaboration. Speech signal contains much information about the speaker, such as age, sex, ethnicity, health condition, emotion, and thoughts. The field of study which analyzes the mood of the person from the speech is called speech emotion recognition (SER). Classifying the emotions from the speech data is a complicated problem for artificial intelligence, and its sub-discipline, machine learning. Because it is hard to analyze the speech signal which contains various frequencies and characteristics. Speech data are digitized with signal processing methods and speech features are obtained. These features vary depending on the emotions such as sadness, fear, anger, happiness, boredom, confusion, etc. Even though different methods have been developed for determining the audio properties and emotion recognition, the success rate varies depending on the languages, cultures, emotions, and data sets. In speech emotion recognition, there is a need for new methods which can be applied in data sets with different sizes, which will increase classification success, in which best properties can be obtained, and which are affordable. The success rates are affected by many factors such as the methods used, lack of speech emotion datasets, the homogeneity of the database, the difficulty of the language (linguistic differences), the noise in audio data and the length of the audio data. Within the scope of this study, studies on emotion recognition from speech signals from past to present have been analyzed in detail. In this study, classification studies based on a discrete emotion model using speech data belonging to the Berlin emotional database (EMO-DB), Italian emotional speech database (EMOVO), The Surrey audio-visual expressed emotion database (SAVEE), Ryerson Audio-Visual Database of Emotional Speech and Song Database (RAVDESS), which are mostly independent of the speaker and content, are examined. The results of both classical classifiers and deep learning methods are compared. Deep learning results are more successful, but classical classification is more important in determining the defining features of speech, song or voice. So It develops feature extraction stage. This study will be able to contribute to the literature and help the researchers in the SER field.}
}