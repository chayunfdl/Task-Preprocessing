@article{YANG2025103904,
title = {Automatically learning linguistic structures for entity relation extraction},
journal = {Information Processing & Management},
volume = {62},
number = {1},
pages = {103904},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103904},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324002632},
author = {Weizhe Yang and Yanping Chen and Jinling Xu and Yongbin Qin and Ping Chen},
keywords = {Relation extraction, Feature combination, Linguistic structures, Semantic dependencies},
abstract = {A sentence is composed of linguistically linked units, such as words or phrases. The dependencies between them compose the linguistic structures of a sentence, which indicates the meanings of linguistic units and encodes the syntactic or semantic relationships between them. Therefore, it is important to learn the linguistic structures of a sentence for entity relation extraction or other natural language processing (NLP) tasks. In related works, manual rules or dependency trees are usually adopted to capture the linguistic structures. These methods heavily depend on prior knowledge or external toolkits. In this paper, we introduce a Supervised Graph Autoencoder Network (SGAN) model to automatically learn the linguistic structures of a sentence. Unlike traditional graph neural networks that use a fixed adjacency matrix initialized with prior knowledge, the SGAN model contains a learnable adjacency matrix that is dynamically tuned by a task-relevant learning objective. It can automatically learn linguistic structures from raw input sentences. After being evaluated on seven public datasets, the SGAN achieves state-of-the-art (SOTA) performance, outperforming all compared models. The results show that automatically learned linguistic structures have better performance than manually designed linguistic patterns. It exhibits great potential for supporting entity relation extraction and other NLP tasks.}
}
@article{SUN2024123092,
title = {DSAMR: Dual-Stream Attention Multi-hop Reasoning for knowledge-based visual question answering},
journal = {Expert Systems with Applications},
volume = {245},
pages = {123092},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.123092},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423035947},
author = {Yanhan Sun and Zhenfang Zhu and Zicheng Zuo and Kefeng Li and Shuai Gong and Jiangtao Qi},
keywords = {Visual question answering, Multi-hop reasoning, Dual-stream attention, Hypergraph},
abstract = {Knowledge-based visual question answering aims to associate external knowledge facts for answering questions about images. Most existing methods emphasize high-order associations between knowledge facts and questions, and fail to consider the negative effects of unnecessary knowledge facts in multi-hop reasoning. In this paper, we propose a Dual-Stream Attention Multi-hop Reasoning (DSAMR) architecture that constructs two different attention streams to mitigate unnecessary knowledge facts. This dual-stream mechanism enables the model to reduce the attention weights on unnecessary knowledge while gathering essential knowledge by learning the implicit correlations between knowledge facts and questions. In addition, we designed a hypergraph knowledge extraction module in the architecture to extract optimal knowledge facts by evaluating the relevance of each knowledge fact to the question. The experimental results demonstrate the effectiveness of our method not only on the knowledge-based visual question answering dataset KVQA, but also on the multi-hop question answering dataset PathQuestion.}
}
@article{WAN2023110228,
title = {A Span-based Multi-Modal Attention Network for joint entity-relation extraction},
journal = {Knowledge-Based Systems},
volume = {262},
pages = {110228},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.110228},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122013247},
author = {Qian Wan and Luona Wei and Shan Zhao and Jie Liu},
keywords = {Information extraction, Named entity recognition, Relation extraction, Natural language processing, Deep learning},
abstract = {Joint extraction of entities and their relations not only depends on entity semantics but also highly correlates with contextual information and entity types. Therefore, an effective joint modelling method designed for handling information from different modalities can lead to a superior performance of the joint entity and relation extraction. Previous span-based models tended to focus on the internal semantics of a span but failed to effectively capture the interactions between the span and other modal information (such as tokens or labels). In this study, a Span-based Multi-Modal Attention Network (SMAN) is proposed for joint entity and relation extraction. The network introduces a cloze mechanism to simultaneously extract the context and span position information, and jointly models the span and label in the relation extraction stage. To determine the fine-grained associations between different modalities, a Modal-Enhanced Attention (MEA) module with two modes is designed and adopted in the modelling process. Experimental results reveal that the proposed model consistently outperforms the state-of-the-art for both entity recognition and relation extraction on the SciERC and ADE datasets, and beats other competing approaches by more than 1.42% F1 score for relation extraction on the CoNLL04 dataset. Extensive additional experiments further verify the effectiveness of the proposed model.}
}
@article{ALAM202250,
title = {Special Issue on Machine Learning and Knowledge Graphs},
journal = {Future Generation Computer Systems},
volume = {129},
pages = {50-53},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.11.022},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21004660},
author = {Mehwish Alam and Anna Fensel and Jorge Martinez-Gil and Bernhard Moser and Diego Reforgiato Recupero and Harald Sack}
}
@article{KRUIPER2024102653,
title = {A platform-based Natural Language processing-driven strategy for digitalising regulatory compliance processes for the built environment},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102653},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102653},
url = {https://www.sciencedirect.com/science/article/pii/S147403462400301X},
author = {Ruben Kruiper and Bimal Kumar and Richard Watson and Farhad Sadeghineko and Alasdair Gray and Ioannis Konstas},
keywords = {Digital Regulatory Compliance, Natural Language Processing, Semantic Web, Machine Learning, Knowledge Graph, Automated Compliance Checking},
abstract = {The digitalisation of the regulatory compliance process has been an active area of research for several decades. However, more recently the level of activities in this area has increased considerably. In the UK, the tragic incident of Grenfell fire in 2017 has been a major catalyst for this as a result of the Hackitt report’s recommendations pointing a lot of the blame on the broken regulatory regime in the country. The Hackitt report emphasises the need to overhaul the building regulations, but the approach to do so remains an open research question. Existing work in this space tends to overlook the processing of actual regulatory documents, or limits their scope to solving a relatively small subtask. This paper presents a new comprehensive platform approach to the digitalisation of the regulatory compliance processing. We present i-ReC (intelligent Regulatory Compliance), a platform approach to digitalisation of regulatory compliance that takes into consideration the enormous diversity of all the stakeholders’ activities. A historical perspective on research in this area is first presented to put things in perspective which identifies the challenges in such an endeavour and identifies the gaps in state-of-the-art. After enumerating all the challenges in implementing a platform-based approach to digitalising the regulatory compliance process, the implementation of some parts of the platform is described. Our research demonstrates that the identification and extraction of all relevant requirements from the corpus of several hundred regulatory documents is a key part of the whole process which underlies the entire process from authoring to eventually compliance checking of designs. Some of the issues that need addressing in this endeavour include ambiguous language, inconsistent use of terms, contradicting requirements and handling multi-word expressions. The implementation of these tools is driven by NLP, ML and Semantic Web technologies. A semantic search engine was developed and validated against other popular and comparable engines with a corpus of 420 (out of about 800) documents used in the UK for compliance checking of building designs. In every search scenario, our search engine performed better on all objective criteria. Limitations of the approach are discussed which includes the challenges around licensing for all the documents in the corpus. Further work includes improving the performance of SPaR.txt (the tool created to identify multi-word expressions) as well as the information retrieval engine by increasing the dataset and providing the model with examples from more diverse formats of regulations. There is also a need to develop and align strategies to collect a comprehensive set of domain vocabularies to be combined in a Knowledge Graph.}
}
@incollection{LAMURIAS202550,
title = {Text Mining for Bioinformatics Using Biomedical Literature},
editor = {Shoba Ranganathan and Mario Cannataro and Asif M. Khan},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {50-61},
year = {2025},
isbn = {978-0-323-95503-4},
doi = {https://doi.org/10.1016/B978-0-323-95502-7.00017-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955027000178},
author = {Andre Lamurias and Diana F. Sousa and Francisco M. Couto},
keywords = {Biomedical literature, Distant supervision, Event extraction, Machine Learning, Named entity recognition, Normalization, Relation extraction, Text mining},
abstract = {Biomedical literature is a large and rich source of information for various applications. Text mining tools aim at extracting information from the literature in an efficient manner since processing scientific texts is a complex task given the formal and highly specialized language. Text mining tools tackle these challenges using different approaches, such as rule-based methods and machine learning algorithms including deep learning. This document overviews the current biomedical text mining tools by describing their approaches, tasks (e.g., Named Entity Recognition, Relation Extraction, Event Extraction, Question Answering), available corpora, toolkits and applications, and community challenges.}
}
@article{JIN2025127501,
title = {A survey on knowledge graph-based click-through rate prediction},
journal = {Expert Systems with Applications},
volume = {281},
pages = {127501},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127501},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425011236},
author = {Ying Jin and Yanwu Yang},
keywords = {Click-through rate, Knowledge graph, CTR prediction, Prediction models},
abstract = {With the rapid development of the Internet era, accurate click-through rate (CTR) prediction is crucial for optimizing recommender systems. Existing CTR prediction models often encounter challenges related to data sparsity and cold start problems. Knowledge graph (KG), with rich semantic and structural relationships, has shown great potential in addressing these issues. In this paper, we provide a comprehensive literature review of recent advances in CTR prediction based on KG. Specifically, we categorize various approaches into embedding-based, path-based and propagation-based models and discuss their advantages and limitations. We further summarize datasets and evaluation metrics used in the literature and discuss the performance of each model on these datasets. Moreover, we identify prevailing research trends, primary challenges and promising future research directions. This literature review aims to offer insights into the progress achieved in KG-based CTR prediction and provide a foundation for future research in this area.}
}
@article{DING2022100681,
title = {An empirical study of representing adjectives over knowledge bases: Approach, lexicon and application},
journal = {Journal of Web Semantics},
volume = {72},
pages = {100681},
year = {2022},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2021.100681},
url = {https://www.sciencedirect.com/science/article/pii/S1570826821000548},
author = {Jiwei Ding and Wei Hu and Xin Yu and Yuzhong Qu},
keywords = {Adjective representation, Question answering, Knowledge base, SPARQL},
abstract = {Adjectives are common in natural language, and their usage and semantics have been studied broadly. In recent years, with the rapid growth of knowledge bases (KBs), many knowledge-based question answering (KBQA) systems are developed to answer users’ natural language questions over KBs. A fundamental task of such systems is to transform natural language questions into structural queries, e.g., SPARQL queries. Thus, such systems require knowledge about how natural language expressions are represented in KBs, including adjectives. In this paper, we specifically address the problem of representing adjectives over KBs. We propose a novel approach, called Adj2SP, to represent adjectives as SPARQL query patterns. Adj2SP contains a statistic-based approach and a neural network-based approach, both of them can effectively reduce the search space for adjective representations and overcome the lexical gap between input adjectives and their target representations. Two adjective representation datasets are built for evaluation, with adjectives used in QALD and Yahoo! Answers, as well as their representations over DBpedia. Experimental results show that Adj2SP can generate representations of high quality and significantly outperform several alternative approaches in F1-score. Furthermore, we publish Lark, a lexicon for adjective representations over KBs. Current KBQA systems show an improvement of over 24% in F1-score by integrating Adj2SP.}
}
@article{YIN2023104902,
title = {Two-stage Text-to-BIMQL semantic parsing for building information model extraction using graph neural networks},
journal = {Automation in Construction},
volume = {152},
pages = {104902},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.104902},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523001620},
author = {Mengtian Yin and Llewellyn Tang and Chris Webster and Jinyang Li and Haotian Li and Zhuoquan Wu and Reynold C.K. Cheng},
abstract = {With the increasing complexity of the building process, it is difficult for project stakeholders to retrieve large and multi-disciplinary building information models (BIMs). A natural language interface (NLI) is beneficial for users to query BIM models using natural language. However, parsing natural language queries (NLQs) is challenging due to ambiguous name descriptions and intricate relationships between entities. To address these issues, this study proposes a graph neural network (GNN)-based semantic parsing method that automatically maps NLQs into executable queries. Firstly, ambiguous mentions are collectively linked to referent ontological entities via a GNN-based entity linking model. Secondly, the logical forms of NLQs are interpreted through a GNN-based relation extraction model, which predicts links between mentioned entities in a heterogeneous graph fusing ontology and NLQ texts. The experiment based on 786 queries shows its outstanding performance. Moreover, a real-world case verifies the practicability of the proposed method for BIM model retrieval.}
}
@article{PANCHENDRARAJAN2024124097,
title = {Synergizing machine learning & symbolic methods: A survey on hybrid approaches to natural language processing},
journal = {Expert Systems with Applications},
volume = {251},
pages = {124097},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124097},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424009631},
author = {Rrubaa Panchendrarajan and Arkaitz Zubiaga},
keywords = {Hybrid NLP, Machine learning, Symbolic methods, Hybrid approaches, Natural language processing},
abstract = {The advancement of machine learning and symbolic approaches have underscored their strengths and weaknesses in Natural Language Processing (NLP). While machine learning approaches are powerful in identifying patterns in data, they often fall short in learning commonsense and the factual knowledge required for the NLP tasks. Meanwhile, the symbolic methods excel in representing knowledge-rich data. However, they struggle to adapt dynamic data and generalize the knowledge. Bridging these two paradigms through hybrid approaches enables the alleviation of weaknesses in both while preserving their strengths. Recent studies extol the virtues of this union, showcasing promising results in a wide range of NLP tasks. In this paper, we present an overview of hybrid approaches used for NLP. Specifically, we delve into the state-of-the-art hybrid approaches used for a broad spectrum of NLP tasks requiring natural language understanding, generation, and reasoning. Furthermore, we discuss the existing resources available for hybrid approaches for NLP along with the challenges and future directions, offering a roadmap for future research avenues.}
}
@article{LI2023104318,
title = {Joint learning-based causal relation extraction from biomedical literature},
journal = {Journal of Biomedical Informatics},
volume = {139},
pages = {104318},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2023.104318},
url = {https://www.sciencedirect.com/science/article/pii/S1532046423000394},
author = {Dongling Li and Pengchao Wu and Yuehu Dong and Jinghang Gu and Longhua Qian and Guodong Zhou},
keywords = {Joint Learning, BEL Statement, Relation Extraction, Function Detection},
abstract = {Causal relation extraction of biomedical entities is one of the most complex tasks in biomedical text mining, which involves two kinds of information: entity relations and entity functions. One feasible approach is to take relation extraction and function detection as two independent sub-tasks. However, this separate learning method ignores the intrinsic correlation between them and leads to unsatisfactory performance. In this paper, we propose a joint learning model, which combines entity relation extraction and entity function detection to exploit their commonality and capture their inter-relationship, so as to improve the performance of biomedical causal relation extraction. Experimental results on the BioCreative-V Track 4 corpus show that our joint learning model outperforms the separate models in BEL statement extraction, achieving the F1 scores of 57.0% and 37.3% on the test set in Stage 2 and Stage 1 evaluations, respectively. This demonstrates that our joint learning system reaches the state-of-the-art performance in Stage 2 compared with other systems.}
}
@article{BAKAGIANNI2025101313,
title = {A systematic survey of natural language processing for the Greek language},
journal = {Patterns},
pages = {101313},
year = {2025},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2025.101313},
url = {https://www.sciencedirect.com/science/article/pii/S2666389925001618},
author = {Juli Bakagianni and Kanella Pouli and Maria Gavriilidou and John Pavlopoulos},
keywords = {monolingual NLP survey, Greek NLP, language resources, task taxonomy, search protocol},
abstract = {Summary
Comprehensive monolingual natural language processing (NLP) surveys are essential for assessing language-specific challenges, resource availability, and research gaps. However, existing surveys often lack standardized methodologies, leading to selection bias and fragmented coverage of NLP tasks and resources. This study introduces a generalizable framework for systematic monolingual NLP surveys. Our approach integrates a structured search protocol to minimize bias, an NLP task taxonomy for classification, and language resource taxonomies to identify potential benchmarks and highlight opportunities for improving resource availability. We apply this framework to Greek NLP (2012–2023), providing an in-depth analysis of its current state, task-specific progress, and resource gaps. The survey results are publicly available and are regularly updated to provide an evergreen resource. This systematic survey of Greek NLP serves as a case study, demonstrating the effectiveness of our framework and its potential for broader application to other not-so-well-resourced languages as regards NLP.}
}
@article{BELLANDI2024105904,
title = {An entity-centric approach to manage court judgments based on Natural Language Processing},
journal = {Computer Law & Security Review},
volume = {52},
pages = {105904},
year = {2024},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2023.105904},
url = {https://www.sciencedirect.com/science/article/pii/S0267364923001140},
author = {Valerio Bellandi and Christian Bernasconi and Fausto Lodi and Matteo Palmonari and Riccardo Pozzi and Marco Ripamonti and Stefano Siccardi},
keywords = {Legal knowledge extraction, Semantic search, Named Entity Recognition, Zero-shot learning},
abstract = {In this paper, we present an entity-centric infrastructure to manage legal documents, especially court judgments, based on the organization of a textual document repository and on the annotation of these documents to serve a variety of downstream tasks. Documents are pre-processed and then iteratively annotated using a set of NLP services that combine complementary approaches based on machine learning and syntactic rules. We present a framework that has been designed to be developed and maintained in a sustainable way, allowing for multiple services and uses of the annotated document repository and considering the scarcity of annotated data as an intrinsic challenge for its development. The design activity is the result of a cooperative project where a scientific team, institutional bodies, and companies appointed to implement the final system are involved in co-design activities. We describe experiments to demonstrate the feasibility of the solution and discuss the main challenges to scaling the system at a national level. In particular, we report the results we obtained in annotating data with different low-resource methods and with solutions designed to combine these approaches in a meaningful way. An essential aspect of the proposed solution is a human-in-the-loop approach to control the output of the annotation algorithms in agreement with the organizational processes in place in Italian courts. Based on these results we advocate for the feasibility of the proposed approach and discuss the challenges that must be addressed to ensure the scalability and robustness of the proposed solution.}
}
@article{STEPHAN2025290,
title = {Fostering Model Reuse in Model-based Systems Engineering using Knowledge Graphs},
journal = {Procedia CIRP},
volume = {136},
pages = {290-295},
year = {2025},
note = {35th CIRP Design 2025},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2025.08.051},
url = {https://www.sciencedirect.com/science/article/pii/S2212827125008042},
author = {Roman Stephan and Thomas Schumacher and David Inkermann},
keywords = {knowledge graph, labeled property graph, model reuse, mode-based systems engineering, transformation concept},
abstract = {The increasing spread of MBSE is associated with the issue of reusing the knowledge contained in different system models in different development projects. Existing approaches for the reuse of system models or elements of these can be classified into framework-based, retrieval-based, and pattern-based. While framework- and pattern-based approaches require standardized modelling methods, retrieval-based approaches are based on merging of different system models. This is not supported by current MBSE-tools. In this contribution we investigate how knowledge graphs can be generated from various SysML system models and whether and how new knowledge, e.g., about cause-effect relationships or structural patterns, can be determined.}
}
@article{YANG2023103945,
title = {Semi-automatic representation of design code based on knowledge graph for automated compliance checking},
journal = {Computers in Industry},
volume = {150},
pages = {103945},
year = {2023},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2023.103945},
url = {https://www.sciencedirect.com/science/article/pii/S0166361523000957},
author = {Mingsong Yang and Qin Zhao and Lei Zhu and Haining Meng and Kehai Chen and Zongjian Li and Xinhong Hei},
keywords = {Automated compliance checking (ACC), Design code representation, Knowledge graph (KG), Building information model (BIM), Building design},
abstract = {Automated compliance checking (ACC) intends to verify the compliance of designs in construction industry by design codes. The ability to interpret and represent semantic information of design codes determines the maximum application scope of ACC. However, design codes are clause texts written in natural languages and existing ACC studies usually use relatively low-complexity code clause samples. At present, the lack of an accurate representation model for design codes leads to difficulties in representing the implicit information, nested logic, and complex relations contained in high-complexity clauses in codes. To address this problem, this research establishes a new representation model based on knowledge graph (KG). Four schemas are proposed into the model including order, complex, event and integration schemas. Further, an accompanying methodology for semi-automatic construction of design code KG (DCKG) is proposed. It includes four parts: interpretation, reconstruction, organization, and implementation. Where the implementation part develops a code annotation platform. In the case study and experiment, a scenario of checking a building information model (BIM) of metro station by GB50157–2013 Code for Design of Metro is adopted to validate the newly proposed representation model and the automated compliance process. The results show that the proposed model and method are correct and feasible, and our model outperforms other models in the representation ability of design codes.}
}
@article{YAO2022102269,
title = {TERQA: Question answering over knowledge graph considering precise dependencies of temporal information on vectors},
journal = {Displays},
volume = {74},
pages = {102269},
year = {2022},
issn = {0141-9382},
doi = {https://doi.org/10.1016/j.displa.2022.102269},
url = {https://www.sciencedirect.com/science/article/pii/S0141938222000890},
author = {Junping Yao and Yijing Wang and Xiaojun Li and Cong Yuan and Kaiyuan Cheng},
keywords = {Temporal knowledge graph, Knowledge graph question answering, Temporal information enhancement, Precise dependency},
abstract = {Time questions involve explicit and implicit constraints as well as complex time interval interactions, making them critical criteria for measuring the effect of knowledge base question answering. Although attention to temporal questions has spurred the development of temporal knowledge graphs, existing studies have focused on the simple splicing and fusion of temporal information with question or knowledge base embeddings, losing sight of the hidden interaction features between temporal information and embedded vectors. In this paper, we proposed TERQA, a temporal knowledge base question-answering approach to explore precise spatial dependencies between temporal information and embedded vectors. The exploration of the deep dependency between time and embedded vectors was divided into two stages. In the first stage, the Transformer model of depth extraction was employed to extract richer features from questions and the representation was enhanced with temporal information; in the second stage, high-level capsules were adopted to extract the low-level vector features for detailed pose determination, allowing a more precise deep dependency of temporal facts on embedded vectors. We conducted an experiment using two temporal question answering datasets, TempQuestions and CronQuestions, and the results showed that accuracy for TERQA improved 11.3% from baseline on the dataset TempQuestions with higher annotated information. Additionally, the adapted TERQA also showed varying degrees of improvements over the baseline in the larger but simply annotated dataset CronQuestions.}
}
@article{FEI2022102,
title = {Research on Intelligent Construction Technology of Information-driven Power Grid Security Situation Knowledge Graph},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {3},
pages = {102-107},
year = {2022},
note = {16th IFAC Symposium on Large Scale Complex Systems: Theory and Applications LSS 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.05.018},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322002828},
author = {Yuan Fei and Yang Hongying and Zhao Gaoshang},
keywords = {Power Grid Security Situation, Knowledge Graph, Intelligent Reasoning, Situation Analysis, Intelligent Feedback},
abstract = {In order to cope with the increasingly complex power grid security operation situation, this article designs a framework for the construction of knowledge graphs in the power field and analyses its key technologies for the comprehensive control and precise analysis of the grid security situation, to realize the framework construction, situation analysis, and intelligent reasoning based on the knowledge graph. And smart feedback. Finally, the application of future knowledge graphs in the security situation of smart grids is discussed in detail, which is of great significance for improving the intelligent level of the security situation of power grids.}
}
@article{YERRAGUNTA2025110142,
title = {Bayesian-error-informed contrastive learning for knowledge-based question answering systems},
journal = {Computers and Electrical Engineering},
volume = {123},
pages = {110142},
year = {2025},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2025.110142},
url = {https://www.sciencedirect.com/science/article/pii/S0045790625000850},
author = {Sudarshan Yerragunta and Rajendra Prasath and G.N. Girish},
keywords = {Question answering, Knowledge-based QA, Incomplete knowledge base, Knowledge aware text reader, Gating mechanisms, Contrastive learning},
abstract = {The Knowledge-base Question Answering (KBQA) system aims to answer a question based on a knowledge base (KB). However, incomplete knowledge bases (KBs) limit the performance of KBQA systems. To address this issue, we propose a contrastive regularization method that considers two modules to tackle this problem: knowledge expansion and a contrastive loss function, Bayesian-error-informed Contrastive Learning (BeCoL). These modules leverage latent knowledge from context KBs and their associated question–answer pairs to generate more such pairs. Additionally, we use these question–answer pairs for informative representation learning, which makes hard positive pairs attract and hard negative pairs separate. This approach will enhance the ability of the system to distinguish the pairs better, ultimately improving the systems performance. We evaluate our proposed approach on the WebQuestionSP (WebQSP), ComplexWebQuestions (CompWebQ), and GrailQA datasets. The results indicate that our approach outperforms existing methods across different KB settings in the WebQSP dataset at 10%, 30%, 50%, and 100% with Hits@1 scores of 43.8, 49.7, 61.3, and 73.7 respectively, and with F1-scores of 28.2, 32.5, 44.3, and 61.1 respectively. Similarly, we achieved Hits@1 score of 52.7 and F1-score of 44.2 on the CompWebQ dataset with 100% KB setting. For the GrailQA dataset under the 100% KB setting, our method attained an Exact Match (EM) score of 67.5 and an F1-score of 76.4. The findings demonstrate the proposed methods capacity to address low-resource settings and significantly improve the performance of KBQA systems. The code is available at https://github.com/ysudarshan-collab/BeCoL.}
}
@article{TANG2024121880,
title = {A Systematic Literature Review of Reinforcement Learning-based Knowledge Graph Research},
journal = {Expert Systems with Applications},
volume = {238},
pages = {121880},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121880},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423023825},
author = {Zifang Tang and Tong Li and Di Wu and Junrui Liu and Zhen Yang},
keywords = {Knowledge graphs, Reinforcement learning, Systematic literature review, Markov decision processes},
abstract = {Knowledge graphs (KGs) model entities or concepts and their relations in a structural manner. The incompleteness has turned out to be the main challenge that hinders the application of KGs. Recently, reinforcement learning (RL) has been recognized as an effective method to deal with such a challenge, which models research tasks into a sequence decision problem without labels. Although an increasing number of studies investigate and analyze KGs using RL, there lacks a systematic literature review that comprehensively and quantitatively analyzes the landscape of RL-based KG research (RL-KG for short). As a result, researchers may have encountered difficulties in appropriately adopting RL techniques in KG research, even reinventing the wheels. In this paper, we follow the Systematic Literature Review (SLR) methodology to survey, screen, and investigate papers of RL-KG. Specifically, we identify 109 highly related papers from 1542, and systematically investigate them with regard to the following five research questions: (1) to what extent RL-KG have been investigated; (2) what application domains have been covered; (3) what RL techniques have been mainly considered; (4) whether there is a connection between the influence and reproducibility of these papers; (5) what specialized datasets, evaluation metrics, and publication venues have been applied. Through an in-depth analysis of the review results, we systematically and comprehensively identify some significant phenomena and analyze the reasons and difficulties of these phenomena. Based on such analysis, we tentatively propose promising future research topics to promote the RL-KG.}
}
@article{ALMOUSA2022101337,
title = {A novel word sense disambiguation approach using WordNet knowledge graph},
journal = {Computer Speech & Language},
volume = {74},
pages = {101337},
year = {2022},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2021.101337},
url = {https://www.sciencedirect.com/science/article/pii/S0885230821001303},
author = {Mohannad AlMousa and Rachid Benlamri and Richard Khoury},
keywords = {Semantic word sense disambiguation, Knowledge-based, Knowledge graph, WordNet},
abstract = {Various applications in computational linguistics and artificial intelligence rely on high-performing word sense disambiguation techniques to solve challenging tasks such as information retrieval, machine translation, question answering, and document clustering. While text comprehension is intuitive for humans, machines face tremendous challenges in processing and interpreting a human’s natural language. This paper presents a novel knowledge-based word sense disambiguation algorithm, namely Sequential Contextual Similarity Matrix Multiplication (SCSMM). The SCSMM algorithm combines semantic similarity, heuristic knowledge, and document context to respectively exploit the merits of local sense-based context between consecutive terms, human knowledge about terms, and a document’s main topic in disambiguating terms. Unlike other algorithms, the SCSMM algorithm guarantees the capture of the maximum sentence context while maintaining the terms’ order within the sentence. The proposed algorithm outperformed all other algorithms when disambiguating nouns on the combined gold standard datasets, while demonstrating comparable results to current state-of-the-art word sense disambiguation systems when dealing with each dataset separately. Furthermore, the paper discusses the impact of granularity level, ambiguity rate, sentence size, and part of speech distribution on the performance of the proposed algorithm.}
}
@article{MAO2022733,
title = {Financial fraud detection using the related-party transaction knowledge graph},
journal = {Procedia Computer Science},
volume = {199},
pages = {733-740},
year = {2022},
note = {The 8th International Conference on Information Technology and Quantitative Management (ITQM 2020 & 2021): Developing Global Digital Economy after COVID-19},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.01.091},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922000928},
author = {Xuting Mao and Hao Sun and Xiaoqian Zhu and Jianping Li},
keywords = {Financial Fraud Detection, Knowledge Graph, Related-Party Transactions, Machine Learning, Knowledge Reasoning},
abstract = {Financial fraud detection has gained constant attention from researchers, practitioners, and regulators. Because of its concealment and ease of manipulation, related-party transactions (RPTs) among firms have become a usual way to implement financial fraud. However, the traditional quantitative analysis methods regard each firm as an independent individual, failing to mine the intricate relation and transactions among related parties. A knowledge graph can mine valuable hidden knowledge from large-scale associated data as a new form of knowledge representation. Therefore, in this paper, the RPT knowledge graph is utilized to detect financial fraud, where the feature of the transaction’s scale and category is obtained. The experiment on the Chinese listed companies from 2000 to 2019 shows that these features enhance financial fraud detection performance, suggesting that type, size, and frequency of RPTs may imply fraud. More details, the feature importance indicates that regulators should pay more attention to the loan-based RPTs and the total number of RPTs.}
}
@article{BEHMANESH2023119973,
title = {Improved relation span detection in question answering systems over extracted knowledge bases},
journal = {Expert Systems with Applications},
volume = {224},
pages = {119973},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.119973},
url = {https://www.sciencedirect.com/science/article/pii/S095741742300475X},
author = {Somayyeh Behmanesh and Alireza Talebpour and Mehrnoush Shamsfard and Mohammad Mahdi Jafari},
keywords = {Knowledge base question answering, Extracted knowledge base, Transformers, Relation detection},
abstract = {In recent years, AI studies have been focused on developing question answering systems to deal with automatic answering natural language questions. Knowledge based open domain question answering systems can generate accurate answers to questions posed by users in various fields. However, these systems need further development to scale the domain of answer retrieval systems and question interpretation. Deep learning methods are one of the current approaches in this research area. Existing knowledge-based question answering systems use either manually curated knowledge bases such as Freebase or knowledge bases automatically extracted from unstructured texts such as Reverb, or a combination of both. In the case of open domain question answering systems, limited access to knowledge bases reduces the expandability of the system. Systems that use only curated knowledge bases have high precision with limited coverage; while systems that use extracted knowledge bases have higher coverage with generally lower precision. To improve the precision of question answering over extracted knowledge bases, this paper presents a solution to enhance detection of the relation span in the question, corresponding to the triples of the extracted knowledge base. First, a dataset with 16,675 simple questions is introduced along with answers based on the Reverb triples. Then, a method based on a fine-tuned BERT model for relation span detection in the questions is proposed. The results showed an increase in the precision of the relation span detection, so that the precision reached 99.65%.}
}
@article{ARAZZI2025100765,
title = {NLP-based techniques for Cyber Threat Intelligence},
journal = {Computer Science Review},
volume = {58},
pages = {100765},
year = {2025},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2025.100765},
url = {https://www.sciencedirect.com/science/article/pii/S1574013725000413},
author = {Marco Arazzi and Dincy {R. Arikkat} and Serena Nicolazzo and Antonino Nocera and Rafidha {Rehiman K.A.} and Vinod P. and Mauro Conti},
keywords = {Cyber threat intelligence, Natural language processing, Security, Named entity recognition, Knowledge graph, Large language model},
abstract = {In the digital era, threat actors employ sophisticated techniques for which, often, digital traces in the form of textual data are available. Cyber Threat Intelligence (CTI) is related to all the solutions inherent to data collection, processing, and analysis useful to understand a threat actor’s targets and attack behavior. Currently, CTI is assuming an always more crucial role in identifying and mitigating threats and enabling proactive defense strategies. In this context, NLP, an artificial intelligence branch, has emerged as a powerful tool for enhancing threat intelligence capabilities. This survey paper provides a comprehensive overview of NLP-based techniques applied in the context of threat intelligence. It begins by describing the foundational definitions and principles of CTI as a major tool for safeguarding digital assets. It then undertakes a thorough examination of NLP-based techniques for CTI data crawling from Web sources, CTI data analysis, Relation Extraction from cybersecurity data, CTI sharing and collaboration, security threats of CTI, and role of LLM in this domain. Finally, the challenges and limitations of NLP in threat intelligence are exhaustively examined, including data quality issues and ethical considerations. This survey draws a complete framework and serves as a valuable resource for security professionals and researchers seeking to understand the state-of-the-art NLP-based threat intelligence techniques and their potential impact on cybersecurity.}
}
@article{HAN2022100696,
title = {A framework for differentially-private knowledge graph embeddings},
journal = {Journal of Web Semantics},
volume = {72},
pages = {100696},
year = {2022},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2021.100696},
url = {https://www.sciencedirect.com/science/article/pii/S1570826821000640},
author = {Xiaolin Han and Daniele Dell’Aglio and Tobias Grubenmann and Reynold Cheng and Abraham Bernstein},
keywords = {Differential privacy, Knowledge graph embeddings},
abstract = {Knowledge graph (KG) embedding methods are at the basis of many KG-based data mining tasks, such as link prediction and node clustering. However, graphs may contain confidential information about people or organizations, which may be leaked via embeddings. Research recently studied how to apply differential privacy to a number of graphs (and KG) analyses, but embedding methods have not been considered so far. This study moves a step toward filling such a gap, by proposing the Differential Private Knowledge Graph Embedding (DPKGE) framework. DPKGE extends existing KG embedding methods (e.g., TransE, TransM, RESCAL, and DistMult) and processes KGs containing both confidential and unrestricted statements. The resulting embeddings protect the presence of any of the former statements in the embedding space using differential privacy. Our experiments identify the cases where DPKGE produces useful embeddings, by analyzing the training process and tasks executed on top of the resulting embeddings.}
}
@article{LI2025113471,
title = {Enhancing named entity recognition with external knowledge from large language model},
journal = {Knowledge-Based Systems},
volume = {318},
pages = {113471},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113471},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125005155},
author = {Qi Li and Tingyu Xie and Jian Zhang and Ke Ma and Jiayuan Su and Kaixiang Yang and Hongwei Wang},
keywords = {Large language model, Named entity recognition, Knowledge distillation, Information extraction, Natural language processing},
abstract = {Inspired by the powerful capabilities of large language models (LLMs), our work enhances the Named Entity Recognition (NER) task by incorporating external knowledge generated with LLMs. We identify two primary challenges: generating external knowledge with sufficient breadth and training the extraction model with the large amount of non-entities. To fully exploit entity features derived from external knowledge, our work establishes robust connections between the quality of external knowledge from LLMs and the utilization of external knowledge within the extraction model. First, we introduce an entity linking strategy that enhances the comprehensiveness of the external knowledge generated by LLMs, thereby improving the extraction model’s understanding of relevant entities. Second, we propose an optimal self-training strategy for selecting pseudo labels, which effectively filters out non-entity interference. This enables the extraction model to better leverage entity-related expressions presented in the external knowledge. Through thorough experiments, we demonstrate the high performance of our method against three distinct benchmarks, each compared to varying external knowledge sources. Further analysis reveals that our proposed external knowledge and training strategies significantly enhance the ability of the extraction model to identify entity features.}
}
@article{LI2025671,
title = {A review on enhancing agricultural intelligence with large language models},
journal = {Artificial Intelligence in Agriculture},
volume = {15},
number = {4},
pages = {671-685},
year = {2025},
issn = {2589-7217},
doi = {https://doi.org/10.1016/j.aiia.2025.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S2589721725000613},
author = {Hongda Li and Huarui Wu and Qingxue Li and Chunjiang Zhao},
keywords = {Large language models, Agricultural knowledge, Knowledge integration, Knowledge Base, Intelligent question-answering},
abstract = {This paper systematically explores the application potential of large language models (LLMs) in the field of agricultural intelligence, focusing on key technologies and practical pathways. The study focuses on the adaptation of LLMs to agricultural knowledge, starting with foundational concepts such as architecture design, pre-training strategies, and fine-tuning techniques, to build a technical framework for knowledge integration in the agricultural domain. Using tools such as vector databases and knowledge graphs, the study enables the structured development of professional agricultural knowledge bases. Additionally, by combining multimodal learning and intelligent question-answering (Q&A) system design, it validates the application value of LLMs in agricultural knowledge services. Addressing core challenges in domain adaptation, including knowledge acquisition and integration, logical reasoning, multimodal data processing, agent collaboration, and dynamic knowledge updating, the paper proposes targeted solutions. The study further explores the innovative applications of LLMs in scenarios such as precision crop management and market dynamics analysis, providing theoretical support and technical pathways for the development of agricultural intelligence. Through the technological innovation of large language models and their deep integration with the agricultural sector, the intelligence level of agricultural production, decision-making, and services can be effectively enhanced.}
}
@article{SHEN2022109597,
title = {A comprehensive overview of knowledge graph completion},
journal = {Knowledge-Based Systems},
volume = {255},
pages = {109597},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.109597},
url = {https://www.sciencedirect.com/science/article/pii/S095070512200805X},
author = {Tong Shen and Fu Zhang and Jingwei Cheng},
keywords = {Knowledge Graph Completion (KGC), Classification, Comparisons and analyses, Performance evaluation, Overview},
abstract = {Knowledge Graph (KG) provides high-quality structured knowledge for various downstream knowledge-aware tasks (such as recommendation and intelligent question-answering) with its unique advantages of representing and managing massive knowledge. The quality and completeness of KGs largely determine the effectiveness of the downstream tasks. But in view of the incomplete characteristics of KGs, there is still a large amount of valuable knowledge is missing from the KGs. Therefore, it is necessary to improve the existing KGs to supplement the missed knowledge. Knowledge Graph Completion (KGC) is one of the popular technologies for knowledge supplement. Accordingly, there has a growing concern over the KGC technologies. Recently, there have been lots of studies focusing on the KGC field. To investigate and serve as a helpful resource for researchers to grasp the main ideas and results of KGC studies, and further highlight ongoing research in KGC, in this paper, we provide a all-round up-to-date overview of the current state-of-the-art in KGC. According to the information sources used in KGC methods, we divide the existing KGC methods into two main categories: the KGC methods relying on structural information and the KGC methods using other additional information. Further, each category is subdivided into different granularity for summarizing and comparing them. Besides, the other KGC methods for KGs of special fields (including temporal KGC, commonsense KGC, and hyper-relational KGC) are also introduced. In particular, we discuss comparisons and analyses for each category in our overview. Finally, some discussions and directions for future research are provided.}
}
@article{MAN2023106244,
title = {Synthesis of multilevel knowledge graphs: Methods and technologies for dynamic networks},
journal = {Engineering Applications of Artificial Intelligence},
volume = {123},
pages = {106244},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106244},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623004281},
author = {Tianxing Man and Alexander Vodyaho and Dmitry I. Ignatov and Igor Kulikov and Nataly Zhukova},
keywords = {Knowledge graphs, Multilevel object model, Inductive synthesis, Deductive synthesis, Ontology, Telecommunication benchmark},
abstract = {Knowledge Graphs is one of the most popular techniques for knowledge-based modelling in various subdomains of modern AI technologies ranging from natural language processing to e-commerce recommendations and cyberphysical systems. Even complex technical systems like telecommunication networks could be modelled by means of Knowledge Graphs. However, there are serious challenges when we deal with such systems having a huge number of interconnected elements (e.g. technical objects and their groups) that change over time. Thus, up-to-date there is no adequate solution for not only telecommunication networks but for any complex dynamic systems where inductive and deductive synthesis of large Knowledge Graph based models that are easily reconfigurable and scalable is required. We state and solve the problem of building such models for one of the most common types of objects where models can be represented as hierarchical re-configurable structures. This representation enables recent advances in multilevel inductive–deductive synthesis for model building. From a methodological viewpoint, we propose a novel complex approach to multilevel synthesis for objects with dynamic hierarchical structure based on modified methods for inductive and deductive synthesis of Knowledge Graphs. From a practical perspective we present a real case-study on an interactive service for digital cable TV networks – which is especially interesting for data engineers and scientists – where various problems ranging from network health monitoring to channel advertising can be solved with the same hierarchical model. We release an openly available domain benchmark, which features two realistic datasets (namely, for SPARQL querying performance analysis, and for our case study on dynamic network monitoring). Last but not least, our experiments with recent state-of-the-art approaches to knowledge graph querying Abdelaziz et al. (2017) show that the developed models of multilevel synthesis reduce the time complexity up to 73% on practice compared to the baselines, and are lossless and able to beat their competitors based on parallel knowledge graph processing from 4% to 91% in terms of computational time (depending on the query type). Further parallelisation of our multilevel models is even more efficient (the reduction of query processing time is about 40%–45%) and opens promising prospects for the creation and exploitation of dynamic Knowledge Graphs in practice.}
}
@article{ZHU2023105074,
title = {Autonomous complex knowledge mining and graph representation through natural language processing and transfer learning},
journal = {Automation in Construction},
volume = {155},
pages = {105074},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.105074},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523003345},
author = {Xiaofeng Zhu and Haijiang Li and Tengxiang Su},
keywords = {Knowledge mining, Natural language processing (NLP), Transfer learning, Knowledge modelling, Regulation document},
abstract = {Regulatory documents play a significant role in securing engineering project quality, standard process management and long-term sustainable developments. With the digitisation of knowledge in the AEC industry, the demand for automated knowledge mining has emerged when confronted with substantial regulations. However, the current interpretation approaches for regulatory documents are still mostly labour-intensive and flawed in complex knowledge. Based on transfer learning (BERT) and natural language processing (e.g., NLP-Syntactic Parsing), this paper proposes a fully automated knowledge mining framework to convert complex knowledge in textual regulations to graph-based knowledge representations. The framework uses a BERT-based engine to extract clauses from regulation documents through fine-tuning with the self-developed domain dataset. A constituent extractor is developed to process the provisions with complex knowledge and extract constituents. A knowledge modelling engine integrates the extracted constituents into a graph-based regulation knowledge model, which can be queried, visualised, and directly applied to downstream applications. The outcome has demonstrated promising performance in complex knowledge mining and knowledge graph modelling based on ISO 19650 case study. This research can effectively convert textual regulation documents to their counterpart regulatory knowledge base, contributing to automated knowledge acquisition and multi-domain knowledge fusion toward regulation digitalization.}
}
@article{HOSSEINI2022102957,
title = {A systemic functional linguistics approach to implicit entity recognition in tweets},
journal = {Information Processing & Management},
volume = {59},
number = {4},
pages = {102957},
year = {2022},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2022.102957},
url = {https://www.sciencedirect.com/science/article/pii/S0306457322000772},
author = {Hawre Hosseini and Mehran Mansouri and Ebrahim Bagheri},
keywords = {Named entity recognition, Tweet analytics, Knowledge graphs, Computational linguistics},
abstract = {The identification of knowledge graph entity mentions in textual content has already attracted much attention. The major assumption of existing work is that entities are explicitly mentioned in text and would only need to be disambiguated and linked. However, this assumption does not necessarily hold for social content where a significant portion of information is implied. The focus of our work in this paper is to identify whether textual social content include implicit mentions of knowledge graph entities or not, hence forming a two-class classification problem. To this end, we adopt the systemic functional linguistic framework that allows for capturing meaning expressed through language. Based on this theoretical framework we systematically introduce two classes of features, namely syntagmatic and paradigmatic features, for implicit entity recognition. In our experiments, we show the utility of these features for the task, report on ablation studies, measure the impact of each feature subset on each other and also provide a detailed error analysis of our technique.}
}
@article{KHAN2022117737,
title = {Categorization of knowledge graph based recommendation methods and benchmark datasets from the perspectives of application scenarios: A comprehensive survey},
journal = {Expert Systems with Applications},
volume = {206},
pages = {117737},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.117737},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422010181},
author = {Nasrullah Khan and Zongmin Ma and Aman Ullah and Kemal Polat},
keywords = {Categorization, Knowledge graph, Side information, Recommendation methods, Benchmark datasets, Knowledge repositories},
abstract = {Recommender Systems (RS) are established to deal with the preferences of users to enhance their experience and interest in innumerable online applications by streamlining the stress persuaded by the reception of excessive information through the recommendation methods. Although researches have put a lot of efforts in making recommendation processes accurate, specific, and personalized; different issues like cold start, data sparsity or gray sheep etc., still pop up in one or the other form of challenges. Recently, exploitation of Knowledge Graph (KG)-based data as Side Information in recommendation methods has revealed as a sign of resolution to the corresponding challenges; and thus, acquired incredible focus, applicability, and popularity. The incorporation of KG in recommendation has not only effectively alleviated the contrasting challenges, but also has provided specific, accurate, personalized and explainable recommendations about the target items to the end users. In this paper, we explore well-known RSs, popular knowledge repositories, benchmark datasets, recommendation methods, and future research dimensions about the current research. Intuitively, we investigate recommendation methods and associated datasets with respect to the corresponding application scenarios in a categorical way.}
}
@article{DING2023102183,
title = {Knowledge graph modeling method for product manufacturing process based on human–cyber–physical fusion},
journal = {Advanced Engineering Informatics},
volume = {58},
pages = {102183},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102183},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623003117},
author = {Chen Ding and Fei Qiao and Juan Liu and Dongyuan Wang},
keywords = {Knowledge graph modeling, Product manufacturing process, Human–cyber–physical, Ontology},
abstract = {The data generated in the product manufacturing process are usually distributed in different formats, triggering fragmented knowledge and disconnected information. To address this problem, we present a knowledge graph modeling method for the product manufacturing process. First, the concepts of human–cyber–physical (HCP) elements are analyzed in detail. The HCP-related classes, attributes, and relations are defined in a formalized manner in the ontology modeling process. Second, a knowledge graph model for the product manufacturing process (KGM/PMP) is constructed by three steps, including knowledge extraction, knowledge fusion, and knowledge reasoning. When constructing the KGM/PMP model, a deep learning method called BERT-D’BiGRU-CRF is presented to automatically extract knowledge from the manufacturing data. Moreover, a set of reasoning rules are designed to infer new knowledge. Finally, a case study is carried out to validate the effectiveness of the proposed method. The validity of the BERT-D’BiGRU-CRF method on knowledge extraction is verified by comparing performance with four other methods. The applicability of the knowledge graph model is demonstrated through developing a prototype system. With this system, manufacturing knowledge can be provided for the demanders rapidly and accurately.}
}
@article{HAO2025130638,
title = {Dynamic task balancing for joint information extraction},
journal = {Neurocomputing},
volume = {648},
pages = {130638},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.130638},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225013104},
author = {Anran Hao and Shuo Sun and Jian Su and Siu Cheung Hui and Anh Tuan Luu},
keywords = {Information extraction, Multi-task learning, Deep learning},
abstract = {Joint Information Extraction (IE) aims for joint extraction of various semantic structures such as entities and relations. Most recent joint IE works use static weighting methods by combining task losses with predefined and fixed weights. In this paper, we identify the limitations of the static weighting methods with empirical analysis. We then study the feasibility of applying several dynamic weighting methods for the joint IE problem and evaluate the methods on three benchmark IE datasets in terms of their performance. We find that existing dynamic weighting methods can achieve reasonably good results in a single run, demonstrating their effectiveness and advantages over the static weighting methods. Further, we propose a hybrid dynamic weighting method, Adaptive Weighting for Joint IE (AWIE), based on gradient dynamic task weighting. Experimental results show that our proposed method obtains competitive performance results across datasets cost-effectively with task preference accommodation.}
}
@article{LUO2024102904,
title = {Pre-trained language models in medicine: A survey},
journal = {Artificial Intelligence in Medicine},
volume = {154},
pages = {102904},
year = {2024},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2024.102904},
url = {https://www.sciencedirect.com/science/article/pii/S0933365724001465},
author = {Xudong Luo and Zhiqi Deng and Binxia Yang and Michael Y. Luo},
keywords = {Natural language processing, Medical science, Healthcare, Pre-trained language model, BERT, GPT},
abstract = {With the rapid progress in Natural Language Processing (NLP), Pre-trained Language Models (PLM) such as BERT, BioBERT, and ChatGPT have shown great potential in various medical NLP tasks. This paper surveys the cutting-edge achievements in applying PLMs to various medical NLP tasks. Specifically, we first brief PLMS and outline the research of PLMs in medicine. Next, we categorise and discuss the types of tasks in medical NLP, covering text summarisation, question-answering, machine translation, sentiment analysis, named entity recognition, information extraction, medical education, relation extraction, and text mining. For each type of task, we first provide an overview of the basic concepts, the main methodologies, the advantages of applying PLMs, the basic steps of applying PLMs application, the datasets for training and testing, and the metrics for task evaluation. Subsequently, a summary of recent important research findings is presented, analysing their motivations, strengths vs weaknesses, similarities vs differences, and discussing potential limitations. Also, we assess the quality and influence of the research reviewed in this paper by comparing the citation count of the papers reviewed and the reputation and impact of the conferences and journals where they are published. Through these indicators, we further identify the most concerned research topics currently. Finally, we look forward to future research directions, including enhancing models’ reliability, explainability, and fairness, to promote the application of PLMs in clinical practice. In addition, this survey also collect some download links of some model codes and the relevant datasets, which are valuable references for researchers applying NLP techniques in medicine and medical professionals seeking to enhance their expertise and healthcare service through AI technology.}
}
@article{SHA2025,
title = {Leveraging Retrieval-Augmented Large Language Models for Dietary Recommendations With Traditional Chinese Medicine’s Medicine Food Homology: Algorithm Development and Validation},
journal = {JMIR Medical Informatics},
volume = {13},
year = {2025},
issn = {2291-9694},
doi = {https://doi.org/10.2196/75279},
url = {https://www.sciencedirect.com/science/article/pii/S2291969425001711},
author = {Hangyu Sha and Fan Gong and Bo Liu and Runfeng Liu and Haofen Wang and Tianxing Wu},
keywords = {Traditional Chinese Medicine, medicine food homology, large language model, retrieval-augmented generation, uncertain knowledge graph, dietary recommendation},
abstract = {Background
Traditional Chinese Medicine (TCM) emphasizes the concept of medicine food homology (MFH), which integrates dietary therapy into health care. However, the practical application of MFH principles relies heavily on expert knowledge and manual interpretation, posing challenges for automating MFH-based dietary recommendations. Although large language models (LLMs) have shown potential in health care decision support, their performance in specialized domains such as TCM is often hindered by hallucinations and a lack of domain knowledge. The integration of uncertain knowledge graphs (UKGs) with LLMs via retrieval-augmented generation (RAG) offers a promising solution to overcome these limitations by enabling a structured and faithful representation of MFH principles while enhancing LLMs’ ability to understand the inherent uncertainty and heterogeneity of TCM knowledge. Consequently, it holds potential to improve the reliability and accuracy of MFH-based dietary recommendations generated by LLMs.
Objective
This study aimed to introduce Yaoshi-RAG, a framework that leverages UKGs to enhance LLMs' capabilities in generating accurate and personalized MFH-based dietary recommendations.
Methods
The proposed framework began by constructing a comprehensive MFH knowledge graph (KG) through LLM-driven open information extraction, which extracted structured knowledge from multiple sources. To address the incompleteness and uncertainty within the MFH KG, UKG reasoning was used to measure the confidence of existing triples and to complete missing triples. When processing user queries, query entities were identified and linked to the MFH KG, enabling retrieval of relevant reasoning paths. These reasoning paths were then ranked based on triple confidence scores and entity importance. Finally, the most informative reasoning paths were encoded into prompts using prompt engineering, enabling the LLM to generate personalized dietary recommendations that aligned with both individual health needs and MFH principles. The effectiveness of Yaoshi-RAG was evaluated through both automated metrics and human evaluation.
Results
The constructed MFH KG comprised 24,984 entities, 22 relations, and 29,292 triples. Extensive experiments demonstrate the superiority of Yaoshi-RAG in different evaluation metrics. Integrating the MFH KG significantly improved the performance of LLMs, yielding an average increase of 14.5% in Hits@1 and 8.7% in F1-score, respectively. Among the evaluated LLMs, DeepSeek-R1 achieved the best performance, with 84.2% in Hits@1 and 71.5% in F1-score, respectively. Human evaluation further validated these results, confirming that Yaoshi-RAG consistently outperformed baseline models across all assessed quality dimensions.
Conclusions
This study shows Yaoshi-RAG, a new framework that enhances LLMs’ capabilities in generating MFH-based dietary recommendations through the knowledge retrieved from a UKG. By constructing a comprehensive TCM knowledge representation, our framework effectively extracts and uses MFH principles. Experimental results demonstrate the effectiveness of our framework in synthesizing traditional wisdom with advanced language models, facilitating personalized dietary recommendations that address individual health conditions while providing evidence-based explanations.}
}
@article{ZENG2023100035,
title = {Multi-aspect attentive text representations for simple question answering over knowledge base},
journal = {Natural Language Processing Journal},
volume = {5},
pages = {100035},
year = {2023},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2023.100035},
url = {https://www.sciencedirect.com/science/article/pii/S2949719123000328},
author = {Zhixiang Zeng and Yuefeng Li and Jianming Yong and Xiaohui Tao and Vicky Liu},
keywords = {Question answering, Knowledge base, Deep learning},
abstract = {With the deepening of knowledge base research and application, question answering over knowledge base, also called KBQA, has recently received more and more attention from researchers. Most previous KBQA models focus on mapping the input query and the fact in KBs into an embedding format. Then the similarity between the query vector and the fact vector is computed eventually. Based on the similarity, each query can obtain an answer representing a tuple (subject, predicate, object) from the KBs. However, the information about each word in the input question will lose inevitably during the process. To retain as much original information as possible, we introduce an attention-based recurrent neural network model with interactive similarity matrixes. It can extract more comprehensive information from the hierarchical structure of words among queries and tuples stored in the knowledge base. This work makes three main contributions: (1) A neural network-based question-answering model for the knowledge base is proposed to handle single relation questions. (2) An attentive module is designed to obtain information from multiple aspects to represent queries and data, which contributes to avoiding losing potentially valuable information. (3) Similarity matrixes are introduced to obtain the interaction information between queries and data from the knowledge base. Experimental results show that our proposed model performs better on simple questions than state-of-the-art in several effectiveness measures.}
}
@article{SUN2022100698,
title = {Skeleton parsing for complex question answering over knowledge bases},
journal = {Journal of Web Semantics},
volume = {72},
pages = {100698},
year = {2022},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2021.100698},
url = {https://www.sciencedirect.com/science/article/pii/S1570826821000652},
author = {Yawei Sun and Pengwei Li and Gong Cheng and Yuzhong Qu},
keywords = {Complex question answering, KBQA, Skeleton parsing, Dependency parsing, Question decomposition},
abstract = {Answering complex questions involving multiple relations over knowledge bases is a challenging task. Many previous works rely on dependency parsing. However, errors in dependency parsing would influence their performance, in particular for long complex questions. In this paper, we propose a novel skeleton grammar to represent the high-level structure of a complex question. This lightweight formalism and its BERT-based parsing algorithm help to improve the downstream dependency parsing. To show the effectiveness of skeleton, we develop two question answering approaches: skeleton-based semantic parsing (called SSP) and skeleton-based information retrieval (called SIR). In SSP, skeleton helps to improve structured query generation. In SIR, skeleton helps to improve path ranking. Experimental results show that, thanks to skeletons, our approaches achieve state-of-the-art results on three datasets: LC-QuAD 1.0, GraphQuestions, and ComplexWebQuestions 1.1.}
}
@article{LI2024124760,
title = {mt4CrossOIE: Multi-stage tuning for cross-lingual open information extraction},
journal = {Expert Systems with Applications},
volume = {255},
pages = {124760},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124760},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424016270},
author = {Tongliang Li and Zixiang Wang and Linzheng Chai and Jian Yang and Jiaqi Bai and Yuwei Yin and Jiaheng Liu and Hongcheng Guo and Liqun Yang and Hebboul {Zine el-abidine} and Zhoujun Li},
keywords = {Information extraction, Cross-lingual transfer, Disentangled training, Multi-lingual training, Mixture of LoRA},
abstract = {Cross-lingual open information extraction aims to extract structured information from raw text across multiple languages. Previous work uses a shared cross-lingual pre-trained model to handle the different languages but underuses the potential of the language-specific representation. In this paper, we propose an effective multi-stage tuning framework called mt4CrossOIE, designed for enhancing cross-lingual open information extraction by injecting language-specific knowledge into the shared model. Specifically, the cross-lingual pre-trained model is first tuned in a shared semantic space (e.g., embedding matrix) in the fixed encoder and then other components are optimized in the second stage. After enough training, we freeze the pre-trained model and tune the multiple extra low-rank language-specific modules using mixture of LoRAs for model-based cross-lingual transfer. In addition, we leverage two-stage prompting to encourage the large language model (LLM) to annotate the multi-lingual raw data for data-based cross-lingual transfer. The model is trained with multi-lingual objectives on our proposed dataset OpenIE4++ by combining the model-based and data-based transfer techniques. Experimental results on various benchmarks emphasize the importance of aggregating multiple plug-in-and-play language-specific modules and demonstrate the effectiveness of mt4CrossOIE in cross-lingual OIE.22https://github.com/CSJianYang/Multilingual-Multimodal-NLP.}
}
@article{JIA2024111545,
title = {Document-level relation extraction with global and path dependencies},
journal = {Knowledge-Based Systems},
volume = {289},
pages = {111545},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.111545},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124001801},
author = {Wei Jia and Ruizhe Ma and Li Yan and Weinan Niu and Zongmin Ma},
keywords = {Relation extraction, Global dependency, Multi-hop path, Path representation},
abstract = {Document-level relation extraction (RE) focuses on extracting relations for each entity pair in the same sentence or across different sentences of a document. Several existing methodologies aim to capture the intricate interactions among entities across a document by constructing diverse document graphs. However, these graphs frequently cannot sufficiently model the intricate global interactions and concurrent explicit path reasoning. Therefore, we introduce a distinctive graph-based model designed to assimilate global and path dependencies within a document for document-level RE, termed graph-based global and path dependencies (GGP). Specifically, the global dependency component captures interactions between mentions, entities, sentences and, the document through two interconnected graphs: the mention-level graph and the entity-level graph (ELG). To integrate relevant paths essential for the designated entity pair, the path dependency component consolidates information from various multi-hop paths of the target entity pair through an attention mechanism on the ELG. In addition, we devised an innovative method for learning path representation, which encapsulates relations and intermediate entities within the multi-hop path in the ELG. Comprehensive experiments conducted on standard document-level RE and CDR datasets reveal the following key findings: (i) GGP achieves an Ign F1 score of 59.98%, surpassing baselines by 0.61% on the test set; and (ii) the integration of various features derived from entities, sentences, documents, and paths enhances GGP's performance in document-level RE.}
}
@article{CHU2025126378,
title = {GeoSMIE: An event extraction framework for Document-Level spatial morphological information extraction},
journal = {Expert Systems with Applications},
volume = {268},
pages = {126378},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.126378},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424032457},
author = {Deping Chu and Bo Wan and Huizhu Ni and Hong Li and Zhuo Tan and Yan Dai and Zijing Wan and Tao Tang and Shunping Zhou},
keywords = {Spatial information extraction, Spatial morphological information, Chinese geological text, Event extraction},
abstract = {Spatial morphological information (SMI) in geological texts provides critical insights into the formation, localization, and distribution of geological bodies. However, SMI is often scattered across multiple sentences or coexists in complex forms within the same document, making it challenging to extract using traditional methods. In this paper, we address this gap by formalizing SMI extraction as an event extraction task and proposed a novel Geological body SMI Extraction model, GeoSMIE. Our approach is innovative in two key ways: first, we implement a no-trigger-word annotation strategy to capture both descriptive and digital SMI, ensuring that SMI without explicit morphological triggers is not missed. Second, we design dual graph neural networks (GNNs) to handle long-distance dependencies and complex interactions between scattered arguments across sentences. To validate its effectiveness, we compared GeoSMIE to state-of-the-art models on the constructed dataset. For SMI extraction, GeoSMIE outperformed the optimal baseline by 0.4%, 2.2%, and 1.5% for accuracy, recall, and Micro-F1 score, respectively. This work provides an innovative idea for extracting complex spatial information from geoscience texts.}
}
@article{LI2022100511,
title = {Neural Natural Language Processing for unstructured data in electronic health records: A review},
journal = {Computer Science Review},
volume = {46},
pages = {100511},
year = {2022},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2022.100511},
url = {https://www.sciencedirect.com/science/article/pii/S1574013722000454},
author = {Irene Li and Jessica Pan and Jeremy Goldwasser and Neha Verma and Wai Pan Wong and Muhammed Yavuz Nuzumlalı and Benjamin Rosand and Yixin Li and Matthew Zhang and David Chang and R. Andrew Taylor and Harlan M. Krumholz and Dragomir Radev},
keywords = {Natural language processing, Electronic health records, Deep learning},
abstract = {Electronic health records (EHRs), digital collections of patient healthcare events and observations, are ubiquitous in medicine and critical to healthcare delivery, operations, and research. Despite this central role, EHRs are notoriously difficult to process automatically. Well over half of the information stored within EHRs is in the form of unstructured text (e.g., provider notes, operation reports) and remains largely untapped for secondary use. Recently, however, newer neural network and deep learning approaches to Natural Language Processing (NLP) have made considerable advances, outperforming traditional statistical and rule-based systems on a variety of tasks. In this survey paper, we summarize current neural NLP methods for EHR applications. We focus on a broad scope of tasks, namely, classification and prediction, word embeddings, extraction, generation, and other topics such as question answering, phenotyping, knowledge graphs, medical dialogue, multilinguality, interpretability, etc.}
}
@article{TIAN2022100159,
title = {Knowledge graph and knowledge reasoning: A systematic review},
journal = {Journal of Electronic Science and Technology},
volume = {20},
number = {2},
pages = {100159},
year = {2022},
issn = {1674-862X},
doi = {https://doi.org/10.1016/j.jnlest.2022.100159},
url = {https://www.sciencedirect.com/science/article/pii/S1674862X2200012X},
author = {Ling Tian and Xue Zhou and Yan-Ping Wu and Wang-Tao Zhou and Jin-Hao Zhang and Tian-Shu Zhang},
keywords = {Knowledge graph (KG), Knowledge graph applications, Knowledge hypergraph, Knowledge reasoning},
abstract = {The knowledge graph (KG) that represents structural relations among entities has become an increasingly important research field for knowledge-driven artificial intelligence. In this survey, a comprehensive review of KG and KG reasoning is provided. It introduces an overview of KGs, including representation, storage, and essential technologies. Specifically, it summarizes several types of knowledge reasoning approaches, including logic rules-based, representation-based, and neural network-based methods. Moreover, this paper analyzes the representation methods of knowledge hypergraphs. To effectively model hyper-relational data and improve the performance of knowledge reasoning, a three-layer knowledge hypergraph model is proposed. Finally, it analyzes the advantages of three-layer knowledge hypergraphs through reasoning and update algorithms which could facilitate future research.}
}
@article{HASSAN2024200458,
title = {Design and implementation of EventsKG for situational monitoring and security intelligence in India: An open-source intelligence gathering approach},
journal = {Intelligent Systems with Applications},
volume = {24},
pages = {200458},
year = {2024},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2024.200458},
url = {https://www.sciencedirect.com/science/article/pii/S2667305324001327},
author = {Hashmy Hassan and Sudheep Elayidom and M.R. Irshad and Christophe Chesneau},
keywords = {Knowledge graphs, Ontology, Domain specific KGs, EventsKG, Open-source intelligence gathering},
abstract = {This paper presents a method to construct and implement an Events Knowledge Graph (EventsKG) for security-related open-source intelligence gathering, focusing on event exploration for situation monitoring in India. The EventsKG is designed to process news articles, extract events of national security significance, and represent them in a consistent and intuitive manner. This method utilizes state-of-the-art natural language understanding techniques and the capabilities of graph databases to extract and organize events. A domain-specific ontology is created for effective storage and retrieval. In addition, we provide a user-friendly dashboard for querying and a complete visualization of events across India. The effectiveness of the EventsKG is assessed through a human evaluation of the information retrieval quality. Our approach contributes to rapid data availability and decision-making through a comprehensive understanding of events, including local events, from every part of India in just a few clicks. The system is evaluated against a manually annotated dataset and by involving human evaluators through a feedback survey, and it has shown good retrieval accuracy. The EventsKG can also be used for other applications such as threat intelligence, incident response, and situational awareness.}
}
@article{TANG2023101426,
title = {Construction and application of an ontology-based domain-specific knowledge graph for petroleum exploration and development},
journal = {Geoscience Frontiers},
volume = {14},
number = {5},
pages = {101426},
year = {2023},
issn = {1674-9871},
doi = {https://doi.org/10.1016/j.gsf.2022.101426},
url = {https://www.sciencedirect.com/science/article/pii/S1674987122000792},
author = {Xianming Tang and Zhiqiang Feng and Yitian Xiao and Ming Wang and Tianrui Ye and Yujie Zhou and Jin Meng and Baosen Zhang and Dongwei Zhang},
keywords = {Knowledge Graph, Petroleum exploration and development, Natural language processing, Ontology},
abstract = {The massive amount and multi-sourced, multi-structured data in the upstream petroleum industry impose great challenge on data integration and smart application. Knowledge graph, as an emerging technology, can potentially provide a way to tackle the challenges associated with oil and gas big data. This paper proposes an engineering-based method that can improve upon traditional natural language processing to construct the domain knowledge graph based on a petroleum exploration and development ontology. The exploration and development knowledge graph is constructed by assembling Sinopec’s multi-sourced heterogeneous database, and millions of nodes. The two applications based on the constructed knowledge graph are developed and validated for effectiveness and advantages in providing better knowledge services for the oil and gas industry.}
}
@article{DEGIORGIS2025104127,
title = {Neurosymbolic graph enrichment for Grounded World Models},
journal = {Information Processing & Management},
volume = {62},
number = {4},
pages = {104127},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2025.104127},
url = {https://www.sciencedirect.com/science/article/pii/S030645732500069X},
author = {Stefano {De Giorgis} and Aldo Gangemi and Alessandro Russo},
keywords = {Neurosymbolic AI, Knowledge representation, Knowledge extraction, Large language models, Graph KAG, Hybrid reasoning},
abstract = {The development of artificial intelligence systems capable of understanding and reasoning about complex real-world scenarios is a significant challenge. In this work we present a novel approach to enhance and exploit LLM reactive capability to address complex problems and interpret deeply contextual real-world meaning. We introduce a method and a tool for creating a multimodal, knowledge-augmented formal representation of meaning that combines the strengths of large language models with structured semantic representations. Our method begins with an image input, utilizing state-of-the-art large language models to generate a natural language description. This description is then transformed into an Meaning Representation (AMR) graph, which is formalized and enriched with logical design patterns, and layered semantics derived from linguistic and factual knowledge bases. The resulting graph is then fed back into the LLM to be extended with implicit knowledge activated by complex heuristic learning, including semantic implicatures, moral values, embodied cognition, and metaphorical representations. By bridging the gap between unstructured language models and formal semantic structures, our method opens new avenues for tackling intricate problems in natural language understanding and reasoning.}
}
@article{ZHU2023107262,
title = {RDKG-115: Assisting drug repurposing and discovery for rare diseases by trimodal knowledge graph embedding},
journal = {Computers in Biology and Medicine},
volume = {164},
pages = {107262},
year = {2023},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2023.107262},
url = {https://www.sciencedirect.com/science/article/pii/S0010482523007278},
author = {Chaoyu Zhu and Xiaoqiong Xia and Nan Li and Fan Zhong and Zhihao Yang and Lei Liu},
keywords = {Rare disease, Drug repurposing, Knowledge graph reasoning, Multimodal data fusion, Clinical trials},
abstract = {Rare diseases (RDs) may affect individuals in small numbers, but they have a significant impact on a global scale. Accurate diagnosis of RDs is challenging, and there is a severe lack of drugs available for treatment. Pharmaceutical companies have shown a preference for drug repurposing from existing drugs developed for other diseases due to the high investment, high risk, and long cycle involved in RD drug development. Compared to traditional approaches, knowledge graph embedding (KGE) based methods are more efficient and convenient, as they treat drug repurposing as a link prediction task. KGE models allow for the enrichment of existing knowledge by incorporating multimodal information from various sources. In this study, we constructed RDKG-115, a rare disease knowledge graph involving 115 RDs, composed of 35,643 entities, 25 relations, and 5,539,839 refined triplets, based on 372,384 high-quality literature and 4 biomedical datasets: DRKG, Pathway Commons, PharmKG, and PMapp. Subsequently, we developed a trimodal KGE model containing structure, category, and description embeddings using reverse-hyperplane projection. We utilized this model to infer 4199 reliable new inferred triplets from RDKG-115. Finally, we calculated potential drugs and small molecules for each of the 115 RDs, taking multiple sclerosis as a case study. This study provides a paradigm for large-scale screening of drug repurposing and discovery for RDs, which will speed up the drug development process and ultimately benefit patients with RDs. The source code and data are available at https://github.com/ZhuChaoY/RDKG-115.}
}
@article{FETTACH2024107779,
title = {JobEdKG: An uncertain knowledge graph-based approach for recommending online courses and predicting in-demand skills based on career choices},
journal = {Engineering Applications of Artificial Intelligence},
volume = {131},
pages = {107779},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.107779},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623019632},
author = {Yousra Fettach and Adil Bahaj and Mounir Ghogho},
keywords = {Knowledge graphs, Skill need prediction, Link confidence prediction, Online courses, Representation learning},
abstract = {Modern job markets often require an intricate combination of multi-disciplinary skills or specialist and technical knowledge, even for entry-level positions. Such requirements pose increased pressure on students, new graduates, and job seekers to find suitable sources of information to enhance their employability. Unlike standard competence management systems, this paper presents JobEdKG, which helps job seekers from various backgrounds choose online courses based on their prospective careers in addition to predicting different skills that may be required in their chosen career path. While existing solutions focus on internal institutional data, such as previous student experiences and a fixed set of skills provided by curated datasets, JobEdKG considers external data, recommending online courses that best cover the knowledge and skills required by selected job roles, in addition to extracting and predicting skills of that particular job roles. To achieve this, we first extract skills from job postings and online courses. These skills are linked to job titles, online courses, and other concepts in order to create a knowledge graph (KG). We assign an uncertainty score to each fact in the KG based on the prevalence of the fact in the source data (i.e. job listings and online courses), which results in an uncertain KG (UKG). Finally, we model the constructed UKG in order to infer different relations between the different concepts. The code and the data are available on our GitHub repository (https://github.com/team611/JobEd) and a user interface to browse the KG is available at (http://jobed.datanets.org/).}
}
@article{WENG2022,
title = {Leveraging Representation Learning for the Construction and Application of a Knowledge Graph for Traditional Chinese Medicine: Framework Development Study},
journal = {JMIR Medical Informatics},
volume = {10},
number = {9},
year = {2022},
issn = {2291-9694},
doi = {https://doi.org/10.2196/38414},
url = {https://www.sciencedirect.com/science/article/pii/S2291969422002277},
author = {Heng Weng and Jielong Chen and Aihua Ou and Yingrong Lao},
keywords = {knowledge graph, knowledge embedding, traditional Chinese medicine, knowledge discovery, medicine, clinical, framework},
abstract = {Background
Knowledge discovery from treatment data records from Chinese physicians is a dramatic challenge in the application of artificial intelligence (AI) models to the research of traditional Chinese medicine (TCM).
Objective
This paper aims to construct a TCM knowledge graph (KG) from Chinese physicians and apply it to the decision-making related to diagnosis and treatment in TCM.
Methods
A new framework leveraging a representation learning method for TCM KG construction and application was designed. A transformer-based Contextualized Knowledge Graph Embedding (CoKE) model was applied to KG representation learning and knowledge distillation. Automatic identification and expansion of multihop relations were integrated with the CoKE model as a pipeline. Based on the framework, a TCM KG containing 59,882 entities (eg, diseases, symptoms, examinations, drugs), 17 relations, and 604,700 triples was constructed. The framework was validated through a link predication task.
Results
Experiments showed that the framework outperforms a set of baseline models in the link prediction task using the standard metrics mean reciprocal rank (MRR) and Hits@N. The knowledge graph embedding (KGE) multitagged TCM discriminative diagnosis metrics also indicated the improvement of our framework compared with the baseline models.
Conclusions
Experiments showed that the clinical KG representation learning and application framework is effective for knowledge discovery and decision-making assistance in diagnosis and treatment. Our framework shows superiority of application prospects in tasks such as KG-fused multimodal information diagnosis, KGE-based text classification, and knowledge inference–based medical question answering.}
}
@article{MANN2023108446,
title = {SUSIE: Pharmaceutical CMC ontology-based information extraction for drug development using machine learning},
journal = {Computers & Chemical Engineering},
volume = {179},
pages = {108446},
year = {2023},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2023.108446},
url = {https://www.sciencedirect.com/science/article/pii/S0098135423003162},
author = {Vipul Mann and Shekhar Viswanath and Shankar Vaidyaraman and Jeya Balakrishnan and Venkat Venkatasubramanian},
keywords = {Ontology, Pharmaceutical drug development, Information extraction, Hybrid machine learning, Chemistry manufacturing and control},
abstract = {Automatically extracting information from unstructured text in pharmaceutical documents is important for drug discovery and development. This information can be integrated with structured datasets to ultimately accelerate pharmaceutical product development. To this end, we report an end-to-end information extraction framework based on a custom-built pharmaceutical drug development ontology, a weak supervision framework, contextualization algorithms, and a fine-tuned BioBERT model (adaptation of BERT or Bidirectional Encoder Representations from Transformers for biomedical text). The proposed framework, SUSIE (Schema-based Unsupervised Semantic Information Extraction), was trained on ICH (International Conference on Harmonization) documents to identify important entities and relations from unstructured text and auto-generate knowledge graphs representing crucial information in a structured format. On the entity identification task, the framework achieves a test accuracy and F1-score of 96% and 88%, respectively, on out-of-sample documents. A major contribution of this work is to build an automated, unsupervised information extraction framework around a domain-specific, custom-built pharmaceutical drug development ontology without the need for manual curation of training datasets for specific tasks. The efficacy of the approach was tested on out-of-sample documents including an internal Eli Lilly technical document.}
}
@article{SUN2023110428,
title = {Document-level relation extraction with two-stage dynamic graph attention networks},
journal = {Knowledge-Based Systems},
volume = {267},
pages = {110428},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.110428},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123001788},
author = {Qi Sun and Kun Zhang and Kun Huang and Tiancheng Xu and Xun Li and Yaodi Liu},
keywords = {Document-level relation extraction, Graph attention networks, Dynamic graph, Two-stage framework, Pretrained language models},
abstract = {Document-level Relation Extraction (RE) aims to infer complex semantic relations between entities in a document. Previous approaches leverage a multi-classification model to predict relation types between each entity pair. However, in contrast to sentence-level RE, document-level RE contains various entities expressed by mentions appearing across multiple sentences in a document. Therefore, the amount of negative instances (‘no relationship’) significantly outnumbers that of other positive instances in document-level RE. In addition, most existing methods construct static graphs with heuristic rules to capture the interactions among entities. However, these heuristic rules ignore the specificities of the documents. In this study, we propose a novel two-stage framework to extract document-level relations based on dynamic graph attention networks, namely TDGAT. In the first stage, we capture the relational links of the entity pairs using a binary classification model. In the second stage, we extract fine-grained relations among entities, including the type of ‘NA (no relationship)’. To reduce error propagation, we regard the entity pair links predicted in the first stage as the prior information and leverage them to reconstruct the document-level graphs of the second stage. In this manner, we can provide extra head and tail entity connection information for predicting relations in a document. Furthermore, we propose a dynamic graph strategy to explore the multi-hop interactions between related information. The experimental results show that our framework outperforms most existing models on the public document-level dataset DocRED. The extensive analysis demonstrates the effectiveness of our TDGAT in extracting inter-sentence relations.}
}
@article{STYLIANOU2022,
title = {Doc2KG:},
journal = {International Journal on Semantic Web and Information Systems},
volume = {18},
number = {1},
year = {2022},
issn = {1552-6283},
doi = {https://doi.org/10.4018/IJSWIS.295552},
url = {https://www.sciencedirect.com/science/article/pii/S1552628322000424},
author = {Nikolaos Stylianou and Danai Vlachava and Ioannis Konstantinidis and Nick Bassiliades and Vassilios Peristeras},
keywords = {eGovernment, Government Portals, Linked Data, Machine Learning, Natural Language Processing, Open Data, Semantic Web},
abstract = {ABSTRACT
Document management systems (DMS) have been used for decades to store large amounts of information in textual form. Their technology paradigm is based on storing vast quantities of textual information enriched with metadata to support searchability. However, this exhibits limitations as it treats textual information as a black box and is based exclusively on user-created metadata, a process that suffers from quality and completeness shortcomings. The use of knowledge graphs in DMS can substantially improve searchability, providing the ability to link data and enabling semantic searching. Recent approaches focus on either creating knowledge graphs from document collections or updating existing ones. In this paper, the authors introduce Doc2KG (Document-to-Knowledge-Graph), an intelligent framework that handles both creation and real-time updating of a knowledge graph, while also exploiting domain-specific ontology standards. They use DIAVGEIA (clarity), an award-winning Greek open government portal, as the case-study and discuss new capabilities for the portal by implementing Doc2KG.}
}
@article{CUI2023103283,
title = {Reinforcement learning with dynamic completion for answering multi-hop questions over incomplete knowledge graph},
journal = {Information Processing & Management},
volume = {60},
number = {3},
pages = {103283},
year = {2023},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2023.103283},
url = {https://www.sciencedirect.com/science/article/pii/S0306457323000201},
author = {Hai Cui and Tao Peng and Ridong Han and Beibei Zhu and Haijia Bi and Lu Liu},
keywords = {Knowledge graph, Question answering, Reinforcement learning, Path-based reasoning, Dynamic completion},
abstract = {Text-enhanced and implicit reasoning methods are proposed for answering questions over incomplete knowledge graph (KG), whereas prior studies either rely on external resources or lack necessary interpretability. This article desires to extend the line of reinforcement learning (RL) methods for better interpretability and dynamically augment original KG action space with additional actions. To this end, we propose a RL framework along with a dynamic completion mechanism, namely Dynamic Completion Reasoning Network (DCRN). DCRN consists of an action space completion module and a policy network. The action space completion module exploits three sub-modules (relation selector, relation pruner and tail entity predictor) to enrich options for decision making. The policy network calculates probability distribution over joint action space and selects promising next-step actions. Simultaneously, we employ the beam search-based action selection strategy to alleviate delayed and sparse rewards. Extensive experiments conducted on WebQSP, CWQ and MetaQA demonstrate the effectiveness of DCRN. Specifically, under 50% KG setting, the Hits@1 performance improvements of DCRN on MetaQA-1H and MetaQA-3H are 2.94% and 1.18% respectively. Moreover, under 30% and 10% KG settings, DCRN prevails over all baselines by 0.9% and 1.5% on WebQSP, indicating the robustness to sparse KGs.}
}
@article{LIU2022572,
title = {Construction of well logging knowledge graph and intelligent identification method of hydrocarbon-bearing formation},
journal = {Petroleum Exploration and Development},
volume = {49},
number = {3},
pages = {572-585},
year = {2022},
issn = {1876-3804},
doi = {https://doi.org/10.1016/S1876-3804(22)60047-8},
url = {https://www.sciencedirect.com/science/article/pii/S1876380422600478},
author = {Guoqiang LIU and Renbin GONG and Yujiang SHI and Zhenzhen WANG and Lan MI and Chao YUAN and Jibin ZHONG},
keywords = {well logging, hydrocarbon bearing formation identification, knowledge graph, graph embedding technique, intelligent identification, neural network},
abstract = {Based on the well logging knowledge graph of hydrocarbon-bearing formation (HBF), a Knowledge-Powered Neural Network Formation Evaluation model (KPNFE) has been proposed. It has the following functions: (1) extracting characteristic parameters describing HBF in multiple dimensions and multiple scales; (2) showing the characteristic parameter-related entities, relationships, and attributes as vectors via graph embedding technique; (3) intelligently identifying HBF; (4) seamlessly integrating expertise into the intelligent computing to establish the assessment system and ranking algorithm for potential pay recommendation. Taking 547 wells encountered the low porosity and low permeability Chang 6 Member of Triassic in the Jiyuan Block of Ordos Basin, NW China as objects, 80% of the wells were randomly selected as the training dataset and the remainder as the validation dataset. The KPNFE prediction results on the validation dataset had a coincidence rate of 94.43% with the expert interpretation results and a coincidence rate of 84.38% for all the oil testing layers, which is 13 percentage points higher in accuracy and over 100 times faster than the primary conventional interpretation. In addition, a number of potential pays likely to produce industrial oil were recommended. The KPNFE model effectively inherits, carries forward and improves the expert knowledge, nicely solving the robustness problem in HBF identification. The KPNFE, with good interpretability and high accuracy of computation results, is a powerful technical means for efficient and high-quality well logging re-evaluation of old wells in mature oilfields.}
}
@incollection{NAGAR2025251,
title = {13 - An integrated framework for knowledge graphs based on battery management},
editor = {Rajesh Kumar Dhanaraj and M. Nalini and Malathy Sathyamoorthy and Manar Mohaisen},
booktitle = {Knowledge Graph-Based Methods for Automated Driving},
publisher = {Elsevier},
pages = {251-272},
year = {2025},
isbn = {978-0-443-30040-0},
doi = {https://doi.org/10.1016/B978-0-443-30040-0.00013-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780443300400000138},
author = {Khushboo Nagar and Ayesha Mandloi and Sumit Jain},
keywords = {Ontology, Knowledge graph, Battery management, Semantic modeling, Graph-based representation, Data integration, Ontological structuring, Graph-based querying mechanisms, Information retrieval, EV battery deployment},
abstract = {The proposed framework addresses the multifaceted challenges associated with EV battery management by consolidating diverse data sources into a unified knowledge graph. Utilizing semantic modeling and graph-based representations, the framework synthesizes fragmented data encompassing battery specifications, charging profiles, usage patterns, environmental conditions, and performance histories. Through ontological structuring and graph-based querying mechanisms, the framework facilitates seamless data retrieval and inference capabilities. By amalgamating information from various sources, including battery manufacturers’ specifications, vehicle telematics, and real-time battery health monitoring systems, the knowledge graph empowers informed decision-making in EV battery deployment, charging strategies, and predictive maintenance.}
}
@article{ZHANG2022108038,
title = {Association Rules Enhanced Knowledge Graph Attention Network},
journal = {Knowledge-Based Systems},
volume = {239},
pages = {108038},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.108038},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121011321},
author = {Zhenghao Zhang and Jianbin Huang and Qinglin Tan},
keywords = {Knowledge Graphs, Graph attention network, Association rules, Knowledge inference, High-order neighborhood, Embedding propagation},
abstract = {Embedding knowledge graphs into continuous vector spaces has recently attracted increasing interest in knowledge base completion. However, in most existing embedding methods, only fact triplets are utilized, and logical rules have not been thoroughly studied for the knowledge base completion task. To overcome the problem, we propose an association rules enhanced knowledge graph attention network (AR-KGAN). In this paper, triplets and logical rules are jointly modeled in the proposed unified framework to achieve more predictive entity and relation embeddings. Association rules and corresponding correlation degrees between them can be automatically obtained according to our designed mining algorithm. The major component of AR-KGAN is an encoder of an effective neighborhood aggregator, which addresses the problems by aggregating neighbors with both association rules based and graph based attention weights. The decoder enables AR-KGAN to be translational between entities and relations while keeping the superior link prediction performance. Then, the global loss is minimized over both atomic and complex formulas to achieve the embedding task. In this manner, we learn embeddings compatible with triplets and association rules, which are certainly more predictive for knowledge acquisition and inference. The results show that the proposed AR-KGAN model achieves significant and consistent improvements over state-of-the-art methods on three benchmark datasets.}
}
@article{SUI2022108943,
title = {Causality-aware Enhanced Model for Multi-hop Question Answering over Knowledge Graphs},
journal = {Knowledge-Based Systems},
volume = {250},
pages = {108943},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.108943},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122004567},
author = {Yuan Sui and Shanshan Feng and Huaxiang Zhang and Jian Cao and Liang Hu and Nengjun Zhu},
keywords = {Knowledge graph-based question answering, Causal representation learning, Constraint pairwise-based clustering, Knowledge graph embedding, Confounding bias},
abstract = {To improve the performance of knowledge graph-based question answering system (KGQA), several approaches have been developed to construct a semantic parser based on entity linking, relation identification and logical/numerical structure identification. However, existing methods arrive at answers only by maximizing the data likelihood only on the sparse or imbalanced explicit relations, ignoring the potentially large number of latent relations. It makes KGQA suffer from a high level of spurious entity relations and missing link challenge. In this paper, we propose a causal filter (CF) model for KGQA (CF-KGQA), which performs causal interference on the relation representation space to reduce the spurious relation representation in a data-driven manner, i.e., the goal of this work is to comprehensively discover disentangled latent factors to alleviate the spurious correlation problem in KGQA. The model comprises a causal pairwise aggregator (AP) and a disentangled latent factor aggregator (AC). The former filters out most spurious entity relations inconsistent to their dense groups’ neighborhood, and generates a causal pairwise matrix among all the candidate relations. The latter learns the latent relation representation via an encoder–decoder on the causal pairwise matrix. It disconnects the latent factor and the causal confounder beneath the knowledge embedding space by causal intervention. To prove the effectiveness and efficiency of the proposed approach, we test CF-KGQA and other state-of-the-art methods on four public real-world datasets. The experiments indicate that our approach outperforms the recent methods and is also less sensitive to the spurious correlation problem, thus demonstrating the robustness of CF-KGQA.}
}
@article{FANG2023118806,
title = {Learning knowledge graph embedding with a dual-attention embedding network},
journal = {Expert Systems with Applications},
volume = {212},
pages = {118806},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118806},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422018243},
author = {Haichuan Fang and Youwei Wang and Zhen Tian and Yangdong Ye},
keywords = {Knowledge graph embedding, Knowledge graph, Graph convolutional network, Representation learning, Attention mechanism},
abstract = {Knowledge Graph Embedding (KGE) aims to retain the intrinsic structural information of knowledge graphs (KGs) via representation learning, which is critical for various downstream tasks including personalized recommendations, intelligent search, and relation extraction. The graph convolutional network (GCN), due to its remarkable performance in modeling graph data, has recently been studied extensively in the KGE field. However, when learning entity representations, most attention-based GCN approaches treat neighborhoods as a whole to measure their importance without considering the direction information of relations. Additionally, these approaches make relation representations perform self-update via a learnable matrix, resulting in ignoring the impact of neighborhood information on representation learning of relations. To this end, this study presents an innovative framework, namely learning knowledge graph embedding with a dual-attention embedding network (D-AEN), to jointly propagate and update the representations of both relations and entities via fusing neighborhood information. Here the dual attentions consist of a bidirectional attention mechanism and a relation-specific attention mechanism for jointly measuring the importance of neighborhoods in respectively learning entity and relation representations. Thus D-AEN enables elements like relations and entities to interact well semantically, which makes their learned representations retain more effective information of KGs. Extensive experimental results on three standard link prediction datasets demonstrate the superiority of D-AEN over several state-of-the-art approaches.}
}
@article{TRAJANOV2023714,
title = {Review of Natural Language Processing in Pharmacology},
journal = {Pharmacological Reviews},
volume = {75},
number = {4},
pages = {714-738},
year = {2023},
issn = {0031-6997},
doi = {https://doi.org/10.1124/pharmrev.122.000715},
url = {https://www.sciencedirect.com/science/article/pii/S0031699724007762},
author = {Dimitar Trajanov and Vangel Trajkovski and Makedonka Dimitrieva and Jovana Dobreva and Milos Jovanovik and Matej Klemen and Aleš Žagar and Marko Robnik-Šikonja},
abstract = {Natural language processing (NLP) is an area of artificial intelligence that applies information technologies to process the human language, understand it to a certain degree, and use it in various applications. This area has rapidly developed in the past few years and now employs modern variants of deep neural networks to extract relevant patterns from large text corpora. The main objective of this work is to survey the recent use of NLP in the field of pharmacology. As our work shows, NLP is a highly relevant information extraction and processing approach for pharmacology. It has been used extensively, from intelligent searches through thousands of medical documents to finding traces of adversarial drug interactions in social media. We split our coverage into five categories to survey modern NLP: methodology, commonly addressed tasks, relevant textual data, knowledge bases, and useful programming libraries. We split each of the five categories into appropriate subcategories, describe their main properties and ideas, and summarize them in a tabular form. The resulting survey presents a comprehensive overview of the area, useful to practitioners and interested observers.
Significance Statement
The main objective of this work is to survey the recent use of NLP in the field of pharmacology in order to provide a comprehensive overview of the current state in the area after the rapid developments that occurred in the past few years. The resulting survey will be useful to practitioners and interested observers in the domain.}
}
@article{YANG2023102204,
title = {BT-CKBQA: An efficient approach for Chinese knowledge base question answering},
journal = {Data & Knowledge Engineering},
volume = {147},
pages = {102204},
year = {2023},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2023.102204},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X23000642},
author = {Erhe Yang and Fei Hao and Jiaxing Shang and Xiaoliang Chen and Doo-Soon Park},
keywords = {Knowledge base question answering, BM25, Predicate mapping, Knowledge base, COVID-19},
abstract = {Knowledge Base Question Answering (KBQA), as an increasingly essential application, can provide accurate responses to user queries. ensuring that users obtain relevant information and make decisions promptly. The deep learning-based approaches have achieved satisfactory QA results by leveraging the neural network models. However, these approaches require numerous parameters, which increases the workload of tuning model parameters. To address this problem, we propose BT-CKBQA, a practical and highly efficient approach incorporating BM25 and Template-based predicate mapping for CKBQA. Besides, a concept lattice based approach is proposed for summarizing the knowledge base, which can largely improve the execution efficiency of QA with little loss of performance. Concretely, BT-CKBQA leverages the BM25 algorithm and custom dictionary to detect the subject of a question sentence. A template-based predicate generation approach is then proposed to generate candidate predicates. Finally, a ranking approach is provided with the joint consideration of character similarity and semantic similarity for predicate mapping. Extensive experiments are conducted over the NLPCC-ICCPOL 2016 and 2018 KBQA datasets, and the experimental results demonstrate the superiority of the proposed approach over the compared baselines. Particularly, the averaged F1-score result of BT-CKBQA for mention detection is up to 98.25%, which outperforms the best method currently available in the literature. For question answering, the proposed approach achieves superior results than most baselines with the F1-score value of 82.68%. Compared to state-of-the-art baselines, the execution efficiency and performance of QA per unit time can be improved with up to 56.39% and 44.06% gains, respectively. The experimental results for the diversification of questions indicate that the proposed approach performs better for diversified questions than domain-specific questions. The case study over a constructed COVID-19 knowledge base illustrates the effectiveness and practicability of BT-CKBQA.}
}
@article{ZENG2025112837,
title = {KoSEL: Knowledge subgraph enhanced large language model for medical question answering},
journal = {Knowledge-Based Systems},
volume = {309},
pages = {112837},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112837},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124014710},
author = {Zefan Zeng and Qing Cheng and Xingchen Hu and Yan Zhuang and Xinwang Liu and Kunlun He and Zhong Liu},
keywords = {Domain-specific, Knowledge graph, Large language model, Medical question answering, Privacy, Retrieval},
abstract = {The integration of medical knowledge graphs (KGs) and large language models (LLMs) for medical question answering (Q&A) has attracted considerable interest in recent studies. However, current approaches that combine KGs and LLMs tend to either integrate KGs directly into the fine-tuning process of LLMs or use entire KGs as a contextual prompt base for LLMs to reason, raising concerns regarding potential data leakage and reasoning confusion. In this study, we propose KoSEL (Knowledge Subgraph Enhanced Large Language Model), a novel medical Q&A framework based on KG-enhanced LLMs. KoSEL comprises two modules: Knowledge Retrieval (KR) and Reasoning and Answering (RA). The KR module is LLM-independent and employs an entity-linking algorithm and a subgraph construction and fusion strategy to retrieve question-relevant knowledge. The RA module conveys prompts to the LLM for information extraction, knowledge fusion, reasoning, and answer generation. KoSEL, which is designed as a plug-and-play framework, effectively fuses structural and textual knowledge while ensuring efficiency and privacy. The construction of a precise and refined subgraph reduces knowledge noise and the number of input graph tokens, thus mitigating hallucination issues. Extensive experiments demonstrated that KoSEL outperformed advanced methods in terms of knowledge retrieval efficiency (20.27% reduction in retrieval time), knowledge utilization (15.16% increase in utilization rate), and data protection (113.50% reduction in data leakage rate), resulting in higher-quality answers for medical Q&A tasks (1.50% improvement in answer score).}
}
@article{HUR2024122269,
title = {Unifying context with labeled property graph: A pipeline-based system for comprehensive text representation in NLP},
journal = {Expert Systems with Applications},
volume = {239},
pages = {122269},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122269},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423027719},
author = {Ali Hur and Naeem Janjua and Mohiuddin Ahmed},
keywords = {Natural language processing, Graph based NLP, Text representation},
abstract = {Extracting valuable insights from vast amounts of unstructured digital text presents significant challenges across diverse domains. This research addresses this challenge by proposing a novel pipeline-based system that generates domain-agnostic and task-agnostic text representations. The proposed approach leverages labeled property graphs (LPG) to encode contextual information, facilitating the integration of diverse linguistic elements into a unified representation. The proposed system enables efficient graph-based querying and manipulation by addressing the crucial aspect of comprehensive context modeling and fine-grained semantics. The effectiveness of the proposed system is demonstrated through the implementation of NLP components that operate on LPG-based representations. Additionally, the proposed approach introduces specialized patterns and algorithms to enhance specific NLP tasks, including nominal mention detection, named entity disambiguation, event enrichments, event participant detection, and temporal link detection. The evaluation of the proposed approach, using the MEANTIME corpus comprising manually annotated documents, provides encouraging results and valuable insights into the system's strengths. The proposed pipeline-based framework serves as a solid foundation for future research, aiming to refine and optimize LPG-based graph structures to generate comprehensive and semantically rich text representations, addressing the challenges associated with efficient information extraction and analysis in NLP.}
}
@article{CHEN2022109576,
title = {Staged query graph generation based on answer type for question answering over knowledge base},
journal = {Knowledge-Based Systems},
volume = {253},
pages = {109576},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.109576},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122007948},
author = {Haoyuan Chen and Fei Ye and Yuankai Fan and Zhenying He and Yinan Jing and Kai Zhang and X. Sean Wang},
keywords = {Knowledge base, Question answering, Semantic parsing, SPARQL, RDF},
abstract = {Question answering over knowledge base (KBQA) enables users to query over the knowledge base without the need to know the details. A range of existing KBQA approaches treats the entities mentioned in the given question as the starting point to find the answers. While helpful in achieving improvements on the existing benchmarks, they have some limitations on the strategy of query graph generation, which creates too many candidate queries and makes it hard to select the best-matching one to get the answer. In this paper, we propose a staged query graph generation approach based on the answer type, which exploits the correlation between questions and answer types to reduce the size of the candidate set and further improve the performance. Besides, we construct a question/answer-type (QAT) dataset aiming to predict the answer type of a given question. Extensive experiments demonstrate our method outperforms existing methods on both simple questions and complex questions.}
}
@article{ELMASHAD2022101641,
title = {Automatic creation of a 3D cartoon from natural language story},
journal = {Ain Shams Engineering Journal},
volume = {13},
number = {3},
pages = {101641},
year = {2022},
issn = {2090-4479},
doi = {https://doi.org/10.1016/j.asej.2021.11.010},
url = {https://www.sciencedirect.com/science/article/pii/S2090447921004184},
author = {Shady Y. El-Mashad and El-Hussein S. Hamed},
keywords = {Computer graphics, Natural language processing, 3D cartoon, Story visualization},
abstract = {The automatic creation of 3D animation from natural language text is used in many fields. The main target of this paper is to produce a 3D cartoon from a text input. Therefore, we need to analyze the input corpus to extract useful information by employing theories and tools from linguistics and natural language processing in addition to computer graphics for human language visualization. The system operates through two phases. The NLP phase, in which input text passes first through a coreference resolution solver in order to remove pronouns and substitute them with their corresponding nouns followed by a dependency parser in order to detect subject-action-object (SAO) relations in the resolved text. The sequence of SAOs resulting from the NLP phase is passed to the graphics phase. In the graphics phase a 3D animated video cartoon is generated by visualizing each SAO extracted in the NLP phase and Storytelling using the Unity game engine platform. The main contribution of this work is that the input does not have to be a screenplay. It is also demonstrated that performing coreference resolution before dependency parsing resulted in a more compact sequence of SAOs.}
}
@article{MIN2022100484,
title = {Applications of knowledge graphs for food science and industry},
journal = {Patterns},
volume = {3},
number = {5},
pages = {100484},
year = {2022},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2022.100484},
url = {https://www.sciencedirect.com/science/article/pii/S2666389922000691},
author = {Weiqing Min and Chunlin Liu and Leyi Xu and Shuqiang Jiang},
keywords = {knowledge graph, artificial intelligence, ontology, food science and industry, nutrition and health, new recipe development, food analysis},
abstract = {Summary
The deployment of various networks (e.g., Internet of Things [IoT] and mobile networks), databases (e.g., nutrition tables and food compositional databases), and social media (e.g., Instagram and Twitter) generates huge amounts of food data, which present researchers with an unprecedented opportunity to study various problems and applications in food science and industry via data-driven computational methods. However, these multi-source heterogeneous food data appear as information silos, leading to difficulty in fully exploiting these food data. The knowledge graph provides a unified and standardized conceptual terminology in a structured form, and thus can effectively organize these food data to benefit various applications. In this review, we provide a brief introduction to knowledge graphs and the evolution of food knowledge organization mainly from food ontology to food knowledge graphs. We then summarize seven representative applications of food knowledge graphs, such as new recipe development, diet-disease correlation discovery, and personalized dietary recommendation. We also discuss future directions in this field, such as multimodal food knowledge graph construction and food knowledge graphs for human health.}
}
@article{XUE2025105525,
title = {How to realize the knowledge reuse and sharing from accident reports? A knowledge-driven modeling method combining ontology and deep learning},
journal = {Journal of Loss Prevention in the Process Industries},
volume = {94},
pages = {105525},
year = {2025},
issn = {0950-4230},
doi = {https://doi.org/10.1016/j.jlp.2024.105525},
url = {https://www.sciencedirect.com/science/article/pii/S0950423024002833},
author = {Nannan Xue and Wei Zhang and Huayu Zhong and Wenbin Liao and Tingsheng Zhao},
keywords = {Process safety, Knowledge graph, Ontology design, Joint extraction model, Knowledge application},
abstract = {The exploration and understanding of past accidents are of great significance in enhancing the process safety. However, manually reading and analyzing a large number of accident reports is a time-consuming and inefficient task. In this study, a novel modeling method is developed to build the knowledge graph of process safety accidents, aiming to overcome the problem of knowledge reuse and sharing. Firstly, the dataset consists of 409 process safety accident reports selected from the official website of the Ministry of Emergency Management of China. Secondly, the ontology design schema is defined based on the seven-step method, including 34 ontology classes and 11 relations. Then, a new joint extraction model for the process domain is proposed based on the CasRel framework, which achieves 95.85% in precision, 61.54% in recall, and 74.95% in F1-score. Finally, the knowledge graph containing 9192 nodes and 11,257 edges is constructed in the Neo4j graph database, followed by the discussion of various related applications such as query, statistics, and analysis. The results indicate that the proposed method is a useful tool for obtaining valuable knowledge from accident reports, contributing to analysis and prevention of accidents.}
}
@article{BELLOMARINI2022407,
title = {Data science with Vadalog: Knowledge Graphs with machine learning and reasoning in practice},
journal = {Future Generation Computer Systems},
volume = {129},
pages = {407-422},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.10.021},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21004179},
author = {Luigi Bellomarini and Ruslan R. Fayzrakhmanov and Georg Gottlob and Andrey Kravchenko and Eleonora Laurenza and Yavor Nenov and Stéphane Reissfelder and Emanuel Sallinger and Evgeny Sherkhonov and Sahar Vahdati and Lianlong Wu},
keywords = {Knowledge Graphs, Data science, Machine learning, Reasoning, Probabilistic reasoning},
abstract = {Following the recent successful examples of large technology companies, many modern enterprises seek to build Knowledge Graphs to provide a unified view of corporate knowledge, and to draw deep insights using machine learning and logical reasoning. There is currently a perceived disconnect between the traditional approaches for data science, typically based on machine learning and statistical modeling, and systems for reasoning with domain knowledge. In this paper, we demonstrate how to perform a broad spectrum of data science tasks in a unified Knowledge Graph environment. This includes data wrangling, complex logical and probabilistic reasoning, and machine learning. We base our work on the state-of-the-art Knowledge Graph Management System Vadalog, which delivers highly expressive and efficient logical reasoning and provides seamless integration with modern data science toolkits such as the Jupyter platform. We argue that this is a significant step forward towards practical, holistic data science workflows that combine machine learning and reasoning in data science.}
}
@article{YANG2025,
title = {The Financial Institution Text Data Mining and Value Analysis Model Based on Big Data and Natural Language Processing},
journal = {Journal of Organizational and End User Computing},
volume = {37},
number = {1},
year = {2025},
issn = {1546-2234},
doi = {https://doi.org/10.4018/JOEUC.374213},
url = {https://www.sciencedirect.com/science/article/pii/S1546223425000267},
author = {Juan Yang and Yu Bai and Jie Gong and Menghui Han},
keywords = {Financial Text Mining, Multi-task Learning, Temporal Graph Convolutional Networks, Knowledge Graph, Value Prediction},
abstract = {ABSTRACT
Financial markets are inherently complex and influenced by a variety of factors, making it challenging to predict trends and detect key events. Traditional models often struggle to integrate both structured, or numerical, and unstructured, or textual, data; additionally, they fail to capture temporal dependencies or the dynamic relationships between financial entities. To address this, the multidimensional integrated model for financial text mining and value analysis (MI-FinText), was proposed. MI-FinText integrated multi-task learning, temporal graph convolutional networks and dynamic knowledge graph construction. MI-FinText simultaneously performed sentiment analysis, event detection, and value prediction by learning shared representations across tasks and modeling time-dependent relationships between financial events. MI-FinText continuously updated a dynamic knowledge graph to reflect the evolving financial landscape, enabling real-time insights.}
}
@article{BOLUCU2024103857,
title = {An adaptive approach to noisy annotations in scientific information extraction},
journal = {Information Processing & Management},
volume = {61},
number = {6},
pages = {103857},
year = {2024},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103857},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324002164},
author = {Necva Bölücü and Maciej Rybinski and Xiang Dai and Stephen Wan},
keywords = {Information extraction, Dataset, Mislabelled, Noisy, Weighted weakly supervised learning, Scientific},
abstract = {Despite recent advances in large language models (LLMs), the best effectiveness in information extraction (IE) is still achieved by fine-tuned models, hence the need for manually annotated datasets to train them. However, collecting human annotations for IE, especially for scientific IE, where expert annotators are often required, is expensive and time-consuming. Another issue widely discussed in the IE community is noisy annotations. Mislabelled training samples can hamper the effectiveness of trained models. In this paper, we propose a solution to alleviate problems originating from the high cost and difficulty of the annotation process. Our method distinguishes clean training samples from noisy samples and then employs weighted weakly supervised learning (WWSL) to leverage noisy annotations. Evaluation of Named Entity Recognition (NER) and Relation Classification (RC) tasks in Scientific IE demonstrates the substantial impact of detecting clean samples. Experimental results highlight that our method, utilising clean and noisy samples with WWSL, outperforms the baseline RoBERTa on NER (＋4.28, ＋4.59, ＋29.27, and ＋5.21 gain for the ADE, SciERC, STEM-ECR, and WLPC datasets, respectively) and the RC (＋6.09 and ＋4.39 gain for the SciERC and WLPC datasets, respectively) tasks. Comprehensive analyses of our method reveal its advantages over state-of-the-art denoising baseline models in scientific NER. Moreover, the framework is general enough to be adapted to different NLP tasks or domains, which means it could be useful in the broader NLP community.}
}
@article{WANG2025110465,
title = {A medical information extraction model with contrastive tuning and tagging layer training},
journal = {Computers in Biology and Medicine},
volume = {193},
pages = {110465},
year = {2025},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2025.110465},
url = {https://www.sciencedirect.com/science/article/pii/S0010482525008169},
author = {Xiaowei Wang},
keywords = {Medical information extraction, Medical entity recognition, Contrastive learning},
abstract = {Medical information extraction, as a core task in medical intelligent systems, focuses on extracting necessary structured information from clinical texts. In recent years, deep learning-based methods have become mainstream and often achieve superior extraction results. However, these existing methods have not fully tapped into the semantic potential of medical information categories, and most rely on a large amount of annotated data. This study proposes a novel semantic guided representation training model for medical information, which trains the representation of medical texts and medical information categories in the same semantic space by contrasting loss mechanisms, effectively reducing the need for annotated data. The experimental results show that our method objectives F1 value of 88.29 on CCKS2019 and 90.68 on CMeEE. Our method also exceeds the baseline by 4.07 on CCKS2019 and 4.95 on CMeEE.}
}
@article{WANG2023190,
title = {Fusing external knowledge resources for natural language understanding techniques: A survey},
journal = {Information Fusion},
volume = {92},
pages = {190-204},
year = {2023},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2022.11.025},
url = {https://www.sciencedirect.com/science/article/pii/S1566253522002354},
author = {Yuqi Wang and Wei Wang and Qi Chen and Kaizhu Huang and Anh Nguyen and Suparna De and Amir Hussain},
keywords = {Natural language understanding, Knowledge graph, Knowledge fusion, Representation learning, Deep learning},
abstract = {Knowledge resources, e.g. knowledge graphs, which formally represent essential semantics and information for logic inference and reasoning, can compensate for the unawareness nature of many natural language processing techniques based on deep neural networks. This paper provides a focused review of the emerging but intriguing topic that fuses quality external knowledge resources in improving the performance of natural language processing tasks. Existing methods and techniques are summarised in three main categories: (1) static word embeddings, (2) sentence-level deep learning models, and (3) contextualised language representation models, depending on when, how and where external knowledge is fused into the underlying learning models. We focus on the solutions to mitigate two issues: knowledge inclusion and inconsistency between language and knowledge. Details on the design of each representative method, as well as their strength and limitation, are discussed. We also point out some potential future directions in view of the latest trends in natural language processing research.}
}
@article{CHEN2022182,
title = {A pattern-first pipeline approach for entity and relation extraction},
journal = {Neurocomputing},
volume = {494},
pages = {182-191},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.04.059},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222004489},
author = {Zheng Chen and Changyu Guo},
keywords = {Information extraction, Named entity recognition, Relation extraction, Machine reading comprehension, Question answering},
abstract = {Entity-relation extraction is the task of extracting entities and their semantic relations from a piece of unstructured text. In recent studies, Machine Reading Comprehension (MRC) based methods have been applied to this task and achieved significant results. As a pipelined approach, these methods always extract head entities first, and then identify related tail entities by enumerating each relationship. These entity-first methods will lead to the entity redundancy problem. They also suffer from the error propagation issue, which is an inherent issue of the multi-step inference process. Moreover, most existing MRC-based models, which use tagging-based methods for entity recognition, could not deal with overlapping entities. To address these, we propose Patti, a Pattern-First Pipeline Approach for Entity and Relation Extraction. Firstly, Patti leverages a novel MRC-based pattern classifier to identify relation patterns. Next, a span-based method was introduced to extract entities under the guidance of questions parameterized by the patterns yield in the first step. Finally, to alleviate the error propagation issue, Patti employs an additional MRC-based classifier to remove falsely extracted candidate entity-relation triples. Experiment results show that our approach significantly outperforms the entity-first baseline models on CoNLL04 and ACE05 datasets.}
}
@article{LIU2022101515,
title = {A knowledge graph-based data representation approach for IIoT-enabled cognitive manufacturing},
journal = {Advanced Engineering Informatics},
volume = {51},
pages = {101515},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101515},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621002639},
author = {Mingfei Liu and Xinyu Li and Jie Li and Yahui Liu and Bin Zhou and Jinsong Bao},
keywords = {Cognitive manufacturing, Industrial Internet of Things, Knowledge graph, Cyber-Physical Production System, Data fusion},
abstract = {The Industrial Internet of Things (IIoT) interconnects a large number of interconnected sensors, actuators, and edge computing devices in the manufacturing systems, where the massive data collected in the manufacturing process has the characteristics of multi-dimensional, heterogeneous, and time series. An effective data representation manner, which can fuse such complex information and enable cognitive manufacturing decision-making from a global perspective, is necessary and challenging. To solve this issue, this paper proposes a knowledge graph-based data representation approach for IIoT-enabled cognitive manufacturing and applies it in a Cyber-Physical Production System (CPPS) scenario. Based on the digital thread of manufacturing process data, a multi-layer manufacturing knowledge graph is established, including device sensing data, production processing data, and business processing data. With the established knowledge graph, a cognition-driven approach is proposed with a perception-cognition dual system, which achieves perception analysis and cognition decision-making in the resource allocation of the manufacturing process. Finally, responding to the orders of personalized products in a workshop is taken as an illustrative example. The performance of allocating resources of workshop devices under dynamic demand changes shows the advantages of the proposed approach. The proposed manner will lay the foundation for a human-like cognition for processing massive real-time industrial information in CPPS, thus paving a pathway towards the era of cognitive manufacturing.}
}
@article{JIANG2025109796,
title = {A two-stage framework for pig disease knowledge graph fusing},
journal = {Computers and Electronics in Agriculture},
volume = {229},
pages = {109796},
year = {2025},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2024.109796},
url = {https://www.sciencedirect.com/science/article/pii/S0168169924011876},
author = {Tingting Jiang and Zhiyi Zhang and Shunxin Hu and Shuai Yang and Jin He and Chao Wang and Lichuan Gu},
keywords = {Pig disease knowledge graph, Knowledge graph embedding, Data augmentation, Entity alignment},
abstract = {Pig disease knowledge graphs (KGs) are crucial for the prevention and treatment of pig diseases. Due to the difficulty of knowledge mining in the field of traditional animal husbandry, there is a lack of high-quality KGs of pig diseases. To tackle this issue, a novel two-stage framework for pig disease KG fusing is proposed in this manuscript. In the first stage, a multi-view augmentation method for pig disease KGs is designed. The domain characteristics in the field of pig disease are considered and four valid strategies are utilized for augmenting triples, which not only enriches the pig disease KGs and provides abundant training data for KG embedding. In the second stage, an unsupervised entity alignment method is introduced to match entities. Importantly, the similarities of entity name, relation, attribute, and structure information are learned alternatively to avoid annotating data manually. Extensive experiments on the pig disease datasets and the public dataset MED_BBK_9K demonstrate that the proposed method can achieve state-of-the-art performance, i.e., the multi-view augmentation method improves hits@1 by 0.387 compared with the suboptimal model on the Pig1 dataset, and the entity alignment model outperforms the second-best model by 0.168 in terms of hits@1 on the Pig1_Pig2 dataset.}
}
@article{BELALTA2024102308,
title = {A graph based named entity disambiguation using clique partitioning and semantic relatedness},
journal = {Data & Knowledge Engineering},
volume = {152},
pages = {102308},
year = {2024},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2024.102308},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X24000326},
author = {Ramla Belalta and Mouhoub Belazzoug and Farid Meziane},
keywords = {Named entity disambiguation, Clique partitioning, Semantic relatedness, Graph-based approaches},
abstract = {Disambiguating name mentions in texts is a crucial task in Natural Language Processing, especially in entity linking. The credibility and efficiency of such systems depend largely on this task. For a given name entity mention in a text, there are many potential candidate entities that may refer to it in the knowledge base. Therefore, it is very difficult to assign the correct candidate from the whole set of candidate entities of this mention. To solve this problem, collective entity disambiguation is a prominent approach. In this paper, we present a novel algorithm called CPSR for collective entity disambiguation, which is based on a graph approach and semantic relatedness. A clique partitioning algorithm is used to find the best clique that contains a set of candidate entities. These candidate entities provide the answers to the corresponding mentions in the disambiguation process. To evaluate our algorithm, we carried out a series of experiments on seven well-known datasets, namely, AIDA/CoNLL2003-TestB, IITB, MSNBC, AQUAINT, ACE2004, Cweb, and Wiki. The Kensho Derived Wikimedia Dataset (KDWD) is used as the knowledge base for our system. From the experimental results, our CPSR algorithm outperforms both the baselines and other well-known state-of-the-art approaches.}
}
@article{WANG2023103511,
title = {Deep purified feature mining model for joint named entity recognition and relation extraction},
journal = {Information Processing & Management},
volume = {60},
number = {6},
pages = {103511},
year = {2023},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2023.103511},
url = {https://www.sciencedirect.com/science/article/pii/S0306457323002480},
author = {Youwei Wang and Ying Wang and Zhongchuan Sun and Yinghao Li and Shizhe Hu and Yangdong Ye},
keywords = {Named entity recognition, Relation extraction, Purified features, Information bottleneck},
abstract = {Table filling based joint named entity recognition and relation extraction task aims to share representation of subtasks in a table to extract structured knowledge. However, most of existing studies need additional labels and dedicated deep neural networks to learn shared representation, imposing heavy burdens to decoders. More seriously, almost all these models suffer from feature confusion problem, failing to capture purified task-specific features from shared representation to perform subtasks. To address these challenging problems, in this paper we propose a novel and effective Deep puRified fEAture Mining (DREAM) model for joint named entity recognition and relation extraction task, which can automatically capture purified task-specific features to improve the classification performance of subtasks. Specifically, unlike introducing additional labels or dedicated network architectures, we design a new lightweight shared representation learning (LSRL) module by the plainest labels of joint task and thus encodes context by the hybrid convolutional neural networks. Afterwards, a task-aware information bottleneck (TIB) module is proposed to explore the relation between the mutual information of the joint distribution of each subtask and its task-specific features. With the above two modules well obtain shared representation and purified task-specific features, the satisfactory classification results of both subtasks can be guaranteed. Experiment results show that the proposed model is highly effective, obtaining the promising results on three different benchmarks: CoNNL04 (general text), ADE (biomedical text) and SciERC (scientific text). For example, DREAM respectively achieves F1-scores of 78.18%, 80.28% and 44.60% in performing the relation extraction subtask on the CoNNL04, ADE and SciERC datasets. The promising performance indicates that the proposed model can be applied to many practical applications such as biomedical information extraction. The source code is publicly available at https://github.com/SWT-AITeam/DREAM.}
}
@article{WANG2024100566,
title = {IDS-KG: An industrial dataspace-based knowledge graph construction approach for smart maintenance},
journal = {Journal of Industrial Information Integration},
volume = {38},
pages = {100566},
year = {2024},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2024.100566},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X24000104},
author = {Yanying Wang and Ying Cheng and Qinglin Qi and Fei Tao},
keywords = {Knowledge graph, Industrial dataspace, Smart maintenance, Industrial knowledge management, BERT-casualKG},
abstract = {With the development of information technology in manufacturing enterprises, a large amount of equipment maintenance data and knowledge are recorded. These rich knowledge resources contain a vast amount of semantic and physical associations that have not yet been developed, resulting in a significant gap between equipment maintenance procedures and experiential knowledge. Therefore, this paper proposes a multi-source maintenance data management method called Industrial Dataspace (IDS), and on this basis, proposes a method for constructing an equipment maintenance knowledge graph (IDS-KG) that considers the causal relationships between faults in the equipment maintenance corpus. The method fixes procedural data on the ontology model at the upper layer of the knowledge graph and automatically mines maintenance information from empirical data, and ultimately achieves the fusion management of equipment maintenance procedure knowledge and empirical knowledge. The method is validated in the practical application of nuclear power equipment maintenance, and the experiments show that the method proposed in this paper is able to effectively fuse the procedural data and empirical data and structured as triplets, and at the same time, it is able to identify the hidden causal relationship between failures in the empirical data.}
}
@article{NAVEEN2024126,
title = {GeoNLU: Bridging the gap between natural language and spatial data infrastructures},
journal = {Alexandria Engineering Journal},
volume = {87},
pages = {126-147},
year = {2024},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2023.12.027},
url = {https://www.sciencedirect.com/science/article/pii/S1110016823011195},
author = {Palanichamy Naveen and Rajagopal Maheswar and Pavel Trojovský},
keywords = {GeoNLU, Natural language processing, Spatial data infrastructure, GeoSpatial data},
abstract = {Integrating natural language processing (NLP) techniques with spatial data infrastructures (SDIs) potentially revolutionize the way users interact with geospatial data. This article presents GeoNLU, a comprehensive framework aimed at bridging the gap between natural language and SDIs. GeoNLU aims to enable seamless interaction and querying of geospatial data through natural language, thereby enhancing accessibility and usability for a wide range of users. This article delves into the theoretical foundations, architectural design, key components, and potential applications of GeoNLU, highlighting its significance in improving geospatial data exploration, analysis, and decision-making.}
}
@article{XIE2024119115,
title = {Intelligent maritime question-answering and recommendation system based on maritime vessel activity knowledge graph},
journal = {Ocean Engineering},
volume = {312},
pages = {119115},
year = {2024},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2024.119115},
url = {https://www.sciencedirect.com/science/article/pii/S0029801824024533},
author = {Cunxiang Xie and Zhaogen Zhong and Limin Zhang},
keywords = {Maritime traffic management, Knowledge graph, Question answering system, Recommendation system, Graph neural networks},
abstract = {Traditional maritime traffic management typically relies on positioning data for data mining without incorporating other multi-source data to analyze the maritime vessel activity, which cannot conduct comprehensive maritime knowledge mining. Thus, this study integrates multi-source data, such as trajectory, maritime accident text, and geographic data, to create a maritime vessel activity knowledge graph. On this basis, a question-answering model is developed based on a bidirectional question-answering attention graph neural network, and a personalized recommendation model is developed based on an attention-enhanced joint knowledge propagation and a user preference graph neural network. The former assists users in extracting valuable information from the maritime vessel activity knowledge graph, while the latter predicts the users' potential interests and automatically recommends vessel entities based on their historical query information. Experimental results show that the proposed question-answering model improved the F1-score by 2.31%–10.09% compared to state-of-the-art baseline models on the MVA question-answering dataset. Similarly, the proposed personalized recommendation model improved the click-through rate prediction accuracy by 2.46%–7.05% compared to state-of-the-art baseline models on the MVA personalized recommendation dataset.}
}
@article{LAI2023104392,
title = {KEBLM: Knowledge-Enhanced Biomedical Language Models},
journal = {Journal of Biomedical Informatics},
volume = {143},
pages = {104392},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2023.104392},
url = {https://www.sciencedirect.com/science/article/pii/S1532046423001132},
author = {Tuan Manh Lai and ChengXiang Zhai and Heng Ji},
keywords = {Pre-trained language models, Knowledge bases, Domain knowledge},
abstract = {Pretrained language models (PLMs) have demonstrated strong performance on many natural language processing (NLP) tasks. Despite their great success, these PLMs are typically pretrained only on unstructured free texts without leveraging existing structured knowledge bases that are readily available for many domains, especially scientific domains. As a result, these PLMs may not achieve satisfactory performance on knowledge-intensive tasks such as biomedical NLP. Comprehending a complex biomedical document without domain-specific knowledge is challenging, even for humans. Inspired by this observation, we propose a general framework for incorporating various types of domain knowledge from multiple sources into biomedical PLMs. We encode domain knowledge using lightweight adapter modules, bottleneck feed-forward networks that are inserted into different locations of a backbone PLM. For each knowledge source of interest, we pretrain an adapter module to capture the knowledge in a self-supervised way. We design a wide range of self-supervised objectives to accommodate diverse types of knowledge, ranging from entity relations to description sentences. Once a set of pretrained adapters is available, we employ fusion layers to combine the knowledge encoded within these adapters for downstream tasks. Each fusion layer is a parameterized mixer of the available trained adapters that can identify and activate the most useful adapters for a given input. Our method diverges from prior work by including a knowledge consolidation phase, during which we teach the fusion layers to effectively combine knowledge from both the original PLM and newly-acquired external knowledge using a large collection of unannotated texts. After the consolidation phase, the complete knowledge-enhanced model can be fine-tuned for any downstream task of interest to achieve optimal performance. Extensive experiments on many biomedical NLP datasets show that our proposed framework consistently improves the performance of the underlying PLMs on various downstream tasks such as natural language inference, question answering, and entity linking. These results demonstrate the benefits of using multiple sources of external knowledge to enhance PLMs and the effectiveness of the framework for incorporating knowledge into PLMs. While primarily focused on the biomedical domain in this work, our framework is highly adaptable and can be easily applied to other domains, such as the bioenergy sector.}
}
@article{GALLOFREOCANA2023110750,
title = {A Software Reference Architecture for Journalistic Knowledge Platforms},
journal = {Knowledge-Based Systems},
volume = {276},
pages = {110750},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.110750},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123005002},
author = {Marc {Gallofré Ocaña} and Andreas L. Opdahl},
keywords = {Software reference architecture, Newsrooms, Knowledge graphs, Journalism, Artificial intelligence, Big data},
abstract = {Newsrooms and journalists today rely on many different artificial-intelligence, big-data and knowledge-based systems to support efficient and high-quality journalism. However, making the different systems work together remains a challenge, calling for new unified journalistic knowledge platforms. A software reference architecture for journalistic knowledge platforms could help news organisations by capturing tried-and-tested best practices and providing a generic blueprint for how their IT infrastructure should evolve. To the best of our knowledge, no suitable architecture has been proposed in the literature. Therefore, this article proposes a software reference architecture for integrating artificial intelligence and knowledge bases to support journalists and newsrooms. The design of the proposed architecture is grounded on the research literature and on our experiences with developing a series of prototypes in collaboration with industry. Our aim is to make it easier for news organisations to evolve their existing independent systems for news production towards integrated knowledge platforms and to direct further research. Because journalists and newsrooms are early adopters of integrated knowledge platforms, our proposal can hopefully also inform architectures in other domains with similar needs.}
}
@article{JIANG2024100331,
title = {APIE: An information extraction module designed based on the pipeline method},
journal = {Array},
volume = {21},
pages = {100331},
year = {2024},
issn = {2590-0056},
doi = {https://doi.org/10.1016/j.array.2023.100331},
url = {https://www.sciencedirect.com/science/article/pii/S2590005623000565},
author = {Xu Jiang and Yurong Cheng and Siyi Zhang and Juan Wang and Baoquan Ma},
keywords = {Knowledge graph, Information extraction, Named Entity Recognition, Relation extraction, Representation learning},
abstract = {Information extraction (IE) aims to discover and extract valuable information from unstructured text. This problem can be decomposed into two subtasks: named entity recognition (NER) and relation extraction (RE). Although the IE problem has been studied for years, most work efforts focused on jointly modeling these two subtasks, either by casting them into a structured prediction framework or by performing multitask learning through shared representations. However, since the contextual representations of entity and relation models inherently capture different feature information, sharing a single encoder to capture the information required by both subtasks in the same space would harm the accuracy of the model. Recent research (Zhong and Chen, 2020) has also proved that using two separate encoders for NER and RE tasks respectively through pipeline method are effective, with the model surpassing all previous joint models in accuracy. Thus, in this paper, we design An Pipeline method Information Extraction module called APIE, APIE combines the advantages of both pipeline methods and joint methods, demonstrating higher accuracy and powerful reasoning abilities. Specifically, we design a multi-level feature NER model based on attention mechanism and a document-level RE model based on local context pooling. To demonstrate the effectiveness of our proposed approach, we conducted tests on multiple datasets. Extensive experimental results have shown that our proposed model outperforms state-of-the-art methods and improves both accuracy and reasoning abilities.}
}
@article{JIANG2024102530,
title = {Product innovation design approach driven by implicit relationship completion via patent knowledge graph},
journal = {Advanced Engineering Informatics},
volume = {61},
pages = {102530},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102530},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624001782},
author = {Shaofei Jiang and Jingwei Yang and Jing Xie and Xuesong Xu and Yubo Dou and Liting Jing},
keywords = {Product innovation design, Patent text, Knowledge graph, RFSB ontology model, Implicit relationship completion},
abstract = {Product innovation design process involves a great deal of discrete engineering knowledge, limiting the ability of designers to quickly utilize this knowledge to support design innovation. Nowadays, innovation design based on knowledge graphs has enhanced the ability to explore design knowledge, improving the efficiency of knowledge retrieval. Previous studies have focused on mining more design knowledge to enrich the knowledge graph overlooks the implicit relationships with potential value among design knowledge, wasting design resources. To address these issues, an approach for product innovation design based on implicit knowledge relationship completion in the patent knowledge graph is proposed, which explores the implicit relationships between design knowledge to provide new knowledge satisfying design preferences and enhance the innovativeness of solutions. First, a requirements-function-structure-benefit (RFSB) knowledge ontology is constructed and extracted from the benefit knowledge of patents to build the knowledge graph. Second, an implicit relationship completion model based on the similarity of function or benefit entities explores the implicit relationships, replacing structure entities directly connected to similar function or benefit entities to generate new relationships and outputs novel ideas. Third, a scheme improvement process based on the co-occurrence frequency of requirement and structure knowledge supplements neglected design preferences. Final, a pipeline inspection robot case study is further employed to verify the proposed approach, and a patent knowledge graph assisted design solution prototype system is developed to assist in the utilization of innovative design knowledge. Evaluation results show the significant design potential of the proposed approach in inspiring innovative thinking and knowledge reuse.}
}
@article{DING2023,
title = {Constructing a Knowledge Graph for the Chinese Subject Based on Collective Intelligence},
journal = {International Journal on Semantic Web and Information Systems},
volume = {19},
number = {1},
year = {2023},
issn = {1552-6283},
doi = {https://doi.org/10.4018/IJSWIS.327355},
url = {https://www.sciencedirect.com/science/article/pii/S1552628323000108},
author = {Guozhu Ding and Peiying Yi and Xinru Feng},
keywords = {Knowledge Graph, Knowledge Ontology, Ontology Construction, Ontology Evolution, Subject Matter Learning Cell},
abstract = {ABSTRACT
Knowledge graphs are a valuable tool for intelligent tutoring systems and are typically constructed with a focus on objectivity and accuracy. However, they may not effectively capture the subjectivity and complex relationships often present in the humanities. To address this issue, a dynamic visualization of subject matter knowledge graph was developed using a collective intelligence approach that integrates the individual intelligence of learners and considers cognitive diversity to construct and evolve the knowledge graph. The approach resulted in the construction of 722 knowledge associations and the evolution of 584 triples. A survey assessed the effectiveness and user-friendliness, revealing that this approach is effective, easy to use, and can improve subject matter knowledge ontology. In conclusion, combining individual and collective intelligence is a promising approach for building effective knowledge graphs in subject areas with subjectivity and complexity.}
}
@article{YANG2024111652,
title = {CL&CD: Contrastive Learning and Cluster Description for Zero-Shot Relation Extraction},
journal = {Knowledge-Based Systems},
volume = {293},
pages = {111652},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.111652},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124002879},
author = {Zongqiang Yang and Junbo Fei and Zhen Tan and Jiuyang Tang and Xiang Zhao},
keywords = {Relation extraction, Clustering description, Contrastive learning, Zero-shot},
abstract = {Zero-shot Relation Extraction (ZRE) is designed to identify new relations when the model is adapted to a new environment in a new domain. The majority of existing ZRE methods employ distant supervision for data labeling, which inevitably leads to incomplete annotations and noise. To handle this problem, we propose a ZRE framework based on Contrastive Learning and Cluster Description (CL&CD), a two-stage contrastive learning method is used to train labeled data which consists pseudo-labeled data and labeled data. The framework can effectively promote the model mapping the instances of the same relations to the adjacent vector space better. The module of clustering description reasonably optimizes the time of manual intervention and enormously reduces the consumption of human resources. Experimental results on Wiki-ZSL and FewRel show the superior performance of the CL&CD, which outperforms all baseline models including the state-of-the-art RCL. Furthermore, CL&CD improves F1-Score by up to 8% in both Wiki-ZSL and FewRel with 15 unseen relations.}
}
@article{LIU2023100761,
title = {From tabular data to knowledge graphs: A survey of semantic table interpretation tasks and methods},
journal = {Journal of Web Semantics},
volume = {76},
pages = {100761},
year = {2023},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2022.100761},
url = {https://www.sciencedirect.com/science/article/pii/S1570826822000452},
author = {Jixiong Liu and Yoan Chabot and Raphaël Troncy and Viet-Phi Huynh and Thomas Labbé and Pierre Monnin},
keywords = {Semantic table interpretation, Table annotation, Tabular data, Knowledge graph},
abstract = {Tabular data often refers to data that is organized in a table with rows and columns. We observe that this data format is widely used on the Web and within enterprise data repositories. Tables potentially contain rich semantic information that still needs to be interpreted. The process of extracting meaningful information out of tabular data with respect to a semantic artefact, such as an ontology or a knowledge graph, is often referred to as Semantic Table Interpretation (STI) or Semantic Table Annotation. In this survey paper, we aim to provide a comprehensive and up-to-date state-of-the-art review of the different tasks and methods that have been proposed so far to perform STI. First, we propose a new categorization that reflects the heterogeneity of table types that one can encounter, revealing different challenges that need to be addressed. Next, we define five major sub-tasks that STI deals with even if the literature has mostly focused on three sub-tasks so far. We review and group the many approaches that have been proposed into three macro families and we discuss their performance and limitations with respect to the various datasets and benchmarks proposed by the community. Finally, we detail what are the remaining scientific barriers to be able to truly automatically interpret any type of tables that can be found in the wild Web.}
}
@article{WU2025100236,
title = {Visual analysis of LLM-based entity resolution from scientific papers},
journal = {Visual Informatics},
volume = {9},
number = {2},
pages = {100236},
year = {2025},
issn = {2468-502X},
doi = {https://doi.org/10.1016/j.visinf.2025.100236},
url = {https://www.sciencedirect.com/science/article/pii/S2468502X25000178},
author = {Siyu Wu and Yi Yang and Weize Wu and Ruiming Li and Yuyang Zhang and Ge Wang and Huobin Tan and Zipeng Liu and Lei Shi},
keywords = {Entity resolution, Large language models (LLMs), Visual analytics, Scientific literature analysis, Interactive visualization, Domain-specific knowledge structuring},
abstract = {This paper focuses on the visual analytics support for extracting domain-specific entities from extensive scientific literature, a task with inherent limitations using traditional named entity resolution methods. With the advent of large language models (LLMs) such as GPT-4, significant improvements over conventional machine learning approaches have been achieved due to LLM’s capability on entity resolution integrate abilities such as understanding multiple types of text. This research introduces a new visual analysis pipeline that integrates these advanced LLMs with versatile visualization and interaction designs to support batch entity resolution. Specifically, we focus on a specific material science field of Metal-Organic Frameworks (MOFs) and a large data collection namely CSD-MOFs. Through collaboration with domain experts in material science, we obtain well-labeled synthesis paragraphs. We propose human-in-the-loop refinement over the entity resolution process using visual analytics techniques, which allows domain experts to interactively integrate insights into LLM intelligence, including error analysis and interpretation of the retrieval-augmented generation (RAG) algorithm. Our evaluation through the case study of example selection for RAG demonstrates that this visual analysis approach effectively improves the accuracy of single-document entity resolution.}
}
@article{JAVEED2023100444,
title = {A hybrid attention mechanism for multi-target entity relation extraction using graph neural networks},
journal = {Machine Learning with Applications},
volume = {11},
pages = {100444},
year = {2023},
issn = {2666-8270},
doi = {https://doi.org/10.1016/j.mlwa.2022.100444},
url = {https://www.sciencedirect.com/science/article/pii/S2666827022001190},
author = {Arshad Javeed},
keywords = {Natural language processing, Named-entity recognition, Embeddings, Graph neural networks},
abstract = {Relation extraction is a cardinal natural language processing task employed to process a given corpus and infer hidden connections and relations between real-world objects. Most contemporary research works employ state-of-the-art language models and techniques for determining the type of relationship existing between a pair of entities in a given sentence but are computationally expensive and fail to identify or match the entities present in the sentence as a single task, rather they breakdown the problem into subtasks or rely on a multi-module framework requiring multiple propagations through the network. The paper presents a novel methodology for extracting relations between multiple pairs of entities present in the sentence and performs the relation classification task. The proposed methodology employs a graph neural network, and in contrast to the existing research, the proposed mechanism makes use of a hybrid attention mechanism to dynamically optimize the graph edges to capture the relevant details in the network graph to aid as an attention mechanism to aid in faster computation, compared to the more typical transformer-based networks that overwhelm CPU-based systems. The paper also studies the effect of the number of hop transformations on the graph and other hyper-parameters controlling the input sentence representation. The proposed model architecture achieves a macro avg. F1 score of 86.2 on the SemEval 2010 relation extraction dataset, with further room for improvement.}
}
@article{ISAEE2025128867,
title = {Enhancing Open N-ary Information Extraction using relation embedding and multihead relation attention mechanism},
journal = {Neurocomputing},
volume = {616},
pages = {128867},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128867},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224016382},
author = {Mitra Isaee and Afsaneh Fatemi and Mohammadali Nematbakhsh},
keywords = {Open N-ary Information Extraction, Natural language processing, SpanBERT, Multihead relation attention, Relation embedding},
abstract = {Open Information Extraction (Open IE) is the task of identifying structured and machine-readable information from natural language text within an open domain context. This research area has gained significant importance in the field of natural language processing (NLP), attracting considerable attention for its potential to extract valuable information from unstructured textual data. Previous investigations heavily relied on manual extraction patterns and various NLP tools. While these methods often produce errors that accumulate and propagate throughout the systems, ultimately affecting the accuracy of the results. Moreover, recent Open IE studies have focused on extracting binary relations involving two entities. However, these binary approaches occasionally lead to the omission of essential information in the text, preventing a deeper comprehension of the content. This limitation arises from the fact that real-world relations often involve multiple entities, but binary approaches may oversimplify these relations and miss additional details crucial for a thorough understanding of text. To address these challenges, our study introduces an innovative system called “Open N-ary Information EXtraction (ONIEX).” This system incorporates two novel techniques: multihead relation attention mechanism and relation embedding. Multihead relation attention, in combination with relation embedding, enables the system to focus on relations extracted through the SpanBERT model and accurately identify associated entities for each relation. The ONIEX system's superior performance is substantiated through extensive experiments conducted on the OpenIE4 and LSOIE datasets, benchmark datasets for Open n-ary Information Extraction (Open n-ary IE). The results demonstrate the superiority of the ONIEX system over the existing state-of-the-art systems.}
}
@article{XIA2023109068,
title = {Maintenance planning recommendation of complex industrial equipment based on knowledge graph and graph neural network},
journal = {Reliability Engineering & System Safety},
volume = {232},
pages = {109068},
year = {2023},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2022.109068},
url = {https://www.sciencedirect.com/science/article/pii/S0951832022006834},
author = {Liqiao Xia and Yongshi Liang and Jiewu Leng and Pai Zheng},
keywords = {Graph neural network, Knowledge graph, Link prediction, Maintenance management, Predictive maintenance},
abstract = {Maintenance planning is a significant part of predictive maintenance, which involves task planning, resource scheduling, and prevention. With large-scale sensor systems in modern factories, much data will be captured during monitoring and maintenance of complex industrial equipment. Accumulated data facilitates maintenance planning becomes more thorough and timely. Recently, a knowledge graph (KG) was offered to handle large-scale, unorganized maintenance data semantically, resulting in better data usage. Some prior studies have utilized KG for maintenance planning with semantic searching or graph structure-based algorithms, nevertheless neglecting the prediction of potential linkage. To fill this gap, a maintenance-oriented KG is established firstly based on a well-defined domain-specific ontology schema and accumulated maintenance data. Then, an Attention-Based Compressed Relational Graph Convolutional Network is proposed to predict potential solutions and explain fault in maintenance tasks. Lastly, a maintenance case of oil drilling equipment is carried out, where the proposed model is compared with other cutting-edge models to demonstrate its effectiveness in link prediction. This research is anticipated to shed light on future adoption of KG in maintenance planning recommendations.}
}
@article{CHEN2024123478,
title = {GAP: A novel Generative context-Aware Prompt-tuning method for relation extraction},
journal = {Expert Systems with Applications},
volume = {248},
pages = {123478},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.123478},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424003439},
author = {Zhenbin Chen and Zhixin Li and Yufei Zeng and Canlong Zhang and Huifang Ma},
keywords = {Relation extraction, Prompt-tuning, Pretrained language model, Few-shot learning, Contrastive learning},
abstract = {Prompt-tuning was proposed to bridge the gap between pretraining and downstream tasks, and it has achieved promising results in Relation Extraction (RE). Although the existing prompt-based RE methods have outperformed the methods based on fine-tuning paradigm, these methods require domain experts to design prompt templates, making them hard to be generalized. In this paper, we propose a Generative context-Aware Prompt-tuning method (GAP) to address these limitations. Our method consists of three crucial modules: (1) a pretrained prompt generator module that extracts or generates the relation triggers from the context and embeds them into the prompt tokens, (2) an in-domain adaptive pretraining module that further trains the Pretrained Language Models (PLMs) to promote the adaptability of the model, and (3) a joint contrastive loss that prevents PLMs from generating unrelated content and optimizes our model more effectively. We observe that the context-enhanced prompt tokens generated by GAP can better guide PLMs to make more accurate predictions. And the in-domain pretraining can effectively inject domain knowledge to enhance the robustness of the model. We conduct experiments on four public RE datasets with supervised and few-shot settings. The experimental results have demonstrated the superiority of GAP over existing benchmark methods and GAP shows remarkable improvements in few-shot settings, with average F1 score enhancements of 3.5%, 2.7%, and 3.4% on the TACRED, TACREV, and Re-TACRED datasets, respectively. Furthermore, GAP still achieved state-of-the-art (SOTA) performance in supervised settings.}
}
@article{WANG2022,
title = {Construction of a Linked Data Set of COVID-19 Knowledge Graphs: Development and Applications},
journal = {JMIR Medical Informatics},
volume = {10},
number = {5},
year = {2022},
issn = {2291-9694},
doi = {https://doi.org/10.2196/37215},
url = {https://www.sciencedirect.com/science/article/pii/S2291969422001533},
author = {Haofen Wang and Huifang Du and Guilin Qi and Huajun Chen and Wei Hu and Zhuo Chen},
keywords = {knowledge graph, linked data, COVID-19, knowledge extraction, knowledge fusion, natural language processing, artificial intelligence, data set, schema modeling, semantic search},
abstract = {Background
With the continuous spread of COVID-19, information about the worldwide pandemic is exploding. Therefore, it is necessary and significant to organize such a large amount of information. As the key branch of artificial intelligence, a knowledge graph (KG) is helpful to structure, reason, and understand data.
Objective
To improve the utilization value of the information and effectively aid researchers to combat COVID-19, we have constructed and successively released a unified linked data set named OpenKG-COVID19, which is one of the largest existing KGs related to COVID-19. OpenKG-COVID19 includes 10 interlinked COVID-19 subgraphs covering the topics of encyclopedia, concept, medical, research, event, health, epidemiology, goods, prevention, and character.
Methods
In this paper, we introduce the key techniques exploited in building COVID-19 KGs in a top-down manner. First, the schema of the modeling process for each KG in OpenKG-COVID19 is described. Second, we propose different methods for extracting knowledge from open government sites, professional texts, public domain–specific sources, and public encyclopedia sites. The curated 10 COVID-19 KGs are further linked together at both the schema and data levels. In addition, we present the naming convention for OpenKG-COVID19.
Results
OpenKG-COVID19 has more than 2572 concepts, 329,600 entities, 513 properties, and 2,687,329 facts, and the data set will be updated continuously. Each COVID-19 KG was evaluated, and the average precision was found to be above 93%. We have developed search and browse interfaces and a SPARQL endpoint to improve user access. Possible intelligent applications based on OpenKG-COVID19 for further development are also described.
Conclusions
A KG is useful for intelligent question-answering, semantic searches, recommendation systems, visualization analysis, and decision-making support. Research related to COVID-19, biomedicine, and many other communities can benefit from OpenKG-COVID19. Furthermore, the 10 KGs will be continuously updated to ensure that the public will have access to sufficient and up-to-date knowledge.}
}
@article{FAN2022109857,
title = {Knowledge base question answering via path matching},
journal = {Knowledge-Based Systems},
volume = {256},
pages = {109857},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.109857},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122009509},
author = {Chunxiao Fan and Wentong Chen and Yuexin Wu},
keywords = {KBQA, Text matching, Knowledge graph},
abstract = {Knowledge base question answering (KBQA) refers to combining the information in the knowledge base to obtain an answer for an objective question. However, most of the existing methods tend to add a large number of hand-crafted features or constraints to improve the performance of the model. This makes the KBQA system more and more complex, but the improvement is limited. In this work, we try a novel method without any hand-crafted features or constraints, which transforms this problem into a text matching problem. In this method, a question and a series of edges in knowledge bases (KBs) are treated as two pieces of text that need to be matched. The network to match the two texts called Path Matching Model (PMM). On the WebQuestions benchmark, our method has 3% improvement compared to the state-of-the-art method.}
}
@incollection{ZAKI2025819,
title = {Chapter 24 - Artificial intelligence for information extraction from cement literature},
editor = {Nakshatra Bahadur Singh and Raju Goyal and Bernhard Middendorf},
booktitle = {Binding Materials for Sustainable Construction},
publisher = {Woodhead Publishing},
pages = {819-832},
year = {2025},
series = {Woodhead Publishing Series in Civil and Structural Engineering},
isbn = {978-0-443-26566-2},
doi = {https://doi.org/10.1016/B978-0-443-26566-2.00024-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780443265662000246},
author = {Mohd Zaki and Keshav Bharadwaj and Raju Goyal and  Jayadeva and N.M. Anoop Krishnan},
keywords = {Artificial intelligence, Cement literature mining, Clinker microstructure analysis, Information extraction, Natural language processing (NLP)},
abstract = {The exponential growth of scientific literature in the field of cement and construction materials poses significant challenges for researchers seeking to synthesize knowledge and derive actionable insights. This chapter explores the transformative role of Artificial Intelligence (AI) in automating information extraction (IE) from cement-related literature, with a particular focus on construction materials research and clinker microstructural analysis. Core IE concepts—including named entity recognition, relation extraction, and document classification—are introduced in the context of cement science. The chapter presents recent advancements in natural language processing (NLP) and computer vision techniques for extracting structured data from unstructured research papers and microstructural imagery. Applications include the automated identification of chemical compositions, performance metrics, phase characterization, and process parameters. Case studies highlight the integration of AI tools in accelerating materials discovery, optimizing mix designs, and supporting data-driven decision-making in sustainable cement technologies. The chapter concludes with a discussion of current limitations, ethical considerations, and future research directions aimed at enhancing the reliability, interpretability, and scalability of AI-driven information extraction in cement research.}
}
@article{SMITH2022101938,
title = {LILLIE: Information extraction and database integration using linguistics and learning-based algorithms},
journal = {Information Systems},
volume = {105},
pages = {101938},
year = {2022},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2021.101938},
url = {https://www.sciencedirect.com/science/article/pii/S030643792100137X},
author = {Ellery Smith and Dimitris Papadopoulos and Martin Braschler and Kurt Stockinger},
keywords = {Information extraction, Data integration, Machine learning for database systems},
abstract = {Querying both structured and unstructured data via a single common query interface such as SQL or natural language has been a long standing research goal. Moreover, as methods for extracting information from unstructured data become ever more powerful, the desire to integrate the output of such extraction processes with “clean”, structured data grows. We are convinced that for successful integration into databases, such extracted information in the form of “triples” needs to be both (1) of high quality and (2) have the necessary generality to link up with varying forms of structured data. It is the combination of both these aspects, which heretofore have been usually treated in isolation, where our approach breaks new ground. The cornerstone of our work is a novel, generic method for extracting open information triples from unstructured text, using a combination of linguistics and learning-based extraction methods, thus uniquely balancing both precision and recall. Our system called LILLIE (LInked Linguistics and Learning-Based Information Extractor) uses dependency tree modification rules to refine triples from a high-recall learning-based engine, and combines them with syntactic triples from a high-precision engine to increase effectiveness. In addition, our system features several augmentations, which modify the generality and the degree of granularity of the output triples. Even though our focus is on addressing both quality and generality simultaneously, our new method substantially outperforms current state-of-the-art systems on the two widely-used CaRB and Re-OIE16 benchmark sets for information extraction. We have made our code publicly available11https://github.com/OIELILLIE/LILLIE. to facilitate further research.}
}
@article{SKJAEVELAND202455,
title = {An ecosystem for personal knowledge graphs: A survey and research roadmap},
journal = {AI Open},
volume = {5},
pages = {55-69},
year = {2024},
issn = {2666-6510},
doi = {https://doi.org/10.1016/j.aiopen.2024.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S2666651024000044},
author = {Martin G. Skjæveland and Krisztian Balog and Nolwenn Bernard and Weronika Łajewska and Trond Linjordet},
keywords = {Personal knowledge graphs, Personal data management},
abstract = {This paper presents an ecosystem for personal knowledge graphs (PKGs), commonly defined as resources of structured information about entities related to an individual, their attributes, and the relations between them. PKGs are a key enabler of secure and sophisticated personal data management and personalized services. However, there are challenges that need to be addressed before PKGs can achieve widespread adoption. One of the fundamental challenges is the very definition of what constitutes a PKG, as there are multiple interpretations of the term. We propose our own definition of a PKG, emphasizing the aspects of (1) data ownership by a single individual and (2) the delivery of personalized services as the primary purpose. We further argue that a holistic view of PKGs is needed to unlock their full potential, and propose a unified framework for PKGs, where the PKG is a part of a larger ecosystem with clear interfaces towards data services and data sources. A comprehensive survey and synthesis of existing work is conducted, with a mapping of the surveyed work into the proposed unified ecosystem. Finally, we identify open challenges and research opportunities for the ecosystem as a whole, as well as for the specific aspects of PKGs, which include population, representation and management, and utilization.}
}
@article{SIVARAJKUMAR2024,
title = {An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study},
journal = {JMIR Medical Informatics},
volume = {12},
year = {2024},
issn = {2291-9694},
doi = {https://doi.org/10.2196/55318},
url = {https://www.sciencedirect.com/science/article/pii/S2291969424000383},
author = {Sonish Sivarajkumar and Mark Kelley and Alyssa Samolyk-Mazzanti and Shyam Visweswaran and Yanshan Wang},
keywords = {large language model, LLM, LLMs, natural language processing, NLP, in-context learning, prompt engineering, evaluation, zero-shot, few shot, prompting, GPT, language model, language, models, machine learning, clinical data, clinical information, extraction, BARD, Gemini, LLaMA-2, heuristic, prompt, prompts, ensemble},
abstract = {Background
Large language models (LLMs) have shown remarkable capabilities in natural language processing (NLP), especially in domains where labeled data are scarce or expensive, such as the clinical domain. However, to unlock the clinical knowledge hidden in these LLMs, we need to design effective prompts that can guide them to perform specific clinical NLP tasks without any task-specific training data. This is known as in-context learning, which is an art and science that requires understanding the strengths and weaknesses of different LLMs and prompt engineering approaches.
Objective
The objective of this study is to assess the effectiveness of various prompt engineering techniques, including 2 newly introduced types—heuristic and ensemble prompts, for zero-shot and few-shot clinical information extraction using pretrained language models.
Methods
This comprehensive experimental study evaluated different prompt types (simple prefix, simple cloze, chain of thought, anticipatory, heuristic, and ensemble) across 5 clinical NLP tasks: clinical sense disambiguation, biomedical evidence extraction, coreference resolution, medication status extraction, and medication attribute extraction. The performance of these prompts was assessed using 3 state-of-the-art language models: GPT-3.5 (OpenAI), Gemini (Google), and LLaMA-2 (Meta). The study contrasted zero-shot with few-shot prompting and explored the effectiveness of ensemble approaches.
Results
The study revealed that task-specific prompt tailoring is vital for the high performance of LLMs for zero-shot clinical NLP. In clinical sense disambiguation, GPT-3.5 achieved an accuracy of 0.96 with heuristic prompts and 0.94 in biomedical evidence extraction. Heuristic prompts, alongside chain of thought prompts, were highly effective across tasks. Few-shot prompting improved performance in complex scenarios, and ensemble approaches capitalized on multiple prompt strengths. GPT-3.5 consistently outperformed Gemini and LLaMA-2 across tasks and prompt types.
Conclusions
This study provides a rigorous evaluation of prompt engineering methodologies and introduces innovative techniques for clinical information extraction, demonstrating the potential of in-context learning in the clinical domain. These findings offer clear guidelines for future prompt-based clinical NLP research, facilitating engagement by non-NLP experts in clinical NLP advancements. To the best of our knowledge, this is one of the first works on the empirical evaluation of different prompt engineering approaches for clinical NLP in this era of generative artificial intelligence, and we hope that it will inspire and inform future research in this area.}
}
@article{CADEDDU2024108166,
title = {A comparative analysis of knowledge injection strategies for large language models in the scholarly domain},
journal = {Engineering Applications of Artificial Intelligence},
volume = {133},
pages = {108166},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.108166},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624003245},
author = {Andrea Cadeddu and Alessandro Chessa and Vincenzo {De Leo} and Gianni Fenu and Enrico Motta and Francesco Osborne and Diego {Reforgiato Recupero} and Angelo Salatino and Luca Secchi},
keywords = {Knowledge injection, Knowledge graphs, Large language models, Transformers, BERT, Classification, Natural language processing},
abstract = {In recent years, transformer-based models have emerged as powerful tools for natural language processing tasks, demonstrating remarkable performance in several domains. However, they still present significant limitations. These shortcomings become more noticeable when dealing with highly specific and complex concepts, particularly within the scientific domain. For example, transformer models have particular difficulties when processing scientific articles due to the domain-specific terminologies and sophisticated ideas often encountered in scientific literature. To overcome these challenges and further enhance the effectiveness of transformers in specific fields, researchers have turned their attention to the concept of knowledge injection. Knowledge injection is the process of incorporating outside knowledge into transformer models to improve their performance on certain tasks. In this paper, we present a comprehensive study of knowledge injection strategies for transformers within the scientific domain. Specifically, we provide a detailed overview and comparative assessment of four primary methodologies, evaluating their efficacy in the task of classifying scientific articles. For this purpose, we constructed a new benchmark including both 24K labelled papers and a knowledge graph of 9.2K triples describing pertinent research topics. We also developed a full codebase to easily re-implement all knowledge injection strategies in different domains. A formal evaluation indicates that the majority of the proposed knowledge injection methodologies significantly outperform the baseline established by Bidirectional Encoder Representations from Transformers.}
}
@article{TANG2023120441,
title = {Boundary regression model for joint entity and relation extraction},
journal = {Expert Systems with Applications},
volume = {229},
pages = {120441},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120441},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423009430},
author = {Ruixue Tang and Yanping Chen and Yongbin Qin and Ruizhang Huang and Qinghua Zheng},
keywords = {Boundary regression, Boundary filter, Joint entity and relation extraction},
abstract = {Joint extraction of named entities and their relations has the advantage of avoiding cascading failures caused by falsely recognized named entities. Recent studies have focused on span classification modes to support end-to-end multiobjective learning. However, the enumeration of a large number of inaccurate entity spans creates a serious data imbalance and incurs high computational complexity. In this study, we propose a boundary regression model for joint entity and relation extraction, where a boundary regression mechanism is adopted to learn the offset of a possible named entity relevant to a true named entity. Instead of exhaustively enumerating all possible entity spans, this model receives only a small number of coarse entities with inaccurate boundaries as inputs. It can locate named entities and extract relations between them simultaneously. Experiments demonstrated that our boundary regression model outperforms state-of-the-art models in terms of the F1 score by ＋2.5%, ＋0.4%, ＋2.1%, and ＋1.3% on ADE, ACE05, ACE04, and CoNLL04 benchmark datasets respectively. Analytical experiments further confirmed the effectiveness of our model for refining entity boundaries and learning accurate span representations.}
}
@article{NIU2022249,
title = {Joint semantics and data-driven path representation for knowledge graph reasoning},
journal = {Neurocomputing},
volume = {483},
pages = {249-261},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222001515},
author = {Guanglin Niu and Bo Li and Yongfei Zhang and Yongpan Sheng and Chuan Shi and Jingyang Li and Shiliang Pu},
keywords = {Knowledge graph reasoning, Path representation, Horn rules, Entity converting, Joint semantics and data-driven},
abstract = {Reasoning on a large-scale knowledge graph (KG) is of great importance for KG applications like question answering. The path-based reasoning models can leverage much information over paths other than pure triples in the KG but face several challenges. Firstly, all the existing path-based methods are data-driven, lacking explainability, namely how the path representations and the reasoning results are obtained with human-understandable explanations. Besides, some approaches either consider only relational paths or ignore the heterogeneity between entities and relations both in paths, which cannot capture the rich semantics of paths well. To address the above challenges, in this work, we propose a novel joint semantics and data-driven path representation that balances explainability and generalization in the framework of KG embedding. Specifically, we inject horn rules to obtain the condensed paths through a transparent and explainable path composition procedure. The entity converter is designed to transform entities along paths into the representations in the semantic level similar to relations for reducing the heterogeneity between entities and relations. The KGs, both with and without type information, are considered. Our proposed model is evaluated on two classes of tasks: link prediction and path query answering. The experimental results show that our model obtains significant performance gains over several state-of-the-art baselines.}
}
@article{JI2023493,
title = {Construction and application of knowledge graph for grid dispatch fault handling based on pre-trained model},
journal = {Global Energy Interconnection},
volume = {6},
number = {4},
pages = {493-504},
year = {2023},
issn = {2096-5117},
doi = {https://doi.org/10.1016/j.gloei.2023.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S2096511723000683},
author = {Zhixiang Ji and Xiaohui Wang and Jie Zhang and Di Wu},
keywords = {Power-grid dispatch fault handling, Knowledge graph, Pre-trained model, Auxiliary decision-making},
abstract = {With the construction of new power systems, the power grid has become extremely large, with an increasing proportion of new energy and AC/DC hybrid connections. The dynamic characteristics and fault patterns of the power grid are complex; additionally, power grid control is difficult, operation risks are high, and the task of fault handling is arduous. Traditional power-grid fault handling relies primarily on human experience. The difference in and lack of knowledge reserve of control personnel restrict the accuracy and timeliness of fault handling. Therefore, this mode of operation is no longer suitable for the requirements of new systems. Based on the multi-source heterogeneous data of power grid dispatch, this paper proposes a joint entity–relationship extraction method for power-grid dispatch fault processing based on a pre-trained model, constructs a knowledge graph of power-grid dispatch fault processing and designs, and develops a fault-processing auxiliary decision-making system based on the knowledge graph. It was applied to study a provincial dispatch control center, and it effectively improved the accident processing ability and intelligent level of accident management and control of the power grid.}
}