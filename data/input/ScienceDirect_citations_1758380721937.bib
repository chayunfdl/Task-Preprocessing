@article{DIXIT2024122579,
title = {Deep CNN with late fusion for real time multimodal emotion recognition},
journal = {Expert Systems with Applications},
volume = {240},
pages = {122579},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122579},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423030816},
author = {Chhavi Dixit and Shashank Mouli Satapathy},
keywords = {CNN, Cross dataset, Ensemble learning, FastText, Multimodal emotion recognition, Stacking},
abstract = {Emotion recognition is a fundamental aspect of human communication and plays a crucial role in various domains. This project aims at developing an efficient model for real-time multimodal emotion recognition in videos of human oration (opinion videos), where the speakers express their opinions about various topics. Four separate datasets contributing 20,000 samples for text, 1,440 for audio, 35,889 for images, and 3,879 videos for multimodal analysis respectively are used. One model is trained for each of the modalities: fastText for text analysis because of its efficiency, robustness to noise, and pre-trained embeddings; customized 1-D CNN for audio analysis using its translation invariance, hierarchical feature extraction, scalability, and generalization; custom 2-D CNN for image analysis because of its ability to capture local features and handle variations in image content. They are tested and combined on the CMU-MOSEI dataset using both bagging and stacking to find the most effective architecture. They are then used for real-time analysis of speeches. Each of the models is trained on 80% of the datasets, the remaining 20% is used for testing individual and combined accuracies in CMU-MOSEI. The emotions finally predicted by the architecture correspond to the six classes in the CMU-MOSEI dataset. This cross-dataset training and testing of the models makes them robust and efficient for general use, removes reliance on a specific domain or dataset, and adds more data points for model training. The proposed architecture was able to achieve an accuracy of 85.85% and an F1-score of 83 on the CMU-MOSEI dataset.}
}
@article{SUNG20233833,
title = {Speech Recognition via CTC-CNN Model},
journal = {Computers, Materials and Continua},
volume = {76},
number = {3},
pages = {3833-3858},
year = {2023},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2023.040024},
url = {https://www.sciencedirect.com/science/article/pii/S1546221823000097},
author = {Wen-Tsai Sung and Hao-Wei Kang and Sung-Jung Hsiao},
keywords = {Artificial intelligence, speech recognition, speech to text, convolutional neural network, automatic speech recognition},
abstract = {In the speech recognition system, the acoustic model is an important underlying model, and its accuracy directly affects the performance of the entire system. This paper introduces the construction and training process of the acoustic model in detail and studies the Connectionist temporal classification (CTC) algorithm, which plays an important role in the end-to-end framework, established a convolutional neural network (CNN) combined with an acoustic model of Connectionist temporal classification to improve the accuracy of speech recognition. This study uses a sound sensor, ReSpeaker Mic Array v2.0.1, to convert the collected speech signals into text or corresponding speech signals to improve communication and reduce noise and hardware interference. The baseline acoustic model in this study faces challenges such as long training time, high error rate, and a certain degree of overfitting. The model is trained through continuous design and improvement of the relevant parameters of the acoustic model, and finally the performance is selected according to the evaluation index. Excellent model, which reduces the error rate to about 18%, thus improving the accuracy rate. Finally, comparative verification was carried out from the selection of acoustic feature parameters, the selection of modeling units, and the speaker’s speech rate, which further verified the excellent performance of the CTCCNN_5 + BN + Residual model structure. In terms of experiments, to train and verify the CTC-CNN baseline acoustic model, this study uses THCHS-30 and ST-CMDS speech data sets as training data sets, and after 54 epochs of training, the word error rate of the acoustic model training set is 31%, the word error rate of the test set is stable at about 43%. This experiment also considers the surrounding environmental noise. Under the noise level of 80∼90 dB, the accuracy rate is 88.18%, which is the worst performance among all levels. In contrast, at 40–60 dB, the accuracy was as high as 97.33% due to less noise pollution.}
}
@article{LEHONG2021107504,
title = {Diacritics generation and application in hate speech detection on Vietnamese social networks},
journal = {Knowledge-Based Systems},
volume = {233},
pages = {107504},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107504},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121007668},
author = {Phuong Le-Hong},
keywords = {Diacritics generation, Hate speech detection, Recurrent neural networks, Transformers, Sentiment analysis, Text, Vietnamese},
abstract = {One of the challenging problems in text processing is diacritics generation where one needs to generate diacritic marks for non-accented text. With an ever increasing amount of informal text without accents such as short text messages, emails or blog posts on social media, a software system which is capable of generating diacritic marks accurately is very useful and necessary in many situations. This paper presents an approach to improve the accuracy of diacritics generation for Vietnamese text. We propose two novel deep learning models which leverage a plausible conceptual representation for the phonetic structure of Vietnamese syllables. Experimental results on real-world datasets show that our models achieve a significant improvement as compared to the state-of-the-art methods for diacritics generation. We also demonstrate that the proposed models can be applied efficiently to improve the accuracy of hate speech detection on Vietnamese social networks.}
}
@article{MIAO2024106248,
title = {DC-BVM: Dual-channel information fusion network based on voting mechanism},
journal = {Biomedical Signal Processing and Control},
volume = {94},
pages = {106248},
year = {2024},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2024.106248},
url = {https://www.sciencedirect.com/science/article/pii/S1746809424003069},
author = {Borui Miao and Yunfeng Xu and Jialin Wang and Yan Zhang},
keywords = {Emotion recognition in conversation, Ensemble learning, Graph neural network, Attention mechanism},
abstract = {Emotion recognition in conversations (ERC) has been challenging due to the dynamics and complexity of emotions in conversations. Most current emotion recognition studies have focused on modeling temporal dimensions, such as context-sensitive dependencies while ignoring the spatial dimensional relationships between discourses. In this paper, we propose a dual-channel information fusion network based on a voting mechanism, in which we model modal information in two dimensions: distance dependence is constructed in the spatial dimension, and context dependence is constructed in the temporal dimension. We aim to extract more comprehensive and accurate sentiment features to recognize verbal emotions from limited data. In addition, we discard the traditional modal fusion method and propose a fusion strategy based on the voting mechanism, which significantly accelerates the model convergence speed and recognition performance. We conducted experiments on two benchmark datasets, IEMOCAP and MELD, respectively, and the experimental results demonstrate the superiority of the designed network in emotion recognition.}
}
@incollection{PEREZESPINOSA2022307,
title = {Chapter 15 - Emotion recognition: from speech and facial expressions},
editor = {Alejandro A. Torres-García and Carlos A. Reyes-García and Luis Villaseñor-Pineda and Omar Mendoza-Montoya},
booktitle = {Biosignal Processing and Classification Using Computational Learning and Intelligence},
publisher = {Academic Press},
pages = {307-326},
year = {2022},
isbn = {978-0-12-820125-1},
doi = {https://doi.org/10.1016/B978-0-12-820125-1.00028-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128201251000282},
author = {Humberto Pérez-Espinosa and Ramón Zatarain-Cabada and María Lucía Barrón-Estrada},
keywords = {Emotion recognition from speech, Acoustic features, Emotion models},
abstract = {Emotions are an essential aspect of communication between human beings. There is a very close relationship between emotions, behavior, and thoughts in such a way that the combination of these aspects governs the way we act and the decisions we make. For this reason, over the past years, there has been a growing interest in this area of scientific research. Automatic recognition of emotions can be applied in several areas to enhance them. For example, human–computer interaction, since detecting the emotional state of a computer system's user will allow generating a more natural, productive, and intelligent interaction. Another area is human–human interaction monitoring, given its allowance to detect conflicts or unwanted situations. This chapter addresses the main challenges of automatic emotion recognition from speech and face, such as the creation of corpora, the feature selection, the design of an appropriate classification scheme, and the fusion with other sources of information, such as text.}
}
@article{SHAHIN2025110963,
title = {Two-stage emotion recognition framework using CNN–transformer architecture and speaker cues},
journal = {Applied Acoustics},
volume = {240},
pages = {110963},
year = {2025},
issn = {0003-682X},
doi = {https://doi.org/10.1016/j.apacoust.2025.110963},
url = {https://www.sciencedirect.com/science/article/pii/S0003682X25004359},
author = {Ismail Shahin and Ali Bou Nassif and Noor Hindawi and Bader Alsabek and Nour Abujabal},
keywords = {Classical classifiers, Convolutional neural network, Emotion identification, Mel-frequency cepstral coefficients, Two-stage approach},
abstract = {Emotion identification has gained increasing attention in recent years due to its relevance in diverse applications ranging from intelligent human-computer interaction to mental health diagnostics. However, performance in emotion recognition remains highly susceptible to degradation caused by speaker variability. To address this, we propose a groundbreaking two-stage framework that incorporates speaker cues to enhance emotion classification performance. In Stage 1, speaker identification module is used to capture speaker-dependent characteristics, followed by Stage 2, where emotion is identified with speaker awareness. This text-independent architecture is built using deep learning models: a Convolutional Neural Network (CNN) and a hybrid CNN-Transformer model. We validate our framework on three distinct speech emotion datasets: Arabic Emirati-accented corpus, Speech Under Simulated and Actual Stress (SUSAS) dataset, and Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS). The input features include 39-dimensional MFCCs, composed of base coefficients, delta, and delta-delta features. Additional variants using LPC, DCT, DWPD, and PLDA are also evaluated. Experimental results show that our CNN-based model achieves a significant performance gain over classical classifiers such as Support Vector Machine (SVM), K-Nearest Neighbor (KNN), Multi-Layer Perceptron (MLP), Radial Basis Function (RBF), and Naïve Bayes (NB), attaining up to 90.7% accuracy. Furthermore, the hybrid CNN-Transformer model outperforms all baseline models across all tested datasets. The proposed two-stage system achieves 99.8% accuracy in speaker identification stage and 75.3% in emotion recognition stage on the Emirati dataset.}
}
@article{MA2021258,
title = {A Parameter Transfer Method for HMM-DNN Heterogeneous Model with the Scarce Mongolian Data Set},
journal = {Procedia Computer Science},
volume = {187},
pages = {258-263},
year = {2021},
note = {2020 International Conference on Identification, Information and Knowledge in the Internet of Things, IIKI2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.04.060},
url = {https://www.sciencedirect.com/science/article/pii/S187705092100853X},
author = {Zhiqiang Ma and Junpeng Zhang and Tuya Li and Rui Yang and Hongbin Wang},
keywords = {HMM-DNN Model, Homogeneous Model, Heterogeneous Model, Parameter Transfer, Transfer Learning},
abstract = {Hidden Markov Model-Deep Neural Network (HMM-DNN) is one of the most successful architecture in speech recognition. Although the HMM-DNN achieved state-of-the-art results on English and Mandarin, we find that there are lots of not updated parameters during the training of HMM-DNN acoustic model on a small scale Mongolian data set. This caused the model’s network training underfitting, which cannot learn the features of the data set. In the speech recognition scenario, the underfitting of speech features leads to a problem that the accuracy of the system decreases. In this regard, we define the concept of the homogeneous model heterogeneous model and propose a parameter learning method for HMM-DNN heterogeneous model in the scarce Mongolian data set. In our experiment, we choose KALDI as the experimental platform with the TIMIT English data set as the source data set and the scarce Mongolian data set as the target data set. Through the proposed parameter transfer method, we achieved much better performance on Mongolian recognition accuracy.}
}
@article{MA2025102753,
title = {Generative technology for human emotion recognition: A scoping review},
journal = {Information Fusion},
volume = {115},
pages = {102753},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102753},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524005311},
author = {Fei Ma and Yucheng Yuan and Yifan Xie and Hongwei Ren and Ivan Liu and Ying He and Fuji Ren and Fei Richard Yu and Shiguang Ni},
keywords = {Emotion recognition, Generative technology, Autoencoder, Generative Adversarial Network, Diffusion model, Large Language Model},
abstract = {Affective computing stands at the forefront of artificial intelligence (AI), seeking to imbue machines with the ability to comprehend and respond to human emotions. Central to this field is emotion recognition, which endeavors to identify and interpret human emotional states from different modalities, such as speech, facial images, text, and physiological signals. In recent years, important progress has been made in generative models, including Autoencoder, Generative Adversarial Network, Diffusion Model, and Large Language Model. These models, with their powerful data generation capabilities, emerge as pivotal tools in advancing emotion recognition. However, up to now, there remains a paucity of systematic efforts that review generative technology for emotion recognition. This survey aims to bridge the gaps in the existing literature by conducting a comprehensive analysis of over 330 research papers until June 2024. Specifically, this survey will firstly introduce the mathematical principles of different generative models and the commonly used datasets. Subsequently, through a taxonomy, it will provide an in-depth analysis of how generative techniques address emotion recognition based on different modalities in several aspects, including data augmentation, feature extraction, semi-supervised learning, cross-domain, etc. Finally, the review will outline future research directions, emphasizing the potential of generative models to advance the field of emotion recognition and enhance the emotional intelligence of AI systems.}
}
@article{CHAWLA2025903,
title = {Recent Trends in Machine and Deep Learning for Verbal and Non-verbal Emotion Detection},
journal = {Recent Advances in Electrical and Electronic Engineering},
volume = {18},
number = {7},
pages = {903-922},
year = {2025},
issn = {2352-0973},
doi = {https://doi.org/10.2174/0123520965303142240430101645},
url = {https://www.sciencedirect.com/science/article/pii/S235209732500063X},
author = {Muskan Chawla and Surya Narayan Panda and Vikas Khullar and Isha Kansal and Rajeev Kumar},
keywords = {Artificial intelligence, emotions, emotion recognition, verbal emotions, nonverbal emotions, machine learning, deep learning, assistive technologies},
abstract = {Emotion recognition, both verbal and non-verbal, is a crucial component of artificial intelligence, psychology, and human-computer interaction. Emotion recognition is an integral component that significantly contributes to the improvement of communication and interaction. The research endeavors to conduct a thorough analysis and synthesis of the most recent developments in Deep Learning (DL) and Machine Learning (ML) techniques. Specifically, the study concentrates on the recognition of both verbal and non-verbal emotions. In contrast to previous research concentrated on verbal or non-verbal emotion detection separately, the study attempts to reconcile the gap between the two by demonstrating how ML and DL can be utilized effectively to detect emotions. The study also examines new methods, including multimodal data and integration of contextual information. Additionally, the research has examined the ethical implications and difficulties associated with emotion detection technologies. Findings have also revealed the wide-reaching implications for various sectors, including healthcare, education, customer service, and entertainment, where comprehending human emotions plays a crucial role in enhancing user experience and outcomes. In conclusion, the study provides invaluable knowledge to practitioners and researchers, which may facilitate the development of more advanced and accurate systems.}
}
@article{AV2024102218,
title = {Multimodal Emotion Recognition with Deep Learning: Advancements, challenges, and future directions},
journal = {Information Fusion},
volume = {105},
pages = {102218},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.102218},
url = {https://www.sciencedirect.com/science/article/pii/S1566253523005341},
author = {Geetha A.V. and Mala T. and Priyanka D. and Uma E.},
keywords = {Affective computing, Deep Learning, Multimodal Emotion Recognition, Multimodal affective computing, Multimodality, Emotion models},
abstract = {In recent years, affective computing has become a topic of considerable interest, driven by its ability to enhance several domains, such as mental health monitoring, human–computer interaction, and personalized advertising. The progress of affective computing has been extensively supported by the emergence of sub-domains such as sentiment analysis and emotion recognition. Furthermore, Deep Learning (DL) techniques have made significant advancements in the realm of emotion recognition, resulting in the emergence of Multimodal Emotion Recognition (MER) systems that are capable of effectively processing data from various sources, such as audio, video, and text. However, despite the considerable progress made, there are still several challenges that persist in MER systems. Moreover, existing surveys often lack a specific focus on MER and the associated DL architectures. To address these research gaps, this study provides an in-depth systematic review of DL-based MER systems. This review encompasses the recent state-of-the-art models, foundational theories, DL architectures, mechanisms for fusing multimodal information, relevant datasets, performance evaluation, and practical applications. Additionally, the study identifies key challenges and limitations in MER systems and suggests future research opportunities. The main objective of this review is to provide a thorough comprehension of the present cutting-edge MER, thus enabling researchers in both academia and industry to stay up to date with the most recent developments in this rapidly evolving domain.}
}
@article{BASAK20221053,
title = {Challenges and Limitations in Speech Recognition Technology: A Critical Review of Speech Signal Processing Algorithms, Tools and Systems},
journal = {CMES - Computer Modeling in Engineering and Sciences},
volume = {135},
number = {2},
pages = {1053-1089},
year = {2022},
issn = {1526-1492},
doi = {https://doi.org/10.32604/cmes.2022.021755},
url = {https://www.sciencedirect.com/science/article/pii/S1526149222002880},
author = {Sneha Basak and Himanshi Agrawal and Shreya Jena and Shilpa Gite and Mrinal Bachute and Biswajeet Pradhan and Mazen Assiri},
keywords = {Speech recognition, automatic speech recognition (ASR), mel-frequency cepstral coefficients (MFCC), hidden Markov model (HMM), artificial neural network (ANN)},
abstract = {Speech recognition systems have become a unique human-computer interaction (HCI) family. Speech is one of the most naturally developed human abilities; speech signal processing opens up a transparent and hand-free computation experience. This paper aims to present a retrospective yet modern approach to the world of speech recognition systems. The development journey of ASR (Automatic Speech Recognition) has seen quite a few milestones and breakthrough technologies that have been highlighted in this paper. A step-by-step rundown of the fundamental stages in developing speech recognition systems has been presented, along with a brief discussion of various modern-day developments and applications in this domain. This review paper aims to summarize and provide a beginning point for those starting in the vast field of speech signal processing. Since speech recognition has a vast potential in various industries like telecommunication, emotion recognition, healthcare, etc., this review would be helpful to researchers who aim at exploring more applications that society can quickly adopt in future years of evolution.}
}
@article{MAO2025129607,
title = {Enhancing music audio signal recognition through CNN-BiLSTM fusion with De-noising autoencoder for improved performance},
journal = {Neurocomputing},
volume = {625},
pages = {129607},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.129607},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225002796},
author = {Xiaoying Mao and Ye Tian and Tairan Jin and Bo Di},
keywords = {Audio signal, Enhanced algorithm, Signal recognition, CNN, BiLSTM, Fusion model, De-noising auto-encoder},
abstract = {This study presents an advanced framework for music audio signal recognition that combines Convolutional Neural Networks (CNNs), Bidirectional Long Short-Term Memory (BiLSTM) networks, and Noise Reduction Auto-encoder models to significantly improve accuracy and robustness. The core innovation is a novel noise reduction auto-encoder that integrates CNN and BiLSTM architectures, enabling superior recognition performance under varying noise levels and environmental conditions. The proposed framework, validated on several datasets including the Zhvoice, Common Voice, and LibriSpeech, demonstrates higher accuracy compared to existing methods. In addition, an optimized CNN architecture called Faster Region-based CNN with Multi-scale Information (FRCNN-MSI) is developed for efficient speech feature extraction, which shows significant improvements in noisy environments. The BiLSTM model is further enhanced with an attention mechanism that improves sequence modeling and contextual relationship capture. Together, these advances establish our approach as a robust solution to real-world speech recognition challenges, with potential implications for improving speech recognition systems in diverse applications.}
}
@article{SIGONA2025101691,
title = {A computational analysis of transcribed speech of people living with dementia: The Anchise 2022 Corpus},
journal = {Computer Speech & Language},
volume = {89},
pages = {101691},
year = {2025},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2024.101691},
url = {https://www.sciencedirect.com/science/article/pii/S0885230824000743},
author = {Francesco Sigona and Daniele P. Radicioni and Barbara {Gili Fivela} and Davide Colla and Matteo Delsanto and Enrico Mensa and Andrea Bolioli and Pietro Vigorelli},
keywords = {MMSE, Automatic speech and language analysis, NLP, Digital linguistic biomarkers, Emotion analysis, Perplexity, Naturalistic conversations, Enabling approach},
abstract = {Introduction
Automatic linguistic analysis can provide cost-effective, valuable clues to the diagnosis of cognitive difficulties and to therapeutic practice, and hence impact positively on wellbeing. In this work, we analyzed transcribed conversations between elderly individuals living with dementia and healthcare professionals. The material came from the Anchise 2022 Corpus, a large collection of transcripts of conversations in Italian recorded in naturalistic conditions. The aim of the work was to test the effectiveness of a number of automatic analyzes in finding correlations with the progression of dementia in individuals with cognitive decline as measured by the Mini-Mental State Examination (MMSE) score, which is the only psychometric-clinical information available on the participants in the conversations. Healthy controls (HC) were not considered in this study, nor does the corpus itself include HCs. The main innovation and strength of the work consists in the high ecological validity of the language analyzed (most of the literature to date concerns controlled language experiments); in the use of Italian (there is little corpora for Italian); in the size of the analyzed data (more than 200 conversations were considered); in the adoption of a wide range of NLP methods, that span from traditional morphosyntactic investigation to deep linguistic models for conducting analyzes such as through perplexity, sentiment (polarity) and emotions.
Methods
Analyzing real-world interactions not designed with computational analysis in mind, such as is the case of the Anchise Corpus, is particularly challenging. To achieve the research goals, a wide variety of tools were employed. These included traditional morphosyntactic analysis based on digital linguistic biomarkers (DLBs), transformer-based language models, sentiment and emotion analysis, and perplexity metrics. Analyzes were conducted both on the continuous range of MMSE values and on the severe/moderate/mild categorization suggested by AIFA (Italian Medicines Agency) guidelines, based on MMSE threshold values.
Results and discussion
Correlations between MMSE and individual DLBs were weak, up to 0.19 for positive, and -0.21 for negative correlation values. Nevertheless, some correlations were statistically significant and consistent with the literature, suggesting that people with a greater degree of impairment tend to show a reduced vocabulary, to have anomia, to adopt a more informal linguist register, and to display a simplified use of verbs, with a decrease in the use of participles, gerunds, subjunctive moods, modal verbs, as well as a flattening in the use of the tenses towards the present to the detriment of the past. The -0.26 inverse correlation between perplexity and MMSE suggests that perplexity captures slightly more specific linguistic information, which can complement the MMSE scores. In the categorization tasks, the classifier based on DLBs achieved an F1 score of 0.79 for binary classification between SEVERE and MILD, and 0.61 for multi-label categorization. Sentiment and emotion analyzes showed inverse trends for joy while MMSE scores suggested that less impaired individuals were less joyful, or more “negative”, than others. Considering the real-world context, this is consistent with the hypothesis of a gradual reduction in awareness in individuals affected by dementia. Finally, integrating various profiles of analysis has been proved to be effective in offering a wider picture of linguistic and communication deficits, as well as more precise data regarding the progression of dementia.}
}
@article{MARTINEK20211073,
title = {Noise Reduction in Industry Based on Virtual Instrumentation},
journal = {Computers, Materials and Continua},
volume = {69},
number = {1},
pages = {1073-1096},
year = {2021},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2021.017568},
url = {https://www.sciencedirect.com/science/article/pii/S1546221821011358},
author = {Radek Martinek and Rene Jaros and Jan Baros and Lukas Danys and Aleksandra Kawala-Sterniuk and Jan Nedoma and Zdenek Machacek and Jiri Koziorek},
keywords = {5G, hybrid algorithms, signal processing, speech recognition},
abstract = {This paper discusses the reduction of background noise in an industrial environment to extend human-machine-interaction. In the Industry 4.0 era, the mass development of voice control (speech recognition) in various industrial applications is possible, especially as related to augmented reality (such as hands-free control via voice commands). As Industry 4.0 relies heavily on radiofrequency technologies, some brief insight into this problem is provided, including the Internet of things (IoT) and 5G deployment. This study was carried out in cooperation with the industrial partner Brose CZ spol. s.r.o., where sound recordings were made to produce a dataset. The experimental environment comprised three workplaces with background noise above 100 dB, consisting of a laser/magnetic welder and a press. A virtual device was developed from a given dataset in order to test selected commands from a commercial speech recognizer from Microsoft. We tested a hybrid algorithm for noise reduction and its impact on voice command recognition efficiency. Using virtual devices, the study was carried out on large speakers with 20 participants (10 men and 10 women). The experiments included a large number of repetitions (100 times for each command under different noise conditions). Statistical results confirmed the efficiency of the tested algorithms. Laser welding environment efficiency was 27% before applied filtering, 76% using the least mean square (LMS) algorithm, and 79% using LMS + independent component analysis (ICA). Magnetic welding environment efficiency was 24% before applied filtering, 70% with LMS, and 75% with LMS + ICA. Press workplace environment efficiency showed no success before applied filtering, was 52% with LMS, and was 54% with LMS + ICA.}
}
@article{BIRJALI2021107134,
title = {A comprehensive survey on sentiment analysis: Approaches, challenges and trends},
journal = {Knowledge-Based Systems},
volume = {226},
pages = {107134},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107134},
url = {https://www.sciencedirect.com/science/article/pii/S095070512100397X},
author = {Marouane Birjali and Mohammed Kasri and Abderrahim Beni-Hssane},
keywords = {Sentiment analysis, Opinion mining, Machine learning, Lexicon-based, Sentiment classification, Deep learning},
abstract = {Sentiment analysis (SA), also called Opinion Mining (OM) is the task of extracting and analyzing people’s opinions, sentiments, attitudes, perceptions, etc., toward different entities such as topics, products, and services. The fast evolution of Internet-based applications like websites, social networks, and blogs, leads people to generate enormous heaps of opinions and reviews about products, services, and day-to-day activities. Sentiment analysis poses as a powerful tool for businesses, governments, and researchers to extract and analyze public mood and views, gain business insight, and make better decisions. This paper presents a complete study of sentiment analysis approaches, challenges, and trends, to give researchers a global survey on sentiment analysis and its related fields. The paper presents the applications of sentiment analysis and describes the generic process of this task. Then, it reviews, compares, and investigates the used approaches to have an exhaustive view of their advantages and drawbacks. The challenges of sentiment analysis are discussed next to clarify future directions.}
}
@article{NGO2022107133,
title = {Computerized analysis of speech and voice for Parkinson's disease: A systematic review},
journal = {Computer Methods and Programs in Biomedicine},
volume = {226},
pages = {107133},
year = {2022},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2022.107133},
url = {https://www.sciencedirect.com/science/article/pii/S0169260722005144},
author = {Quoc Cuong Ngo and Mohammod Abdul Motin and Nemuel Daniel Pah and Peter Drotár and Peter Kempster and Dinesh Kumar},
keywords = {Acoustics, Articulation, Parkinson's disease, Phonation, Speech, Voice},
abstract = {Background and objective
Speech impairment is an early symptom of Parkinson's disease (PD). This study has summarized the literature related to speech and voice in detecting PD and assessing its severity.
Methods
A systematic review of the literature from 2010 to 2021 to investigate analysis methods and signal features. The keywords “Automatic analysis” in conjunction with “PD speech” or “PD voice” were used, and the PubMed and ScienceDirect databases were searched. A total of 838 papers were found on the first run, of which 189 were selected. One hundred and forty-seven were found to be suitable for the review. The different datasets, recording protocols, signal analysis methods and features that were reported are listed. Values of the features that separate PD patients from healthy controls were tabulated. Finally, the barriers that limit the wide use of computerized speech analysis are discussed.
Results
Speech and voice may be valuable markers for PD. However, large differences between the datasets make it difficult to compare different studies. In addition, speech analytic methods that are not informed by physiological understanding may alienate clinicians.
Conclusions
The potential usefulness of speech and voice for the detection and assessment of PD is confirmed by evidence from the classification and correlation results.}
}
@article{BELLAGHA202159,
title = {Using the MGB-2 challenge data for creating a new multimodal Dataset for speaker role recognition in Arabic TV Broadcasts},
journal = {Procedia Computer Science},
volume = {192},
pages = {59-68},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921014940},
author = {Mohamed Lazhar Bellagha and Mounir Zrigui},
keywords = {Speaker role recognition, Multimodal Dataset, Multimodal learning, Arabic Multi-Genre Broadcast.},
abstract = {Speaker role recognition is an important component in multimedia analysis for applications such as speaker naming, speaker diarization and video summarization. The lack of labeled datasets for this task has constrained algorithm evaluations. In this paper, we present a new multimodal dataset for speaker role recognition in Arabic TV programs. The dataset is artificially created using data provided by the Multi-Genre Broadcast challenge dataset. We also describe our algorithm for the processing and creation of speaker segments and their corresponding transcripts from audio documents. The spoken transcript and the speaker segments are automatically annotated for their speaker role of presenter, reporter, or a guest speaker. Based on these artificial annotations, we demonstrate for the speaker role labeling the importance of taking into account multimodal information for predicting speaker role. We present a monomodal and multimodal speaker role recognition approaches on speaker segments mined from television programs, with audio and textual classification baselines over a three-way speaker role labeling of presenter, reporter and guest.}
}
@article{BARRON2025103227,
title = {Stance scaffolding and the recognition of commercialisation opportunities in the life science industry},
journal = {Technovation},
volume = {144},
pages = {103227},
year = {2025},
issn = {0166-4972},
doi = {https://doi.org/10.1016/j.technovation.2025.103227},
url = {https://www.sciencedirect.com/science/article/pii/S0166497225000598},
author = {Nicola Barron and Mark Palmer and Monica Masucci and Steven McGuire},
keywords = {Stance scaffolding, Science commercialisation, Opportunity, Life sciences},
abstract = {The challenge of the commercialisation of science from the ‘laboratory environment’ into the ‘field environment’ is a longstanding one. This paper explores the way that this challenge is rooted in the epistemic stance of institutional members working in the life science industry. Drawing upon their lived experiences, this study contributes both conceptually and analytically towards an in-depth understanding of the micro-foundations of the recognition of opportunities to commercialise science. We unravel two unique epistemic stance perspectives from members' experiences – a science stance and a market stance –that members enact in the recognition of opportunities for science commercialisation. We uncover that members may not have to shift from a science stance to a market stance, but critically, iterate between the scaffolding mechanisms of aligning, exchanging, and integrating in order to address the challenge of bridging science and market stances in the commercialisation of scientific opportunities.}
}
@article{GUPTA2025127864,
title = {Investigating the impact of sentiments on stock market using digital proxies: Current trends, challenges, and future directions},
journal = {Expert Systems with Applications},
volume = {285},
pages = {127864},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127864},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425014861},
author = {Tapas Gupta and Shridev Devji and Ashish Kumar Tripathi},
keywords = {Stock market prediction, Social media, Digital news, Machine learning, Deep learning, Large language models},
abstract = {Social media and online news have emerged as significant sources of market sentiment, influencing stock market dynamics globally. With the growing availability of digital data, the current research focus is on leveraging advanced computational techniques for sentiment-driven stock market prediction. The era of financial forecasting has been revolutionized by integrating cutting-edge technologies such as Machine Learning, Deep Learning, and Large Language Models. In this paper, a comprehensive survey of 108 research articles has been undertaken to explore the recent advancements in these technologies, with a focus on utilizing sentiment data extracted from social media platforms and news sources. The technology-wise state-of-the-art findings, current trends, challenges, and literature gaps in this domain are analyzed, and potential future directions are proposed to address these gaps. Additionally, publicly available benchmark datasets for social media and news sentiment indices are compiled and analyzed, with insights into their limitations and potential improvements. A comparative evaluation of prediction methods across heterogeneous user-generated datasets is performed, identifying the most effective techniques for various data types and problem formulations. Recommendations are offered for selecting suitable techniques based on the nature of the data and the specific problem formulation. By incorporating the latest advancements in the field of sentiment analysis and stock market prediction, this work provides actionable insights for researchers and practitioners, advancing the understanding and development of sentiment-driven financial forecasting.}
}
@incollection{GHOSH2023209,
title = {Chapter 9 - Sentiment-aware design of human–computer interactions: How research in human–computer interaction and sentiment analysis can lead to more user-centered systems?},
editor = {Dipankar Das and Anup Kumar Kolya and Abhishek Basu and Soham Sarkar},
booktitle = {Computational Intelligence Applications for Text and Sentiment Data Analysis},
publisher = {Academic Press},
pages = {209-224},
year = {2023},
series = {Hybrid Computational Intelligence for Pattern Analysis and Understanding},
isbn = {978-0-323-90535-0},
doi = {https://doi.org/10.1016/B978-0-32-390535-0.00014-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323905350000148},
author = {Souvick Ghosh},
keywords = {Human-computer interaction (HCI), Sentiment analysis, Emotionally intelligent interfaces, Sentiment-aware systems, Emotion detection and expressiveness, Embodied conversational agents (ECAs), Maslow's hierarchy},
abstract = {In this chapter, we examine how human-computer interaction (HCI) and sentiment analysis research can create more user-centered systems. We define HCI and sentiment analysis and explore their symbiotic relationship. By acknowledging the emotional nature of human users, we discuss the importance of emotionally intelligent interfaces in enhancing user experiences and promoting user retention. Drawing on interdisciplinary knowledge from machine learning, psychology, and cognitive science, we propose affective systems capable of adapting to users' emotional states. We outline the challenges of incorporating emotional intelligence, with an emphasis on emotion detection and expressiveness. We present Embodied Conversational Agents (ECAs) as an example of sentiment-aware systems that facilitate human-like interactions. Utilizing Maslow's hierarchy, we emphasize the significance of recognizing and prioritizing user needs, and propose essential properties for effective conversational agents. Lastly, we suggest methods for multimodal sentiment detection and conclude the chapter with future research directions and ethical considerations.}
}
@article{DUVVURI20241722,
title = {Unravelling stress levels in continuous speech through optimal feature selection and deep learning},
journal = {Procedia Computer Science},
volume = {235},
pages = {1722-1731},
year = {2024},
note = {International Conference on Machine Learning and Data Engineering (ICMLDE 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.04.163},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924008391},
author = {Kavya Duvvuri and Harshitha Kanisettypalli and Teja Nikhil Masabattula and Susmitha Vekkot and Deepa Gupta and Mohammed Zakariah},
keywords = {Feature extraction, MFCC, Chroma features, GFCC, Stress level recognition, CNN, LSTM},
abstract = {Stress is a psychological or emotional strain that occurs due to adverse experiences in human life. This paper showcases the application of deep learning in detecting stress levels in continuous audio signals in the Distress Analysis Interview Corpus Wizard of Oz (DAIC-WOZ) database. The features that have been experimented with are Gammatone Frequency Cepstral Coefficients (GFCC), Log Filter Bank (Log-Filter Bank), Mel Frequency Cepstral Coefficients (MFCC), chroma, and Linear Predictive Coding (LPC). Five deep learning models were evaluated: Long Short-Term Memory (LSTM), Convolutional Neural Network (CNN), Bidirectional LSTM (Bi-LSTM), k-fold CNN with the k value as 5, and a fusion model of CNN, LSTM, and attention. Upon evaluating the performance metrics of all the models, it is concluded that the k-fold CNN model with k as 5 performs well with continuous audio signals. The model has achieved an accuracy of 80% when it is trained on MFCC, GFCC, and Log-F Bank features which are observed to be the optimal features in the stress analysis.}
}
@article{VASHISHTHA2021114323,
title = {Highlighting keyphrases using senti-scoring and fuzzy entropy for unsupervised sentiment analysis},
journal = {Expert Systems with Applications},
volume = {169},
pages = {114323},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.114323},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420310162},
author = {Srishti Vashishtha and Seba Susan},
keywords = {Sentiment analysis, Social media, Keyphrases, N-grams, Linguistic hedges, Fuzzy entropy},
abstract = {Sentiment Analysis is a process that aids in assessing the performance of products or services from user generated online posts. In present time, there are various websites that allow customers to post reviews about movies, products, events or services, etc. This has led to cumulative aggregation of a lot of reviews written in natural language. Prevailing factors such as availability of online reviews and raised end-user expectations have motivated the evolution of opinion mining systems that can automatically classify customers' reviews. It is observed that in Sentiment Analysis (SA), to highlight the significant keyphrases which contribute towards correct sentiment cognition is a tedious task. In this paper, we have proposed an unsupervised sentiment classification system that comprehensively formulates phrases, computes their senti-scores (sentiment scores) and polarity using the SentiWordNet lexicon and fuzzy linguistic hedges. Further it extracts the keyphrases significant for SA using fuzzy entropy filter and k-means clustering. We have deployed document level SA on online reviews using n-gram techniques, specifically combination of unigram, bigram and trigram. Experiments on two benchmark movie review datasets- polarity dataset by Pang and Lee and IMDB dataset, achieve high accuracy for our approach as compared to the other state-of-the-art-methods for phrase-level SA.}
}
@article{SINGHTOMAR202394,
title = {Unimodal approaches for emotion recognition: A systematic review},
journal = {Cognitive Systems Research},
volume = {77},
pages = {94-109},
year = {2023},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2022.10.012},
url = {https://www.sciencedirect.com/science/article/pii/S1389041722000626},
author = {Pragya {Singh Tomar} and Kirti Mathur and Ugrasen Suman},
keywords = {Emotion recognition, Affective computing, Human-computer interaction, Systematic literature review},
abstract = {Affective computing is a rising interdisciplinary field of research spanning the areas from artificial intelligence, natural language processing to cognitive and social sciences. Potential applications comprise of man–machine interaction, healthcare, entertainment, teaching, marketing and many more. Despite the increasing number of papers published in the domains of affective computing, emotion recognition, and human–computer interaction (HCI), there are still gaps in the comprehensive literature review that covers all relevant studies in a single study, which this review attempts to address. As a result, this study provides a systematic literature review (SLR) on existing modalities (unimodals) for emotion recognition, emotion models, and trends in relevant studies by selecting articles published from January 2010 to June 2021. To ensure the retrieval of all relevant studies, a review protocol is used that includes both automatic and manual searches. Based on the research questions, the final 129 papers are reviewed and relevant information is extracted. This SLR provides future research directions to assist novice researchers and practitioners in more efficiently utilizing affective computing techniques.}
}
@article{BAYERL2023101519,
title = {Classification of stuttering – The ComParE challenge and beyond},
journal = {Computer Speech & Language},
volume = {81},
pages = {101519},
year = {2023},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2023.101519},
url = {https://www.sciencedirect.com/science/article/pii/S0885230823000384},
author = {Sebastian P. Bayerl and Maurice Gerczuk and Anton Batliner and Christian Bergler and Shahin Amiriparian and Björn Schuller and Elmar Nöth and Korbinian Riedhammer},
keywords = {Dysfluency, Stuttering, ComParE challenge, Paralinguistics, Pathological speech},
abstract = {The ACM Multimedia 2022 Computational Paralinguistics Challenge (ComParE) featured a sub-challenge on the classification of stuttering in order to bring attention to this important topic and engage a wider research community. Stuttering is a complex speech disorder characterized by blocks, prolongations of sounds and syllables, and repetitions of sounds and words. Accurately classifying the symptoms of stuttering has implications for the development of self-help tools and specialized automatic speech recognition systems (ASR) that can handle atypical speech patterns. This paper provides a review of the challenge contributions and improves upon them with new state-of-the-art classification results for the KSF-C dataset, and explores cross-language training to demonstrate the potential of datasets in multiple languages. To facilitate further research and reproducibility, the full KSF-C dataset, including test-set labels, is also released.}
}
@article{GUO2022110042,
title = {Multiview nonlinear discriminant structure learning for emotion recognition},
journal = {Knowledge-Based Systems},
volume = {258},
pages = {110042},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.110042},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122011352},
author = {Shuai Guo and Li Song and Rong Xie and Lin Li and Shenglan Liu},
keywords = {Multiview subspace learning, Emotion recognition, Nonlinear, Uncorrelated, Out-of-sample},
abstract = {Multiview subspace learning (MSL) has been widely used in various practical applications including emotion recognition. Despite the recent progress in MSL, two challenges remain to address. First, most existing MSL methods indiscriminately utilize both helpful and defective information contained in different views. Second, the most recent methods are linear approaches that do not perform well on emotion datasets with weak linear separability. Therefore, in this study, we introduce a framework for emotion recognition: multiview nonlinear discriminant structure learning (MNDSL). MNDSL fully exploits useful information in each input through local information preservation and discriminant reconstruction (LPDR) and obtains latent subspaces using multiview discriminant latent subspace learning (MDLSL). In addition, an out-of-sample extension was introduced to satisfy the requirements of large-scale applications and obtain the projections of new samples. The proposed framework constructs interviews and intra-view-weighted connections to explore discriminant structures and preserve locality under complementarity and correlation principles. The results demonstrate the superiority of the proposed framework compared with state-of-the-art methods.}
}
@article{HUANG2024102576,
title = {Multimodal federated learning: Concept, methods, applications and future directions},
journal = {Information Fusion},
volume = {112},
pages = {102576},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102576},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524003543},
author = {Wei Huang and Dexian Wang and Xiaocao Ouyang and Jihong Wan and Jia Liu and Tianrui Li},
keywords = {Multimodal learning, Multimodal fusion, Federated learning, Privacy protection, Machine learning},
abstract = {Multimodal learning mines and analyzes multimodal data in reality to better understand and appreciate the world around people. However, how to exploit this rich multimodal data without violating user privacy is a key issue. Federated learning is a privacy-conscious alternative to centralized machine learning, therefore many researchers have combined federated learning with multimodal learning to break down data barriers for the purpose of jointly leveraging multiple modal data from different clients for modeling. In order to provide a systematic summarize of multimodal federated learning, this paper describes the basic mode of multimodal federated learning, multimodal fusion based on federated learning, multimodal federated learning optimization and multimodal federated learning application, and introduces each type of multimodal federated learning methods in detail. Finally, the future research trends of multimodal federated learning are discussed and analyzed, mainly including the optimization of multimodal federated learning, privacy-preserving techniques for multimodal federated learning, multimodal federated few-shot learning & multimodal federated semi-supervised learning, and data and knowledge-driven multimodal federated learning.}
}
@article{DASMENON20213666,
title = {Real time speech analysis},
journal = {Materials Today: Proceedings},
volume = {43},
pages = {3666-3674},
year = {2021},
note = {International Conference on Nanoelectronics, Nanophotonics, Nanomaterials, Nanobioscience & Nanotechnology},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2020.09.841},
url = {https://www.sciencedirect.com/science/article/pii/S2214785320377749},
author = {H. Kishan {Das Menon}},
keywords = {Natural language processing, Real time analysis, Classification, Auto-tagging, Feature Extraction},
abstract = {In the combat area, a great deal of transmissions happens through radio channels. Despite the fact that armed forces have the innovation to catch adversary radio channels, it is as of now intense to identify decipher the voices continuously on the off chance that it is in another dialect. Starting at now, the captured messages are listened physically by human administrators to extricate significant data. Administrators now and again face troubles to isolate voluminous number of calls and unavoidably drop data. Subsequently, there is a need of customized naming them subject to modified voice recognizing confirmation through A.I and following size of model for database on profiling which would engage quick powerful and counter movement in a battle situation. Since the errands include treatment of touchy information, the arrangement doesn't contain any utilization of open APIs which include an outsider assistance or server. The application ought not be web based and all calculation must be done inside the nearby machine.}
}
@article{HAJEK2024103820,
title = {Corporate financial distress prediction using the risk-related information content of annual reports},
journal = {Information Processing & Management},
volume = {61},
number = {5},
pages = {103820},
year = {2024},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103820},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324001791},
author = {Petr Hajek and Michal Munk},
keywords = {Financial distress, Prediction, Annual report, Financial sentiment, Semi-supervised learning, XGBoost},
abstract = {This study presents a financial distress prediction model focusing on the linguistic analysis of risk-related sections of corporate annual reports. Here, we introduce a novel methodology that leverages BERT-based contextualized embedding models for nuanced extraction of financial sentiment and topic coherence. This stands in contrast to existing research, which predominantly relies on dictionary-based or non-contextual word embeddings and addresses their limitations in context sensitivity. Furthermore, we apply an innovative financial distress prediction model that combines the robust XGBoost algorithm with unsupervised outlier detection techniques. This hybrid model is specifically designed to tackle the issue of class imbalance, a persistent challenge in financial distress prediction. The efficacy of the proposed model is empirically validated using a comprehensive dataset of 2545 companies listed on major global stock exchanges. Our findings indicate that the introduced model not only significantly outperforms most existing state-of-the-art financial distress prediction models in terms of predictive accuracy, but also significantly outperforms the Loughran & McDonald dictionary-based approach and the Word2Vec model, underlining its potential as a superior analytical tool for financial distress prediction.}
}
@article{ZHANG2023282,
title = {A Multitask learning model for multimodal sarcasm, sentiment and emotion recognition in conversations},
journal = {Information Fusion},
volume = {93},
pages = {282-301},
year = {2023},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S1566253523000040},
author = {Yazhou Zhang and Jinglin Wang and Yaochen Liu and Lu Rong and Qian Zheng and Dawei Song and Prayag Tiwari and Jing Qin},
keywords = {Multimodal sarcasm recognition, Sentiment analysis, Emotion recognition, Multitask learning, Affective computing},
abstract = {Sarcasm, sentiment and emotion are tightly coupled with each other in that one helps the understanding of another, which makes the joint recognition of sarcasm, sentiment and emotion in conversation a focus in the research in artificial intelligence (AI) and affective computing. Three main challenges exist: Context dependency, multimodal fusion and multitask interaction. However, most of the existing works fail to explicitly leverage and model the relationships among related tasks. In this paper, we aim to generically address the three problems with a multimodal joint framework. We thus propose a multimodal multitask learning model based on the encoder–decoder architecture, termed M2Seq2Seq. At the heart of the encoder module are two attention mechanisms, i.e., intramodal (Ia) attention and intermodal (Ie) attention. Ia attention is designed to capture the contextual dependency between adjacent utterances, while Ie attention is designed to model multimodal interactions. In contrast, we design two kinds of multitask learning (MTL) decoders, i.e., single-level and multilevel decoders, to explore their potential. More specifically, the core of a single-level decoder is a masked outer-modal (Or) self-attention mechanism. The main motivation of Or attention is to explicitly model the interdependence among the tasks of sarcasm, sentiment and emotion recognition. The core of the multilevel decoder contains the shared gating and task-specific gating networks. Comprehensive experiments on four bench datasets, MUStARD, Memotion, CMU-MOSEI and MELD, prove the effectiveness of M2Seq2Seq over state-of-the-art baselines (e.g., CM-GCN, A-MTL) with significant improvements of 1.9%, 2.0%, 5.0%, 0.8%, 4.3%, 3.1%, 2.8%, 1.0%, 1.7% and 2.8% in terms of Micro F1.}
}
@article{ALSELWI2024102068,
title = {RNN-LSTM: From applications to modeling techniques and beyond—Systematic review},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {36},
number = {5},
pages = {102068},
year = {2024},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2024.102068},
url = {https://www.sciencedirect.com/science/article/pii/S1319157824001575},
author = {Safwan Mahmood Al-Selwi and Mohd Fadzil Hassan and Said Jadid Abdulkadir and Amgad Muneer and Ebrahim Hamid Sumiea and Alawi Alqushaibi and Mohammed Gamal Ragab},
keywords = {Machine learning, Deep learning, Recurrent neural networks, Long short-term memory, Weights initialization, Weights optimization, Systematic literature review},
abstract = {Long Short-Term Memory (LSTM) is a popular Recurrent Neural Network (RNN) algorithm known for its ability to effectively analyze and process sequential data with long-term dependencies. Despite its popularity, the challenge of effectively initializing and optimizing RNN-LSTM models persists, often hindering their performance and accuracy. This study presents a systematic literature review (SLR) using an in-depth four-step approach based on the PRISMA methodology, incorporating peer-reviewed articles spanning 2018–2023. It aims to address how weight initialization and optimization techniques can bolster RNN-LSTM performance. This SLR offers a detailed overview across various applications and domains, and stands out by comprehensively analyzing modeling techniques, datasets, evaluation metrics, and programming languages associated with these networks. The findings of this SLR provide a roadmap for researchers and practitioners to enhance RNN-LSTM networks and achieve superior results.}
}
@article{WANG2022105907,
title = {Multi-modal emotion recognition using EEG and speech signals},
journal = {Computers in Biology and Medicine},
volume = {149},
pages = {105907},
year = {2022},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2022.105907},
url = {https://www.sciencedirect.com/science/article/pii/S0010482522006503},
author = {Qian Wang and Mou Wang and Yan Yang and Xiaolei Zhang},
keywords = {Multi-modal emotion database, EEG emotion recognition, Speech emotion recognition, Physiological signal, Data fusion},
abstract = {Automatic Emotion Recognition (AER) is critical for naturalistic Human–Machine Interactions (HMI). Emotions can be detected through both external behaviors, e.g., tone of voice and internal physiological signals, e.g., electroencephalogram (EEG). In this paper, we first constructed a multi-modal emotion database, named Multi-modal Emotion Database with four modalities (MED4). MED4 consists of synchronously recorded signals of participants’ EEG, photoplethysmography, speech and facial images when they were influenced by video stimuli designed to induce happy, sad, angry and neutral emotions. The experiment was performed with 32 participants in two environment conditions, a research lab with natural noises and an anechoic chamber. Four baseline algorithms were developed to verify the database and the performances of AER methods, Identification-vector + Probabilistic Linear Discriminant Analysis (I-vector + PLDA), Temporal Convolutional Network (TCN), Extreme Learning Machine (ELM) and Multi-Layer Perception Network (MLP). Furthermore, two fusion strategies on feature-level and decision-level respectively were designed to utilize both external and internal information of human status. The results showed that EEG signals generate higher accuracy in emotion recognition than that of speech signals (achieving 88.92% in anechoic room and 89.70% in natural noisy room vs 64.67% and 58.92% respectively). Fusion strategies that combine speech and EEG signals can improve overall accuracy of emotion recognition by 25.92% when compared to speech and 1.67% when compared to EEG in anechoic room and 31.74% and 0.96% in natural noisy room. Fusion methods also enhance the robustness of AER in the noisy environment. The MED4 database will be made publicly available, in order to encourage researchers all over the world to develop and validate various advanced methods for AER.}
}
@article{JIANG2023911,
title = {A Robust Conformer-Based Speech Recognition Model for Mandarin Air Traffic Control},
journal = {Computers, Materials and Continua},
volume = {77},
number = {1},
pages = {911-940},
year = {2023},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2023.041772},
url = {https://www.sciencedirect.com/science/article/pii/S1546221823001200},
author = {Peiyuan Jiang and Weijun Pan and Jian Zhang and Teng Wang and Junxiang Huang},
keywords = {Air traffic control, automatic speech recognition, conformer, robustness evaluation, T5 error correction model},
abstract = {This study aims to address the deviation in downstream tasks caused by inaccurate recognition results when applying Automatic Speech Recognition (ASR) technology in the Air Traffic Control (ATC) field. This paper presents a novel cascaded model architecture, namely Conformer-CTC/Attention-T5 (CCAT), to build a highly accurate and robust ATC speech recognition model. To tackle the challenges posed by noise and fast speech rate in ATC, the Conformer model is employed to extract robust and discriminative speech representations from raw waveforms. On the decoding side, the Attention mechanism is integrated to facilitate precise alignment between input features and output characters. The Text-To-Text Transfer Transformer (T5) language model is also introduced to handle particular pronunciations and code-mixing issues, providing more accurate and concise textual output for downstream tasks. To enhance the model’s robustness, transfer learning and data augmentation techniques are utilized in the training strategy. The model’s performance is optimized by performing hyperparameter tunings, such as adjusting the number of attention heads, encoder layers, and the weights of the loss function. The experimental results demonstrate the significant contributions of data augmentation, hyperparameter tuning, and error correction models to the overall model performance. On the Our ATC Corpus dataset, the proposed model achieves a Character Error Rate (CER) of 3.44%, representing a 3.64% improvement compared to the baseline model. Moreover, the effectiveness of the proposed model is validated on two publicly available datasets. On the AISHELL-1 dataset, the CCAT model achieves a CER of 3.42%, showcasing a 1.23% improvement over the baseline model. Similarly, on the LibriSpeech dataset, the CCAT model achieves a Word Error Rate (WER) of 5.27%, demonstrating a performance improvement of 7.67% compared to the baseline model. Additionally, this paper proposes an evaluation criterion for assessing the robustness of ATC speech recognition systems. In robustness evaluation experiments based on this criterion, the proposed model demonstrates a performance improvement of 22% compared to the baseline model.}
}
@article{LEE2025101678,
title = {TadaStride: Using time adaptive strides in audio data for effective downsampling},
journal = {Computer Speech & Language},
volume = {89},
pages = {101678},
year = {2025},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2024.101678},
url = {https://www.sciencedirect.com/science/article/pii/S0885230824000615},
author = {Yoonhyung Lee and Kyomin Jung},
keywords = {Deep learning, Downsampling, Audio data, Speech processing},
abstract = {In this paper, we introduce a new downsampling method for audio data called TadaStride, which can adaptively adjust the downsampling ratios across an audio data instance. Unlike previous methods using a fixed downsampling ratio, TadaStride can preserve more information from task-relevant parts of a data instance by using smaller strides for those parts and larger strides for less relevant parts. Additionally, we also introduce TadaStride-F, which is developed as a more efficient version of TadaStride while maintaining minimal performance loss. In experiments, we evaluate our TadaStride, primarily focusing on a range of audio processing tasks. Firstly, in audio classification experiments, TadaStride and TadaStride-F outperform other widely used standard downsampling methods, even with comparable memory and time usage. Furthermore, through various analyses, we provide an understanding of how TadaStride learns effective adaptive strides and how it leads to improved performance. In addition, through additional experiments on automatic speech recognition and discrete speech representation learning, we demonstrate that TadaStride and TadaStride-F consistently outperform other downsampling methods and examine how the adaptive strides are learned in these tasks.}
}
@article{WALI2022101308,
title = {Generative adversarial networks for speech processing: A review},
journal = {Computer Speech & Language},
volume = {72},
pages = {101308},
year = {2022},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2021.101308},
url = {https://www.sciencedirect.com/science/article/pii/S0885230821001066},
author = {Aamir Wali and Zareen Alamgir and Saira Karim and Ather Fawaz and Mubariz Barkat Ali and Muhammad Adan and Malik Mujtaba},
keywords = {GANs, Speech synthesis, Speech enhancement, Data augmentation, Speech GANs},
abstract = {Generative adversarial networks (GANs) have seen remarkable progress in recent years. They are used as generative models for all kinds of data such as text, images, audio, music, videos, and animations. This paper presents a comprehensive review of the novel and emerging GAN-based speech frameworks and algorithms that have revolutionized speech processing. We have categorized speech GANs based on application areas: speech synthesis, speech enhancement & conversion, and data augmentation in automatic speech recognition and emotion speech recognition systems. This review also includes a summary of the data sets and evaluation metrics commonly used in speech GANs. We also suggest some interesting research directions for future work and highlight the issues faced by current state-of-the-art speech GANs.}
}
@article{WU20251027,
title = {A dual-engine fusion optical character recognition method for fast identification and key information extraction of drug labels},
journal = {Alexandria Engineering Journal},
volume = {128},
pages = {1027-1036},
year = {2025},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2025.05.037},
url = {https://www.sciencedirect.com/science/article/pii/S111001682500657X},
author = {Siyu Wu and Feng Chang},
keywords = {Optical character recognition (OCR), Drug label recognition, Deep learning, Information extraction, Image preprocessing},
abstract = {In the context of smart healthcare and information-driven drug supervision, the automatic recognition and extraction of drug label information presents a significant challenge. Traditional Optical Character Recognition (OCR) methods often struggle with complex backgrounds, diverse fonts, and mixed languages. This paper proposes a dual-engine fusion OCR method combining EasyOCR and CnOCR to enhance recognition accuracy. The method integrates IoT-based data collection for real-time drug information monitoring, utilizing multi-threaded parallel recognition for efficiency and an image preprocessing pipeline (including tilt correction, deblurring, and contrast enhancement). Additionally, a field area positioning and template matching mechanism ensures the precise extraction of key information such as drug name, ingredients, specifications, and expiration date. The approach achieves over 92% accuracy across various real-world scenarios, demonstrating improved robustness and promising potential for digital drug management, as well as IoT-based drug traceability and supervision.}
}
@article{PAN2023126866,
title = {A review of multimodal emotion recognition from datasets, preprocessing, features, and fusion methods},
journal = {Neurocomputing},
volume = {561},
pages = {126866},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.126866},
url = {https://www.sciencedirect.com/science/article/pii/S092523122300989X},
author = {Bei Pan and Kaoru Hirota and Zhiyang Jia and Yaping Dai},
keywords = {Emotion recognition, Multimodal information fusion, Feature learning, Classifier},
abstract = {Affective computing is one of the most important research fields in modern human–computer interaction (HCI). The goal of affective computing is to study and develop the theories, methods, and systems that can recognize, explain, process, and simulate human emotions. As a branch of affective computing, emotion recognition aims to enlighten the machine/computer automatically analyzing human emotions, which has received increasing attention from researchers in various fields. Human beings generally observe and understand the emotional states of one person by integrating the perceived information from his/her facial expressions, voice tone, speech content, behavior, or physiological features. To imitate the emotion observation manner of humans, researchers have been devoted to constructing multimodal emotion recognition models by fusing information from two or more modalities. In this paper, we provide a comprehensive review of multimodal emotion recognition from the perspectives of multimodal datasets, data preprocessing, unimodal feature extraction, and multimodal information fusion methods in recent decades. Furthermore, challenges and future research directions of the topic are specified and discussed. The main motivations of this review are to conclude the recent emergence of abundant works on multimodal emotion recognition and to provide potential guidance to researchers in the related field for understanding the pipeline and mainstream approaches to multimodal emotion recognition.}
}
@article{NGUYEN20231458,
title = {Multimodal Machine Learning for Mental Disorder Detection:A Scoping Review},
journal = {Procedia Computer Science},
volume = {225},
pages = {1458-1467},
year = {2023},
note = {27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.134},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923012929},
author = {Thuy Trinh Nguyen and Viet Hoang-Quoc Pham and Duc-Trong Le and Xuan-Son Vu and Fani Deligianni and Hoang D. Nguyen},
keywords = {Multimodal machine learning, mental disorder diagnosis, depression, stress disorders, bipolar disorders},
abstract = {Recent advancements in machine learning and multimedia technologies have paved new ways for automatic medical diagnosis. In mental health, multimodal inputs such as visual and audible sensing data are promising to investigate the underlying mechanisms of many conditions, such as depression and bipolar disorders. With the increasing burden on healthcare systems, timely diagnosis of mental diseases using multiple modalities might benefit millions of people worldwide. This scoping review provides an exploratory overview of recent multimodal machine learning approaches for mental disorder screening. We also discuss a generalised end-to-end multimodal machine learning pipeline for future research and development of multimodal disease detection.}
}
@article{PARVIN202172,
title = {An Ensemble Technique to Classify Multi-Class Textual Emotion},
journal = {Procedia Computer Science},
volume = {193},
pages = {72-81},
year = {2021},
note = {10th International Young Scientists Conference in Computational Science, YSC2021, 28 June – 2 July, 2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921020494},
author = {Tanzia Parvin and Mohammed Moshiul Hoque},
keywords = {Natural language processing, Textual emotion classification, Emotion corpus, Machine learning, Ensemble},
abstract = {Classifying textual emotion plays a critical role in several HCI applications where the text is utilized as a central means of communication such as messages, reviews, blogs and other Web 2.0 platforms. The extensive usage of the Internet has emerged as an unprecedented means for people to express their feelings or emotion on blogs, social media, and e-commerce sites in recent years. Most of the emotions displayed on the online platforms are in textual forms (such as posts, tweets, comments and reviews), which are unorganized and time-consuming to structured due to their disordered forms. Although several emotion analysis tools are available in high-resource languages, it is critical to developing an automatic emotion classification system for low-resource languages, including Bengali, due to its constrained resources. This paper presents an ML-based ensemble method to classify six primary textual emotions (anger, fear, disgust, sadness, surprise and joy) from Bengali texts. An emotion corpus containing 8047 Bengali texts is developed to perform the textual emotion classification task. This work investigates eight standard ML-based techniques such as logistic regression (LR), multinomial naive Bayes (MNB), support vector machine (SVM), random forest (RF), decision tree (DT), K-nearest neighbour (KNN) and adaptive boosting (AdaBoost) and an ensemble method (a combination of LR, RF, SVM) with Bag of words (BoW) and tf-idf feature extraction techniques. The experimental result demonstrates that the ensemble with tf-idf achieved the highest weighted f1-score of 62.39% compared to other methods.}
}
@article{AHMED2023200171,
title = {A systematic survey on multimodal emotion recognition using learning algorithms},
journal = {Intelligent Systems with Applications},
volume = {17},
pages = {200171},
year = {2023},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2022.200171},
url = {https://www.sciencedirect.com/science/article/pii/S2667305322001089},
author = {Naveed Ahmed and Zaher Al Aghbari and Shini Girija},
keywords = {Multimodal emotion recognition, Virtual reality, Deep learning, Machine learning, Data fusion methods, Fine grained emotions},
abstract = {Emotion recognition is the process to detect, evaluate, interpret, and respond to people's emotional states and emotions, ranging from happiness to fear to humiliation. The COVID- 19 epidemic has provided new and essential impetus for emotion recognition research. The numerous feelings and thoughts shared and posted on social networking sites throughout the COVID-19 outbreak mirrored the general public's mental health. To better comprehend the existing ecology of applied emotion recognition, this work presents an overview of different emotion acquisition tools that are readily available and provide high recognition accuracy. It also compares the most widely used emotion recognition datasets. Finally, it discusses various machine and deep learning classifiers that can be employed to acquire high level features for classification. Different data fusion methods are also explained in detail highlighting their benefits and limitations.}
}
@article{ZHOU2023104562,
title = {Developing a machine learning model for detecting depression, anxiety, and apathy in older adults with mild cognitive impairment using speech and facial expressions: A cross-sectional observational study},
journal = {International Journal of Nursing Studies},
volume = {146},
pages = {104562},
year = {2023},
issn = {0020-7489},
doi = {https://doi.org/10.1016/j.ijnurstu.2023.104562},
url = {https://www.sciencedirect.com/science/article/pii/S002074892300127X},
author = {Ying Zhou and Wei Han and Xiuyu Yao and JiaJun Xue and Zheng Li and Yingxin Li},
keywords = {Anxiety, Apathy, Depression, Emotion recognition, Facial analysis, Machine learning, Mild cognitive impairment, Speech analysis},
abstract = {Background
Depression, anxiety, and apathy are highly prevalent in older people with preclinical dementia and mild cognitive impairment. These symptoms have also proven valuable in predicting the progression from mild cognitive impairment to dementia, enabling a timely diagnosis and treatment. However, objective and reliable indicators to detect and distinguish depression, anxiety, and apathy are relatively scarce.
Objective
This study aimed to develop a machine learning model to detect and distinguish depression, anxiety, and apathy based on speech and facial expressions.
Design
An observational, cross-sectional study design.
Setting(s)
The memory outpatient department of a tertiary hospital.
Participants
319 older adults diagnosed with mild cognitive impairment.
Methods
Depression, anxiety, and apathy were evaluated by the Public Health Questionnaire, General Anxiety Disorder, and Apathy Evaluation Scale, respectively. Speech and facial expressions of older adults with mild cognitive impairment were digitally captured using audio and video recording software. Open-source data analysis toolkits were utilized to extract speech, facial, and text features. The multiclass classification was used to develop classification models, and shapely additive explanations were used to explain the contribution of each feature within the model.
Results
The random forest method was used to develop a multiclass emotion classification model, which performed well in classifying emotions with a weighted-average F1 score of 96.6 %. The model also demonstrated high accuracy, precision, and recall, with 87.4 %, 86.6 %, and 87.6 %, respectively.
Conclusions
The machine learning model developed in this study demonstrated strong classification performance in detecting and differentiating depression, anxiety, and apathy. This innovative approach combines text, audio, and video to provide objective methods for precise classification and remote monitoring of these symptoms in nursing practice.
Registration
This study was registered at the Chinese Clinical Trial Registry (registration number: ChiCTR1900023892; registration date: June 19th, 2019).}
}
@article{NATARAJAN2025103405,
title = {Deep neural networks for speech enhancement and speech recognition: A systematic review},
journal = {Ain Shams Engineering Journal},
volume = {16},
number = {7},
pages = {103405},
year = {2025},
issn = {2090-4479},
doi = {https://doi.org/10.1016/j.asej.2025.103405},
url = {https://www.sciencedirect.com/science/article/pii/S2090447925001467},
author = {Sureshkumar Natarajan and Syed Abdul {Rahman Al-Haddad} and Faisul Arif Ahmad and Raja Kamil and Mohd Khair Hassan and Syaril Azrad and June Francis Macleans and Sadiq H. Abdulhussain and Basheera M. Mahmmod and Nurbek Saparkhojayev and Aigul Dauitbayeva},
keywords = {Acoustic modeling, Denoising, Reverberation, Beamforming, Speech enhancement, Speech recognition, Machine learning, Deep neural network, Systematic review},
abstract = {The field of speech signal processing has undergone significant transformation through extensive research. There is growing interest in Speech Enhancement (SE) and Automatic Speech Recognition (ASR), with SE serving as a crucial preliminary step to enhance ASR performance. This paper addresses key challenges, particularly the need to maintain speech quality and improve intelligibility in ASR systems. Recently, deep learning techniques have emerged as powerful tools for tackling these challenges. This systematic review examines speech enhancement and recognition techniques, emphasizing denoising, acoustic modeling, and beamforming. Various deep learning architectures, such as Deep Neural Networks (DNN), Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), Long Short-Term Memory (LSTM) networks, and Hybrid Neural Networks, are reviewed to highlight their roles in enhancement and recognition. The review specifically details their usage, the features utilized in each study, the databases employed, performance, and limitations, all presented in a structured tabular format. This approach provides valuable insights into the strengths and weaknesses of each method, guiding future advancements in the field. In particular, it emphasizes that LSTM-RNN models excel in temporal signal processing, while hybrid models demonstrate superior performance in optimizing task outcomes. The paper conducts a comprehensive statistical analysis of 187 research papers that exclusively utilize deep neural networks to address the challenges of speech enhancement and recognition, presenting the latest advances in the field. The review examines publications from 2012 to 2024, shedding light on research trends and patterns, while the proposed solutions aim to bridge gaps for researchers in this evolving domain.}
}
@article{SHAHIN2022116080,
title = {Novel dual-channel long short-term memory compressed capsule networks for emotion recognition},
journal = {Expert Systems with Applications},
volume = {188},
pages = {116080},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.116080},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421014172},
author = {Ismail Shahin and Noor Hindawi and Ali Bou Nassif and Adi Alhudhaif and Kemal Polat},
keywords = {Capsule networks, Convolutional neural network, Deep neural network, Dual-channel, Emotion recognition, LSTM},
abstract = {Recent analysis on speech emotion recognition (SER) has made considerable advances with the use of MFCC’s spectrogram features and the implementation of neural network approaches such as convolutional neural networks (CNNs). The fundamental issue of CNNs is that the spatial information is not recorded in spectrograms. Capsule networks (CapsNet) have gained gratitude as alternatives to CNNs with their larger capacities for hierarchical representation. However, the concealed issue of CapsNet is the compression method that is employed in CNNs cannot be directly utilized in CapsNet. To address these issues, this research introduces a text-independent and speaker-independent SER novel architecture, where a dual-channel long short-term memory compressed-CapsNet (DC-LSTM COMP-CapsNet) algorithm is proposed based on the structural features of CapsNet. Our proposed novel classifier can ensure the energy efficiency of the model and adequate compression method in speech emotion recognition, which is not delivered through the original structure of a CapsNet. Moreover, the grid search (GS) approach is used to attain optimal solutions. Results witnessed an improved performance and reduction in the training and testing running time. The speech datasets used to evaluate our algorithm are: Arabic Emirati-accented corpus, English “speech under simulated and actual stress (SUSAS)” corpus, English Ryerson audio-visual database of emotional speech and song (RAVDESS) corpus, and crowd-sourced emotional multimodal actors dataset (CREMA-D). This work reveals that the optimum feature extraction method compared to other known methods is MFCCs delta-delta. Using the four datasets and the MFCCs delta-delta, DC-LSTM COMP-CapsNet surpasses all the state-of-the-art systems, classical classifiers, CNN, and the original CapsNet. Using the Arabic Emirati-accented corpus, our results demonstrate that the proposed work yields average emotion recognition accuracy of 89.3% compared to 84.7%, 82.2%, 69.8%, 69.2%, 53.8%, 42.6%, and 31.9% based on CapsNet, CNN, support vector machine (SVM), multi-layer perceptron (MLP), k-nearest neighbor (KNN), radial basis function (RBF), and naïve Bayes (NB), respectively.}
}
@article{LI2025130308,
title = {Hierarchical cross-modal interaction network for multimodal fake news detection},
journal = {Neurocomputing},
volume = {647},
pages = {130308},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.130308},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225009804},
author = {Zihao Li and Xiaofei Li and Jiaxin Yang and Xianghan Wang and Shuohao Li and Jun Zhang},
keywords = {Multimodal fake news detection, Contrastive learning, Cross-modal fusion},
abstract = {In the rapidly evolving digital information age, multimodal fake news detection has become a critical field of study to ensure the veracity of content that blends text and imagery. Existing methods utilize attention mechanism to fuse the features extracted from unimodal encoder and perform surface-level cross-modal interaction, which is inefficient due to misalignment of multimodal features and limited ability to interpret the consistency between textual and visual information. To address these issues, we present the Hierarchical Cross-Modal Interaction Network (HCMIN) for multimodal fake news detection, a novel architecture that robustly achieve intra- and inter-modality interaction. Specifically, we design dual unimodal branches that independently enhance textual and visual representation, coupled with unimodal-guided co-attention mechanisms that ensure complex intra- and inter-modality feature interactions are captured. Additionally, we propose a hierarchical mutual contrastive learning strategy, which refines the alignment and interactions of modalities at both feature and decision levels, utilizing contrastive loss to spotlight discriminative characteristics of fake news. Extensive experiments on three benchmarks demonstrate the superiority of our HCMIN.}
}
@article{KHEDDAR2023110851,
title = {Deep transfer learning for automatic speech recognition: Towards better generalization},
journal = {Knowledge-Based Systems},
volume = {277},
pages = {110851},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.110851},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123006019},
author = {Hamza Kheddar and Yassine Himeur and Somaya Al-Maadeed and Abbes Amira and Faycal Bensaali},
keywords = {Automatic speech recognition, Deep transfer learning, Fine-tuning, Domain adaptation, Models fusion, Large language model},
abstract = {Automatic speech recognition (ASR) has recently become an important challenge when using deep learning (DL). It requires large-scale training datasets and high computational and storage resources. Moreover, DL techniques and machine learning (ML) approaches in general, hypothesize that training and testing data come from the same domain, with the same input feature space and data distribution characteristics. This assumption, however, is not applicable in some real-world artificial intelligence (AI) applications. Moreover, there are situations where gathering real data is challenging, expensive, or rarely occurring, which cannot meet the data requirements of DL models. deep transfer learning (DTL) has been introduced to overcome these issues, which helps develop high-performing models using real datasets that are small or slightly different but related to the training data. This paper presents a comprehensive survey of DTL-based ASR frameworks to shed light on the latest developments and helps academics and professionals understand current challenges. Specifically, after presenting the DTL background, a well-designed taxonomy is adopted to inform the state-of-the-art. A critical analysis is then conducted to identify the limitations and advantages of each framework. Moving on, a comparative study is introduced to highlight the current challenges before deriving opportunities for future research.}
}
@article{TANVEER2023126436,
title = {Ensemble deep learning in speech signal tasks: A review},
journal = {Neurocomputing},
volume = {550},
pages = {126436},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.126436},
url = {https://www.sciencedirect.com/science/article/pii/S0925231223005593},
author = {M. Tanveer and Aryan Rastogi and Vardhan Paliwal and M.A. Ganaie and A.K. Malik and Javier {Del Ser} and Chin-Teng Lin},
keywords = {Deep learning, Ensemble deep learning, Speech signal, Speech recognition, Speech enhancement},
abstract = {Machine learning methods are extensively used for processing and analysing speech signals by virtue of their performance gains over multiple domains. Deep learning and ensemble learning are the two most commonly used techniques, which results in benchmark performance across different downstream tasks. Ensemble deep learning is a recent development which combines these two techniques to result in a robust architecture having substantial performance gains, as well as better generalization performance over the individual techniques. In this paper, we extensively review the use of ensemble deep learning methods for different speech signal related tasks, ranging from general objectives such as automatic speech recognition and voice activity detection, to more specific areas such as biomedical applications involving the detection of pathological speech or music genre detection. We provide a discussion on the use of different ensemble strategies such as bagging, boosting and stacking in the context of speech signals, and identify the various salient features and advantages from a broader perspective when coupled with deep learning architectures. The main objective of this study is to comprehensively evaluate existing works in the area of ensemble deep learning, and highlight the future directions that may be explored to further develop it as a tool for several speech related tasks. To the best of our knowledge, this is the first review study which primarily focuses on ensemble deep learning for speech applications. This study aims to serve as a valuable resource for researchers in academia and in industry working with speech signals, supporting advanced novel applications of ensemble deep learning models towards solving challenges in existing speech processing systems.}
}
@article{2025iii,
title = {Contents},
journal = {Procedia Computer Science},
volume = {258},
pages = {iii-xxvi},
year = {2025},
note = {International Conference on Machine Learning and Data Engineering},
issn = {1877-0509},
doi = {https://doi.org/10.1016/S1877-0509(25)01840-X},
url = {https://www.sciencedirect.com/science/article/pii/S187705092501840X}
}
@article{LIU2022100616,
title = {Audio self-supervised learning: A survey},
journal = {Patterns},
volume = {3},
number = {12},
pages = {100616},
year = {2022},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2022.100616},
url = {https://www.sciencedirect.com/science/article/pii/S2666389922002410},
author = {Shuo Liu and Adria Mallol-Ragolta and Emilia Parada-Cabaleiro and Kun Qian and Xin Jing and Alexander Kathan and Bin Hu and Björn W. Schuller},
keywords = {self-supervised learning, audio and speech processing, multi-modal SSL, representation learning, unsupervised learning},
abstract = {Summary
Similar to humans’ cognitive ability to generalize knowledge and skills, self-supervised learning (SSL) targets discovering general representations from large-scale data. This, through the use of pre-trained SSL models for downstream tasks, alleviates the need for human annotation, which is an expensive and time-consuming task. Its success in the fields of computer vision and natural language processing have prompted its recent adoption into the field of audio and speech processing. Comprehensive reviews summarizing the knowledge in audio SSL are currently missing. To fill this gap, we provide an overview of the SSL methods used for audio and speech processing applications. Herein, we also summarize the empirical works that exploit audio modality in multi-modal SSL frameworks and the existing suitable benchmarks to evaluate the power of SSL in the computer audition domain. Finally, we discuss some open problems and point out the future directions in the development of audio SSL.}
}
@incollection{REEJA2021107,
title = {Chapter 7 - EEG signal-based human emotion detection using an artificial neural network},
editor = {Hemanth D. Jude},
booktitle = {Handbook of Decision Support Systems for Neurological Disorders},
publisher = {Academic Press},
pages = {107-124},
year = {2021},
isbn = {978-0-12-822271-3},
doi = {https://doi.org/10.1016/B978-0-12-822271-3.00007-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128222713000074},
author = {S.R. Reeja and Rino Cherian and Kiran Waghmare and  Jothimani},
keywords = {ANN, DEAP dataset, EEG, Emotion recognition, Emotional intelligence},
abstract = {Our lives are filled with different emotions. Emotion is a remarkable feature in human relationships. It is classified into positive and negative emotions. Whenever a negative emotion lasts for a long time, it will cause stress or mental issues. With the increasing number of individuals suffering from stress and/or life’s pressures, it is essential to have the option to recognize these issues at their initial stages and assist individuals by acknowledging and correcting them before too much harm is caused. Recognition of emotions by physiological means is a subject of interest for many researchers. Due to having versatile role, it persuades human to react to stimuli in their environment rapidly for improving their communication, learning and decision-making. Emotion recognition could be done from the facial appearance, motion, speech and text, and could be record in different ways, e.g., Electroencephalography, Positron Emission Tomography, Magnetic Resonance Imaging, and so forth. The fundamental favorable reason for using EEG signals is that they recognize genuine feelings emerging directly from the brain and disregard outside influences like outward appearances or emotions. Thus EEG can be genuine indicator of feelings delineated by the subject. Utilizing suitable features for extracting emotional state and suitable learner like Artificial Neural Network (ANN), we can find emotions correctly. As a result, an ANN classifier gives an accuracy of 96.5%.}
}
@article{TRIANTAFYLLOPOULOS2025101802,
title = {Vishing: Detecting social engineering in spoken communication — A first survey & urgent roadmap to address an emerging societal challenge},
journal = {Computer Speech & Language},
volume = {94},
pages = {101802},
year = {2025},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2025.101802},
url = {https://www.sciencedirect.com/science/article/pii/S0885230825000270},
author = {Andreas Triantafyllopoulos and Anika A. Spiesberger and Iosif Tsangko and Xin Jing and Verena Distler and Felix Dietz and Florian Alt and Björn W. Schuller},
keywords = {Vishing, Social engineering, Human–computer interaction, Computational paralinguistics},
abstract = {Vishing – the use of voice calls for phishing – is a form of Social Engineering (SE) attacks. The latter have become a pervasive challenge in modern societies, with over 300,000 yearly victims in the US alone. An increasing number of those attacks is conducted via voice communication, be it through machine-generated ‘robocalls’ or human actors. The goals of ‘social engineers’ can be manifold, from outright fraud to more subtle forms of persuasion. Accordingly, social engineers adopt multi-faceted strategies for voice-based attacks, utilising a variety of ‘tricks’ to exert influence and achieve their goals. Importantly, while organisations have set in place a series of guardrails against other types of SE attacks, voice calls still remain ‘open ground’ for potential bad actors. In the present contribution, we provide an overview of the existing speech technology subfields that need to coalesce into a protective net against one of the major challenges to societies worldwide. Given the dearth of speech science and technology works targeting this issue, we have opted for a narrative review that bridges the gap between the existing psychological literature on the topic and research that has been pursued in parallel by the speech community on some of the constituent constructs. Our review reveals that very little literature exists on addressing this very important topic from a speech technology perspective, an omission further exacerbated by the lack of available data. Thus, our main goal is to highlight this gap and sketch out a roadmap to mitigate it, beginning with the psychological underpinnings of vishing, which primarily include deception and persuasion strategies, continuing with the speech-based approaches that can be used to detect those, as well as the generation and detection of AI-based vishing attempts, and close with a discussion of ethical and legal considerations.}
}
@article{SAADI2024122784,
title = {Driver’s facial expression recognition: A comprehensive survey},
journal = {Expert Systems with Applications},
volume = {242},
pages = {122784},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122784},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423032864},
author = {Ibtissam Saadi and Douglas W. cunningham and Abdelmalik Taleb-Ahmed and Abdenour Hadid and Yassin El Hillali},
keywords = {Driver’s facial expression recognition, Driver emotion recognition, Facial expression recognition datasets, Advanced driver assistant systems (ADAS), Driving safety},
abstract = {Driving is an integral part of daily life for millions of people worldwide, and it has a profound impact on road safety and human health. The emotional state of the driver, including feelings of anger, happiness, or fear, can significantly affect their ability to make safe driving decisions. Recognizing the facial expressions of drivers(DFER) has emerged as a promising technique for improving road safety and can provide valuable information about their emotions, This information can be used by intelligent transportation systems (ITS), like advanced driver assistance systems (ADAS) to take appropriate decision, such as alerting the driver or intervening in the driving process, to prevent the potential risks. This survey paper presents a comprehensive survey of recent studies that focus on the problem of recognizing the facial expression of driver recognition in the driving context from 2018 to March 2023. Specifically, we examine studies that address the recognition of the driver’s emotion using facial expressions and explore the challenges that exist in this field, such as illumination conditions, occlusion, and head poses. Our survey includes an analysis of different techniques and methods used to identify and categorize specific expressions or emotions of the driver. We begin by reviewing and comparing available datasets and summarizing state-of-the-art methods, including machine learning-based methods, deep learning-based methods, and hybrid methods. We also identify limitations and potential areas for improvement. Overall, our survey highlights the importance of recognizing driver facial expressions in improving road safety and provides valuable insights into recent developments and future research directions in this field.}
}
@article{VENKATESHPERUMAL2025100010,
title = {LLM-Augmented VoIP for Emergency Call Triage},
journal = {Measurement: Digitalization},
volume = {2-3},
pages = {100010},
year = {2025},
issn = {3050-6441},
doi = {https://doi.org/10.1016/j.meadig.2025.100010},
url = {https://www.sciencedirect.com/science/article/pii/S3050644125000106},
author = {Danush Venkateshperumal and Rahman Abdul Rafi and Shakil Ahmed and Ashfaq Khokhar},
keywords = {LLM, Retrieval-augmented input conditioning, FAISS, TF-IDF, Emergency services, Prioritization, Signal reconstruction, APIs, VoIP, Packet loss},
abstract = {Emergency communication systems often suffer from disruptions due to packet loss, bandwidth limitations, and poor VoIP signal quality, making it difficult for dispatchers to assess the urgency of a situation. Additionally, victims in distress may struggle to convey critical information due to panic, speech disorders, or background noise, leading to incomplete or ambiguous communication. This paper proposes an LLM-based real-time speech reconstruction and emergency call prioritization system to address these challenges. The proposed system integrates real-time audio transcription with retrieval-augmented input conditioning using TF-IDF and FAISS indexing to retrieve relevant emergency transcripts, enhancing contextual understanding. Unlike traditional Retrieval-Augmented Generation (RAG) models, this approach eliminates the need for fine-tuning, making it lightweight and efficient for real-time applications. The reconstructed speech output is further classified using a GPT-based severity detection model, ensuring that critical emergencies receive immediate attention via Twilio and AssemblyAI APIs. Experimental evaluations demonstrate high Conceptual Precision and favorable BLEU and ROUGE scores, indicating strong alignment with real-world emergency scenarios. The results highlight the system’s potential to improve emergency response efficiency, optimize call triaging, and reduce communication delays in high-risk situations}
}
@article{PUDASAINI2025130334,
title = {A comprehensive study of audio profiling: Methods, applications, challenges, and future directions},
journal = {Neurocomputing},
volume = {640},
pages = {130334},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.130334},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225010069},
author = {Anil Pudasaini and Muna Al-Hawawreh and Mohamed Reda Bouadjenek and Hakim Hacid and Sunil Aryal},
keywords = {Audio profiling, User profiling, Speaker profiling, Voice privacy, Privacy preservation, Personality detection, Age detection, Emotion detection, Acoustic event detection, Personality traits detection, Voice pathology detection, Mental health inference, Acoustic scene classification, Gender detection},
abstract = {Audio profiling is at the forefront of a technological breakthrough, offering rich insights into human behavior, emotions, physical attributes, and environmental contexts through detailed analysis of voice data. As we embrace an era where the integration of smart technologies equipped with the ability to capture sound is becoming ubiquitous, the capacity to accurately infer personal traits such as age, gender, height, weight, emotional state, personality, and even environmental contexts through voice analysis opens up vast opportunities across law enforcement, healthcare, social and commercial services, and entertainment. This emerging field promises to enhance our interaction with technology by not only understanding who we are but also by interpreting the world around us. However, the remarkable landscape is fraught with challenges, including data imbalances, the complexity of predictive models, and significant privacy concerns regarding the handling of sensitive paralinguistic information. This survey explores deep into the current landscape of audio profiling, examining the techniques and datasets in use, and showcasing its diverse applications while highlighting the need for advanced methodologies, enriched dataset development, and robust privacy preservation techniques.}
}
@article{MAI2023542,
title = {Excavating multimodal correlation for representation learning},
journal = {Information Fusion},
volume = {91},
pages = {542-555},
year = {2023},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2022.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S1566253522002135},
author = {Sijie Mai and Ya Sun and Ying Zeng and Haifeng Hu},
keywords = {Multimodal sentiment analysis, Multimodal representation learning, Correlation learning, Multimodal emotion recognition, Multimodal humor detection},
abstract = {A majority of previous methods for multimodal representation learning ignore the rich correlation information inherently stored in each sample, leading to a lack of robustness when trained on small datasets. Although a few contrastive learning frameworks leverage that information in a self-supervised manner, they generally encourage the intra-sample unimodal representations to be identical, neglecting the modality-specific information carried by individual modalities. In contrast, we propose a novel algorithm that learns the correlations between modalities to facilitate downstream multimodal tasks by leveraging the prior information across samples, and we explore the feasibility of the proposed method on elaborately designed unsupervised and supervised auxiliary learning tasks. Specifically, we construct the positive and negative sets for correlation learning as unimodal embeddings from the same sample and from different samples, respectively. A weak predictor is employed on the concatenated unimodal embeddings to learn the correspondence relationship for each set. In this way, the model can correlate unimodal features and discover the shared information across modalities. In contrast to contrastive learning methods, the proposed framework is compatible with any number of modalities and can retain modality-specific information, enabling multimodal representation to capture richer information. Moreover, in the supervised version, one of the main novelties is that the sample labels are further utilized to learn more discriminative features, where the assigned correlation scores of negative sets vary according to the label variations between the associated samples. Extensive experiments suggest that the proposed method reaches state-of-the-art performance on the tasks of multimodal sentiment analysis, emotion recognition, and humor detection, and can improve the performance of various fusion approaches.}
}
@article{FAVARO2023107559,
title = {Interpretable speech features vs. DNN embeddings: What to use in the automatic assessment of Parkinson’s disease in multi-lingual scenarios},
journal = {Computers in Biology and Medicine},
volume = {166},
pages = {107559},
year = {2023},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2023.107559},
url = {https://www.sciencedirect.com/science/article/pii/S0010482523010247},
author = {Anna Favaro and Yi-Ting Tsai and Ankur Butala and Thomas Thebaud and Jesús Villalba and Najim Dehak and Laureano Moro-Velázquez},
keywords = {Parkinson’s disease, Machine learning, Deep learning, Interpretable features, Speech},
abstract = {Speech-based approaches for assessing Parkinson’s Disease (PD) often rely on feature extraction for automatic classification or detection. While many studies prioritize accuracy by using non-interpretable embeddings from Deep Neural Networks, this work aims to explore the predictive capabilities and language robustness of both feature types in a systematic fashion. As interpretable features, prosodic, linguistic, and cognitive descriptors were adopted, while x-vectors, Wav2Vec 2.0, HuBERT, and TRILLsson representations were used as non-interpretable features. Mono-lingual, multi-lingual, and cross-lingual machine learning experiments were conducted leveraging six data sets comprising speech recordings from various languages: American English, Castilian Spanish, Colombian Spanish, Italian, German, and Czech. For interpretable feature-based models, the mean of the best F1-scores obtained from each language was 81% in mono-lingual, 81% in multi-lingual, and 71% in cross-lingual experiments. For non-interpretable feature-based models, instead, they were 85% in mono-lingual, 88% in multi-lingual, and 79% in cross-lingual experiments. Firstly, models based on non-interpretable features outperformed interpretable ones, especially in cross-lingual experiments. Specifically, TRILLsson provided the most stable and accurate results across tasks and data sets. Conversely, the two types of features adopted showed some level of language robustness in multi-lingual and cross-lingual experiments. Overall, these results suggest that interpretable feature-based models can be used by clinicians to evaluate the deterioration of the speech of patients with PD, while non-interpretable feature-based models can be leveraged to achieve higher detection accuracy.}
}
@article{YU2025129195,
title = {TACL: A Trusted Action-enhanced Curriculum Learning Approach to Multimodal Affective Computing},
journal = {Neurocomputing},
volume = {620},
pages = {129195},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.129195},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224019660},
author = {Tan Yu and Jingjing Wang and Jiamin Luo and Jiawen Wang and Guodong Zhou},
keywords = {Dynamic and temporal action information, Trusted learning, Curriculum learning, Multimodal Affective Computing},
abstract = {Previous studies on Multimodal Affective Computing (MAC) predominantly focus on leveraging language, acoustic, and facial information to identify human’s affective states, which largely ignore the dynamic and temporal action information, despite such information being crucial for precisely inferring the affective states. In this way, this paper first attempts to consider the action information for MAC and further argues that exploiting the action information faces two key challenges, i.e., credibility and sparsity challenges. To this end, this paper proposes a new Trusted Action-enhanced Curriculum Learning (TACL) approach to incorporate the action information for boosting MAC. Specifically, this approach designs two main components, i.e., the Trusted Curriculum Learning block and the Action-enhanced Vision Regulator, to address the above credibility challenge and sparsity challenge. Furthermore, a high-quality action-enhanced video dataset is constructed to evaluate TACL and detailed evaluations show the great advantage of TACL over the state-of-the-art baselines. Particularly, an interesting finding is observed that action information is more conducive to facilitating the recognition of negative emotions, which aligns with the intuition that humans prefer using actions more when expressing negative emotions.}
}
@article{JI2025129138,
title = {Multimodal large model pretraining, adaptation and efficiency optimization},
journal = {Neurocomputing},
volume = {619},
pages = {129138},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.129138},
url = {https://www.sciencedirect.com/science/article/pii/S092523122401909X},
author = {Lixia Ji and Shijie Xiao and Jingmei Feng and Wenzhao Gao and Han Zhang},
keywords = {Multi-modal learning, Pretrained model, Adaptation fine-tuning, Efficient training},
abstract = {Multimodal large models, leveraging extensive datasets and parameters, have provided superior solutions for multimodal tasks and have been widely applied across various domains in everyday life. However, the development of multimodal large models also faces numerous challenges. Among the most critical challenges are how to scientifically and rationally pre-train multimodal large models and how to apply these pre-trained models to downstream tasks more effectively and at a lower cost. In response to these challenges, this paper initially analyzes and summarizes various multimodal pre-training methods employed by state-of-the-art multimodal large models. By analyzing the characteristics these methods, the paper elucidates the different capabilities imparted to multimodal large models by various multimodal pre-training methods. This provides a basis for researchers to select multimodal pre-training methods according to specific needs. Subsequently, the paper categorizes and outlines general adaptation methods for downstream tasks using multimodal pre-trained models. By comparing their advantages and disadvantages, the distinct characteristic of each method are highlighted. Furthermore, we categorize multimodal downstream tasks into five types. Then, for different types of tasks based on their characteristics and application scenarios, we outline suitable adaptation methods, offering researchers a more definitive direction for adaptation studies related to various types of tasks. Additionally, from the perspectives of parameter efficiency, memory efficiency, and efficiency of data utilization, a comparative analysis of research on the efficiency optimization of multimodal large models is conducted. In conclusion, the paper summarizes the shortcomings of existing research and looks forward to the issues and development directions that need to be addressed for multimodal large models. It points out that integrating more modalities into multimodal large models, reducing resource consumption costs, enhancing model explainability and transparency, and addressing privacy and security issues will be hotspots for future research.}
}
@article{WANG2025100759,
title = {Towards cognitive intelligence-enabled product design: The evolution, state-of-the-art, and future of AI-enabled product design},
journal = {Journal of Industrial Information Integration},
volume = {43},
pages = {100759},
year = {2025},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2024.100759},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X24002024},
author = {Zuoxu Wang and Xinxin Liang and Mingrui Li and Shufei Li and Jihong Liu and Lianyu Zheng},
keywords = {Engineering product design, Cognitive computing, Knowledge graph, Industrial products and services design, Design knowledge support, Knowledge reasoning},
abstract = {Engineering design researchers have increasing interests in leveraging artificial intelligence (AI) techniques to a wide range of product design tasks, such as customer requirement analysis, product concept generation, design synthesis, and decision-making in product design. Indeed, AI techniques perform excellently on well-defined design tasks with clear problem definition, specialized solutions, and abundant training data. However, facing the ever-evolving AI techniques rapidly and radically changing the product design manner, there is still a lack of a systematic summary about the current stage of AI-enabled product design. Besides, although the current AI-enabled product design performs excellently on the well-defined tasks, the other advanced design tasks that need cognitive capability can still hardly be satisfyingly completed by the current product design system. This study systematically reviewed the literature on AI-enabled product design to understand its evolution and state-of-the-arts. To bridge the semantic gap between humans and systems, a novel cognitive intelligence-enabled product design (CIPD) framework is proposed, in which cognitive intelligence is the key enabler. The CIPD's key aspects, including its system architecture, human-like capabilities, enabling technologies, and potential applications, are also systematically discussed. It is hoped that this study could contribute to the future directions of the product design field and offer insightful guidance to the practitioners and researchers in their product design process.}
}
@article{ELFAIK2023462,
title = {Leveraging feature-level fusion representations and attentional bidirectional RNN-CNN deep models for Arabic affect analysis on Twitter},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {35},
number = {1},
pages = {462-482},
year = {2023},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2022.12.015},
url = {https://www.sciencedirect.com/science/article/pii/S1319157822004335},
author = {Hanane Elfaik and El Habib Nfaoui},
keywords = {Emotional analysis, Feature-level fusion, Deep learning, Attention mechanism, Multilabel classification, Arabic language},
abstract = {Arabic affect analysis on Twitter avidly helps to capture the emotional states of individuals being expressed regarding many targets, such as world-level events, products, and services. It is the key to monitoring and advancing human intelligence, which impacts human decision-making processes efficiently. However, state-of-the-art models have not witnessed serious developments yet since they have just achieved an accuracy of around 54%. This inaccuracy is mainly due to the agglutination, dialectal variation, and morphological richness of the Arabic language, as well as the unique features of tweets, such as shortness, noisiness, and informal language. This paper presents an approach that tackles these challenges and then improves the performance of Arabic affect analysis on Twitter. First, we propose a novel feature-based fusion representation for Arabic tweets to capture the polysemy, semantic/syntactic information, and conveyed emotional knowledge; and to deal with Out-Of-Vocabulary (OOV) words. Second, we propose an attentional deep learning model based on Bidirectional Gated Recurrent Unit (BiGRU), Bidirectional Long Short-Term Memory (BiLSTM), and Convolution Neural Network (CNN) to effectively learn local and global features and provide a multilabel emotional classification. The experimental results indicate that our proposal outperforms twelve state-of-the-art and baseline methods with a significant improvement of 6% in accuracy.}
}
@article{M2023102513,
title = {Cognitive computing technological trends and future research directions in healthcare — A systematic literature review},
journal = {Artificial Intelligence in Medicine},
volume = {138},
pages = {102513},
year = {2023},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2023.102513},
url = {https://www.sciencedirect.com/science/article/pii/S0933365723000271},
author = {Srivani M. and Abirami Murugappan and Mala T.},
keywords = {Cognitive computing systems, Intelligent systems, Healthcare, Preclusion, Prognosis, Medical prodigy},
abstract = {Background and aim:
Cognitive Computing systems are the intelligent systems that thinks, understands and augments the capabilities of human brain by blending the technologies of Artificial Intelligence, Machine Learning and Natural Language Processing. In recent days, maintenance or enhancement of health by preclusion, prognosis, and analysis of diseases has become a challenging task. The increasing diseases and its causes becomes a big question before humanity. Limited risk analysis, meticulous training process, and automated critical decision-making are some of the issues of cognitive computing. To overcome this issue, cognitive computing in healthcare works like a medical prodigy which anticipates the disease or illness of the human being and helps the doctors with technological facts to take the timely action. The main aim of this survey article is to explore the present and futuristic technological trends of cognitive computing in healthcare. In this work, different cognitive computing applications are reviewed, and the best application is recommended to the clinicians. Based on this recommendation, the clinicians are able to monitor and analyze the physical health of patients.
Methods:
This article presents the systematic literature on the different aspects of cognitive computing in healthcare. Nearly seven online databases such as SCOPUS, IEEE Xplore, Google Scholar, DBLP, Web of Science, Springer and PubMed were screened and the published articles related to cognitive computing in healthcare is collected from 2014 to 2021. In total, 75 articles were selected, examined and their pros and cons are analyzed. The analysis is done with respect to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines.
Results:
The basic findings of this review article and their significance for theory and practice are mindmaps portraying the cognitive computing platforms, cognitive applications in healthcare, and use cases of cognitive computing in healthcare. A detailed discussion section highlighting the present issues, future research directions and recent applications of cognitive computing in healthcare. Accuracy analysis of different cognitive systems conclude that the Medical Sieve achieves 0.95 and Watson For Oncology (WFO) achieves 0.93 and hence proves to be the prominent computing systems for healthcare.
Conclusions:
Cognitive computing, an evolving technology in healthcare augments the clinical thought process and enable the doctors to make the right diagnosis and preserve the patient’s health in good condition. These systems provides timely care, optimal and cost-effective treatment. This article provides an extensive survey of the importance of cognitive computing in the health sector by highlighting the platforms, techniques, tools, algorithms, applications, and use cases. This survey also explores about the works in the literature on present issues and proposes the future research directions of applying cognitive systems in healthcare.}
}
@article{BENDIAB2025103935,
title = {Deepfakes in digital media forensics: Generation, AI-based detection and challenges},
journal = {Journal of Information Security and Applications},
volume = {88},
pages = {103935},
year = {2025},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2024.103935},
url = {https://www.sciencedirect.com/science/article/pii/S2214212624002370},
author = {Gueltoum Bendiab and Houda Haiouni and Isidoros Moulas and Stavros Shiaeles},
keywords = {Deepfake, Artificial intelligence, Digital media forensics, Security, Deepfake detection},
abstract = {Deepfake technology presents significant challenges for digital media forensics. As deepfakes become increasingly sophisticated, the ability to detect and attribute manipulated media becomes more difficult. The main challenge lies in the realistic and convincing nature of deepfakes, which can deceive human perception and traditional forensic techniques. Furthermore, the widespread availability of open-source deepfake tools and increasing computational power contribute to the ease with which malicious actors can create and disseminate deepfakes. The challenges posed by deepfakes for digital media forensics are multifaceted. Therefore, the development of sophisticated detection algorithms, the creation of comprehensive datasets, and the establishment of legal frameworks are crucial in addressing these challenges. This paper provides a comprehensive analysis of current methods for deepfake generation and the issues surrounding their detection. It also explores the potential of modern AI-based detection techniques in combating the proliferation of deepfakes. This analysis aims to contribute to advancing deepfake detection by highlighting the limits of current detection techniques, the most relevant issues, the upcoming challenges, and suggesting future directions for research.}
}
@article{FATEHI2025103151,
title = {An overview of high-resource automatic speech recognition methods and their empirical evaluation in low-resource environments},
journal = {Speech Communication},
volume = {167},
pages = {103151},
year = {2025},
issn = {0167-6393},
doi = {https://doi.org/10.1016/j.specom.2024.103151},
url = {https://www.sciencedirect.com/science/article/pii/S0167639324001225},
author = {Kavan Fatehi and Mercedes {Torres Torres} and Ayse Kucukyilmaz},
keywords = {Automatic speech recognition, End-to-end model, Deep learning models, Low-resource environment},
abstract = {Deep learning methods for Automatic Speech Recognition (ASR) often rely on large-scale training datasets, which are typically unavailable in low-resource environments (LREs). This lack of sufficient and representative training data poses a significant challenge for applying ASR systems in specific domains categorized as LREs. In this paper, we provide a comprehensive overview and empirical analysis of state-of-the-art deep learning techniques for ASR, which are primarily designed for high-resource environments (HREs). Our aim is to explore their potential effectiveness in LRE settings. We focus on identifying key factors that influence the adaptation of HRE models to LRE tasks. To this end, we survey advanced deep learning models and conduct a comparative evaluation of their performance in LRE contexts. Additionally, we propose that pre-training ASR models on HRE datasets, followed by domain-specific fine-tuning on LRE data, can significantly enhance performance in data-scarce settings. Using LibriSpeech and WSJ as our HRE datasets, we evaluate these models on two LRE datasets: UASpeech for dysarthria speech and iCUBE, our novel human–robot interaction dataset. Our systematic experiments, involving varying dataset sizes for pre-training, demonstrate the efficacy of combining pre-training and fine-tuning strategies to improve recognition accuracy in LREs.}
}
@article{DECLEEN2025115192,
title = {The influence of emotions and communication style on customer satisfaction and recommendation in a call center context: An NLP-based analysis},
journal = {Journal of Business Research},
volume = {189},
pages = {115192},
year = {2025},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2025.115192},
url = {https://www.sciencedirect.com/science/article/pii/S0148296325000153},
author = {Thomas {De Cleen} and Philippe Baecke and Frank Goedertier},
keywords = {Call Center, Emotion, Customer Satisfaction, Net promotor score (NPS), Computational Linguistics},
abstract = {We study the impact of customer sentiment, agent sentiment, and emotional matching (i.e., call center agents matching emotional expressive states of customers) on satisfaction and recommendation intentions in a utilitarian service context. We methodologically contribute by text mining observed data using advanced transformer-based NLP algorithms and compare findings with those of previous survey-based research. An analysis of 25,008 call center conversations reveals that positive (vs negative) customer sentiment more strongly impacts satisfaction and recommendation. For recommendation (vs satisfaction) we observe that negative emotional expressions have a relatively stronger weight, albeit less strong than that of positive ones. We find that emotional expressions of call center agents (vs those of clients) have a smaller impact on these outcomes. Emotional matching is observed as beneficial, but not necessarily when faced with negative high-arousal emotional expressions. As conceptual grounding, we refer to theorizing around delight, formality, source credibility, emotional arousal and loss aversion.}
}
@article{MISHRA2022393,
title = {Uncanny valley for interactive social agents: an experimental study},
journal = {Virtual Reality & Intelligent Hardware},
volume = {4},
number = {5},
pages = {393-405},
year = {2022},
note = {Computer graphics for metaverse},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2022.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S209657962200078X},
author = {Nidhi Mishra and Manoj Ramanathan and Gauri Tulsulkar and Nadia Magneat Thalmann},
keywords = {Uncanny valley hypothesis, Human robot interaction, Interactive robots, Humanoid robots, Virtual humans},
abstract = {Background
The uncanny valley hypothesis states that users may experience discomfort when interacting with almost human-like artificial characters. Advancements in artificial intelligence, robotics, and computer graphics have led to the development of life-like virtual humans and humanoid robots. Revisiting this hypothesis is necessary to check whether they positively or negatively affect the current population, who are highly accustomed to the latest technologies.
Methods
In this study, we present a unique evaluation of the uncanny valley hypothesis by allowing participants to interact live with four humanoid robots that have varying levels of human-likeness. Each participant completed a survey questionnaire to evaluate the affinity of each robot. Additionally, we used deep learning methods to quantify the participants’ emotional states using multimodal cues, including visual, audio, and text cues, by recording the participant–robot interactions.
Results
Multi-modal analysis and surveys provided interesting results and insights into the uncanny valley hypothesis.}
}
@article{ABBAS2025129073,
title = {Context-based emotion recognition: A survey},
journal = {Neurocomputing},
volume = {618},
pages = {129073},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.129073},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224018447},
author = {Rizwan Abbas and Bingnan Ni and Ruhui Ma and Teng Li and Yehao Lu and Xi Li},
keywords = {Emotion recognition, Contextual information, Emotion detection techniques, Real-life applications},
abstract = {Emotions play a crucial role in human communication, and accurately recognizing them is essential for developing intelligent systems capable of effective human-computer interaction. Contextual information, including body language, tone of voice, situational cues, and facial expressions, significantly influences the analysis of emotions. This survey paper investigates the critical role of context in emotion recognition and comprehensively analyzes the various techniques employed in Context-Based Emotion Recognition (CBER). It highlights the associated challenges and limitations, examines the datasets used in emotion recognition research, and discusses the performance evaluation metrics utilized to assess the accuracy and effectiveness of algorithms and models in understanding emotions within their respective contexts. Furthermore, the paper explores practical applications of context-based emotion recognition across diverse fields. It serves as an invaluable resource for researchers seeking insights into the latest developments in this field and identifies future research directions to advance context-based emotion recognition.}
}
@article{MAITHRI2022106646,
title = {Automated emotion recognition: Current trends and future perspectives},
journal = {Computer Methods and Programs in Biomedicine},
volume = {215},
pages = {106646},
year = {2022},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2022.106646},
url = {https://www.sciencedirect.com/science/article/pii/S0169260722000311},
author = {M. Maithri and U. Raghavendra and Anjan Gudigar and Jyothi Samanth and  {Prabal Datta Barua} and Murugappan Murugappan and Yashas Chakole and U. Rajendra Acharya},
keywords = {Human emotions, Electroencephalogram (EEG), CAD, Machine learning, Facial, Voice},
abstract = {Background
Human emotions greatly affect the actions of a person. The automated emotion recognition has applications in multiple domains such as health care, e-learning, surveillance, etc. The development of computer-aided diagnosis (CAD) tools has led to the automated recognition of human emotions.
Objective
This review paper provides an insight into various methods employed using electroencephalogram (EEG), facial, and speech signals coupled with multi-modal emotion recognition techniques. In this work, we have reviewed most of the state-of-the-art papers published on this topic.
Method
This study was carried out by considering the various emotion recognition (ER) models proposed between 2016 and 2021. The papers were analysed based on methods employed, classifier used and performance obtained.
Results
There is a significant rise in the application of deep learning techniques for ER. They have been widely applied for EEG, speech, facial expression, and multimodal features to develop an accurate ER model.
Conclusion
Our study reveals that most of the proposed machine and deep learning-based systems have yielded good performances for automated ER in a controlled environment. However, there is a need to obtain high performance for ER even in an uncontrolled environment.}
}
@article{CHEN2025105322,
title = {A gated leaky integrate-and-fire spiking neural network based on attention mechanism for multi-modal emotion recognition},
journal = {Digital Signal Processing},
volume = {165},
pages = {105322},
year = {2025},
issn = {1051-2004},
doi = {https://doi.org/10.1016/j.dsp.2025.105322},
url = {https://www.sciencedirect.com/science/article/pii/S1051200425003446},
author = {Guoming Chen and Zhuoxian Qian and Shuang Qiu and Dong Zhang and Ruqi Zhou},
keywords = {GLIF, Vision transformer, Multi-modal emotion recognition, Wavelet threshold},
abstract = {Multi-modal emotion recognition is a key research area in human-computer interaction. It involves processing heterogeneous multi-modal signals, which present challenges in signal alignment while aiming to enhance accuracy and reduce computational complexity. To address these challenges, we apply swarm decomposition to EEG signals to reduce noise and extract Short-Time Fourier Transform features. Heatmap features are then derived from these signals, as well as from other non-physiological signals such as facial expressions, voice, and text. These features from various sources are aligned using Discrete Wavelet Transform. We propose a Gated Leaky Integrate-and-Fire Spiking Convolutional Vision Transformer (GLIFCVT) framework for multimodal emotion recognition. This framework utilizes visual features as the primary modality and incorporates a spiking gated attention mechanism to enhance multimodal fusion and classification. In addition, we propose a novel loss function that integrates Focal and Dice losses to address class imbalance. Experiments demonstrate our proposed model consistently outperform state-of-the-art methods in both accuracy and energy efficiency.}
}
@article{ARYA2021100399,
title = {A survey of multidisciplinary domains contributing to affective computing},
journal = {Computer Science Review},
volume = {40},
pages = {100399},
year = {2021},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2021.100399},
url = {https://www.sciencedirect.com/science/article/pii/S1574013721000393},
author = {Resham Arya and Jaiteg Singh and Ashok Kumar},
keywords = {Affective computing, Computer science, Literature survey, Physiology, Psychology, Linguistics, Mathematics, Sociology},
abstract = {Affective computing intends to train computers with human-like abilities. It is a multidisciplinary research field, in which interrelated domains like sociology, psychology, computer science, physiology, mathematics, and linguistics have a great contribution. Till now, these domains have been explored by the researchers independently. On the contrary, the cumulative impact on Affective computing of these domains has never been investigated. In this paper, the contribution of these fields along with their theories, concepts, models, and implications in Affective computing is explained in detail. Along with this, some existing affective databases are also presented in this work. Subsequently, various applications where Affective computing has a great impact are discussed which will help future researchers in understanding the importance of this field and its applicability in real life. At last, it reveals some of the challenges along with suggested solutions that exist in this field for future work.}
}
@article{COHEN2025103283,
title = {Robust prosody modeling for synthetic speech detection},
journal = {Speech Communication},
volume = {174},
pages = {103283},
year = {2025},
issn = {0167-6393},
doi = {https://doi.org/10.1016/j.specom.2025.103283},
url = {https://www.sciencedirect.com/science/article/pii/S0167639325000986},
author = {Ariel Cohen and Denis Shyrman and Aleksandr Solonskyi and Roman Frenkel and Arkady Krishtul and Oren Gal},
keywords = {Anti-spoofing, Prosody, Speaker-verification, Deep fake, Synthetic speech detection},
abstract = {This paper presents a comprehensive study on developing and implementing a speech prosody extractor to enhance audio security in Automatic Speaker Verification (ASV) systems. Our novel training approach, which operates without exposure to spoofing examples, significantly improves the modeling of essential prosodic elements often overlooked in deep fake attacks. By integrating codec and recording device embeddings, the prosody extractor effectively neutralizes codec-specific distortions, enhancing robustness across various audio transmission channels. Combined with state-of-the-art ASV systems, our prosody extractor reduces the Equal Error Rate (EER) by an average of 49.15% without codecs, 50.53% with the g711 codec, 44.77% with the g729 codec, 43.43% with the Vonage11https://www.vonage.com/. channel, 42.05% with ECAPA-TDNN, and 45.17% with TitaNet across diverse datasets, including high-quality commercial deep fakes.22https://elevenlabs.io/.,33https://play.ht/voice-cloning/. This integration markedly improves the detection and mitigation of sophisticated spoofing attempts, especially in compressed or altered audio environments. Our methodology also eliminates the dependency on textual data during training, enabling the use of larger and more varied datasets.}
}
@article{LIU2024104757,
title = {How to identify patient perception of AI voice robots in the follow-up scenario? A multimodal identity perception method based on deep learning},
journal = {Journal of Biomedical Informatics},
volume = {160},
pages = {104757},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104757},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424001758},
author = {Mingjie Liu and Kuiyou Chen and Qing Ye and Hong Wu},
keywords = {Follow-up, Post-diagnosis management, AI voice robots, Multimodal analysis, Human-AI interactions},
abstract = {Objectives
Post-discharge follow-up stands as a critical component of post-diagnosis management, and the constraints of healthcare resources impede comprehensive manual follow-up. However, patients are less cooperative with AI follow-up calls or may even hang up once AI voice robots are perceived. To improve the effectiveness of follow-up, alternative measures should be taken when patients perceive AI voice robots. Therefore, identifying how patients perceive AI voice robots is crucial. This study aims to construct a multimodal identity perception model based on deep learning to identify how patients perceive AI voice robots.
Methods
Our dataset includes 2030 response audio recordings and corresponding texts from patients. We conduct comparative experiments and perform an ablation study. The proposed model employs a transfer learning approach, utilizing BERT and TextCNN for text feature extraction, AST and LSTM for audio feature extraction, and self-attention for feature fusion.
Results
Our model demonstrates superior performance against existing baselines, with a precision of 86.67%, an AUC of 84%, and an accuracy of 94.38%. Additionally, a generalization experiment was conducted using 144 patients’ response audio recordings and corresponding text data from other departments in the hospital, confirming the model’s robustness and effectiveness.
Conclusion
Our multimodal identity perception model can identify how patients perceive AI voice robots effectively. Identifying how patients perceive AI not only helps to optimize the follow-up process and improve patient cooperation, but also provides support for the evaluation and optimization of AI voice robots.}
}
@article{PARK202141,
title = {Survey and challenges of story generation models - A multimodal perspective with five steps: Data embedding, topic modeling, storyline generation, draft story generation, and story evaluation},
journal = {Information Fusion},
volume = {67},
pages = {41-63},
year = {2021},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2020.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S156625352030378X},
author = {Sang-Min Park and Young-Gab Kim},
keywords = {Story generation, Multimodal content, Multimodal storytelling, Multimodal story},
abstract = {The story is the description of events in chronological order that have occurred between people. By delivering facts to the people reading the story, it enables them to feel emotions. Such a story is composed using the following method: each event is analyzed and a storyline is composed, which becomes a skeleton text by linking relationships between major events. As the content of users becomes more diverse, multimodal story composition has become more essential than unimodal text-based story composition. This paper discusses modality integration based on multimodal data types and type conversion for multimodal story composition. We propose a story-graph model to create a story based on the integrated analysis of various modal data. In terms of architecture, the proposed multimodal storytelling model consists of modal data and a topic modeling module that performs clustering based on cross-modal similarities and extracts a topic of clustered modalities. From the perspective of utilization, to visualize a story-graph, the proposed model summarizes nodes with a representative image. Furthermore, the latest techniques are discussed with respect to five main modules and twelve sub-modules for story composition. Lastly, problems that can become issues when composing multimodal story modules are explained.}
}
@article{MIAO202246,
title = {Fusing features of speech for depression classification based on higher-order spectral analysis},
journal = {Speech Communication},
volume = {143},
pages = {46-56},
year = {2022},
issn = {0167-6393},
doi = {https://doi.org/10.1016/j.specom.2022.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0167639322001029},
author = {Xiaolin Miao and Yao Li and Min Wen and Yongyan Liu and Ibegbu Nnamdi Julian and Hao Guo},
keywords = {Higher-order spectrum analysis, Speech-related features, Depression, Machine learning, Classifier},
abstract = {Approximately 300 million people worldwide suffer from depression, and more than 60% of psychiatric patients do not have access to mental health services due to the shortage of psychiatrists and the high costs associated with clinical diagnosis and treatment. Correct and efficient diagnosis of depression can help overcome these straits. Automatic detection of depressive symptoms can help improve the accuracy and availability of diagnosis. In this paper, a fusion feature for Bispectral Features and Bicoherent Features by using higher-order spectral analysis. Experiments were performed on the Depression Sub-Challenge Dataset of the Audio/Visual Emotion Challenge 2017. The fusion feature fuses higher-order spectral features and traditional speech features with classification weights greater than 100 extracted by using A Collaborative Voice Analysis Repository. The support vector machine and k-nearest neighbor classification algorithms were used as the traditional machine learning models, and the convolutional neural network was used as the deep learning model to verify the proposed features. The experimental results show that under the support vector machine algorithm, the accuracies of extraction of speech-related features by using a collaborative voice analysis repository, The higher-order spectral analysis, and their fusion features were 63.15%, 68.42%, and 73.68%, respectively. Under the k-nearest neighbor classification algorithms model algorithm, the corresponding accuracies were 68.18%, 72.73%, and 77.27%, respectively. For the convolutional neural network model, the corresponding accuracies were 70%, 77%, and 85%, respectively. The results demonstrate that the fusion feature recognition accuracy is high and can be employed to improve the accuracy of depression identification by using traditional machine learning and deep learning models.}
}
@article{RAHATE2022203,
title = {Multimodal Co-learning: Challenges, applications with datasets, recent advances and future directions},
journal = {Information Fusion},
volume = {81},
pages = {203-239},
year = {2022},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2021.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S1566253521002530},
author = {Anil Rahate and Rahee Walambe and Sheela Ramanna and Ketan Kotecha},
keywords = {Multimodal co-learning, Multimodal deep learning, Missing modalities, Multimodal taxonomy, Multimodal datasets},
abstract = {Multimodal deep learning systems that employ multiple modalities like text, image, audio, video, etc., are showing better performance than individual modalities (i.e., unimodal) systems. Multimodal machine learning involves multiple aspects: representation, translation, alignment, fusion, and co-learning. In the current state of multimodal machine learning, the assumptions are that all modalities are present, aligned, and noiseless during training and testing time. However, in real-world tasks, typically, it is observed that one or more modalities are missing, noisy, lacking annotated data, have unreliable labels, and are scarce in training or testing, and or both. This challenge is addressed by a learning paradigm called multimodal co-learning. The modeling of a (resource-poor) modality is aided by exploiting knowledge from another (resource-rich) modality using the transfer of knowledge between modalities, including their representations and predictive models. Co-learning being an emerging area, there are no dedicated reviews explicitly focusing on all challenges addressed by co-learning. To that end, in this work, we provide a comprehensive survey on the emerging area of multimodal co-learning that has not been explored in its entirety yet. We review implementations that overcome one or more co-learning challenges without explicitly considering them as co-learning challenges. We present the comprehensive taxonomy of multimodal co-learning based on the challenges addressed by co-learning and associated implementations. The various techniques, including the latest ones, are reviewed along with some applications and datasets. Additionally, we review techniques that appear to be similar to multimodal co-learning and are being used primarily in unimodal or multi-view learning. The distinction between them is documented. Our final goal is to discuss challenges and perspectives and the important ideas and directions for future work that we hope will benefit for the entire research community focusing on this exciting domain.}
}
@article{HIREMATH2021115476,
title = {Sarcasm Detection using Cognitive Features of Visual Data by Learning Model},
journal = {Expert Systems with Applications},
volume = {184},
pages = {115476},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.115476},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421008873},
author = {Basavaraj N. Hiremath and Malini M. Patil},
keywords = {Natural language processing, Multiclass neural network, Sentiment analysis, Sarcasm, Cognitive features, Pragmatic},
abstract = {The objective of the paper is to detect sarcasm in human communication. The methodology uses basic cognitive features of human utterances by capturing three modes of data viz., voice, text, and temporal facial features. The captured data is unstructured as it consists of parameters of feelings and emotions to generate sarcasm which affects expressions through glottal and facial organs. The data capturing method is equally challenging as compared to the method of data processing to acquire features. The significant work is aligned to make natural decisions in the prediction processes using cognitive information in the data lineage. Sarcasm detection in natural human communication is a challenging process. The Linguistic features of natural language processing (NLP) methods help identify sentiment as negative and positive sentences based on polarity using the pre-labelled samples. The multiclass neural network model is used as a soft cognition method for the detection of sarcasm under cloud resources. Identified cognitive features have information like voice cues and eye movements, they tend to influence the decision of detecting sarcasm. The visual data are found to be quite interesting and can establish a strong platform in the area of NLP for further research work.}
}
@incollection{2024265,
title = {Index},
editor = {D. Jude Hemanth},
booktitle = {Computational Intelligence Methods for Sentiment Analysis in Natural Language Processing Applications},
publisher = {Morgan Kaufmann},
pages = {265-278},
year = {2024},
isbn = {978-0-443-22009-8},
doi = {https://doi.org/10.1016/B978-0-443-22009-8.00020-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780443220098000203}
}
@article{GORER2024112018,
title = {Exploring the REIT architecture for requirements elicitation interview training with robotic and virtual tutors},
journal = {Journal of Systems and Software},
volume = {212},
pages = {112018},
year = {2024},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112018},
url = {https://www.sciencedirect.com/science/article/pii/S016412122400061X},
author = {Binnur Görer and Fatma Başak Aydemir},
keywords = {Software engineering education, Requirements elicitation interview training, Empirical evaluation, Generic architecture, Robotic tutor, Virtual tutor},
abstract = {Requirements elicitation interviews are a widely adopted technique where the interview success depends on the interviewer’s preparedness and communication skills. Students can enhance these skills through practice interviews. However, organizing practice interviews for many students presents scalability challenges, given the time and effort required to involve stakeholders in each session. To address this, we propose REIT, an extensible architecture for Requirements Elicitation Interview Training system leveraging technologies such as robots and voice systems. REIT has components to support both the interview phase, wherein students act as interviewers while the system assumes the role of an interviewee, and the feedback phase, during which the system assesses students’ performance and offers contextual and behavioral feedback to enhance their interviewing skills. We demonstrate the applicability of REIT through two implementations: RoREIT with a physical robotic agent and VoREIT with a virtual voice-only agent. We empirically evaluated both instances with a group of graduate students. The participants appreciated both systems. They demonstrated higher learning gain when trained with RoREIT, but they found VoREIT more engaging and easier to use. These findings indicate that each system has distinct benefits and drawbacks, suggesting that educators can customize REIT for various settings, considering preferences and available resources.}
}
@article{TSAREGORODTSEV2022338,
title = {The architecture of the emotion recognition program by speech segments},
journal = {Procedia Computer Science},
volume = {213},
pages = {338-345},
year = {2022},
note = {2022 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence: The 13th Annual Meeting of the BICA Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.11.076},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922017677},
author = {A.V. Tsaregorodtsev and V.E. Samoylov and A.E. Zenov and A.N. Zelenina and D.A. Petrosov and E.S. Pleshakova and A.V. Osipov and M.N. Ivanov and N.V. Petrosova and L.A. Lopatnuk and V.Y. Radygin and S.N. Roga},
keywords = {data analysis, classification, speech recognition, emotion classification, natural language processing},
abstract = {The article discusses the issues of automatic recognition of the emotional coloring of a person's utterance using machine learning methods. In the software being developed, a change in the basic tone of a person's voice is taken as a basis. The architecture of the program is based on a multilayer perceptron. The result of the classifier is the choice of one of eight possible classes of emotions. In the course of the study, the accuracy of the classification of emotions was determined and the main directions of the development of the program were outlined.}
}
@article{KHEDDAR2024102422,
title = {Automatic speech recognition using advanced deep learning approaches: A survey},
journal = {Information Fusion},
volume = {109},
pages = {102422},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102422},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524002008},
author = {Hamza Kheddar and Mustapha Hemis and Yassine Himeur},
keywords = {Automatic speech recognition, Deep transfer learning, Transformers, Federated learning, Reinforcement learning},
abstract = {Recent advancements in deep learning (DL) have posed a significant challenge for automatic speech recognition (ASR). ASR relies on extensive training datasets, including confidential ones, and demands substantial computational and storage resources. Enabling adaptive systems improves ASR performance in dynamic environments. DL techniques assume training and testing data originate from the same domain, which is not always true. Advanced DL techniques like deep transfer learning (DTL), federated learning (FL), and deep reinforcement learning (DRL) address these issues. DTL allows high-performance models using small yet related datasets, FL enables training on confidential data without dataset possession, and DRL optimizes decision-making in dynamic environments, reducing computation costs. This survey offers a comprehensive review of DTL, FL, and DRL-based ASR frameworks, aiming to provide insights into the latest developments and aid researchers and professionals in understanding the current challenges. Additionally, Transformers, which are advanced DL techniques heavily used in proposed ASR frameworks, are considered in this survey for their ability to capture extensive dependencies in the input ASR sequence. The paper starts by presenting the background of DTL, FL, DRL, and Transformers and then adopts a well-designed taxonomy to outline the state-of-the-art (SOTA) approaches. Subsequently, a critical analysis is conducted to identify the strengths and weaknesses of each framework. Additionally, a comparative study is presented to highlight the existing challenges, paving the way for future research opportunities.}
}
@article{CUI2024102092,
title = {A high speed inference architecture for multimodal emotion recognition based on sparse cross modal encoder},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {36},
number = {5},
pages = {102092},
year = {2024},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2024.102092},
url = {https://www.sciencedirect.com/science/article/pii/S1319157824001812},
author = {Lin Cui and Yuanbang Zhang and Yingkai Cui and Boyan Wang and Xiaodong Sun},
keywords = {Emotion recognition, Deep learning, Pre-trained network, Cross-modal encoder, Attention mechanism},
abstract = {In recent years, multimodal emotion recognition models are using pre-trained networks and attention mechanisms to pursue higher accuracy, which increases the training burden and slows down the training and inference speed. In order to strike a balance between speed and accuracy, this paper proposes a speed-optimized multimodal emotion recognition architecture for speech and text emotion recognition. In the feature extraction part, a lightweight residual graph convolutional network (ResGCN) is selected as the speech feature extractor, and an efficient RoBERTa pre-trained network is used as the text feature extractor. Then, an algorithm complexity-optimized sparse cross-modal encoder (SCME) is proposed and used to fuse these two types of features. Finally, a new gated fusion module (GF) is used to weight multiple results and input them into a fully connected layer (FC) for classification. The proposed method is tested on the IEMOCAP dataset and the MELD dataset, achieving weighted accuracies (WA) of 82.4% and 65.0%, respectively. This method achieves higher accuracy than the listed methods while having an acceptable training and inference speed.}
}
@article{LI2025103049,
title = {QMLSC: A quantum multimodal learning model for sentiment classification},
journal = {Information Fusion},
volume = {120},
pages = {103049},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2025.103049},
url = {https://www.sciencedirect.com/science/article/pii/S1566253525001228},
author = {YaoChong Li and Yi Qu and Ri-Gui Zhou and Jing Zhang},
keywords = {Quantum neural network, Sentiment classification, Multimodal learning},
abstract = {Sentiment classification research is gaining prominence for enhancing user experience, facilitating targeted marketing, and supporting mental health assessments while driving technological innovation. Due to the complexity and diversity of emotional expression, this study proposes quantum multimodal learning for sentiment classification (QMLSC), a novel quantum–classical hybrid model that integrates text and speech data to capture emotional signals more effectively. To address the limitations of the noisy intermediate-scale quantum era, we designed advanced variational quantum circuit (VQC) architectures to efficiently process high-dimensional data, maximizing feature retention and minimizing information loss. Our approach employs a residual structure that fuses quantum and classical components, enhancing the benefits of quantum features and conventional machine learning attributes. By using randomized expressive circuits, we improve system flexibility, accuracy, and robustness in sentiment classification tasks. Integrating VQC significantly reduces the number of parameters compared to fully connected layers, resulting in improved accuracy and computational efficiency. Empirical findings validate the superior performance of our fusion approach in effectively mitigating noise and error impacts associated with quantum computing and demonstrate strong potential for future applications in complex emotional information processing. This study provides new insights and methodologies for advancing sentiment classification technology and highlights the broad application potential for advancing quantum computing in information processing fields.}
}
@article{DAWOODBHOY2021e06993,
title = {AI in patient flow: applications of artificial intelligence to improve patient flow in NHS acute mental health inpatient units},
journal = {Heliyon},
volume = {7},
number = {5},
pages = {e06993},
year = {2021},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2021.e06993},
url = {https://www.sciencedirect.com/science/article/pii/S2405844021010963},
author = {Fatema Mustansir Dawoodbhoy and Jack Delaney and Paulina Cecula and Jiakun Yu and Iain Peacock and Joseph Tan and Benita Cox},
keywords = {Mental health, NHS, AI, Patient flow, Machine learning, Natural language processing, Digital phenotyping},
abstract = {Introduction
Growing demand for mental health services, coupled with funding and resource limitations, creates an opportunity for novel technological solutions including artificial intelligence (AI). This study aims to identify issues in patient flow on mental health units and align them with potential AI solutions, ultimately devising a model for their integration at service level.
Method
Following a narrative literature review and pilot interview, 20 semi-structured interviews were conducted with AI and mental health experts. Thematic analysis was then used to analyse and synthesise gathered data and construct an enhanced model.
Results
Predictive variables for length-of-stay and readmission rate are not consistent in the literature. There are, however, common themes in patient flow issues. An analysis identified several potential areas for AI-enhanced patient flow. Firstly, AI could improve patient flow by streamlining administrative tasks and optimising allocation of resources. Secondly, real-time data analytics systems could support clinician decision-making in triage, discharge, diagnosis and treatment stages. Finally, longer-term, development of solutions such as digital phenotyping could help transform mental health care to a more preventative, personalised model.
Conclusions
Recommendations were formulated for NHS trusts open to adopting AI patient flow enhancements. Although AI offers many promising use-cases, greater collaborative investment and infrastructure are needed to deliver clinically validated improvements. Concerns around data-use, regulation and transparency remain, and hospitals must continue to balance guidelines with stakeholder priorities. Further research is needed to connect existing case studies and develop a framework for their evaluation.}
}
@article{MA2025103506,
title = {Towards empathic medical conversation in Narrative Medicine: A visualization approach based on intelligence augmentation},
journal = {International Journal of Human-Computer Studies},
volume = {199},
pages = {103506},
year = {2025},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2025.103506},
url = {https://www.sciencedirect.com/science/article/pii/S1071581925000631},
author = {Hua Ma and Effie Lai-Chong Law and Xu Sun and Weili Yang and Xiangjian He and Glyn Lawson and Huizhong Zheng and Qingfeng Wang and Qiang Li and Xiaoru Yuan},
keywords = {Visualization, Interaction, Intelligence augmentation, Empathic communication, Narrative medicine},
abstract = {Empathic medical conversation is central to patient-centered care within Narrative Medicine. However, difficulties, such as physicians’ limited empathic capabilities and lack of time, impede the practice. Research on real-time, on-site empathic medical exchanges has been limited in exploring technology to assist and enhance physicians’ capabilities. This paper proposed the Empathic Opportunity Perception and Distinction (EOPD) framework for building physician-AI collaboration based on Intelligence Augmentation (IA) for empathic conversations. The EOPD integrates two multi-modal machine learning (ML) models based on facial and verbal cues, presenting a physician-AI interaction framework and three distinctive visualization components: emotional reference, opportunity reminding and keyword collection, and situation understanding. To assess EOPD's effectiveness and gauge physicians’ and patients’ receptiveness, a prototype system named EMVIS (EMotional VISualization) was designed and developed. Results from the study demonstrated improvements in physicians’ empathy efforts and perceived empathy performance when using EMVIS, particularly for junior physicians. Physicians and patients held positive attitudes towards EMVIS, with patients expressing a high expectation that EMVIS would improve the physician-patient relationship. The research showed the efficacy of the multi-modal ML models in supporting complex affective empathy and EMVIS in facilitating and complementing empathy concerns. It highlighted the tailored support to junior and senior physicians and emphasized physician-AI collaboration to maintain user autonomy and mitigate potential biases. Future research should explore extensive system applications, tailor visual and interactive support for physicians, and implement adaptive and reflective ML models to improve the effectiveness and efficiency of empathy communications.}
}
@article{XIE2023126649,
title = {A multimodal fusion emotion recognition method based on multitask learning and attention mechanism},
journal = {Neurocomputing},
volume = {556},
pages = {126649},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.126649},
url = {https://www.sciencedirect.com/science/article/pii/S0925231223007725},
author = {Jinbao Xie and Jiyu Wang and Qingyan Wang and Dali Yang and Jinming Gu and Yongqiang Tang and Yury I. Varatnitski},
keywords = {Multitasking learning, Attention mechanism, Multimodal, Emotion recognition},
abstract = {With new developments in the field of human–computer interaction, researchers are now paying attention to emotion recognition, especially multimodal emotion recognition, as emotion is a multidimensional expression. In this study, we propose a multimodal fusion emotion recognition method (MTL-BAM) based on multitask learning and the attention mechanism to tackle the major problems encountered in multimodal emotion recognition tasks regarding the lack of consideration of emotion interactions among modalities and the focus on emotion similarity among modalities while ignoring the differences. By improving the attention mechanism, the emotional contribution of each modality is further analyzed so that the emotional representations of each modality can learn from and complement each other to achieve better interactive fusion effect, thereby building a multitask learning framework. By introducing three types of monomodal emotion recognition tasks as auxiliary tasks, the model can detect emotion differences. Simultaneously, the label generation unit is introduced into the auxiliary tasks, and the monomodal emotion label value can be obtained more accurately through two proportional formulas while preventing the zero value problem. Our results show that the proposed method outperforms selected state-of-the-art methods on four evaluation indexes of emotion classification (i.e., accuracy, F1 score, MAE, and Pearson correlation coefficient). The proposed method achieved accuracy rates of 85.36% and 84.61% on the published multimodal datasets of CMU-MOSI and CMU-MOSEI, respectively, which are 2–6% higher than those of existing state-of-the-art models, demonstrating good multimodal emotion recognition performance and strong generalizability.}
}
@article{FALCON2023104375,
title = {Teachers’ engaging messages, students’ motivation to learn and academic performance: The moderating role of emotional intensity in speech},
journal = {Teaching and Teacher Education},
volume = {136},
pages = {104375},
year = {2023},
issn = {0742-051X},
doi = {https://doi.org/10.1016/j.tate.2023.104375},
url = {https://www.sciencedirect.com/science/article/pii/S0742051X23003633},
author = {Samuel Falcon and Jesús B. Alonso and Jaime Leon},
keywords = {Teacher professional development, Secondary education, Pedagogical issues, Improving classroom teaching, Evaluation methodologies},
abstract = {This study examined how emotional intensity of speech affects the relationship between teachers’ engaging messages, and students’ motivation to learn and academic performance. To achieve our goal, we recorded and transcribed teachers’ lessons. Results revealed that messages appealing to external stimuli had lower emotional intensity than those appealing to internal stimuli. Our results also suggest that emotional intensity moderates the relationship between engaging messages and academic performance, with the effect decreasing as emotional intensity increases. This study offers insights into the role of acoustic features in teachers’ influence on students’ motivation and academic performance and suggests avenues for further research.}
}
@article{TOSHPULATOV2023119678,
title = {Talking human face generation: A survey},
journal = {Expert Systems with Applications},
volume = {219},
pages = {119678},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.119678},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423001793},
author = {Mukhiddin Toshpulatov and Wookey Lee and Suan Lee},
keywords = {Talking human face animation, 3 face generation, Deep generative model, Autoencoder, Neural radiance field, Datasets, Evaluation metrics, Neural networks, Unsupervised learning, Mel spectogram},
abstract = {Talking human face generation aims at synthesizing a natural human face that talks in correspondence to the given text or audio series. Implementing the recently developed Deep Learning (DL) methods such as Convolutional Neural Networks (CNN), Generative Adversarial Networks (GAN)s, Neural Rendering Fields (NeRF) for data generation, and talking human face generation has attracted significant research interest from academia and industry. They have been explored and exploited recently and have been used to address several problems in image processing and computer vision. Notwithstanding notable advancements, implementing them to real-world problems such as talking human face generation remains challenging. The generation of deepfakes created by the abovementioned methods would greatly promote many fascinating applications, including augmented reality, virtual reality, computer games, teleconferencing, virtual try-on, special movie effects, and avatars. This research reviews and discusses DL related methods, including CNN, GANs, NeRF, and their implementation in talking human face generation. We aim to analyze existing approaches regarding their implementation to talking face generation, investigate the related general problems, and highlight the open study issues. We also provide quantitative and qualitative evaluations of the existing research approaches in the related field.}
}
@article{JOSHY20231,
title = {Dysarthria severity classification using multi-head attention and multi-task learning},
journal = {Speech Communication},
volume = {147},
pages = {1-11},
year = {2023},
issn = {0167-6393},
doi = {https://doi.org/10.1016/j.specom.2022.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0167639322001534},
author = {Amlu Anna Joshy and Rajeev Rajan},
keywords = {Dysarthria, Multi-head attention, Multi-task learning, Convolutional neural network},
abstract = {Identifying the severity of dysarthria is considered a diagnostic step in monitoring the patient’s progress and a beneficial step in the transcription of dysarthric speech. In this paper, the effectiveness of using the multi-head attention mechanism (MHA) and the multi-task learning approach is explored for automated dysarthria severity level classification. Dysarthric speech utterances are represented by mel spectrograms and fed to a residual convolutional neural network for effective feature learning. Then the MHA module is added to identify the salient severity-highlighting periods. At the classification end, gender, age, and disorder-type identifications are employed as auxiliary tasks to share mutual information and leverage the severity classification. The performance of the proposed method is evaluated on the Universal Access Speech database. By giving a gain of 11.51% classification accuracy over the baseline system under the speaker-dependent scenario and 11.58% under the speaker-independent scenario, the proposed system demonstrates its potential for the dysarthria severity classification.}
}
@article{V2025110984,
title = {A lightweight ECA-based DCNN approach for speech command recognition},
journal = {Computers in Biology and Medicine},
volume = {197},
pages = {110984},
year = {2025},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2025.110984},
url = {https://www.sciencedirect.com/science/article/pii/S0010482525013368},
author = {Karthikeyan V and Saranya P and Natchiyar M},
keywords = {Speech recognition, CNN, Accuracy, ECA, Deep learning},
abstract = {Background
Speech recognition allows the recognition of audio streams. It is a tool used by professionals across a range of industries that require accurate transcriptions. In the context of authentication, speech recognition can be used as a biometric factor to verify a user's identity and can be incredibly helpful for individuals with disabilities, particularly those with speech impairments. This evolving technology enables seamless and intuitive communication that closely resembles human conversation.
Method
A lightweight end-to-end deep convolutional neural network with an efficient channel attention framework (LW-DCNN-ECA) is proposed in this work for speech recognition to ease communication and flexibility. We address speech recognition in this work. Our framework involves a layer-modified end-to-end deep CNN with an efficient channel attention (ECA) mechanism to recognize the spoken word for speech recognition. The suggested ECA layer is a computationally efficient module employed in lightweight deep convolutional neural networks to improve overall model performance.
Results
In this work, the suggested framework is tested for speech recognition employing the standard datasets of speech commands and mini-speech commands. The presented model has obtained a recognition rate of 98.28 % and a loss of 0.5691 for the mini-speech commands dataset, and similarly for the speech commands dataset, it has obtained a recognition rate of 99.98 % and a loss of 0.2634. The performance of the recommended framework is validated using the larger volume Fisher's corpus and the system robustness is tested using the CHiME-4 corpus.}
}
@article{ALOBAID2025200529,
title = {Disruptive attacks on artificial neural networks: A systematic review of attack techniques, detection methods, and protection strategies},
journal = {Intelligent Systems with Applications},
volume = {26},
pages = {200529},
year = {2025},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2025.200529},
url = {https://www.sciencedirect.com/science/article/pii/S2667305325000559},
author = {Ahmad Alobaid and Talal Bonny and Maher Alrahhal},
keywords = {Fault injection attacks, Adversarial attacks, Deep neural network, Machine learning, Security analysis},
abstract = {This paper provides a systematic review of disruptive attacks on artificial neural networks (ANNs). As neural networks become increasingly integral to critical applications, their vulnerability to various forms of attack poses significant security challenges. This review categorizes and analyzes recent advancements in attack techniques, detection methods, and protection strategies for ANNs. It explores various attacks, including adversarial attacks, data poisoning, fault injections, membership inference, model inversion, timing, and watermarking attacks, examining their methodologies, limitations, impacts, and potential improvements. Key findings reveal that while detection and protection mechanisms such as adversarial training, noise injection, and hardware-based defenses have advanced significantly, many existing solutions remain vulnerable to adaptive attack strategies and scalability challenges. Additionally, fault injection attacks at the hardware level pose an emerging threat with limited countermeasures. The review identifies critical gaps in defense strategies, particularly in balancing robustness, computational efficiency, and real-world applicability. Future research should focus on scalable defense solutions to ensure effective deployment across diverse ANN architectures and critical applications, such as autonomous systems. Furthermore, integrating emerging technologies, including generative AI models and hybrid architectures, should be prioritized to better understand and mitigate their vulnerabilities.}
}
@article{YANG2025128532,
title = {Fine-grained multimodal fusion for depression assisted recognition based on hierarchical knowledge-enhanced prompt learning},
journal = {Expert Systems with Applications},
volume = {291},
pages = {128532},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.128532},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425021517},
author = {Shanliang Yang and Shaojie Liu and Guangjun Nie and Lei Wang and Tao Wang and Jiebing You and Erik Cambria},
keywords = {Depression assisted recognition, Multimodal data, Knowledge enhancement, Hierarchical prompt learning},
abstract = {Depression is a multifaceted mental health disorder that presents significant diagnostic challenges, traditionally relying on subjective evaluations and clinical interviews, which can lead to biases and inaccuracies. While existing AI-based methods for depression diagnosis have been developed, they still face significant issues related to the difficulty of learning highly discriminative representations, as well as the challenge of ensuring that the features extracted by complex neural network models are highly relevant to depression. To address these challenges, we propose a novel Hierarchical Knowledge-Enhanced Prompt Fusion (HKEPF) model for depression-assisted recognition. Guided by diagnostic knowledge and hierarchical prompt learning, our approach promotes fine-grained multimodal fusion. Initially, psychological and psychiatric expertise are utilized for knowledge enhancement to improve the accuracy of feature extraction and data integration. Subsequently, hierarchical prompt learning is employed to progressively extract and integrate text, audio, and visual data, allowing for a comprehensive understanding and fusion of multimodal information at different levels. We evaluated the proposed method on multiple comprehensive multimodal datasets, demonstrating its potential to outperform traditional diagnostic techniques. The experimental results demonstrate that our HKEPF model enhances the precision of depression diagnosis while also delivering a more interpretable and transparent diagnostic process. This research advances the field of mental health diagnostics, paving the way for more accurate and personalized interventions for individuals experiencing depression.}
}
@article{WANG2022101588,
title = {Sentiment analysis from Customer-generated online videos on product review using topic modeling and Multi-attention BLSTM},
journal = {Advanced Engineering Informatics},
volume = {52},
pages = {101588},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101588},
url = {https://www.sciencedirect.com/science/article/pii/S147403462200060X},
author = {Zheng Wang and Peng Gao and Xuening Chu},
keywords = {Sentiment analysis, Online video, Topic modeling, Deep learning, Product improvement},
abstract = {With the popularity of social websites and mobile applications including Instagram, YouTube, TikTok, etc., online videos shared by customers presenting their thoughts and reviews on products are posted daily in increasing numbers. Such online videos containing Voice of Customer (VOC) are precious for product designers or managers to capture customer sentiment and understand customer preference. For this purpose, we propose a novel method for analyzing customer sentiment from online videos on product review. Firstly, latent Dirichlet allocation (LDA) modeling is applied to identify the topics from the online videos after data preprocessing. Then sentiment polarity corresponding to each topic of each speaker in videos can be identified using our newly designed multi-attention bi-directional LSTM (BLSTM(MA)), which can better mine complex relationships among a speaker’s sentiments on different topics. This paper is of great practical value for company managers and researchers to better understand a large number of customer opinions on specific products. To explain the application of this method and prove its effectiveness, two cases respectively on smartphones and several published datasets are developed finally.}
}
@article{DUAN2025104222,
title = {When emotions don’t match: Effects of multimodal emotional misalignment in virtual streamers on viewer engagement},
journal = {Information & Management},
volume = {62},
number = {8},
pages = {104222},
year = {2025},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2025.104222},
url = {https://www.sciencedirect.com/science/article/pii/S0378720625001259},
author = {Menghan Duan and Qi Zhang and Yueyue Zhang and Cheng Zhang},
keywords = {Multimodal emotions, Emotional alignment, Streamer-viewer emotional synchrony, Elaboration likelihood model, Cognitive tuning theory, Virtual streamer, Live streaming},
abstract = {Virtual streamers have been increasingly adopted in entertainment live streaming, yet the effectiveness of their social interactions remains unexplored. Owing to the avatar abstraction of faces and the limited flexibility of facial expression, they rely primarily on vocal tone and textual content to convey emotion. Drawing on the Elaboration Likelihood Model and Cognitive Tuning Theory, this study examines how streamers' cross-modal emotional misalignment between voice and text influences viewer engagement through streamer-viewer emotional synchrony. Using moment-to-moment data and machine learning–based emotion recognition techniques, we find that greater cross-modal emotional misalignment of streamers increases viewer engagement by heightening viewers’ emotional responses to vocal cues. Additionally, the positivity of the streamer’s vocal tone strengthens the effect of cross-modal emotional misalignment on vocal–emotional synchrony. Finally, we reveal the dual effects of cross-modal emotional misalignment on viewer consumption; while it increases short-term spending on paid comments and virtual gifting, it reduces long-term commitment in the form of premium subscriptions. Our study contributes to the research on live streaming and emotional interaction, and provides practical implications for designing emotionally intelligent virtual streamers.}
}
@article{BINI2024107938,
title = {Robust speech command recognition in challenging industrial environments},
journal = {Computer Communications},
volume = {228},
pages = {107938},
year = {2024},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2024.107938},
url = {https://www.sciencedirect.com/science/article/pii/S0140366424002858},
author = {Stefano Bini and Vincenzo Carletti and Alessia Saggese and Mario Vento},
keywords = {Industry 5.0, Speech recognition, Noisy environment, Curriculum learning},
abstract = {Speech is among the main forms of communication between humans and robots in industrial settings, being the most natural way for a human worker to issue commands. However, the presence of pervasive and loud environmental noise poses significant challenges to the adoption of Speech-Command Recognition systems onboard manufacturing robots; indeed, they are expected to perform in real time on hardware with limited computational capabilities and also to be robust and accurate in such complex environments. In this paper, we propose an innovative system based on an End-to-End architecture with a Conformer backbone. Our system is specifically designed to achieve high accuracy in noisy industrial environments and to guarantee a minimal computational burden to meet stringent real-time requirements while running on computing devices that are embedded in robots. In order to increase the generalization capability of the system, the training procedure is driven by a Curriculum Learning strategy combined with dynamic data augmentation techniques, that progressively increase the complexity of input samples by increasing the noise during the training phase. We have conducted extensive experimentation to assess the effectiveness of our system, using a dataset composed of more than 50,000 samples, of which about 2,000 have been acquired during the daily operations of a Stellantis Italian factory. The results confirm the suitability of the proposed approach to be adopted in a real industrial environment; indeed, it is able to achieve, on both English and Italian commands, an accuracy higher than 90%, maintaining a compact model size (the network is 1.81 MB) and running in real-time on an industrial embedded device (namely 41ms over an NVIDIA Xavier NX).}
}
@article{KUMAR2023118651,
title = {Artificial Emotional Intelligence: Conventional and deep learning approach},
journal = {Expert Systems with Applications},
volume = {212},
pages = {118651},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118651},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422016931},
author = {Himanshu Kumar and A. Martin},
keywords = {Artificial emotional intelligence, Automated decision-making, Machine learning, Deep learning emotion detection, Neural Network, Facial recognition Pattern, Facial Emotion Recognition},
abstract = {Artificial intelligence substantially changes the global world, influencing technologies, machines, and objects in various encouraging aspects nowadays; emotion recognition is also one of them. This paper describes a significant contribution of emotion recognition by applying conventional and deep learning methodologies by focusing on limitations and demanding challenges. It also intends to explore the comparative study on recently applied machine learning and deep learning-based algorithms, which provide the best accuracy rates to recognize emotions. This Comparative study consists of different feature extractions, classifier models, and datasets that recognize the emotions within a facial image, speech, and non-verbal communication and describes their features and principles for future research work. We have shown the balancing accuracy, and efficiency of using hybrid classification techniques briefly explained in Speech emotion recognition. This review study would be more beneficial in enhancing automated decision-making services in various customer-based industries and observing patients in the health care sector, industries, public sectors, private sectors, and production firms.}
}
@article{ZHANG2023120948,
title = {Trusted emotion recognition based on multiple signals captured from video},
journal = {Expert Systems with Applications},
volume = {233},
pages = {120948},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120948},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423014501},
author = {Junjie Zhang and Kun Zheng and Sarah Mazhar and Xiaohui Fu and Jiangping Kong},
keywords = {Trusted emotion recognition, Eye region segment, rPPG, Facial expression},
abstract = {Emotion recognition based on facial expressions has low accuracy and doubtful reliability because of the existence of fake expressions. In this paper, a method is proposed to recognize fake emotions based on multivisual information generated from single information sources, including facial expressions, eye states and physiological signals captured from video. An algorithm based on a graph neural network is used to extract spatial and spectral domain features from facial images for facial expression recognition. A model-based method is used for decomposing RGB signals into heart rates. A deep model trained by a labeled dataset that we created is used to segment the eye region. After obtaining the signals extracted from video, different fusion strategies are applied to evaluate emotion recognition performance based on multiple signals. In the experiment, the CK+, TFEID, JAFFE, RAF, PURE, and ESLD datasets are used to measure the accuracy of facial expression recognition, heart rate detection and eye region segmentation. The results show that multimodality is effective in improving accuracy and that eye state can be considered a cue for trusted emotion recognition. Compared with the method based on the eye state and noncontact physiological signal, the accuracy of multimodality can be improved by 26.19% and 9.52%, respectively.}
}
@article{SLAVOVA20211514,
title = {On the revealing the emotional valence in communication by text},
journal = {Procedia Computer Science},
volume = {192},
pages = {1514-1523},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.08.155},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921016495},
author = {Velina Slavova},
keywords = {Text emotion recognition, Sound-symbolism, Emotion, semantics},
abstract = {The interaction between humans and between humans and machines is often based on texts. The statistical analysis, presented here, provides one more reason to suppose that meaningful language communication integrates an emotional component involved at the phonemic level. The performed analysis is of written English language, presented by means words’ phonetic transcripts. The phonemic content of the texts is thought in layers, supposing that the phonological forms involved in the communication have the role of phonological markers assisting emotion transfer. The words are considered as composed of biphones – phonemic pairs consisting of one vowel and one consonant, in either order. Phonemic metadata for this sublexical representation was obtained from a vast corpus of emotionally evaluated texts and submitted to statistical analysis. The result of the applied principle component regression showed that the phonemic content is very strongly related (r = 0.96) with the ratings of emotional valence (positive-negative emotion), provided by readers. The analysis showed the big importance of the first biphone in the words, in alliance with results for single phonemes reported in recent studies. The result shows that the phonemic content presented as a stream of biphones is strongly related to the emotional impact that a text has on the reader. This leads to suppose that the mechanisms that have shaped the phonological forms to communicate conceptual meaning involve emotional valence too.}
}
@article{ZHANG2025121787,
title = {Multi-scale interaction network for multimodal entity and relation extraction},
journal = {Information Sciences},
volume = {699},
pages = {121787},
year = {2025},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.121787},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524017018},
author = {Ying Zhang and Gaoxiang Li and Hu Gao and Depeng Dang},
keywords = {Multimodal data, Relation extraction, Named entity extraction},
abstract = {Multimodal Named Entity Recognition (MNER) and Relation Extraction (MRE) are fundamental subtasks of information extraction. These tasks involve identifying entities and determining the semantic relations between them by analyzing contextual information from paired sentences and images. Current research faces two major challenges: (1) noise sensitivity caused by low-relevance visual information, and (2) inefficiency in modality fusion due to incorrect semantic alignment. To this end, we propose a multi-scale fusion network that facilitates modal interaction. Our approach consists of two main stages. In the first stage, we refine the original visual features using cross-modal correlations, treating these refined features as insertable prefixes to generate noise-resistant textual representations. Simultaneously, we introduce a flexible routing strategy to allocate attention across the image and text encoders dynamically. In the second stage, we align semantic features at varying granularities and design a cross-attention mechanism to enhance interaction between modalities, thereby improving fusion efficiency and generating robust multimodal representations for prediction. We conduct comprehensive experiments on two standard datasets (MNRE and Twitter-2017). The results demonstrate significant improvements over state-of-the-art methods. The code will be published on: https://github.com/ZhangYing2022/MoSiNet.}
}
@article{ZHANG2023328,
title = {Guest editorial: Special issue on advances in deep learning based speech processing},
journal = {Neural Networks},
volume = {158},
pages = {328-330},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.11.033},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022004750},
author = {Xiao-Lei Zhang and Lei Xie and Eric Fosler-Lussier and Emmanuel Vincent}
}
@article{DOKUZ2021107573,
title = {Mini-batch sample selection strategies for deep learning based speech recognition},
journal = {Applied Acoustics},
volume = {171},
pages = {107573},
year = {2021},
issn = {0003-682X},
doi = {https://doi.org/10.1016/j.apacoust.2020.107573},
url = {https://www.sciencedirect.com/science/article/pii/S0003682X20306770},
author = {Yesim Dokuz and Zekeriya Tufekci},
keywords = {Mini-batch gradient descent, Sample selection strategies, Deep learning, Speech recognition, LSTM},
abstract = {With the use of deep learning technologies, speech recognition systems gained more success and human–computer interactions became more prevalent. Deep learning based speech recognition systems are getting more attention and are having tremendous success in all areas of speech recognition, such as voice search, mobile communication, and personal digital assistance. However, speech recognition is still challenging due to hardness of adapting new languages, difficulty in handling variations in speech datasets, and overcoming distorting factors. Deep learning systems have the ability to overcome these challenges using high-level abstractions in the datasets by using a deep graph with multiple processing layers using training algorithms, such as gradient descent optimization. In this study, a variant of gradient descent optimization, mini-batch gradient descent is used. We proposed four strategies for selecting mini-batch samples to represent variations of each feature in the dataset for speech recognition tasks to increase model performance of deep learning based speech recognition. For this purpose, gender and accent adjusted strategies are proposed for selecting mini-batch samples. The experiments show that proposed strategies perform better in comparison with standard mini-batch sample selection strategy.}
}
@article{LI20243041,
title = {A Novel 6G Scalable Blockchain Clustering-Based Computer Vision Character Detection for Mobile Images},
journal = {Computers, Materials and Continua},
volume = {78},
number = {3},
pages = {3041-3070},
year = {2024},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2023.045741},
url = {https://www.sciencedirect.com/science/article/pii/S1546221824003473},
author = {Yuejie Li and Shijun Li},
keywords = {6G technology, blockchain, end-to-end recognition, Chinese characters, natural scene, computer vision algorithms, convolutional neural network},
abstract = {6G is envisioned as the next generation of wireless communication technology, promising unprecedented data speeds, ultra-low Latency, and ubiquitous Connectivity. In tandem with these advancements, blockchain technology is leveraged to enhance computer vision applications’ security, trustworthiness, and transparency. With the widespread use of mobile devices equipped with cameras, the ability to capture and recognize Chinese characters in natural scenes has become increasingly important. Blockchain can facilitate privacy-preserving mechanisms in applications where privacy is paramount, such as facial recognition or personal healthcare monitoring. Users can control their visual data and grant or revoke access as needed. Recognizing Chinese characters from images can provide convenience in various aspects of people’s lives. However, traditional Chinese character text recognition methods often need higher accuracy, leading to recognition failures or incorrect character identification. In contrast, computer vision technologies have significantly improved image recognition accuracy. This paper proposed a Secure end-to-end recognition system (SE2ERS) for Chinese characters in natural scenes based on convolutional neural networks (CNN) using 6G technology. The proposed SE2ERS model uses the Weighted Hyperbolic Curve Cryptograph (WHCC) of the secure data transmission in the 6G network with the blockchain model. The data transmission within the computer vision system, with a 6G gradient directional histogram (GDH), is employed for character estimation. With the deployment of WHCC and GDH in the constructed SE2ERS model, secure communication is achieved for the data transmission with the 6G network. The proposed SE2ERS compares the performance of traditional Chinese text recognition methods and data transmission environment with 6G communication. Experimental results demonstrate that SE2ERS achieves an average recognition accuracy of 88% for simple Chinese characters, compared to 81.2% with traditional methods. For complex Chinese characters, the average recognition accuracy improves to 84.4% with our system, compared to 72.8% with traditional methods. Additionally, deploying the WHCC model improves data security with the increased data encryption rate complexity of ∼12 & higher than the traditional techniques.}
}
@article{JAAFAR2023118523,
title = {Multimodal fusion methods with deep neural networks and meta-information for aggression detection in surveillance},
journal = {Expert Systems with Applications},
volume = {211},
pages = {118523},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118523},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422016013},
author = {Noussaiba Jaafar and Zied Lachiri},
keywords = {Aggression detection, Deep learning, Multimodal fusion, Audio–visual fusion, Text-based features, Meta-features},
abstract = {Multimodal fusion has become one of the hottest topics in affective computing and other research areas. Yet, this topic is less studied in surveillance systems. In general, it focused only on a single modality mainly video or audio. Nevertheless, the fusion of multiple data from different sources and modalities is challenging especially for the detection of aggression because of its complexity. Indeed, aggression is difficult to be defined due to its ambiguity. Therefore, this high level concept is hard to be described through simple rules or trivial algorithms and classifiers. This article presents an approach based on four multimodal fusion methods using the audio, video and text modalities, as well as extra-information with Deep Learning techniques to detect aggression in surveillance. We investigate the combination of acoustic, visual and text-based features, as well as a set of five meta-features which have an influence on the fusion process and the aggression intensity. The proposed architectures use Deep Neural Networks (DNNs) although the dataset is relatively small. The neural network models show their effectiveness not only in prediction tasks, but also in feature extraction and dimension reduction. The first multimodal fusion method is based on an intermediate level that contains the predictions of audio, video and the five meta-features by implementing multiple DNNs. As our previous works on aggression detection based on an intermediate level and deep neural networks on a restricted subset of data also indicate the same findings, we extend these works by working on the whole dataset. The second fusion method uses the concatenation of the different features with the meta-features as labels without the intermediate level. The other two fusion methods are based on element-wise operations as a first step then on the concatenation as a second one. One method uses the element-wise product and the other one uses the element-wise addition. Among these proposed models, we find that the second method surpasses the performance of the other methods with an unweighted average accuracy of 85.66%, and weighted average accuracy of 86.35%, as well as the existing similar approach, for the detection of aggression, that provided accuracies of 75% and 77% for the unweighted average and weighted average. Overall, we think that each fusion method has its advantages and shortcomings for the prediction of different aggression intensities. Furthermore, we observe that meta-features are very informative and show significant improvements for the prediction performance of all the fusion methods.}
}
@article{PARK2024113972,
title = {Energy justice: Lessons from offshore wind farm siting conflicts in South Korea},
journal = {Energy Policy},
volume = {185},
pages = {113972},
year = {2024},
issn = {0301-4215},
doi = {https://doi.org/10.1016/j.enpol.2023.113972},
url = {https://www.sciencedirect.com/science/article/pii/S0301421523005578},
author = {Seona Park and Sun-Jin Yun and Kongjang Cho},
keywords = {Energy justice, Offshore wind farm, Energy transition conflicts, Renewable energy conflicts, Restorative justice, Participatory planning},
abstract = {Given the growing salience of energy transition conflicts, policymakers need better tools to explain, prevent, and resolve them. The concept of energy justice highlights the normative direction of the energy system, and therefore, we examined the offshore wind farm siting conflict in Yeonggwang-gun, South Korea, through the lens of energy justice. In this study, we interviewed representatives of local stakeholder groups relevant to offshore wind farms and conducted text-mining analysis to extract the primary opinions of stakeholders. Through text-analysis, four primary factors were identified and discussed in relation to energy justice: gillnet fishing, consultation with the fishing village cooperative, damage by private developers, and a government-led conditional agreement. This mixed-method approach showed that the voices of fishing communities correlate with the four tenets of energy justice—distributional, procedural, recognition, and restorative justice. We discuss how recognition and restorative justice explain energy transition conflict under inadequate policy arrangements in South Korea, where little has been investigated in energy justice literature. By filling the gaps in energy transition conflict, we suggest inclusive policy strategies, revisiting the meaning and utility of participatory planning, scoping methods for Environmental and Social Impact Assessments, and just transition.}
}