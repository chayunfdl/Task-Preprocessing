@article{ZHANG2026104481,
title = {Demand forecasting in cross-border live streaming commerce: An explainable multimodal AI framework for market segmentation},
journal = {Journal of Retailing and Consumer Services},
volume = {88},
pages = {104481},
year = {2026},
issn = {0969-6989},
doi = {https://doi.org/10.1016/j.jretconser.2025.104481},
url = {https://www.sciencedirect.com/science/article/pii/S0969698925002607},
author = {Qi Zhang and Feng Ye and Xue Li and Pengbin Gao},
keywords = {Multimodal, Temporal fusion transformer, Cross-border E-commerce, Live streaming, Demand forecasting},
abstract = {Cross-border live streaming e-commerce serves as a powerful driver of international trade growth. However, critical gaps remain in current research: (1) inadequate theoretical modelling of multimodal information fusion, (2) limitations in the interpretability of predictive algorithms, (3) single-modality approaches restricting generalisability, and (4) unclear mechanisms linking cultural variation to live stream sales cycles. To address these gaps, we propose a novel Multimodal Dynamic Prediction Framework that integrates Temporal Fusion Transformers (TFT), Bidirectional Encoder Representations from Transformers (BERT), and Contrastive Language-Image Pretraining (CLIP) for capturing multimodal dynamics in live-commerce forecasting. Empirical evaluation demonstrates that our framework outperforms baseline approaches significantly. Key findings include: (1) Elucidation of a novel dual-track evolution of KOL influence, characterised by high sensitivity and rapid decay in emerging markets versus gradual penetration and sustained impact in mature markets. (2) Identification of core cyclical patterns underpinning the live streaming restructuring of traditional retail. (3) Temporal variation in multimodal feature contributions confirms an inverted U-shaped relationship between cognitive load and information digestion efficiency. Our framework provides actionable insights for precision marketing and dynamic KOL allocation, advancing both theory and practice in global e-commerce.}
}
@article{YANG2023104,
title = {Efficient data-driven behavior identification based on vision transformers for human activity understanding},
journal = {Neurocomputing},
volume = {530},
pages = {104-115},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.01.067},
url = {https://www.sciencedirect.com/science/article/pii/S092523122300098X},
author = {Jiachen Yang and Zhuo Zhang and Shuai Xiao and Shukun Ma and Yang Li and Wen Lu and Xinbo Gao},
keywords = {Human activity understanding, Vision transformers, Data problems, Computer vision, Deep learning, Active learning},
abstract = {With the development of computer vision, the research on human activity understanding has been greatly promoted. The recognition algorithm based on vision transformer has made some achievements in a large number of computer vision tasks, but it still needs to be driven by a large amount of data. How to get rid of the constraints of large amounts of data is crucial for human behavior recognition based on vision transformer. This paper focuses on solving the dilemma of big data, and tries to achieve a high-performance model through a small amount of high information human activity data. The advantage of our work is that by studying feature distribution, we proposed a core weight entropy data information evaluation method for obtaining high information data, and through redundant information elimination strategy, we can avoid introducing similar data. A large number of experimental results show the effectiveness of the proposed method. Compared with existing methods, our method reduces the data consumption by 5% to 30%, and can achieve the performance of using only 50% of 100% data. More importantly, the data our method selected has no redundancy, which is not available in other methods. In addition, we carried out a large number of ablation experiments to prove the rationality of the method. The work of this paper solves the challenge of relying on a large amount of data when using the visual converter to recognize human behavior, which is of practical significance for realizing efficient human activity understanding research with low data.}
}
@article{WANG2023101799,
title = {BDBRC: A Chinese military entity recognition model combining context contribution and residual dilatation convolutional networks},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {35},
number = {10},
pages = {101799},
year = {2023},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2023.101799},
url = {https://www.sciencedirect.com/science/article/pii/S1319157823003531},
author = {Jintao Wang and Jiayi Qu and Zuyi Zhao and Yulong Yin},
keywords = {Knowledge graph, Entity recognition, Joint recognition, Deep learning},
abstract = {Military information is gradually overloaded due to the diversity of sources and the exponential growth in quantity, which greatly affects the accuracy of intelligence personnel in extracting and analyzing military information. The modern warfare approach has also evolved from the traditional physical domain to the cognitive domain, and competing for advantages in the cognitive domain has become a key objective of combat. Therefore, constructing domain knowledge graphs and mining the relationships between data play an important role in cognitive domain analysis. In this paper, we propose a convolutional network recognition method based on improved two-layer bi-directional BiLSTM networks named the BERT-α BiLSTMs-RECNN-CRF (BDBRC) model. For the difficulties of military entities generally having long names and low extraction accuracy, as well as the existence of a large number of composite entities that are difficult to recognize, an improved two-layer BiLSTM model is devised first. In view of the fact that the BiLSTMs model always extracts features equally in long-distance text sequences without actually considering the different influences of different sentence contexts, contribution factor a is added to extract the contribution of the above and below to the target entity in different sentences respectively. Then, aiming at the strong problem of the domain of military news texts and the high level of inter-entity ambiguity, we propose a method that utilizes a modified convolutional network (RECNN) for partial feature extraction and jointly with a modified two-layer BiLSTM network for entity recognition. The experiment on the self-constructed dataset shows that the F1 value of the model proposed in this paper reaches 93.18%, and the F1 value, P, and H of our model are all better than the baseline model, which verifies the performance of the model. At the same time, we use public data sets MSRA and CLUB2020, and the experimental results show that the model proposed in this paper also has a good performance in the public data set, verifying the universality of the model. It can provide methodological support for the construction of the military knowledge graph.}
}
@article{WEI2025114047,
title = {MFDB: Multimodal feature fusion and dynamic behavior modeling for interactive recommendation systems},
journal = {Knowledge-Based Systems},
volume = {326},
pages = {114047},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.114047},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125010925},
author = {Rongxuan Wei and Jiaming Lan and Kangkang Li and Yu Luo and Yongbin Hu},
keywords = {Recommendation system, Reinforcement learning, BERT, GNN, SlateQ},
abstract = {Recommendation systems have shown its potential in online shopping and video push. However, previous methods struggle to effectively capture the dynamic changes in user behavior sequences while integrating multi-source heterogeneous data, resulting in difficulties in achieving precise, context-sensitive, and sustainably adaptive personalized recommendations that meet user needs. To address these challenges, this study introduces the MFDB (Multimodal Feature Fusion and Dynamic Behavior Modeling) framework, a novel approach based on interactive recommendation strategies. The MFDB framework employs Graph Neural Networks (GNN) and Transformer architectures to model multimodal item data and users’ long-term and short-term preferences, respectively. By conceptualizing multimodal contextual information as states and framing the recommendation process as a sequential decision-making procedure, the framework utilizes reinforcement learning techniques to guide model updates, dynamically incorporating user feedback and behavioral trajectories to ensure that decision-making aligns with user preferences. Extensive experimental results demonstrate that MFDB significantly outperforms various advanced baseline methods across multiple metrics on two real-world datasets, validating its ability to adaptively model user behavior characteristics and efficiently integrate multimodal data sources. This provides robust support for the development of recommendation systems with enhanced robustness and contextual sensitivity.}
}
@article{RENDONSEGADOR2023318,
title = {CrimeNet: Neural Structured Learning using Vision Transformer for violence detection},
journal = {Neural Networks},
volume = {161},
pages = {318-329},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.01.048},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023000606},
author = {Fernando J. Rendón-Segador and Juan A. Álvarez-García and Jose L. Salazar-González and Tatiana Tommasi},
keywords = {Deep learning, Neural Structured Learning, Vision Transformer, Violence detection, Adversarial Learning},
abstract = {The state of the art in violence detection in videos has improved in recent years thanks to deep learning models, but it is still below 90% of average precision in the most complex datasets, which may pose a problem of frequent false alarms in video surveillance environments and may cause security guards to disable the artificial intelligence system. In this study, we propose a new neural network based on Vision Transformer (ViT) and Neural Structured Learning (NSL) with adversarial training. This network, called CrimeNet, outperforms previous works by a large margin and reduces practically to zero the false positives. Our tests on the four most challenging violence-related datasets (binary and multi-class) show the effectiveness of CrimeNet, improving the state of the art from 9.4 to 22.17 percentage points in ROC AUC depending on the dataset. In addition, we present a generalisation study on our model by training and testing it on different datasets. The obtained results show that CrimeNet improves over competing methods with a gain of between 12.39 and 25.22 percentage points, showing remarkable robustness.}
}
@article{KOLESNIKOVA2025e43437,
title = {Advanced machine learning techniques for social support detection on social media},
journal = {Heliyon},
volume = {11},
number = {10},
pages = {e43437},
year = {2025},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2025.e43437},
url = {https://www.sciencedirect.com/science/article/pii/S2405844025018225},
author = {Olga Kolesnikova and Moein {Shahiki Tash} and Zahra Ahani and Ameeta Agrawal and Raúl Monroy and Grigori Sidorov},
keywords = {Social support detection, Social media, GPT, Zero-shot learning},
abstract = {The widespread use of social media highlights the need to understand its impact, particularly the role of online social support. In this study, we present a dataset of YouTube comments, initially comprising 66,272 entries, which was refined to 42,695, with a subset of 10,000 comments selected for detailed analysis without additional filtering. The dataset is annotated for three classification tasks: (1) distinguishing supportive from non-supportive comments, (2) determining whether the support is directed at an individual or a group, and (3) further categorizing group support into six subtypes (Nation, LGBTQ, Black Community, Women, Religion, and Other). To address data imbalances in these tasks, we employed K-means clustering to balance the dataset and compared the results with the original unbalanced data. We use state-of-the-art transformer models and zero-shot learning techniques—including GPT-3, GPT-4, and GPT-4o. Our approach, evaluated using macro F1-scores, demonstrates strong performance on the imbalanced dataset, with our transformer-based model (roberta-base) achieving scores of 0.78, 0.84, and 0.80, respectively, across the three classification tasks. These results also demonstrate a macro F1-score improvement of 0.2% for Task 2 and 0.7% for Task 3 compared to previous work that used CNN with GloVe embeddings and traditional machine learning baselines based on TF-IDF and LIWC features.}
}
@article{MA2025,
title = {Big Data-Driven Ecosystem Integration in the Film and Television Industry:},
journal = {Information Resources Management Journal},
volume = {38},
number = {1},
year = {2025},
issn = {1040-1628},
doi = {https://doi.org/10.4018/IRMJ.388907},
url = {https://www.sciencedirect.com/science/article/pii/S1040162825000217},
author = {Dan Ma},
keywords = {Information Resource Management, Big Data, Film and Television Ecosystem, Industrial Chain Integration, Audience Profiling, Blockchain, Cross-Cultural Adaptation},
abstract = {ABSTRACT
This study developed a big data-driven information resource management framework to integrate creation, production, distribution, and consumption across the global film and television value chain. This research addressed the need to break down data silos, adapt for cross-cultural content, and balance technological efficiency with artistic autonomy. The framework was built by combining artificial intelligence tools (bidirectional encoder representations from transformers and latent Dirichlet allocation), blockchain-based alliance chains, and cultural label conversion rules; these were then validated through controlled experiments and multi-market testing. Results showed higher audience retention, more successful cross-cultural performance, faster production cycles, and reduced levels of disputes. The findings suggested that information resource management optimized data governance, enhanced cultural adaptability, and improved industrial efficiency, thereby offering a scalable model for sustainable digital transformation in the media industry.}
}
@article{LI2024102010,
title = {Punctuation and lexicon aid representation: A hybrid model for short text sentiment analysis on social media platform},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {36},
number = {3},
pages = {102010},
year = {2024},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2024.102010},
url = {https://www.sciencedirect.com/science/article/pii/S1319157824000995},
author = {Zhenyu Li and Zongfeng Zou},
keywords = {Sentiment analysis, Short text classification, BERT representation, Attention mechanism, Social media mining},
abstract = {Sentiment analysis measures user experience on social media. With the emergence of pre-trained models, text classification tasks have become homogeneous, without a significant improvement in accuracy. Therefore, we present a hybrid model called PLASA to classify the sentiment polarity of short comments, particularly barrages. PLASA introduces a collaborative attention module that integrates information about relative position and knowledge from summarized lexicons to better adjust the relationship between word representations. Our model is evaluated using three new curated sentiment analysis datasets: SentiTikTok-2023 (4613 items), SentiBilibili-2023 (7755 items), and SentiWeibo-2023 (5614 items). Although the comment length varies across datasets, all maintain a consistent punctuation percentage at approximately 13%. Consequently, PLASA with the optimal combination demonstrates notable performance improvements compared to both the baseline and commonly used models. It achieves micro-F1 scores of 93.94%, 90.34%, and 88.79% on the respective datasets. We also observed that the representation capacity of the pre-trained model decreases as the text length increases. Moreover, the proposed collaborative attention module effectively addresses this limitation, as confirmed by our ablation study.}
}
@article{HUERTASGARCIA2023110552,
title = {Countering malicious content moderation evasion in online social networks: Simulation and detection of word camouflage},
journal = {Applied Soft Computing},
volume = {145},
pages = {110552},
year = {2023},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2023.110552},
url = {https://www.sciencedirect.com/science/article/pii/S1568494623005707},
author = {Álvaro Huertas-García and Alejandro Martín and Javier Huertas-Tato and David Camacho},
keywords = {Information disorders, Leetspeak, Word camouflage, Multilingualism, Content evasion},
abstract = {Content moderation is the process of screening and monitoring user-generated content online. It plays a crucial role in stopping content resulting from unacceptable behaviors such as hate speech, harassment, violence against specific groups, terrorism, racism, xenophobia, homophobia, or misogyny, to mention some few, in Online Social Platforms. These platforms make use of a plethora of tools to detect and manage malicious information; however, malicious actors also improve their skills, developing strategies to surpass these barriers and continuing to spread misleading information. Twisting and camouflaging keywords are among the most widely used techniques to evade platform content moderation systems. In response to this recent ongoing issue This paper presents an innovative approach to address this linguistic trend in social networks through the simulation of different content evasion techniques and a multilingual transformer model for content evasion detection. In this way a multilingual public tool Named “pyleetspeak” is shared with the scientific community, enabling the generation and simulation of content evasion through automatic word camouflage in a customizable way. Additionally a multilingual named-entity recognition (NER) transformer-based model is provided Designed for the recognition and detection of such evasion technique. The developed tool is multilingual Supporting over 20 languages (ar, az, da, de, el, en, es, fi, fr, hu, id, it, kk, nb, ne, nl, pt, ro, ru, sl, sv, tg, tr) and the NER model has been tested in English, Spanish, French, Italian, and German. This multilingual NER model is evaluated in different textual scenarios Detecting different types and mixtures of camouflage techniques Achieving an overall weighted F1 score of 0.8795. This article contributes significantly to countering malicious information by developing multilingual tools to simulate and detect new methods of evasion of content on social networks Making the fight against information disorders more effective}
}
@incollection{WANG2025301,
title = {8 - Transformer and its companions∗},
editor = {Shuhao Wang and Gang Xu},
booktitle = {Deep Learning},
publisher = {Elsevier},
pages = {301-368},
year = {2025},
isbn = {978-0-443-43954-4},
doi = {https://doi.org/10.1016/B978-0-443-43954-4.00008-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780443439544000081},
author = {Shuhao Wang and Gang Xu},
keywords = {Attention mechanism, BERT, GPT, Transformer, TransUNet, Vision transformer},
abstract = {This chapter introduces the Transformer model and its variants, highlighting their significance in natural language processing and computer vision. It begins by discussing the limitations of RNNs, such as the vanishing gradient problem and sequential dependencies, which led to the development of the Transformer. The chapter explains the attention mechanism, crucial for tasks like image captioning and language translation, and introduces the self-attention mechanism that forms the core of the Transformer. It also covers the evolution of the Transformer into models like BERT and GPT, and their impact on natural language processing. Additionally, the chapter explores the application of Transformers in computer vision, including the Vision Transformer and TransUNet, demonstrating their effectiveness in image classification and segmentation tasks. The chapter concludes with a discussion on the potential future developments of deep learning models.}
}
@article{WAN2024103609,
title = {Emotion-cognitive reasoning integrated BERT for sentiment analysis of online public opinions on emergencies},
journal = {Information Processing & Management},
volume = {61},
number = {2},
pages = {103609},
year = {2024},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2023.103609},
url = {https://www.sciencedirect.com/science/article/pii/S0306457323003461},
author = {Bingtao Wan and Peng Wu and Chai Kiat Yeo and Gang Li},
keywords = {Emergency, Sentiment analysis, OCC model, BERT, Hybrid approach},
abstract = {Sentiment analysis of online public opinions on emergencies (OPOEs) requires accurate and explainable results to facilitate a better understanding of public sentiment and effective crisis management, but it is challenging due to the complexity and diversity of emotions contained in OPOEs. In this paper, we propose an Emotion-Cognitive Reasoning integrated BERT (ECR-BERT) for sentiment analysis of OPOEs. ECR-BERT combines an emotion model and deep learning to provide reliable auxiliary knowledge to improve BERT. Specifically, we use the emotion model proposed by Ortony, Clore, and Collins (OCC) to build emotion-cognitive rules and perform emotion-cognitive reasoning to discover emotion-cognitive knowledge. To mitigate the impact of knowledge noise, we propose a novel self-adaptive fusion algorithm that provides a selection mechanism for the incorporation of knowledge. In addition, we utilize knowledge-enabled feature representation to efficiently exploit inferred knowledge. Our evaluation on four real-world OPOE datasets shows that ECR-BERT significantly outperforms other BERT-based models, achieving state-of-the-art results with an absolute average accuracy improvement of 0.82%, 1.74%, 0.98%, and 1.37% over BERT, respectively. In addition, ECR-BERT provides a detailed explanation of how sentiment polarity is derived from fine-grained emotion categories. The ablation study demonstrates the effectiveness of each technique. In conclusion, ECR-BERT is an excellent choice for sentiment analysis of OPOEs, providing accurate and explainable results for crisis management.}
}
@article{ZHAO2024102552,
title = {Multimodal Aspect-Based Sentiment Analysis: A survey of tasks, methods, challenges and future directions},
journal = {Information Fusion},
volume = {112},
pages = {102552},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102552},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524003300},
author = {Tianyu Zhao and Ling-ang Meng and Dawei Song},
keywords = {Multimodal sentiment analysis, Multimodal Named Entity Recognition, Multimodal Aspect Based Sentiment Analysis, Multimodal Category Based Sentiment Analysis},
abstract = {With the development of social media, users increasingly tend to express their sentiments (broadly including sentiment polarities, emotions and sarcasm, etc.) associated with fine-grained aspects (e.g., entities) in multimodal content (mostly encompassing images and texts). Consequently, automated recognition of sentiments within multimodal content over different aspects, namely Multimodal Aspect-Based Sentiment Analysis (MABSA), has recently become an emergent research area. This paper assesses the state-of-the-art methods in MABSA based on a systematic taxonomy over different subtasks of MABSA. It compiles advanced models for each task and offers a concise overview of popular datasets and evaluation standards. Finally, we discuss the limitations of current research and highlight promising future research directions.}
}
@article{ZHANG2023109894,
title = {A time-aware self-attention based neural network model for sequential recommendation},
journal = {Applied Soft Computing},
volume = {133},
pages = {109894},
year = {2023},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2022.109894},
url = {https://www.sciencedirect.com/science/article/pii/S1568494622009437},
author = {Yihu Zhang and Bo Yang and Haodong Liu and Dongsheng Li},
keywords = {Neural recommender systems, Sequential recommendation, Self-attention, Transformer},
abstract = {Sequential recommendation is one of the hot research topics in recent years. Various sequential recommendation models have been proposed, of which Self-Attention (SA)-based models are shown to have state-of-the-art performance. However, most of the existing SA-based sequential recommendation models do not make use of temporal information, i.e., timestamps of user–item interactions, except for an initial attempt (Li et al., 2020). In this paper, we propose a Time-Aware Transformer for Sequential Recommendation (TAT4SRec), an SA-based neural network model which utilizes the temporal information and captures users’ preferences more precisely. TAT4SRec has two salient features: (1) TAT4SRec utilizes an encoder–decoder structure to model timestamps and interacted items separately and this structure appears to be a better way of making use of the temporal information. (2) in the proposed TAT4SRec, two different embedding modules are designed to transform continuous data (timestamps) and discrete data (item IDs) into embedding matrices respectively. Specifically, we propose a window function-based embedding module to preserve the continuous dependency contained in similar timestamps. Finally, extensive experiments demonstrate the effectiveness of the proposed TAT4SRec over various state-of-the-art MC/RNN/SA-based sequential recommendation models under several widely-used metrics. Furthermore, experiments are also performed to show the rationality of the different proposed structures and demonstrate the computation efficiency of TAT4SRec. The promising experimental results make it possible to apply TAT4SRec in various online applications.}
}
@article{NAJAFI2024124737,
title = {TurkishBERTweet: Fast and reliable large language model for social media analysis},
journal = {Expert Systems with Applications},
volume = {255},
pages = {124737},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124737},
url = {https://www.sciencedirect.com/science/article/pii/S095741742401604X},
author = {Ali Najafi and Onur Varol},
keywords = {TurkishBERTweet, Sentiment analysis, HateSpeech detection, ChatGPT},
abstract = {Turkish is one of the most spoken languages in the world; however, it is still among the low-resource languages. Wide us of this language on social media platforms such as Twitter, Instagram, or Tiktok and strategic position of the country in the world politics makes it appealing for the social network researchers and industry. To address this need, we introduce TurkishBERTweet, the first large scale pre-trained language model for Turkish social media built using over 894 million Turkish tweets. The model shares the same architecture as RoBERTa-base model with smaller input length, making TurkishBERTweet lighter than the most used model, called BERTurk, and can have significantly lower inference time. We trained our model using the same approach for RoBERTa model and evaluated on two tasks: Sentiment Classification and Hate Speech Detection. We demonstrate that TurkishBERTweet outperforms the other available alternatives on generalizability and its lower inference time gives significant advantage to process large-scale datasets. We also show custom preprocessors for social media can acquire information from platform specific entities. We also conduct comparison with the commercial solutions like OpenAI and Gemini, and other available Turkish LLMs in terms of cost and performance to demonstrate TurkishBERTweet is scalable and cost-effective.}
}
@article{ISNAN2023168,
title = {Sentiment Analysis for TikTok Review Using VADER Sentiment and SVM Model},
journal = {Procedia Computer Science},
volume = {227},
pages = {168-175},
year = {2023},
note = {8th International Conference on Computer Science and Computational Intelligence (ICCSCI 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.514},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923016800},
author = {Mahmud Isnan and Gregorius Natanael Elwirehardja and Bens Pardamean},
keywords = {Sentiment Analysis, TikTok, SVM, VADER Sentiment, Google Play Store},
abstract = {TikTok, a social networking site for uploading short videos, has become one of the most popular. Despite this, not all users are happy with the app; there are criticisms and suggestions, one of which is reviewed via the TikTok app on the Google Play Store. The reviews were extracted and then used for training a sentiment analysis model. The VADER sentiment method was utilized to offer the review's initial labeling (positive, neutral, and negative). The result revealed that most reviews were classified as positive, meaning that the data were imbalanced and challenging to handle in further analysis. Therefore, Random Under-sampling (RUS) and Random Over-sampling (ROS) methods were deployed to deal with that condition. The labeled reviews were subsequently pre-processed using tools such as case folding, noise removal, normalization, and stopwords before being used for training a Support Vector Machine (SVM) model for sentiment classification. The SVM trained without resampling produced the most favorable results, with an F1-score of 0.80.}
}
@article{ZHANG2023282,
title = {A Multitask learning model for multimodal sarcasm, sentiment and emotion recognition in conversations},
journal = {Information Fusion},
volume = {93},
pages = {282-301},
year = {2023},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S1566253523000040},
author = {Yazhou Zhang and Jinglin Wang and Yaochen Liu and Lu Rong and Qian Zheng and Dawei Song and Prayag Tiwari and Jing Qin},
keywords = {Multimodal sarcasm recognition, Sentiment analysis, Emotion recognition, Multitask learning, Affective computing},
abstract = {Sarcasm, sentiment and emotion are tightly coupled with each other in that one helps the understanding of another, which makes the joint recognition of sarcasm, sentiment and emotion in conversation a focus in the research in artificial intelligence (AI) and affective computing. Three main challenges exist: Context dependency, multimodal fusion and multitask interaction. However, most of the existing works fail to explicitly leverage and model the relationships among related tasks. In this paper, we aim to generically address the three problems with a multimodal joint framework. We thus propose a multimodal multitask learning model based on the encoder–decoder architecture, termed M2Seq2Seq. At the heart of the encoder module are two attention mechanisms, i.e., intramodal (Ia) attention and intermodal (Ie) attention. Ia attention is designed to capture the contextual dependency between adjacent utterances, while Ie attention is designed to model multimodal interactions. In contrast, we design two kinds of multitask learning (MTL) decoders, i.e., single-level and multilevel decoders, to explore their potential. More specifically, the core of a single-level decoder is a masked outer-modal (Or) self-attention mechanism. The main motivation of Or attention is to explicitly model the interdependence among the tasks of sarcasm, sentiment and emotion recognition. The core of the multilevel decoder contains the shared gating and task-specific gating networks. Comprehensive experiments on four bench datasets, MUStARD, Memotion, CMU-MOSEI and MELD, prove the effectiveness of M2Seq2Seq over state-of-the-art baselines (e.g., CM-GCN, A-MTL) with significant improvements of 1.9%, 2.0%, 5.0%, 0.8%, 4.3%, 3.1%, 2.8%, 1.0%, 1.7% and 2.8% in terms of Micro F1.}
}
@article{ONIKOYI2023100018,
title = {Gender prediction with descriptive textual data using a Machine Learning approach},
journal = {Natural Language Processing Journal},
volume = {4},
pages = {100018},
year = {2023},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2023.100018},
url = {https://www.sciencedirect.com/science/article/pii/S2949719123000158},
author = {Babatunde Onikoyi and Nonso Nnamoko and Ioannis Korkontzelos},
keywords = {Gender prediction, Gender classification, Machine Learning, Twitter, Natural Language Processing, Pre-trained word embeddings},
abstract = {Social media are well-established means of online communication, generating vast amounts of data. In this paper, we focus on Twitter and investigate behavioural differences between male and female users on social media. Using Natural Language Processing and Machine Learning approaches, we propose a user gender identification method that considers both the tweets and the Twitter profile description of a user. For experimentation and evaluation, we enriched and used an existing Twitter User Gender Classification dataset, which is freely available on Kaggle. We considered a variety of methods and components, such as the Bag of Words model, pre-trained word embeddings (GLOVE, BERT, GPT2 and Word2Vec) and machine learners, e.g., Naïve Bayes, Support Vector Machines and Random Forests. Evaluation results have shown that including the Twitter profile description of a user significantly improves gender classification accuracy, by 10% approximately. Stanford’s GLOVE embedding model, pre-trained on 2 billion tweets, 27 billion tokens and a vocabulary size of 1.2 million words, achieved the highest gender prediction accuracy, considering both the tweets and the profile description of a user. Statistical significance has been assessed using McNemar’s two-tailed test.}
}
@article{WU2025105560,
title = {Research on the influence mechanism of emotional communication on Twitter (X) and the effect of spreading public anger},
journal = {Acta Psychologica},
volume = {260},
pages = {105560},
year = {2025},
issn = {0001-6918},
doi = {https://doi.org/10.1016/j.actpsy.2025.105560},
url = {https://www.sciencedirect.com/science/article/pii/S000169182500873X},
author = {Chunqiong Wu and Shan Jiang and Jianhong Sun and Yingqi Liu},
keywords = {Anger propagation, Twitter, Algorithmic amplification, Toxic content, Network analysis, Machine learning, Sentiment analysis, Social media discourse},
abstract = {Background
Anger-driven content has a significant influence on engagement and discourse on social media platforms, particularly Twitter. This study investigates the mechanisms through which anger spreads on Twitter, analyzing the role of user interactions, algorithmic amplification, and network dynamics in shaping emotional discourse.
Methods
A dataset of 5000 tweets was collected from Twitter using API access, spanning political discourse, social movements, and crisis-related discussions. Sentiment and toxicity analysis were conducted using VADER and the Perspective API, while network analysis applied the Louvain method to identify clusters of anger-driven interactions. Regression models, propensity score matching, and time-series analysis were employed to examine the relationship between toxicity, engagement, and anger amplification. The predictive performance of other models was compared with the meta-model.
Results
Among 5000 tweets analyzed, 70.0 % originated from ordinary users and 30.0 % from public figures. Toxic content (20.0 %) was strongly correlated with engagement (retweets: r = 0.52, p < 0.001; likes: r = 0.48, p < 0.001; replies: r = 0.43, p = 0.002). Regression confirmed that toxicity significantly predicted anger spread (β = 0.320, p < 0.001), with visibility score also exerting a strong effect (β = 0.045, p < 0.001). Anger-driven discussions formed large clusters (avg. 215.4 users), and toxic tweets received 27.1 % higher visibility and 85.7 % more retweets. The proposed meta-model (RF + SVM + ARIMA) achieved the highest accuracy (0.89), outperforming deep learning approaches such as BERT (0.87) and LSTM (0.86). These amplification results are based on observational modeling of engagement patterns and should be interpreted as indicative of algorithmic intervention.
Conclusion
Our findings underscore the need for targeted interventions, including algorithmic adjustments to mitigate the disproportionate amplification of anger, as well as enhanced content moderation strategies to foster healthier digital interactions.}
}
@article{WANG2025129163,
title = {Transformer-based correlation mining network with self-supervised label generation for multimodal sentiment analysis},
journal = {Neurocomputing},
volume = {618},
pages = {129163},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.129163},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224019349},
author = {Ruiqing Wang and Qimeng Yang and Shengwei Tian and Long Yu and Xiaoyu He and Bo Wang},
keywords = {Multimodal sentiment analysis, Transformer, Multimodal fusion, Collaborative learning},
abstract = {Multimodal Sentiment Analysis (MSA) aims to recognize and understand a speaker's sentiment state by integrating information from natural language, facial expressions, and voice, has gained much attention in recent years. However, modeling multimodal data poses two main challenges: 1) There are potential sentiment correlations between modalities and within contextual contexts, making it difficult to perform deep-level sentiment correlation mining and information fusion; 2) Sentiment information tends to be unevenly distributed across different modalities, posing challenges in fully leveraging information from each modality for collaborative learning. To address the above challenges, we propose CMLG based on correlation mining and label generation. This approach utilizes a Squeeze and Excitation Network (SEN) to recalibrate modality features and employs Transformer-based intra-modal and inter-modal feature extractors to mine the intrinsic connections between different modalities. In addition, we designed a Self-Supervised Label Generation Module (SLGM) that relies on the positive correlation between feature distances and label offsets to generate single-peak labels, and jointly train multi-peak and single-peak tasks to detect sentiment differences. Extensive experiments on three benchmark dataset (MOSI, MOSEI and SIMS) have shown that the above proposed method CMLG achieves excellent results.}
}
@article{WU2025127823,
title = {Recent development on online public opinion communication and early warning technologies: Survey},
journal = {Expert Systems with Applications},
volume = {284},
pages = {127823},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127823},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425014459},
author = {Wei Wu and Yawen Yang and Tianlu Qiao and Haipeng Peng},
keywords = {Online public opinion, Public opinion communication, Public opinion analyzing, Public opinion warning, Artificial intelligence},
abstract = {The rapid development of the Internet and communication technologies has led to the emergence of online public opinion, which plays a crucial role in disseminating information and guiding the public. However, due to its large volume of data and fast-paced changes, automated monitoring of online public opinion is challenging. Currently, advancements in artificial intelligence (AI) technology provides new solutions for detecting and early warning of online public opinion. This paper reviews the literature in the field of online public opinion research over the years. Firstly, it systematically elucidates the formation mechanism and dissemination model of online public opinion. Secondly, it focuses on the analysis of public opinion monitoring technology in the era of artificial intelligence, clarifying the research branches of online public opinion early warning technology. Finally, it identifies the shortcomings in current research on online public opinion and proposes directions for future research.}
}
@article{AZMEE2025100566,
title = {Human AI Collaboration Framework for Detecting Mental Illness Causes from Social Media},
journal = {Smart Health},
volume = {36},
pages = {100566},
year = {2025},
issn = {2352-6483},
doi = {https://doi.org/10.1016/j.smhl.2025.100566},
url = {https://www.sciencedirect.com/science/article/pii/S2352648325000273},
author = {Abm Adnan Azmee and Francis Nweke and Mason Pederson and Md Abdullah Al Hafiz Khan and Yong Pei},
keywords = {Natural language processing, Neural networks, Human AI teaming, Mental health, Social media},
abstract = {Mental health is a critical aspect of our overall well-being. Mental illness refers to conditions that impact an individual’s psychological state, resulting in considerable distress and limitations in functioning day-to-day tasks. Due to the progress of technology, social media has emerged as a platform for individuals to share their thoughts and emotions. The psychological state of individuals can be accessed with the help of data from these platforms. However, it is challenging for conventional machine learning models to analyze the diverse linguistic contexts of social media data. Moreover, to effectively analyze the data, we need the support of human experts. In this work, we propose a novel human AI-collaboration framework that leverages the strength of human expertise and artificial intelligence (AI) to overcome these challenges. Our proposed framework utilizes multi-level data along with feedback from human experts to identify the causes behind mental illness. The efficacy and effectiveness of our proposed model are shown by extensive evaluation on Reddit data. Experimental results demonstrate that our proposed model outperforms the baseline model by 3 to 17% performance improvement.}
}
@article{IBITOYE2025100262,
title = {Clustering digital mental health perceptions using transformer-based models},
journal = {Franklin Open},
volume = {11},
pages = {100262},
year = {2025},
issn = {2773-1863},
doi = {https://doi.org/10.1016/j.fraope.2025.100262},
url = {https://www.sciencedirect.com/science/article/pii/S2773186325000520},
author = {Ayodeji O.J. Ibitoye and Oladosu O. Oladimeji and Oluwaseyi F. Afe},
keywords = {Mental health, Text clustering, Transformer-based architectures, Decision support, Mental health perspectives, Digital mental health},
abstract = {The rise in online mental health discussions underscores the need to understand diverse perspectives to inform targeted interventions. Addressing the granularity of existing research on mapping such perspectives, this study proposes a model combining contextual and sentiment analysis, keyword scoring, and clustering techniques to identify themes in online comments. Using transformer-based models (BERT, ALBERT, ELECTRA), the study achieved high-quality clustering of seven distinct mental health perspectives: stigma, empowerment, treatment approaches, recovery, social/environmental factors, advocacy, and cultural dimensions. ELECTRA outperformed others in clustering quality (silhouette score: 0.73; Davies-Bouldin Index: 0.34). The findings reveal cohesive, well-separated clusters that enhance understanding of digital mental health discourse. These insights provide a foundation for data-driven advocacy, tailored interventions, and broader awareness, addressing the complex dynamics of mental health narratives in online spaces. This study bridges a critical research gap by offering a systematic approach to analysing and interpreting mental health perspectives in digital environments.}
}
@article{LI2024122260,
title = {T3SRS: Tensor Train Transformer for compressing sequential recommender systems},
journal = {Expert Systems with Applications},
volume = {238},
pages = {122260},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122260},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423027628},
author = {Hao Li and Jianli Zhao and Huan Huo and Sheng Fang and Jianjian Chen and Lutong Yao and Yiran Hua},
keywords = {Sequential recommender systems, Model compression, Transformer, Tensor train network},
abstract = {In recent years, attention mechanisms have gained popularity in sequential recommender systems (SRSs) due to obtaining dynamic user preferences efficiently. However, over-parameterization of these models often increases the risk of overfitting. To address this challenge, we propose a Transformer model based on tensor train networks. Initially, we propose a tensor train layer (TTL) to accommodate the original weight matrix, thus reducing the space complexity of the mapping layer. Based on the TTL, we reconfigure the multi-head attention module and the position-wise feed-forward network. Finally, a tensor train layer replaces the output layer to complete the overall compression. According to the experimental results, the proposed model compresses SRSs parameters effectively, achieving compression rates of 76.2%−85.0%, while maintaining or enhancing sequence recommendation performance. To our knowledge, the Tensor Train Transformer is the first model compression approach for Transformer-based SRSs, and the model is broadly applicable.}
}
@article{LOKALA2024,
title = {Detecting Substance Use Disorder Using Social Media Data and the Dark Web: Time- and Knowledge-Aware Study},
journal = {JMIRx Med},
volume = {5},
year = {2024},
issn = {2563-6316},
doi = {https://doi.org/10.2196/48519},
url = {https://www.sciencedirect.com/science/article/pii/S2563631624000360},
author = {Usha Lokala and Orchid Chetia Phukan and Triyasha Ghosh Dastidar and Francois Lamy and Raminta Daniulaityte and Amit Sheth},
keywords = {opioid, substance use, substance use disorder, social media, US, opioid crisis, mental health, substance misuse, crypto, dark web, users, user perception, fentanyl, synthetic opioids, United States},
abstract = {Background
Opioid and substance misuse has become a widespread problem in the United States, leading to the “opioid crisis.” The relationship between substance misuse and mental health has been extensively studied, with one possible relationship being that substance misuse causes poor mental health. However, the lack of evidence on the relationship has resulted in opioids being largely inaccessible through legal means.
Objectives
This study aims to analyze social media posts related to substance use and opioids being sold through cryptomarket listings. The study aims to use state-of-the-art deep learning models to generate sentiment and emotion from social media posts to understand users’ perceptions of social media. The study also aims to investigate questions such as which synthetic opioids people are optimistic, neutral, or negative about; what kind of drugs induced fear and sorrow; what kind of drugs people love or are thankful about; which drugs people think negatively about; and which opioids cause little to no sentimental reaction.
Methods
The study used the drug abuse ontology and state-of-the-art deep learning models, including knowledge-aware Bidirectional Encoder Representations From Transformers–based models, to generate sentiment and emotion from social media posts related to substance use and opioids being sold through cryptomarket listings. The study crawled cryptomarket data and extracted posts for fentanyl, fentanyl analogs, and other novel synthetic opioids. The study performed topic analysis associated with the generated sentiments and emotions to understand which topics correlate with people’s responses to various drugs. Additionally, the study analyzed time-aware neural models built on these features while considering historical sentiment and emotional activity of posts related to a drug.
Results
The study found that the most effective model performed well (statistically significant, with a macro–F1-score of 82.12 and recall of 83.58) in identifying substance use disorder. The study also found that there were varying levels of sentiment and emotion associated with different synthetic opioids, with some drugs eliciting more positive or negative responses than others. The study identified topics that correlated with people’s responses to various drugs, such as pain relief, addiction, and withdrawal symptoms.
Conclusions
The study provides insight into users’ perceptions of synthetic opioids based on sentiment and emotion expressed in social media posts. The study’s findings can be used to inform interventions and policies aimed at reducing substance misuse and addressing the opioid crisis. The study demonstrates the potential of deep learning models for analyzing social media data to gain insights into public health issues.}
}
@article{WANG2023102552,
title = {Flow2Flow: Audio-visual cross-modality generation for talking face videos with rhythmic head},
journal = {Displays},
volume = {80},
pages = {102552},
year = {2023},
issn = {0141-9382},
doi = {https://doi.org/10.1016/j.displa.2023.102552},
url = {https://www.sciencedirect.com/science/article/pii/S0141938223001853},
author = {Zhangjing Wang and Wenzhi He and Yujiang Wei and Yupeng Luo},
keywords = {Audio-visual, Talking face, Video synthesis, Multimodal, Speech-driven face animation, Cross-modality generation},
abstract = {Audio-visual cross-modality generation refers to the generation of audio or visual content based on input from another modality. One of the key tasks in this field is the generation of realistic talking facial videos from audio and head pose information, which has significant applications in human–computer interaction, virtual reality, and video production. However, previous work has limitations such as the inability to generate natural head poses or interact with audio, which compromises the realism and expressive power of the generated videos. This paper aims to address these issues and improve the state-of-the-art in this field. To this end, we propose an autoregressive generation method called Flow2Flow and collect a large-scale in-the-wild solo-singing-themed audio-visual dataset called AVVS to investigate the rhythmic head movement patterns. The Flow2Flow model involves a multimodal transformer block with cross-attention, which can encode audio features and historical head poses to establish potential audio-visual motion entanglement and uses normalizing flows to generate future facial motion representation sequences. The generated motion representations are identity-independent, allowing the method to be transferred to any face identity. We model the motion of image content using warping flows generated from 3D keypoints based on the facial motion representation sequences, carefully manipulate animation generation, and estimate dense motion fields based on deformation flows using a neural rendering model to present photo-realistic talking facial videos. Experimental results show that our proposed method generates photo-realistic videos with natural head poses and lip-syncing, and we validate the effectiveness of our method compared to state-of-the-art methods on two public datasets.}
}
@article{MA2024105362,
title = {Investigating disaster response for resilient communities through social media data and the Susceptible-Infected-Recovered (SIR) model: A case study of 2020 Western U.S. wildfire season},
journal = {Sustainable Cities and Society},
volume = {106},
pages = {105362},
year = {2024},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2024.105362},
url = {https://www.sciencedirect.com/science/article/pii/S2210670724001902},
author = {Zihui Ma and Lingyao Li and Libby Hemphill and Gregory B. Baecher and Yubai Yuan},
keywords = {Wildfire response, Community resilience, Social media, BERT topic modeling, SIR model},
abstract = {Effective disaster response is critical for communities to remain resilient and advance the development of smart cities. Responders and decision-makers would benefit from reliable, timely measures of the issues impacting their communities during a disaster, and social media offers a potentially rich data source. Social media can reflect public concerns and behaviors during a disaster, offering valuable insights for decision-makers to understand evolving situations and optimize resource allocation. We used Bidirectional Encoder Representations from Transformers (BERT) topic modeling to cluster topics from Twitter data. Then, we conducted a temporal-spatial analysis to examine the distribution of these topics across different regions during the 2020 western U.S. wildfire season. Our results show that Twitter users mainly focused on three topics: “health impact,” “damage,” and “evacuation.” We used the Susceptible-Infected-Recovered (SIR) theory to explore the magnitude and velocity of topic diffusion on Twitter. The results displayed a clear relationship between topic trends and wildfire propagation patterns. The estimated parameters obtained from the SIR model in selected cities revealed that residents exhibited a high level of several concerns during the wildfire. Our study offers a quantitative approach to measure disaster response and support community resilience enhancement.}
}
@article{MA2025100418,
title = {Exploring Food Safety Emergency Incidents on Sina Weibo: Using Text Mining and Sentiment Evolution},
journal = {Journal of Food Protection},
volume = {88},
number = {1},
pages = {100418},
year = {2025},
issn = {0362-028X},
doi = {https://doi.org/10.1016/j.jfp.2024.100418},
url = {https://www.sciencedirect.com/science/article/pii/S0362028X24002023},
author = {Biao Ma and Ruihan Zheng},
keywords = {Deep learning, Food safety, Sentiment analysis, Text mining},
abstract = {Food safety remains a crucial concern in both public health and societal stability. In the age of information technology, social media has emerged as a pivotal channel for shaping public opinion and disseminating information, exerting a substantial influence on how the public perceives incidents related to food safety. This study specifically focuses on the “Rat-Headed Duck Neck” incident as a case study, conducting a comprehensive analysis of extensive social media data to investigate how online public discourse molds perceptions of such events. To accomplish this research, data were initially gathered using a custom web crawler technology. These data encompassed various aspects, including user interactions, emotional expressions, and the evolution of topics. Subsequently, the study employed an innovative approach by combining BERT-TextCNN and BERTopic models for a thorough analysis of sentiment and thematic aspects of the textual data. This analysis provided insights into the intricate emotions and primary concerns of the public regarding incidents related to food safety. Furthermore, the research harnessed Gephi, a network analysis tool, to scrutinize the dissemination of information within the network and to monitor dynamic shifts in public opinion. The findings from this study not only shed light on the role of online public sentiment in the propagation of food safety events but also provide fresh perspectives for policymakers and business leaders when responding to similar crises, taking into account the subtleties of online public sentiment. These innovative methodologies and findings significantly enhance our comprehension of public responses to food safety incidents within the realm of social media.}
}
@article{ZHANG2025105690,
title = {InpaintingPose: Enhancing human pose transfer by image inpainting},
journal = {Image and Vision Computing},
volume = {162},
pages = {105690},
year = {2025},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2025.105690},
url = {https://www.sciencedirect.com/science/article/pii/S0262885625002781},
author = {Wei Zhang and Chenglin Zhou and Xuekang Peng and Zhichao Lian},
keywords = {Human pose transfer, Diffusion model, Image inpainting},
abstract = {Human pose transfer involves transforming a human subject in a reference image from a source pose to a target pose while maintaining consistency in both appearance and background. Most existing methods treat the appearance and background in the reference image as a unified entity, which causes the background to be disrupted by pose transformations and prevents the model from focusing on the complex relationship between appearance and pose. In this paper, we propose InpaintingPose, a novel human pose transfer framework based on image inpainting, which enables precise pose control without affecting the background. InpaintingPose separates the background from the appearance, applying transformations only where necessary. This strategy prevents the background from being affected by pose transformations and allows the model to focus on the coupling between appearance and pose. Additionally, we introduce an appearance control mechanism to ensure appearance consistency between the generated images and the reference images. Finally, we propose an initial noise optimization strategy to address the instability in generating human images with extremely bright backgrounds. By decoupling appearance and background, InpaintingPose can also recombine the appearance and background from different reference images to produce realistic human images. Extensive experiments demonstrate the effectiveness of our method, achieving state-of-the-art FID scores of 4.74 and 26.74 on DeepFashionv2 and TikTok datasets, respectively, significantly outperforming existing approaches.}
}
@article{LIU2025648,
title = {MiM-UNet: An efficient building image segmentation network integrating state space models},
journal = {Alexandria Engineering Journal},
volume = {120},
pages = {648-656},
year = {2025},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2025.02.035},
url = {https://www.sciencedirect.com/science/article/pii/S1110016825002029},
author = {Dong Liu and Zhiyong Wang and Ankai Liang},
keywords = {Building segmentation, Complex terrain, State space models, Remote sensing images, Deep learning},
abstract = {With the advancement of remote sensing technology, the analysis of complex terrain images has become crucial for urban planning and geographic information extraction. However, existing models face significant challenges in processing intricate building structures: Transformer-based models suffer from high computational complexity and memory demands, while Convolutional Neural Networks (CNNs) often struggle to capture features across multiple scales and hierarchical levels. To address these limitations, we propose a novel architecture, Mamba-in-Mamba U-Net (MiM-UNet), which integrates the design principles of state–space models (SSMs) to enhance both computational efficiency and feature extraction capacity. Specifically, MiM-UNet refines the traditional encoder–decoder framework by introducing Mamba-in-Mamba blocks, enabling precise multi-scale feature capture and efficient information fusion. Experimental results demonstrate that MiM-UNet outperforms state-of-the-art models in segmentation accuracy on the Massachusetts building dataset, while substantially reducing computational overhead, highlighting its superior performance and promising potential for practical applications.}
}
@article{SINGH2025103632,
title = {Unmasking Digital Deceptions: An Integrative Review of Deepfake Detection, Multimedia Forensics, and Cybersecurity Challenges},
journal = {MethodsX},
pages = {103632},
year = {2025},
issn = {2215-0161},
doi = {https://doi.org/10.1016/j.mex.2025.103632},
url = {https://www.sciencedirect.com/science/article/pii/S2215016125004765},
author = {Sonam Singh and Amol Dhumane},
keywords = {Deepfake detection, Generative Adversarial Networks (GANs), Synthetic media, Biometric spoofing, Cyber security threats, Multimedia forensics, AI policy frameworks, Explainable AI, Federated learning, Digital deception, Face synthesis, Speech cloning, Identity theft, Cross-dataset evaluation, Ethical AI},
abstract = {Deepfakes, which are driven by developments in generative AI, seriously jeopardize public trust, cybersecurity, and the veracity of information. This study offers a comprehensive analysis of the most recent methods for creating and detecting deepfakes in image, video, and audio modalities. With a focus on their advantages and disadvantages in cross-dataset and real-world scenarios, we compile the latest developments in transformer-based detection models, multimodal biometric defenses, and Generative Adversarial Networks (GANs). We provide implementation-level information such as pseudocode workflows, hyperparameter settings, and preprocessing pipelines for popular detection frameworks to improve reproducibility. We also examine the implications of cybersecurity, including identity theft and biometric spoofing, as well as policy-oriented solutions that incorporate federated learning, explainable AI, and ethical protections. By enriching technical insights with interdisciplinary perspectives, this review charts a roadmap for building robust, scalable, and trustworthy deepfake detection systems.}
}
@article{SUN2024102454,
title = {Similar modality completion-based multimodal sentiment analysis under uncertain missing modalities},
journal = {Information Fusion},
volume = {110},
pages = {102454},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102454},
url = {https://www.sciencedirect.com/science/article/pii/S156625352400232X},
author = {Yuhang Sun and Zhizhong Liu and Quan Z. Sheng and Dianhui Chu and Jian Yu and Hongxiang Sun},
keywords = {Multimodal sentiment analysis, Uncertain missing modalities, Similar modality completion, Transformer},
abstract = {Recently, uncertain missing modalities in multimodal sentiment analysis (MSA) brings a new challenge for sentiment analysis. However, existing research cannot accurately complete the missing modalities, and fail to explore the advantages of the text modality in MSA. For the above problems, this work develops a Similar Modality Completion based-MSA model under uncertain missing modalities (termed as SMCMSA). Firstly, we construct the full modalities samples database (FMSD) by screening out the full modality samples from the whole multimodal dataset, and then predicting and marking the sentiment labels of each modality of the samples with three pre-trained unimodal sentiment analysis model (PTUSA). Next, for completing the uncertain missing modalities, we propose a set of missing modalities completion strategies based on the similar modalities selected from FMSD. For the completed multimodal data, we first encode the text, video and audio modality using the encoder of transformer, then we fuse the representation of text into the representations of video and audio under the guidance of a pre-trained model, thereby improving the quality of video and audio. Finally, we conduct sentiment classification based on the representations of text, video and audio with the softmax function respectively, and get the final decision with the decision-level fusion method. Based on benchmark datasets CMU-MOSI and IEMOCAP, extensive experiments have been conducted to verify that our proposed model SMCMSA has better performance than that of the state-of-the-art baseline models.The codes of our model are available at https://github.com/Astro2Sun/SMCMSA.}
}
@article{MAHARJAN2025,
title = {Differential Analysis of Age, Gender, Race, Sentiment, and Emotion in Substance Use Discourse on Twitter During the COVID-19 Pandemic: A Natural Language Processing Approach},
journal = {JMIR Infodemiology},
volume = {5},
year = {2025},
issn = {2564-1891},
doi = {https://doi.org/10.2196/67333},
url = {https://www.sciencedirect.com/science/article/pii/S2564189125000477},
author = {Julina Maharjan and Ruoming Jin and Jennifer King and Jianfeng Zhu and Deric Kenne},
keywords = {substance use, social media, deep learning, natural language processing, NLP, COVID-19, age, gender, race, sentiment, emotion, artificial intelligence, AI},
abstract = {Background
User demographics are often hidden in social media data due to privacy concerns. However, demographic information on substance use (SU) can provide valuable insights, allowing public health policy makers to focus on specific cohorts and develop efficient prevention strategies, especially during global crises such as the COVID-19 pandemic.
Objective
This study aimed to analyze SU trends at the user level across different demographic dimensions, such as age, gender, race, and ethnicity, with a focus on the COVID-19 pandemic. The study also establishes a baseline for SU trends using social media data.
Methods
The study was conducted using large-scale English-language data from Twitter (now known as X) over a 3-year period (2019, 2020, and 2021), comprising 1.13 billion posts. Following preprocessing, the SU posts were identified using our custom-trained deep learning model (Robustly Optimized Bidirectional Encoder Representations From Transformers Pretraining Approach [RoBERTa]), which resulted in the identification of 9 million SU posts. Then, demographic attributes, such as user type, age, gender, race, and ethnicity, as well as sentiments and emotions associated with each post, were extracted via a collection of natural language processing modules. Finally, various qualitative analyses were performed to obtain insight into user behaviors based on demographics.
Results
The highest level of user participation in SU discussions was observed in 2020, with a 22.18% increase compared to 2019 and a 25.24% increase compared to 2021. Throughout the study period, male users and teenagers increasingly dominated the SU discussions across all substance types. During the COVID-19 pandemic, user participation in prescription medication discussions was notably higher among female users compared to other substance types. In addition, alcohol use increased by 80% within 2 weeks after the global pandemic declaration in 2020.
Conclusions
This study presents a large-scale, fine-grained analysis of SU on social media data, examining trends by age, gender, race, and ethnicity before, during, and after the COVID-19 pandemic. Our findings, contextualized with sociocultural and pandemic-specific factors, provide actionable insights for targeted public health interventions. This study establishes social media data (powered with artificial intelligence and natural language processing tools) as a valuable platform for real-time SU surveillance and prevention during crises.}
}
@article{ISLAM2024100069,
title = {Sentiment analysis of Bangla language using a new comprehensive dataset BangDSA and the novel feature metric skipBangla-BERT},
journal = {Natural Language Processing Journal},
volume = {7},
pages = {100069},
year = {2024},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2024.100069},
url = {https://www.sciencedirect.com/science/article/pii/S2949719124000177},
author = {Md. Shymon Islam and Kazi Masudul Alam},
keywords = {Sentiment analysis, Bangla dataset, Bangla-BERT, Skipgram, CNN, Bi-LSTM},
abstract = {In this modern technologically advanced world, Sentiment Analysis (SA) is a very important topic in every language due to its various trendy applications. But SA in Bangla language is still in a dearth level. This work focuses on examining different hybrid feature extraction techniques and learning algorithms on Bangla Document level Sentiment Analysis using a new comprehensive dataset (BangDSA) of 203,493 comments collected from various microblogging sites. The proposed BangDSA dataset approximately follows the Zipf’s law, covering 32.84% function words with a vocabulary growth rate of 0.053, tagged both on 15 and 3 categories. In this study, we have implemented 21 different hybrid feature extraction methods including Bag of Words (BOW), N-gram, TF-IDF, TF-IDF-ICF, Word2Vec, FastText, GloVe, Bangla-BERT etc with CBOW and Skipgram mechanisms. The proposed novel method (Bangla-BERT＋Skipgram), skipBangla-BERT outperforms all other feature extraction techniques in machine leaning (ML), ensemble learning (EL) and deep learning (DL) approaches. Among the built models from ML, EL and DL domains the hybrid method CNN-BiLSTM surpasses the others. The best acquired accuracy for the CNN-BiLSTM model is 90.24% in 15 categories and 95.71% in 3 categories. Friedman test has been performed on the obtained results to observe the statistical significance. For both real 15 and 3 categories, the results of the statistical test are significant.}
}
@article{XU2025100998,
title = {How do different ski resort attributes affect skiers' positive sentiments? Evidence from China},
journal = {Journal of Destination Marketing & Management},
volume = {36},
pages = {100998},
year = {2025},
issn = {2212-571X},
doi = {https://doi.org/10.1016/j.jdmm.2025.100998},
url = {https://www.sciencedirect.com/science/article/pii/S2212571X25000101},
author = {Haibin Xu and Yan Fang and Yiyi Jiang and Chengyi Jiang and Xujia Huang},
keywords = {Ski resort attribute, Positive sentiments, Random forest model, Partial dependence plot},
abstract = {Understanding how ski resort attributes specifically influence positive sentiments is crucial for improving the skiing tourism experience. In this study, a framework is developed to explore the relationship between visitors' positive sentiments and the intrinsic attributes of different types of ski resorts. First, the BERT model is used to carefully evaluate the subtle differences in positive sentiments expressed by skiers at various ski resorts. The ski resort attribute evaluation indicators are subsequently constructed based on multisource big data. Finally, a random forest model is used to analyze the complex relationships between skiers' positive sentiments and the diverse attributes of ski resorts. By defining partial dependence plots, this study elucidates the key factors that influence skiers' positive sentiments at different types of ski resorts. This research, which focuses on 251 outdoor ski resorts in China, reveals the varying contributions of different ski resort attributes to skiers’ positive sentiments. These findings are crucial for improving overall visitor experience, satisfaction, loyalty, and the likelihood of return visits, thereby promoting the sustainable development of snow and ice tourism.}
}
@article{XING2024124651,
title = {CoSTA: Co-training spatial–temporal attention for blind video quality assessment},
journal = {Expert Systems with Applications},
volume = {255},
pages = {124651},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124651},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424015185},
author = {Fengchuang Xing and Yuan-Gen Wang and Weixuan Tang and Guopu Zhu and Sam Kwong},
keywords = {Video quality assessment, In-the-wild videos, Transformer, Self-attention, Co-training},
abstract = {Self-attention-based Transformer has achieved great success in many computer vision tasks. However, its application to blind video quality assessment (VQA) is far from comprehensive. Evaluating the quality of in-the-wild videos is challenging due to the unknown of pristine reference and shooting distortion. This paper presents a Co-trained Space–Time Attention network for the blind VQA problem, termed CoSTA. Specifically, we first build CoSTA by alternately concatenating the divided space–time attention. Then, to facilitate the training of CoSTA, we design a vectorized regression loss by encoding the mean opinion score (MOS) to the probability vector and embedding a special token as the learnable variable of MOS, leading to the better fitting of the human rating process. Finally, to solve the data-hungry problem within Transformer, we propose to co-train the spatial and temporal attention weights using both images and videos. Various experiments are conducted on the de-facto in-the-wild video datasets, including LIVE-Qualcomm, LIVE-VQC, KoNViD-1k, YouTube-UGC, LSVQ, LSVQ-1080p, and DVL2021. Experimental results demonstrate the superiority of the proposed CoSTA over the state-of-the-art. The source code is publicly available at https://github.com/GZHU-DVL/CoSTA.}
}
@article{JIN2025106294,
title = {Understanding the internet-famous tourist city: Interaction within digital technology in an accelerated society},
journal = {Cities},
volume = {167},
pages = {106294},
year = {2025},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2025.106294},
url = {https://www.sciencedirect.com/science/article/pii/S0264275125005955},
author = {Honglei Jin and Pengfei Zhang and Liyu Yang and Enrique Santiago-Iglesias},
keywords = {Internet-famous tourist city, Accelerated society, Digital interaction, Capital logic},
abstract = {Under the influence of social media marketing, internet-famous tourist cities have become a hot topic in contemporary society. Tourists' attention to these cities is significantly shaped by social media interactions. By using Zibo, Harbin, and Tianshui as case studies, the study explored the effect of interaction between tourists and destinations within an accelerated societal context. The study collected a dataset of 702 videos and 67,568 comments from TikTok, Kwai, and Bilibili. Through interaction index, sentiment analysis, and BERTopic theme modeling, it measured audience interaction dynamics. The findings reveal that in the process of urban virality, social media-driven emotional resonance emerges as the core factor driving active interaction. However, cognitive conflicts arising from excessive commercial penetration have become prominent with capital expansion and market competition. Notably, audience sentiment scores exhibit a stepped decline across cities from Zibo to Harbin and Tianshui, a trend that profoundly exposes the development paradox of accelerated society. This study constructs a novel theoretical framework that integrates digital technology interaction with social acceleration, unveiling the developmental dynamics of internet-famous tourist city and guiding tourism development toward the goal of fostering a good life.}
}
@article{VERMA2025111195,
title = {Navigating sentiment analysis through fusion, learning, utterance, and attention Methods: An extensive four-fold perspective survey},
journal = {Engineering Applications of Artificial Intelligence},
volume = {156},
pages = {111195},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.111195},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625011960},
author = {Bhavana Verma and Priyanka Meel and Dinesh Kumar Vishwakarma},
keywords = {Sentiment analysis (SA), Fusion techniques, Multimodal sentiment analysis (MSA), Deep learning, Hybrid learning, Attention mechanism},
abstract = {The growing need to automate user sentiment evaluation for goods and services, especially given the rising popularity of video content for online opinion expression, highlights the importance of sentiment analysis in artificial intelligence. This survey introduces a novel taxonomy and analyzes sentiment analysis approaches from four perspectives: Fusion-based, Learning-based, Utterance-based, and Attention-based methods. The study is organized around fundamental elements, significance, and applications of sentiment analysis. The proposed literature provides details about fusion techniques, which may be categorized based on levels, modalities, and procedures, including rule-based and classification-based approaches. This study seeks to provide a concise overview of several learning approaches, accompanied by a brief discussion of a benchmark dataset specific to each model. This work offers a concise analysis of utterance acquisition, focusing on the classification of sentiments according to language specificity and bilingual or multilingual proficiency, as well as the factors of linguistic basis, granularity, and multimodality. Additionally, a benchmark set derived from publicly accessible datasets is presented, accompanied by comprehensive insights. Critical analysis of sentiment analysis limitations provides valuable insights into challenges and potential solutions. The survey concludes by highlighting prospects in the dynamic landscape of sentiment analysis.}
}
@article{RAVI2024100279,
title = {RICo: Reddit ideological communities},
journal = {Online Social Networks and Media},
volume = {42},
pages = {100279},
year = {2024},
issn = {2468-6964},
doi = {https://doi.org/10.1016/j.osnem.2024.100279},
url = {https://www.sciencedirect.com/science/article/pii/S2468696424000041},
author = {Kamalakkannan Ravi and Adan Ernesto Vela},
keywords = {Social networking (online), Learning (artificial intelligence), Predictive models, Transformers, Support vector machines, Text analysis, Context modeling, Natural language processing},
abstract = {The main objective of our research is to gain a comprehensive understanding of the relationship between language usage within different communities and delineating the ideological narratives. We focus specifically on utilizing Natural Language Processing techniques to identify underlying narratives in the coded or suggestive language employed by non-normative communities associated with targeted violence. Earlier studies addressed the detection of ideological affiliation through surveys, user studies, and a limited number based on the content of text articles, which still require label curation. Previous work addressed label curation by using ideological subreddits (r/Liberal and r/Conservative for Liberal and Conservative classes) to label the articles shared on those subreddits according to their prescribed ideologies, albeit with a limited dataset. Building upon previous work, we use subreddit ideologies to categorize shared articles. In addition to the conservative and liberal classes, we introduce a new category called “Restricted” which encompasses text articles shared in subreddits that are restricted, privatized, or banned, such as r/TheDonald. The “Restricted” class encompasses posts tied to violence, regardless of conservative or liberal affiliations. Additionally, we augment our dataset with text articles from self-identified subreddits like r/progressive and r/askaconservative for the liberal and conservative classes, respectively. This results in an expanded dataset of 377,144 text articles, consisting of 72,488 liberal, 79,573 conservative, and 225,083 restricted class articles. Our goal is to analyze language variances in different ideological communities, investigate keyword relevance in labeling article orientations, especially in unseen cases (922,522 text articles), and delve into radicalized communities, conducting thorough analysis and interpretation of the results.}
}
@article{LIU20251161,
title = {Research on Multimodal AIGC Video Detection for Identifying Fake Videos Generated by Large Models},
journal = {Computers, Materials and Continua},
volume = {85},
number = {1},
pages = {1161-1184},
year = {2025},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2025.062330},
url = {https://www.sciencedirect.com/science/article/pii/S1546221825007611},
author = {Yong Liu and Tianning Sun and Daofu Gong and Li Di and Xu Zhao},
keywords = {Multimodal information fusion, artificial intelligence generated content, authenticity detection, feature extraction, multi-layer perceptron, attention mechanism},
abstract = {To address the high-quality forged videos, traditional approaches typically have low recognition accuracy and tend to be easily misclassified. This paper tries to address the challenge of detecting high-quality deepfake videos by promoting the accuracy of Artificial Intelligence Generated Content (AIGC) video authenticity detection with a multimodal information fusion approach. First, a high-quality multimodal video dataset is collected and normalized, including resolution correction and frame rate unification. Next, feature extraction techniques are employed to draw out features from visual, audio, and text modalities. Subsequently, these features are fused into a multilayer perceptron and attention mechanisms-based multimodal feature matrix. Finally, the matrix is fed into a multimodal information fusion layer in order to construct and train a deep learning model. Experimental findings show that the multimodal fusion model achieves an accuracy of 93.8% for the detection of video authenticity, showing significant improvement against other unimodal models, as well as affirming better performance and resistance of the model to AIGC video authenticity detection.}
}
@article{WANG2026104284,
title = {DiffSBR: A diffusion model for session-based recommendation},
journal = {Information Processing & Management},
volume = {63},
number = {1},
pages = {104284},
year = {2026},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2025.104284},
url = {https://www.sciencedirect.com/science/article/pii/S0306457325002250},
author = {Zihe Wang and Bo Jin},
keywords = {Session-based recommendation, Diffusion model, Graph neural network, User preference modeling},
abstract = {Session-based recommendation (SBR) focuses on recommending items to anonymous users within short interaction sequences. Existing solutions focus on modeling item representations as fixed embedding vectors within the discriminative learning paradigm, which fail to accurately capture the diverse preferences that user exhibit during dynamic decision-making. We argue that users in the anonymous environment can fundamentally be regarded as a normative implicit group, exhibiting both homogeneous preference and heterogeneous preference when selecting items. To tackle this, we propose a Diffusion Model for Session-based Recommendation (DiffSBR). Specifically, we first model the aforementioned user diverse preferences from both local and global views. Next, we introduce a cluster-aware diffusion model, which directly represents heterogeneous preference clusters as distribution through forward and reverse processes, while indirectly influencing homogeneous preference via the attention mechanism in the final prediction stage, thereby improving the learning of item and session representations and enhancing the next-item recommendation. Experimental results show that DiffSBR outperforms the strong baseline, demonstrating that this sampling-allocation approach accurately reflects the uncertainty and variability in user preferences.}
}
@article{MA2025102333,
title = {Analyzing public response to wildfires: A socio-spatial study using SIR models and NLP techniques},
journal = {Computers, Environment and Urban Systems},
volume = {121},
pages = {102333},
year = {2025},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2025.102333},
url = {https://www.sciencedirect.com/science/article/pii/S0198971525000869},
author = {Zihui Ma and Guangxiao Hu and Ting-Syuan Lin and Lingyao Li and Songhua Hu and Loni Hagen and Gregory B. Baecher},
keywords = {Wildfire response, Wildland-urban interface (WUI) communities, Social inequity, Equitable strategy, Social media analysis},
abstract = {The increasing frequency and severity of wildfires pose significant risks to communities, infrastructure, and the environment, especially in Wildland-Urban Interface (WUI) areas. Effective disaster management requires understanding how the public perceives and responds to wildfire threats in near-real-time. This study uses social media data to assess public responses (including collective perceptions/reactions) and explores how these responses are linked to city-level community characteristics. Specifically, we leveraged a transformer-based topic modeling technique called BERTopic to identify wildfire response-related topics and then utilized the Susceptible-Infectious-Recovered (SIR) model to compute two key metrics — public response awareness and resilience indicators. Additionally, we used GIS-based spatial analysis to map wildfire responses and the relationships with four groups of city-level factors (racial/ethnic, socioeconomic, demographic, and wildfire-specific). Our findings reveal significant geographic and socio-spatial differences in public responses. Southern California cities with larger Hispanic populations demonstrate higher wildfire awareness and resilience. In contrast, urbanized regions in Central and Northern California exhibit lower awareness levels. Furthermore, resilience is negatively correlated with unemployment rates, particularly in southern regions where higher unemployment aligns with reduced resilience. These findings highlight the need for targeted and equitable wildfire management strategies to improve the adaptive capacity of WUI communities.}
}
@article{ASRI2026102594,
title = {Adaptive Personalized Recommendation Systems: A systematic Review},
journal = {Information Systems},
volume = {135},
pages = {102594},
year = {2026},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2025.102594},
url = {https://www.sciencedirect.com/science/article/pii/S030643792500078X},
author = {Bachir Asri and Sara Qassimi and Said Rakrak},
keywords = {Adaptive recommender systems, Personalized recommender systems, Recommender systems, Evolving preferences, Systematic review},
abstract = {Recommender systems assist users in navigating the vast selection of choices by offering personalized suggestions based on preferences. Originally used in e-commerce and streaming services, these systems are now applied in various sectors such as healthcare, education, and more, making them increasingly important. Despite their growth, recommender systems still face challenges, especially when addressing users whose preferences change over time. This paper presents a review of recent research on recommender systems that deliver personalized and adaptive recommendations for users with evolving preferences. Analyzing 97 studies published between 2020 and 2024, the review categorizes them across multiple dimensions to address key research questions. The findings reveal a diverse landscape of evaluation metrics, datasets, adaptation mechanisms, and application domains within adaptive personalized recommender systems (AdPRSs), with MovieLens as the most widely used dataset and the attention mechanism as the predominant adaptation approach. Furthermore, the review introduces a novel categorization of AdPRSs based on adaptation mechanism. By synthesizing current research, this review highlights key challenges faced in the field and identifies future directions for enhancing the efficiency and effectiveness of AdPRSs. These insights are of significant value to both practitioners and academic researchers, providing a foundation for advancing the development and optimization of AdPRSs.}
}
@article{HOU2024112220,
title = {TCHFN: Multimodal sentiment analysis based on Text-Centric Hierarchical Fusion Network},
journal = {Knowledge-Based Systems},
volume = {300},
pages = {112220},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112220},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124008542},
author = {Jingming Hou and Nazlia Omar and Sabrina Tiun and Saidah Saad and Qian He},
keywords = {Multimodal sentiment analysis, Text-centric, Fusion network, Transformer, Contrastive learning, Knowledge distillation},
abstract = {Multimodal sentiment analysis (MSA) has become a popular field of research in recent years. The aim is to combine the three modalities of text, video, and audio to obtain comprehensive emotional information. However, current research often treats these three modalities equally, downplaying the crucial role of text modality in MSA and ignoring the redundant information generated during multimodal fusion. To address these problems, we propose the Text-Centric Hierarchical Fusion Network (TCHFN), employing a hierarchical fusion strategy. In this framework, low-level fusion involves cross-modal interactions between pairs of modalities, while high-level fusion extends these interactions to involve all three modalities. Through the design of the Cross-modal Reinforced Transformer (CRT), we achieve cross-modal enhancement of the target modality, facilitating a nuanced fusion process with text serving as its core. Additionally, we design Text-Centric Contrastive Learning (TCCL) to align non-text modalities with the text modality, emphasising the central role of text in the fusion process. After fusion, a multimodal fusion output gate is employed to mitigate redundant information within the multimodal fusion representation, which is subsequently processed by a linear layer for prediction. Simultaneously, to fully leverage limited labelled datasets, we introduced knowledge distillation. This approach involves preserving the model parameters that yield the best performance during training as a teacher model. The teacher model aids in capturing rich emotional information, enabling the model to transcend local optima and discover more optimal parameters, thereby enhancing overall model performance. Extensive experiments on the CMU-MOSI, CMU-MOSEI, and CH-SIMS datasets demonstrate the superiority of our model over state-of-the-art methods in MSA tasks.}
}
@article{WANG2025130170,
title = {Multimodal understanding of human values in videos: A benchmark dataset and PLM-based method},
journal = {Neurocomputing},
volume = {638},
pages = {130170},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.130170},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225008422},
author = {Yuchen Wang and Zhulin Tao and He Chang and Nanxin Huang and Libiao Jin and Xiaofang Luo},
keywords = {Human values, Multimodal, Value understanding, Video understanding, PLM, Transformer},
abstract = {Multimodal content has become the mainstream communication medium in video sharing platforms such as TikTok and Twitter, containing rich values information. Understanding human values is of great significance to multimodal content analysis and can be applied to downstream tasks such as recommendation systems and value alignment. However, current studies on human values mainly focus on text and lack a multimodal perspective. In this work, we present a new multimodal human values video dataset called VVALUES, which contains 5,104 annotated videos along with their titles. The dataset is labeled with coarse-grained polarity tags of positive and neutral, and fine-grained tags including 13 classes of value vocabulary. Based on VVALUES, we further develop a pre-trained language model (PLM)-based multimodal method adopting a dual-transformer variant for value recognition, MMVR. Extensive experiments demonstrate that our method significantly improves the performance of understanding values in videos. To the best of our knowledge, we are the first to try to incorporate human values in video understanding, and VVALUES is the first multimodal video dataset for human values.}
}
@article{EDINGER2023,
title = {Misinformation and Public Health Messaging in the Early Stages of the Mpox Outbreak: Mapping the Twitter Narrative With Deep Learning},
journal = {Journal of Medical Internet Research},
volume = {25},
year = {2023},
issn = {1438-8871},
doi = {https://doi.org/10.2196/43841},
url = {https://www.sciencedirect.com/science/article/pii/S143888712300434X},
author = {Andy Edinger and Danny Valdez and Eric Walsh-Buhi and Jennifer S Trueblood and Lorenzo Lorenzo-Luaces and Lauren A Rutter and Johan Bollen},
keywords = {COVID-19, deep learning, misinformation, monkeypox, mpox, outbreak, public health, social media, Twitter},
abstract = {Background
Shortly after the worst of the COVID-19 pandemic, an outbreak of mpox introduced another critical public health emergency. Like the COVID-19 pandemic, the mpox outbreak was characterized by a rising prevalence of public health misinformation on social media, through which many US adults receive and engage with news. Digital misinformation continues to challenge the efforts of public health officials in providing accurate and timely information to the public. We examine the evolving topic distributions of social media narratives during the mpox outbreak to map the tension between rapidly diffusing misinformation and public health communication.
Objective
This study aims to observe topical themes occurring in a large-scale collection of tweets about mpox using deep learning.
Methods
We leveraged a data set comprised of all mpox-related tweets that were posted between May 7, 2022, and July 23, 2022. We then applied Sentence Bidirectional Encoder Representations From Transformers (S-BERT) to the content of each tweet to generate a representation of its content in high-dimensional vector space, where semantically similar tweets will be located closely together. We projected the set of tweet embeddings to a 2D map by applying principal component analysis and Uniform Manifold Approximation Projection (UMAP). Finally, we group these data points into 7 topical clusters using k-means clustering and analyze each cluster to determine its dominant topics. We analyze the prevalence of each cluster over time to evaluate longitudinal thematic changes.
Results
Our deep-learning pipeline revealed 7 distinct clusters of content: (1) cynicism, (2) exasperation, (3) COVID-19, (4) men who have sex with men, (5) case reports, (6) vaccination, and (7) World Health Organization (WHO). Clusters that largely communicated erroneous or irrelevant information began earlier and grew faster, reaching a wider audience than later communications by official instances and health officials.
Conclusions
Within a few weeks of the first reported mpox cases, an avalanche of mostly false, misleading, irrelevant, or damaging information started to circulate on social media. Official institutions, including the WHO, acted promptly, providing case reports and accurate information within weeks, but were overshadowed by rapidly spreading social media chatter. Our results point to the need for real-time monitoring of social media content to optimize responses to public health emergencies.}
}
@article{IMRAN2024426,
title = {Large-scale Probabilistic Forecasting of Consumer Engagement of CPG Products using Heterogeneous Web Data},
journal = {Procedia Computer Science},
volume = {237},
pages = {426-436},
year = {2024},
note = {International Conference on Industry Sciences and Computer Science Innovation},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.05.124},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924011402},
author = {Abdullah Al Imran and Kasun S Perera},
keywords = {Consumer Packaged Goods (CPG), FMCG, Food, FoodTech, USA, Probabilistic Forecasting Models, Large-scale Time Series Forecasting, Multiple Time-series, Global Models, Prediction Intervals, Long-term Forecasts, Trend Forecast, Deep Learning, DeepAR, Transformer Models for Time Series, LSTM, Autoregressive Models, Recurrent Network, Consumer Engagement, Customer Analytics, Social Media Analytics, Web Anaytics, Product Development, Product Growth, Supply Chain Management, Demand Forecast, Market Research},
abstract = {Consumer Packaged Goods (CPG) play a pivotal role in customer-centric industries. Understanding the distinctive features of such products and how customers engage with them is essential for CPG manufacturers to create customer-winning products. In this study, we explore an innovative data-centric approach to forecast customer engagement for CPG products by monitoring their digital evolution, particularly in the Snacks category in the USA. Traditional methods for consumer analysis such as surveys, focus groups are time consuming, costly and ineffective with limited scope. They also lack a forecasting component, making it difficult for CPG companies to make forward-looking decisions. However, with the emergence of big data, we can leverage user generated public data from social media, web search, e-commerce platforms to estimate consumer engagement and make long-term forecasts. To achieve this, we propose a systematic approach to accumulate and prepare large datasets from heterogeneous web sources for CPG products. We then use state-of-the-art deep learning based time series forecasting models to efficiently train and predict consumer engagement for the next 12 months, benchmarking their computational efficiency and forecasting performance. Our findings indicate that the DeepAR model outperforms all other models, with the lowest NRMSE (=0.378), RMSE (=14.848) MSE (=220.457), MASE (=0.871), and sMAPE (=0.306) values. Furthermore, we demonstrate methods for computing single forecasting points and prediction intervals using the forecasted sample distribution from the probabilistic models. The proposed approach will provide CPG businesses with valuable insights to make informed decisions about product development, marketing strategies, and supply chain management.}
}
@article{PU2025,
title = {The Impact of Social Media Information Dissemination on Consumer Brand Perception:},
journal = {Journal of Organizational and End User Computing},
volume = {37},
number = {1},
year = {2025},
issn = {1546-2234},
doi = {https://doi.org/10.4018/JOEUC.388646},
url = {https://www.sciencedirect.com/science/article/pii/S1546223425000450},
author = {Yang Pu and Rui Ma and Jiachen Li and Yuanyuan Wei and Xingchen Pan and Xiaotian Zhang},
keywords = {Brand Perception, Social Media Analytics, Multi-Dimensional Modeling, User Engagement, Platform Affordance},
abstract = {ABSTRACT
With the increasing influence of social media in shaping consumer attitudes, modeling brand perception has become a critical task for computational marketing. Traditional approaches have predominantly relied on textual sentiment analysis, which fails to capture the dynamic interplay of user interaction and platform-specific affordances. To address this limitation, the authors propose a novel Multi-Dimensional Perception Modeling (MPM) framework that jointly incorporates content semantics, social engagement signals, and platform-level features into an end-to-end neural architecture. MPM leverages BERT-based encoders for text, structured encodings for likes, comments, and influencer metrics, and visibility-aware metadata to represent platform context. These heterogeneous inputs are fused using attention mechanisms to predict both brand attitude categories and perception scores. Experiments on large-scale datasets from Weibo and Xiaohongshu demonstrate the effectiveness of the model.}
}
@article{CAO2025121941,
title = {Short video rumor detection based on causal graph},
journal = {Information Sciences},
volume = {703},
pages = {121941},
year = {2025},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2025.121941},
url = {https://www.sciencedirect.com/science/article/pii/S0020025525000738},
author = {Donglin Cao and Xiong Tang and Yanghao Lin and Dazhen Lin},
keywords = {Rumor detection, Causal graph, Multimodality, Social media short videos},
abstract = {In recent years, the short video industry has experienced rapid growth, leading to the emergence of numerous knowledge-based rumors. These rumors often disguise themselves as professional knowledge, making it difficult for fact-checkers to identify their falsehoods without external expertise. Furthermore, existing Chinese short video rumor datasets lack support from external knowledge. To solve that problem and apply to a real-world scenario, this paper constructs a Chinese short video rumor dataset from Douyin, which is the largest short video platform in China, and build a related rumor evidence base. To further characterize the knowledge association between short video entities which is important for the interpretation of knowledge distortion, this paper also constructs causal relationships between entities using causal discovery algorithms. Finally, to tackle and visualize the knowledge distortion in social media short videos, this paper proposes a Causal Short Video Rumor Pretrain Model (CSVRPM). This model obtains relevant causal subgraphs from the causal knowledge repository and integrates the causal relationships within these subgraphs using an attention mechanism in the short video rumor detection model. The experiment results show that the model outperforms some state-of-the-art approaches and greatly improves the interpretability of short video rumor detection results.}
}
@article{TA2025100827,
title = {Detecting signs of depression on social media: A machine learning analysis and evaluation},
journal = {Sustainable Futures},
volume = {10},
pages = {100827},
year = {2025},
issn = {2666-1888},
doi = {https://doi.org/10.1016/j.sftr.2025.100827},
url = {https://www.sciencedirect.com/science/article/pii/S2666188825003922},
author = {Phi Ta and Nha Tran and Hung Nguyen and Hien D. Nguyen},
keywords = {Depression detection, Mental illness, Social media, Natural language processing, Survey},
abstract = {Depression has become a growing concern due to its detrimental effects on both personal functioning and interpersonal relationships. In contemporary society, it is of utmost urgency to research and develop systems capable of detecting symptoms of depression on social media. Our study is not merely a survey, but a comprehensive investigation aimed at uncovering valuable insights and trends in the detection of depression on social media platforms. Our findings present a consolidated map of current methodologies, highlight key trends, and, through experimental results, provide clear performance benchmarks for both post-level and user-level techniques. By integrating insights from both the extensive literature review and practical experiments, this work clarifies existing challenges, establishes performance baselines, and proposes empirically grounded future directions to advance the development of more effective and reliable depression detection systems on social networks. This work opens a promising future for addressing the challenge of detecting depression on social media and contributes to enhancing the effectiveness of depression detection systems, ultimately aiding individuals affected by the adverse effects of depression.}
}
@article{UCHIYAMA2025,
title = {Characterizing Experiences With Hikikomori Syndrome on Twitter Among Japanese-Language Users: Qualitative Infodemiology Content Analysis},
journal = {JMIR Infodemiology},
volume = {5},
year = {2025},
issn = {2564-1891},
doi = {https://doi.org/10.2196/65610},
url = {https://www.sciencedirect.com/science/article/pii/S2564189125000106},
author = {Misa Ashley Uchiyama and Hirofumi Bekki and Tiana McMann and Zhuoran Li and Tim Mackey},
keywords = {hikikomori, social withdrawal, hikikomori syndrome, mental health, social isolation},
abstract = {Background
Hikikomori syndrome is a form of severe social withdrawal prevalent in Japan but is also a worldwide psychiatric issue. Twitter (subsequently rebranded X) offers valuable insights into personal experiences with mental health conditions, particularly among isolated individuals or hard-to-reach populations.
Objective
This study aimed to examine trends in firsthand and secondhand experiences reported on Twitter between 2021 and 2023 in the Japanese language.
Methods
Tweets were collected using the Twitter academic research application programming interface filtered for the following keywords: “#引きこもり,” “#ひきこもり,” “#hikikomori,” “#ニート,” “#脱ひきこもり,” “#不登校,” and “#自宅警備員.” The Bidirectional Encoder Representations From Transformers language model was used to analyze all Japanese-language posts collected. Themes and subthemes were then inductively coded for in-depth exploration of topic clusters relevant to first- and secondhand experiences with hikikomori syndrome.
Results
We collected 2,018,822 tweets, which were narrowed down to 379,265 (18.79%) tweets in Japanese from January 2021 to January 2023. After examining the topic clusters output by the Bidirectional Encoder Representations From Transformers model, 4 topics were determined to be relevant to the study aims. A total of 400 of the most highly interacted with tweets from these topic clusters were manually annotated for inclusion and exclusion, of which 148 (37%) tweets from 89 unique users were identified as relevant to hikikomori experiences. Of these 148 relevant tweets, 71 (48%) were identified as firsthand accounts, and 77 (52%) were identified as secondhand accounts. Within firsthand reports, the themes identified included seeking social support, personal anecdotes, debunking misconceptions, and emotional ranting. Within secondhand reports, themes included seeking social support, personal anecdotes, seeking and giving advice, and advocacy against the negative stigma of hikikomori.
Conclusions
This study provides new insights into experiences reported by web-based users regarding hikikomori syndrome specific to Japanese-speaking populations. Although not yet found in diagnostic manuals classifying mental disorders, the rise of web-based lifestyles as a consequence of the COVID-19 pandemic has increased the importance of discussions regarding hikikomori syndrome in web-based spaces. The results indicate that social media platforms may represent a web-based space for those experiencing hikikomori syndrome to engage in social interaction, advocacy against stigmatization, and participation in a community that can be maintained through a web-based barrier and minimized sense of social anxiety.}
}
@article{NADEEM2025127307,
title = {Protecting social networks against Dual-Vector attacks using Swarm OpenAI, Large Language Models, Swarm Intelligence, and Transformers},
journal = {Expert Systems with Applications},
volume = {278},
pages = {127307},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127307},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425009297},
author = {Muhammad Nadeem and Chen Hongsong},
keywords = {Traffic Analysis, User Behavior, Social Network, OpenAI, GPT, Swarm OpenAI, Transformer, LLM},
abstract = {Swarm Intelligence, OpenAI LLM, and Transformer are the three models that comprise the Swarm OpenAI Framework. Swarm OpenAI is an innovative framework used to generate substantial content in natural language processing. The implementation of Swarm OpenAI, in contrast to traditional NLP approaches, introduces an entirely new paradigm in social network security. This research marks the first application of the Swarm OpenAI model to protect social networks from both internal and external threats. It will not only provide a robust system for detecting and classifying social network attacks but will also leverage Swarm’s capabilities to pinpoint the attacker’s address using KNN. This innovative approach redefines the landscape of social network defense, offering a dynamic and comprehensive solution to modern security challenges. Swarm OpenAI has not yet been employed for social network security, and no framework has been proposed to give the same effectiveness and stability as swarm OpenAI. Researchers evaluated numerous papers to classify distinct threats and then deliberated on multiple strategies to address these attacks. No existing algorithm has yet been developed that can comprehensively address all forms of social network attacks using Swarm OpenAI. The innovation of this study lies in the rigorous statistical and experimental validation of all the proposed algorithms, ensuring that the Swarm OpenAI model, when implemented in a social network, not only mitigates attacks but also significantly fortifies security. In addition, the challenges related to the dependability, performance, and time and space complexity of social networks will also be thoroughly addressed. The proposed framework will be critically compared to existing surveys and experimental studies, with a focus on highlighting the distinct contributions and groundbreaking nature of this work relative to others. The proposed framework in this study is truly pioneering, as it directly tackles the critical need for continuous adaptation and dynamic protection. By doing so, it positions itself as a transformative, game-changing solution for social network security, setting a new benchmark in the field.}
}
@article{ZHANG2024112346,
title = {Reconstructing representations using diffusion models for multimodal sentiment analysis through reading comprehension},
journal = {Applied Soft Computing},
volume = {167},
pages = {112346},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.112346},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624011207},
author = {Hua Zhang and Yongjian Yan and Zijing Cai and Peiqian Zhan and Bi Chen and Bo Jiang and Bo Xie},
keywords = {Multimodal sentiment analysis, Machine reading comprehension, Diffusion model, Diffusion denoising autoencoder, Multimodal fusion representation},
abstract = {The primary challenge in multimodal sentiment analysis (MSA), which utilizes textual, audio, and visual information to analyze speakers' emotions, lies in constructing representation vectors that incorporate both unimodal semantic and multimodal interaction information. While existing research has extensively focused on multimodal fusion strategies, there remains insufficient exploration of the intrinsic potential within concurrently enhancing unimodal and multimodal representations. To address this gap, we introduce two additional steps to traditional three-step MSA: text modality enhancement through the machine reading comprehension (MRC) framework and multimodal representation reconstruction via proposing the diverse diffusion denoising autoencoder (D3AE) module. The MRC queries are integrated to locate sentiment-related prior knowledge, thereby deepening textual semantic understanding from a pretrained language model. Meanwhile, D3AE employs a single-step denoising strategy along with diffusion models across multiple time intervals, enabling efficient reconstruction and enhancement of multimodal representations. Extensive experiments conducted on two benchmark datasets, CMU-MOSI and CMU-MOSEI, validate that our model, MRC-D3AE, achieves state-of-the-art performance. The superiority of our model over existing baselines is primarily attributed to integrating MRC for enhancing text modality and D3AE for reconstructing multimodal representations.}
}
@article{ZHANG2025104076,
title = {A Local context enhanced Consistency-aware Mamba-based Sequential Recommendation model},
journal = {Information Processing & Management},
volume = {62},
number = {3},
pages = {104076},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2025.104076},
url = {https://www.sciencedirect.com/science/article/pii/S0306457325000184},
author = {Zhu Zhang and Bo Yang and Yimeng Lu},
keywords = {Sequential recommendation, Mamba, Consistency training, Graph neural network, Contrastive learning},
abstract = {Sequential recommendation (SR) focuses on capturing users’ interests from their historical behaviors. Transformer-based SR models have demonstrated promising performance by leveraging self-attention for sequential modeling. Recently, Mamba, a novel sequential model, has shown competitive performance compared to Transformers. In SR tasks, item representation learning involves both global and local context information. While several existing SR models attempt to address this integration, they suffer from inferior performance or computational inefficiency. Moreover, existing Mamba-based SR model appears to capture only the global context information. Given Mamba’s merits in enhancing model performance and efficiency, there is substantial potential to more effectively integrate both global and local context information within a Mamba-based framework. Additionally, consistency training, which is pivotal for enhancing model performance, remains underexplored in existing SR models. To tackle these challenges, we propose a Local Context Enhanced Consistency-aware Mamba-based Sequential Recommendation Model (LC-Mamba). LC-Mamba captures both global and local context information to improve recommendation performance. Specifically, LC-Mamba leverages a GNN-based sequence encoder to extract information from local neighbors for each item (local context information) in a graph view, while utilizing a Mamba-based sequence encoder to capture dependencies between items in the sequence (global context information) in a sequential view. Furthermore, we introduce consistency training, including model-level and representation-level consistency, to further enhance performance. Specifically, we incorporate R-Drop regularization into the Mamba-based sequence encoder to mitigate the inconsistency between training and inference caused by random dropout (model-level consistency). Additionally, we leverage contrastive learning to enhance consistency between the item representations learned from the sequential and graph views (representation-level consistency). Extensive experiments on three widely used datasets illustrate that LC-Mamba outperforms baseline models in HR and NDCG, achieving up to a 31.03% improvement in NDCG. LC-Mamba can be applied to real-world applications such as e-commerce and content platforms.}
}
@article{ZENG2023119240,
title = {Heterogeneous graph convolution based on In-domain Self-supervision for Multimodal Sentiment Analysis},
journal = {Expert Systems with Applications},
volume = {213},
pages = {119240},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.119240},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422022588},
author = {Yufei Zeng and Zhixin Li and Zhenjun Tang and Zhenbin Chen and Huifang Ma},
keywords = {Multimodal sentiment analysis, Self-supervised learning, In-domain pretrained, Heterogeneous graph convolution, Multi-task learning},
abstract = {The inability to fully exploit domain-specific knowledge and the lack of an effective integration method have been the difficulties and focus of multimodal sentiment analysis. In this paper, we propose heterogeneous graph convolution with in-domain self-supervised multi-task learning for multimodal sentiment analysis (HIS-MSA) to solve these problems. Firstly, HIS-MSA carries out the second pre-trained with different self-supervised training strategies to fully mine the unique knowledge of the in-domain corpus, and give BERT the awareness of professional field. Secondly, HIS-MSA uses heterogeneous graph, which is good at integrating heterogeneous knowledge, to fuse feature from multiple sources. Finally, a unimodal label generation module is used to jointly guide multimodal tasks and unimodal tasks to balance independent and complementary information between the modalities. We conducted experiments on the datasets MOSI and MOSEI, which have 2199 and 23454 video segments respectively. The results show an average improvement of approximately 1.5 points in all metrics compared to the current state-of-the-art model.}
}
@article{OKEY2023103476,
title = {Investigating ChatGPT and cybersecurity: A perspective on topic modeling and sentiment analysis},
journal = {Computers & Security},
volume = {135},
pages = {103476},
year = {2023},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2023.103476},
url = {https://www.sciencedirect.com/science/article/pii/S0167404823003863},
author = {Ogobuchi Daniel Okey and Ekikere Umoren Udo and Renata Lopes Rosa and Demostenes Zegarra Rodríguez and João Henrique Kleinschmidt},
keywords = {ChatGPT, Cybersecurity, Sentiment analysis, Generative pre-trained transformers, Artificial intelligence, Data security},
abstract = {In early 2023, the Artificial Intelligence (AI) industry experienced a significant advancement with the emergence of OpenAI's ChatGPT, a research product that demonstrated remarkable capabilities and garnered widespread attention. ChatGPT is an advanced chatbot powered by the Generative Pretrained Transformers (GPT) architecture, designed to generate human-like conversations encompassing a wide range of knowledge domains. Many AI researchers are currently engaging with the new technology to understand its functionality and limitations. Various expressions across a range of social media platforms, including Twitter, YouTube, Facebook, and numerous others, are currently under investigation. This research seeks to analyze the opinions of ChatGPT users as it regards cybersecurity. This research is important due to its contribution towards gaining enhanced understanding and devising intricate improvements for the chatbot. The Latent Dirichlet Allocation (LDA) algorithm is utilized to extract relevant topics from the texts. Additionally, to analyze user opinions and decipher the sentiments as either positive, negative, or neutral, we use the Natural language tool kit Valence Aware Dictionary for sEntiment Reasoning (NLTK's VADER) and Robustly Optimized BERT Pretraining Approach (roBERTa) libraries. The data used is obtained from Twitter via the SNScrape library, which aided in the retrieval of over 700,000 tweets via the search terms #chatgptsecurity, #chatgpthackers, #chatgptcybersecurity, and #chatgptcyberthreats. The analysis of the results by the VADER model shows 43.8% positive, 36.3% neutral, and 19.9% negative sentiments. Similarly, the roBERTa model shows 14.1% positive, 53.2% neutral, and 32.7% negative. These results show that there is an ongoing concern about ChatGPT and cybersecurity, especially in malware code generation, hacking, intelligence gathering, and phishing attacks.}
}
@article{ZHENG2024124894,
title = {Modeling multi-factor user preferences based on Transformer for next point of interest recommendation},
journal = {Expert Systems with Applications},
volume = {255},
pages = {124894},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124894},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424017615},
author = {Yongshang Zheng and Xu Zhou},
keywords = {Location-based social network, Next POI recommendation, Graph neural network, Transformer},
abstract = {With the rapid development of the mobile network and the gradual popularization of mobile devices, more and more users try to find attractive places to visit through WeChat, Twitter applications. In this trend, personalized next point of interest(POI) recommendation in the Location-based Social Network (LBSN) has become the focus of research and practice. Most existing studies capture user interest changes between different days (i.e. weekend and weekday), however, they ignore seasonal factors in time transition and category factors and thus fail to capture seasonal-level and category-level movement patterns in users’ mobile trajectories. Besides, they neglect the relevance between POIs from all users’ trajectory and fail to generate expressive POI embedding representation without constructing trajectory graph, which will reduce the accuracy of the next POI recommendation. To address the issues above, a next POI recommendation method for modeling Multi-factor User Preferences based on Transformer (MUPT) is developed, which consists of a global POI relationship modeling, a local multi-factor user preference modeling and a prediction module. It first learns the collaborative information of users with similar behavior to generate expressive POI embedding representation. Then it captures the personalized movement patterns of users at the POI, category and time levels based on Transformer mechanism in the local module. Especially the seasonal and other fine-grained information on the time series are learned in the time preference modeling part. The prediction module designed tracks the relationship between multi-level motion pattern representation of user check-in behavior and the next POI accessed by the user, and it finally obtains user’s preference probability for next POIs. An extensive experiment has been conducted on four datasets, and the experimental results analysis demonstrates that our proposed MUPT method is superior to other methods in terms of accuracy(ACC), mean reciprocal rank(MRR) and normalized discounted cumulative gain(NDCG).}
}
@article{WANG2025,
title = {Quality and Reliability of Adolescent Sexuality Education on Chinese Video Platforms: Sentiment-Topic Analysis and Cross-Sectional Study},
journal = {JMIR Formative Research},
volume = {9},
year = {2025},
issn = {2561-326X},
doi = {https://doi.org/10.2196/77100},
url = {https://www.sciencedirect.com/science/article/pii/S2561326X25006158},
author = {Lan Wang and Xiantao Shu and Jianmei Huang and Weiqian Yan and Duo Zhao},
keywords = {adolescent sexuality education, videos quality, Bilibili, TikTok, Kwai, Global Quality Score, GQS, modified DISCERN, mDISCERN, Patient Education Materials Assessment Tool-Audiovisual, PEMAT-A/V, information reliability, China, Spearman correlation analysis, stepwise regression analysis, topic modeling, sentiment analysis},
abstract = {Background
Adolescence is a critical period for lifelong health, which makes access to accurate and comprehensive sexuality education essential. As video platforms become a primary source of information for adolescents, the quality of their content significantly impacts their physical and mental health.
Objective
This study aimed to evaluate the quality, reliability, understandability, and actionability of adolescent sexuality education videos on major Chinese platforms (Bilibili, TikTok or Douyin, and Kwai), analyze associated user comment sentiment and topics, identify predictors of quality and reliability, and provide recommendations.
Methods
A cross-sectional analysis was conducted (April 2025) on the top 100 comprehensively ranked comprehensive sexuality education videos (N=300 total) retrieved from each platform using the keyword 青春期性教育 (“adolescent sexuality education”). Videos were assessed using the Global Quality Score, modified DISCERN, and Patient Education Materials Assessment Tool (PEMAT-U/A), with interrater reliability assessed via Cohen κ. A corpus of over 49,000 user comments underwent sentiment analysis (fine-tuned RoBERTa) and topic modeling (BERTopic, yielding 29 topics grouped into 6 themes). Statistical analyses included Kruskal-Wallis H tests, Spearman correlations, and stepwise linear regressions (SPSS [version 27.0]; P<.05).
Results
Video quality and reliability were moderate on Bilibili and TikTok but generally poor on Kwai. Content from verified sources (physicians, educators, and institutional media) demonstrated superior quality and stability compared to highly variable content from individual media (the predominant source type, especially on Kwai; 87/100, 87%). Paradoxically, Kwai exhibited the highest user engagement despite the lowest quality scores. Understandability (PEMAT-U) was consistently the strongest positive predictor for both quality (Global Quality Score, final model adjusted R2=0.383, β=0.485) and reliability (modified DISCERN, final model adjusted R2=0.209, β=0.319). Actionability (PEMAT-A) and video duration were also significant positive predictors. Understandability scores (PEMAT-U) were generally high (approximately 69%), while actionability scores (PEMAT-A) were moderate to low (33%-50%). Sentiment analysis revealed that comments were predominantly neutral (35,372/49,680, 71.2%), with negative comments (9141/49,680, 18.4%) significantly outweighing positive ones (5167/49,680, 10.4%). Key discussion themes identified included sources of knowledge acquisition, sexual safety and prevention, physiology, and sexual health and practices.
Conclusions
While online video platforms offer accessible channels for adolescent sexuality education in China, the current content is often of moderate-to-poor quality, with questionable reliability and limited actionability. Understandability is paramount, but high engagement does not necessarily correlate with high quality or reliability, potentially amplifying misinformation. To effectively empower youth, critical steps include enhancing content quality by adhering to evidence-based frameworks like the International Technical Guidance on Sexuality Education; strengthening platform accountability through improved verification and algorithms; and promoting user media literacy. These measures aim to foster a healthier and more equitable future for Chinese adolescents, helping to achieve goals related to sexually transmitted infections and pregnancy prevention and promoting more open societal attitudes toward sexuality.}
}
@article{DARAQEL2024652,
title = {The performance of artificial intelligence models in generating responses to general orthodontic questions: ChatGPT vs Google Bard},
journal = {American Journal of Orthodontics and Dentofacial Orthopedics},
volume = {165},
number = {6},
pages = {652-662},
year = {2024},
issn = {0889-5406},
doi = {https://doi.org/10.1016/j.ajodo.2024.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S0889540624000593},
author = {Baraa Daraqel and Khaled Wafaie and Hisham Mohammed and Li Cao and Samer Mheissen and Yang Liu and Leilei Zheng},
abstract = {Introduction
This study aimed to evaluate and compare the performance of 2 artificial intelligence (AI) models, Chat Generative Pretrained Transformer-3.5 (ChatGPT-3.5; OpenAI, San Francisco, Calif) and Google Bidirectional Encoder Representations from Transformers (Google Bard; Bard Experiment, Google, Mountain View, Calif), in terms of response accuracy, completeness, generation time, and response length when answering general orthodontic questions.
Methods
A team of orthodontic specialists developed a set of 100 questions in 10 orthodontic domains. One author submitted the questions to both ChatGPT and Google Bard. The AI-generated responses from both models were randomly assigned into 2 forms and sent to 5 blinded and independent assessors. The quality of AI-generated responses was evaluated using a newly developed tool for accuracy of information and completeness. In addition, response generation time and length were recorded.
Results
The accuracy and completeness of responses were high in both AI models. The median accuracy score was 9 (interquartile range [IQR]: 8-9) for ChatGPT and 8 (IQR: 8-9) for Google Bard (Median difference: 1; P <0.001). The median completeness score was similar in both models, with 8 (IQR: 8-9) for ChatGPT and 8 (IQR: 7-9) for Google Bard. The odds of accuracy and completeness were higher by 31% and 23% in ChatGPT than in Google Bard. Google Bard’s response generation time was significantly shorter than that of ChatGPT by 10.4 second/question. However, both models were similar in terms of response length generation.
Conclusions
Both ChatGPT and Google Bard generated responses were rated with a high level of accuracy and completeness to the posed general orthodontic questions. However, acquiring answers was generally faster using the Google Bard model.}
}
@article{DUNAGAN2025106080,
title = {Evaluating the timecourses of morpho-orthographic, lexical, and grammatical processing following rapid parallel visual presentation: An EEG investigation in English},
journal = {Cognition},
volume = {257},
pages = {106080},
year = {2025},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2025.106080},
url = {https://www.sciencedirect.com/science/article/pii/S0010027725000204},
author = {Donald Dunagan and Tyson Jordan and John T. Hale and Liina Pylkkänen and Dustin A. Chacón},
keywords = {Sentence processing, Syntax, Agreement, Reading, EEG, Psycholinguistics},
abstract = {Theories of language processing – and typical experimental methodologies – emphasize the word-by-word processing of sentences. This paradigm is good for approximating speech or careful text reading, but arguably, not for the common, cursory glances used while reading short sentences (e.g., cellphone notifications, social media posts). How can we interpret a sentence in a single glance? In an electroencephalography (EEG) study, brain responses to grammatical sentences (the dogs chase a ball) presented for 200 ms diverged from non-lexical consonant strings (thj rjxb zkhtb w lhct) ∼160 ms post-sentence onset and from scrambled constructions (a dogs chase ball the) ∼250 ms post-sentence onset, demonstrating – at different time points – rapid recognition and cursory analysis of linguistic stimuli. In the grammatical sentences, unigram probability correlated with EEG data ∼150–300 ms post-sentence onset, and probability of the word given its context estimated by BERT correlated with EEG data after ∼700–800 ms. EEG responses did not diverge between grammatical sentences and their counterparts with ungrammatical agreement (the dogs chases a ball), although EEG responses did diverge for plural vs. singular morphology at ∼200 ms. These results suggest that ‘at-a-glance’ reading is possible, based on coactivation of individual lexical items, morphological structures, and constituent structure at ∼200-300 ms, but that words are not integrated into a coherent syntactic/semantic analysis, as evidenced by the substantially later responses to BERT probability and the absence of sensitivity to agreement errors.}
}
@article{ZHANG2025126852,
title = {FireSeg: A weakly supervised fire segmentation framework via pre-trained latent diffusion models},
journal = {Expert Systems with Applications},
volume = {275},
pages = {126852},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.126852},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425004749},
author = {Wei Zhang and Hongtao Zheng and Weiran Li and Shuwen Pan and Yan Liu},
keywords = {Fire, Segmentation, Latent diffusion model, Textual information, Feature extraction},
abstract = {Accurate fire segmentation is crucial for emergency fire response. However, current deep learning-based fire segmentation methods heavily rely on large-scale annotated data, and the scarcity of high-quality public datasets limits their application in practical, diverse scenarios. To address this, we propose an innovative weakly-supervised fire segmentation framework, FireSeg, which consists of two core modules: 1) a feature extraction module based on a pre-trained latent diffusion model, generating cross-attention maps through diffusion inversion and textual prompts; and 2) a Transformer decoder-based Flame-Decoder module, which progressively decodes the attention maps and outputs segmentation masks by incorporating Dynamic Focus-aware Positional Queries (DFPQ) and a High-Resolution Cross-Attention (HRCA) mechanism. Experimental results demonstrate that FireSeg significantly outperforms other models on three fully supervised datasets covering different fire scenarios. For example, on the BowFire dataset, FireSeg outperforms the second-best model by 4.10%; on the Corsican dataset, it exceeds the second-best model by 3.83%; and on the FLAME dataset, it surpasses the second-best model by 4.24%. Additionally, FireSeg also shows a significant improvement over the second-best model on the weakly supervised FireDM dataset, which contains diverse scenes but lower-quality annotations. Notably, when only 20% of the weakly supervised data is used, FireSeg achieves a 9.23% increase in IoU. These results indicate that FireSeg not only provides excellent segmentation performance in fully supervised settings but also demonstrates strong adaptability and effectiveness in weakly supervised scenarios with limited labeled data.}
}
@article{ZHANG2025114470,
title = {Product return prediction in live streaming e-commerce with cross-modal contrastive transformer},
journal = {Decision Support Systems},
volume = {194},
pages = {114470},
year = {2025},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2025.114470},
url = {https://www.sciencedirect.com/science/article/pii/S0167923625000715},
author = {Wen Zhang and Rui Xie and Pei Quan and Zhenzhong Ma},
keywords = {Live streaming e-commerce, Product return prediction, Multimodal features fusion, Contrastive learning, Transformer},
abstract = {The live-streaming e-commerce industry is suffering heavy economic losses due to the high product return rate, which leads to rising logistics costs, greater inventory pressure, and unsatisfactory consumer experiences. Accurate product return prediction is highly desirable for the vendors to optimize their business operations in advance to reduce return-related costs. This paper proposes a novel approach, called Contraformer (Contrastive transformer), to predict product returns in live streaming e-commerce by leveraging fine-grained streamer behavior features extracted from three modalities (i.e., visual, acoustic, and language). The primary contribution lies in that we adopt Transformer with the encoder-decoder architecture with a novel class-supervised contrastive learning (CSCL) to fuse streamer behavior for multimodal representation alignment and inter-modal interaction characterization. By using a real-world dataset with 2584 product streamers and 864 items collected from Tiktok China live streaming platform, we demonstrate that the proposed Contrasformer approach outperforms the baseline methods in predicting product return rate with a 25 % reduction in terms of mean absolute error. This study offers great managerial implications for vendors to manage their practice in live streaming commerce.}
}
@article{YIN2025103935,
title = {Enhancing video rumor detection through multimodal deep feature fusion with time-sync comments},
journal = {Information Processing & Management},
volume = {62},
number = {1},
pages = {103935},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103935},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324002942},
author = {Ming Yin and Wei Chen and Dan Zhu and Jijiao Jiang},
keywords = {Rumor verification, Multimodal fusion, Video content analysis, Time-sync comment},
abstract = {Rumors in videos have a stronger propagation compared to traditional text or image rumors. Most current studies on video rumor detection often rely on combining user and video modal information while neglecting the internal multimodal aspects of the video and the relationship between user comments and local segment of the video. To address this problem, we propose a method called Time-Sync Comment Enhanced Multimodal Deep Feature Fusion Model (TSC-MDFFM). It introduces time-sync comments to enhance the propagation structure of videos on social networks, supplementing missing contextual or additional information in videos. Time-sync comments focus on expressing users' views on specific points in time in the video, which helps to obtain more valuable segments from videos with high density information. The time interval from one keyframe to the next in a video is defined as a local segment. We thoroughly described this segment using time-sync comments, video keyframes, and video subtitle texts. The local segment sequences are ordered based on the video timeline and assigned time information, then fused to create the local feature representation of the video. Subsequently, we fused the text features, video motion features, and visual features of video comments at the feature level to represent the global features of the video. This feature not only captures the overall propagation trend of video content, but also provides a deep understanding of the overall features of the video. Finally, we will integrate local and global features for video rumor classification, to combine the local and global information of the video. We created a dataset called TSC-VRD, which includes time-sync comments and encompasses all visible information in videos. Extensive experimental results have shown superior performance of our proposed model compared to existing methods on the TSC-VRD dataset.}
}
@article{ZHANG2024103793,
title = {How real-time interaction and sentiment influence online sales? Understanding the role of live streaming danmaku},
journal = {Journal of Retailing and Consumer Services},
volume = {78},
pages = {103793},
year = {2024},
issn = {0969-6989},
doi = {https://doi.org/10.1016/j.jretconser.2024.103793},
url = {https://www.sciencedirect.com/science/article/pii/S0969698924000894},
author = {Yihan Zhang and Kai Li and Chen Qian and Xiaotong Li and Qinjian Yuan},
keywords = {BERT model, Live streaming danmaku, Sentiment analysis, Social commerce, SOR model, User-user interaction},
abstract = {Live streaming danmaku can reflect both real-time interaction and user sentiment, two key characteristics of live streaming e-commerce. Using a sentiment analysis of live streaming danmaku comments, our study explores the mechanism through which user-user interaction influences live streaming sales. We collect real world data including live streaming danmaku, user stay time, sales from the Douyin live streaming e-commerce platform (the Chinese version of TikTok). Quantifying user-user interaction with BERT model, we then perform a sentiment analysis using the Baidu API. The results of our analysis demonstrate that entertainment-type and information-type user interaction have a significant positive effect on flow, including user stay time, number of danmaku, number of people sending danmaku. Our study also identifies that number of danmaku and number of people sending danmaku play a partial mediating role between user interactions and live streaming sales. Furthermore, we find that user sentiment plays a positive moderating role in the relationships between several flow variables and user purchase behavior. Theoretically, our study proposes a new method to empirically analyze user behavior in live streaming e-commerce. Practically, in light of these findings, we propose several optimization strategies to enhance the interaction functions of live streaming commerce, so as to enhance user experience and stimulate purchase intention.}
}
@article{HU2025114298,
title = {MAGE-fend: Multimodal adaptive fusion with guidance from LLM expertise for fake news detection on short video platforms},
journal = {Knowledge-Based Systems},
volume = {329},
pages = {114298},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.114298},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125013395},
author = {Lingtong Hu and Zituo Wang and Jiayi Zhu and Yifan Hu and Xianbing Wang},
keywords = {Fake news detection, Multimodal analysis, Cross-modal fusion, Large language models, Short-video platforms, Pandemic misinformation},
abstract = {The proliferation of fake news on short-video social media platforms presents significant challenges to public awareness and social stability. While prior research has largely concentrated on text-image fake news, fake news in video format remains underexplored due to limited dataset availability and the complexities of multimodal analytical techniques. To bridge these gaps, we introduce TikCron, a large-scale, open-source dataset of short videos collected from Douyin (TikTok China). TikCron provides news videos and rich social context, specifically curated for studying pandemic-related misinformation in the health and political domains. Furthermore, we propose MAGE-fend (Multimodal Adaptive Fusion Guided by LLM Expertise), a novel framework that utilizes Large Language Models (LLMs) to extract high-level semantic information from images and provide inferential reasoning to enhance fake news detection. MAGE-fend integrates an adaptive attention-based fusion mechanism to dynamically integrate multiple modalities, effectively capturing cross-modal consistency and complementary cues. Comprehensive experiments conducted on the TikCron dataset and the publicly available FakeSV dataset demonstrate that MAGE-fend outperforms state-of-the-art methods in various evaluation metrics. This detection framework makes a substantial contribution to addressing potential future pandemic misinformation crises.}
}
@article{LABADIETAMAYO2026104309,
title = {Distilling knowledge from large language models: A concept bottleneck model for hate and counter speech recognition},
journal = {Information Processing & Management},
volume = {63},
number = {2, Part A},
pages = {104309},
year = {2026},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2025.104309},
url = {https://www.sciencedirect.com/science/article/pii/S030645732500250X},
author = {Roberto Labadie-Tamayo and Djordje Slijepčević and Xihui Chen and Adrian Jaques Böck and Andreas Babic and Liz Freimann and Christiane Atzmüller and Matthias Zeppelzauer},
keywords = {Concept bottleneck model, Natural language processing, Large language model, Explainability, Explainable artificial intelligence, Interpretability, Online hate, Self-learned explanations},
abstract = {The rapid increase in hate speech on social media has exposed an unprecedented impact on society, making automated methods for detecting such content important. Unlike prior black-box models, we propose a novel transparent method for automated hate and counter speech recognition, i.e., “Speech Concept Bottleneck Model” (SCBM), using adjectives as human-interpretable bottleneck concepts. SCBM leverages large language models (LLMs) to map input texts to an abstract adjective-based representation, which is then sent to a light-weight classifier for downstream tasks. Across five benchmark datasets spanning multiple languages and platforms (e.g., Twitter, Reddit, YouTube), SCBM achieves an average macro-F1 score of 0.69 which outperforms the most recently reported results from the literature on four out of five datasets. Aside from high recognition accuracy, SCBM provides a high level of both local and global interpretability. Furthermore, fusing our adjective-based concept representation with transformer embeddings, leads to a 1.8% performance increase on average across all datasets, showing that the proposed representation captures complementary information. Our results demonstrate that adjective-based concept representations can serve as compact, interpretable, and effective encodings for hate and counter speech recognition. With adapted adjectives, our method can also be applied to other NLP tasks.}
}
@article{LEE2024110009,
title = {MODELING APPROACH},
journal = {Drug and Alcohol Dependence},
volume = {260},
pages = {110009},
year = {2024},
note = {Abstracts from the 2023 Annual Meeting of the College on Problems of Drug Dependence},
issn = {0376-8716},
doi = {https://doi.org/10.1016/j.drugalcdep.2023.110009},
url = {https://www.sciencedirect.com/science/article/pii/S0376871623002478},
author = {Juhan Lee and Ben Pretzer and Tanvi Anand and Dhiraj Murthy and Grace Kong}
}
@article{KWAO2026103470,
title = {MCDF: Multimodal information fusion and causal analysis for election misinformation detection},
journal = {Information Fusion},
volume = {125},
pages = {103470},
year = {2026},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2025.103470},
url = {https://www.sciencedirect.com/science/article/pii/S1566253525005433},
author = {Lazarus Kwao and Jing Ma and Sophyani Banaamwini Yussif and Matthew Quayson},
keywords = {Multimodal rumor detection, Causal analysis, DEMATEL, African election misinformation, Noise-Gating Mechanisms, Tensor Fusion Networks, Ghana elections},
abstract = {The rapid spread of election-related misinformation on social media poses a serious threat to public trust, democratic decision-making, and social stability. This form of misinformation is particularly persuasive and difficult to detect as it uses different types of content (modalities), including text, images, captions, and social interactions. These challenges undermine efforts to ensure trustworthy elections and enable timely intervention by policymakers and fact-checkers. However, existing detection approaches struggle with feature misalignment, cross-modal inconsistencies, and noisy social data, thereby limiting their ability to accurately classify misinformation and explain its propagation. To address these challenges, we propose MCDF, a Multimodal Causal Detection Framework, integrating fusion-driven misinformation detection with causal analysis. Our framework consists of three key components: (1) a multimodal rumor detection module, which employs Graph Convolutional Networks (GCNs) for social interaction modeling, Vision Transformers (ViTs) for visual feature extraction, and RoBERTa for text-caption encoding, dynamically aligned via Tensor Fusion Networks (TFNs); (2) a Noise-Gating Mechanism, which refines feature alignment by filtering misleading or redundant inputs, ensuring robust misinformation classification; and (3) DEMATEL, a causal inference module that quantifies misinformation drivers, bridging misinformation classification with explainability. We evaluate our model on Twitter (X), FakeNewsNet (GossipCO and PolitiFact), and a curated Ghana-specific election dataset, demonstrating state-of-the-art performance in both classification and causal inference. MCDF offers a practical and interpretable framework for combating misinformation in real-world political communication, providing actionable insights for electoral stakeholders, fact-checkers, and social media analysts.}
}
@article{PETERS2024108381,
title = {Social media use is predictable from app sequences: Using LSTM and transformer neural networks to model habitual behavior},
journal = {Computers in Human Behavior},
volume = {161},
pages = {108381},
year = {2024},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2024.108381},
url = {https://www.sciencedirect.com/science/article/pii/S0747563224002498},
author = {Heinrich Peters and Joseph B. Bayer and Sandra C. Matz and Yikun Chi and Sumer S. Vaid and Gabriella M. Harari},
keywords = {Social media, Habits, User modeling, LSTM, Transformer, Neural networks},
abstract = {The present paper introduces a novel approach to studying social media habits through predictive modeling of sequential smartphone user behaviors. While much of the literature on media and technology habits has relied on self-report questionnaires and simple behavioral frequency measures, we examine an important yet understudied aspect of media and technology habits: their embeddedness in repetitive behavioral sequences. Leveraging Long Short-Term Memory (LSTM) and transformer neural networks, we show that (i) social media use is predictable at the within and between-person level and that (ii) there are robust individual differences in the predictability of social media use. We examine the performance of several modeling approaches, including (i) global models trained on the pooled data from all participants, (ii) idiographic person-specific models, and (iii) global models fine-tuned on person-specific data. Neither person-specific modeling nor fine-tuning on person-specific data substantially outperformed the global models, indicating that the global models were able to represent a variety of idiosyncratic behavioral patterns. Additionally, our analyses reveal that individual differences in the predictability of social media use were not substantially related to differences in the frequency of smartphone use in general or the frequency of social media use, indicating that our approach captures an aspect of habits that is distinct from behavioral frequency. Implications for habit modeling and theoretical development are discussed.}
}
@article{DEINER2025,
title = {Use of Large Language Models to Classify Epidemiological Characteristics in Synthetic and Real-World Social Media Posts About Conjunctivitis Outbreaks: Infodemiology Study},
journal = {Journal of Medical Internet Research},
volume = {27},
year = {2025},
issn = {1438-8871},
doi = {https://doi.org/10.2196/65226},
url = {https://www.sciencedirect.com/science/article/pii/S1438887125008994},
author = {Michael S Deiner and Russell Y Deiner and Cherie Fathy and Natalie A Deiner and Vagelis Hristidis and Stephen D McLeod and Thomas J Bukowski and Thuy Doan and Gerami D Seitzman and Thomas M Lietman and Travis C Porco},
keywords = {conjunctivitis, microblog, social media, generative pretrained transformers, epidemic classification, forums, Twitter/X, YouTube, infectious eye disease, large language models, infodemiology, ocular, outbreak, epidemic},
abstract = {Background
The use of web-based search and social media can help identify epidemics, potentially earlier than clinical methods or even potentially identifying unreported outbreaks. Monitoring for eye-related epidemics, such as conjunctivitis outbreaks, can facilitate early public health intervention to reduce transmission and ocular comorbidities. However, monitoring social media content for conjunctivitis outbreaks is costly and laborious. Large language models (LLMs) could overcome these barriers by assessing the likelihood that real-world outbreaks are being described. However, public health actions for likely outbreaks could benefit more by knowing additional epidemiological characteristics, such as outbreak type, size, and severity.
Objective
We aimed to assess whether and how well LLMs can classify epidemiological features from social media posts beyond conjunctivitis outbreak probability, including outbreak type, size, severity, etiology, and community setting. We used a validation framework comparing LLM classifications to those of other LLMs and human experts.
Methods
We wrote code to generate synthetic conjunctivitis outbreak social media posts, embedded with specific preclassified epidemiological features to simulate various infectious eye disease outbreak and control scenarios. We used these posts to develop effective LLM prompts and test the capabilities of multiple LLMs. For top-performing LLMs, we gauged their practical utility in real-world epidemiological surveillance by comparing their assessments of Twitter/X, forum, and YouTube conjunctivitis posts. Finally, human raters also classified the posts, and we compared their classifications to those of a leading LLM for validation. Comparisons entailed correlation or sensitivity and specificity statistics.
Results
We assessed 7 LLMs for effectively classifying epidemiological data from 1152 synthetic posts, 370 Twitter/X posts, 290 forum posts, and 956 YouTube posts. Despite some discrepancies, the LLMs demonstrated a reliable capacity for nuanced epidemiological analysis across various data sources and compared to humans or between LLMs. Notably, GPT-4 and Mixtral 8x22b exhibited high performance, predicting conjunctivitis outbreak characteristics such as probability (GPT-4: correlation=0.73), size (Mixtral 8x22b: correlation=0.82), and type (infectious, allergic, or environmentally caused); however, there were notable exceptions. Assessing synthetic and real-world posts for etiological factors, infectious eye disease specialist validations revealed that GPT-4 had high specificity (0.83-1.00) but variable sensitivity (0.32-0.71). Interrater reliability analyses showed that LLM-expert agreement exceeded expert-expert agreement for severity assessment (intraclass correlation coefficient=0.69 vs 0.38), while agreement varied by condition type (κ=0.37-0.94).
Conclusions
This investigation into the potential of LLMs for public health infoveillance suggests effectiveness in classifying key epidemiological characteristics from social media content about conjunctivitis outbreaks. Future studies should further explore LLMs’ potential to support public health monitoring through the automated assessment and classification of potential infectious eye disease or other outbreaks. Their optimal role may be to act as a first line of documentation, alerting public health organizations for the follow-up of LLM-detected and -classified small, early outbreaks, with a focus on the most severe ones.}
}
@article{KHAN2024121288,
title = {Deep multi-scale pyramidal features network for supervised video summarization},
journal = {Expert Systems with Applications},
volume = {237},
pages = {121288},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121288},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423017906},
author = {Habib Khan and Tanveer Hussain and Samee {Ullah Khan} and Zulfiqar {Ahmad Khan} and Sung Wook Baik},
keywords = {Video summarization, Supervised learning, Keyframes, Feature fusion, Keyshots, Feature refinement},
abstract = {Video data are witnessing exponential growth, and extracting summarized information is challenging. It is always necessary to reduce the load of video traffic for the efficient video storage, transmission, and retrieval requirements. The aim of video summarization (VS) is to extract the most important contents from video repositories effectively. Recent attempts have used fewer representative features, which have been fed to recurrent networks to achieve VS. However, generating the desired summaries can become challenging due to the limited representativeness of extracted features and a lack of consideration for feature refinement. In this article, we introduce a vision transformer (ViT)-assisted deep pyramidal refinement network that can extract and refine multi-scale features and can predict an importance score for each frame. The proposed network comprises four main modules; initially, a dense prediction transformer with a ViT backbone is applied for the first time in this domain to acquire the optimal representations from the input frames. Then, feature maps are obtained from various layers separately and processed individually to support multi-scale progressive feature fusion and refinement before the data are passed to the ultimate prediction module. Next, a customized pyramidal refinement block is employed to refine the multi-level feature set before predicting the importance scores. Finally, video summaries are produced by selecting keyframes based on the predictions. To explore the performance of the proposed network, extensive experiments are conducted on the TVSum and SumMe datasets, and our network is found to achieve F1-scores of 62.4% and 51.9%, respectively, outperforming state-of-the-art alternatives by 0.9% and 0.5%.}
}
@article{LIU2025,
title = {Behavioral analysis of ChatGPT users based on the ABC model: Focusing on a socio-technical approach},
journal = {European Management Journal},
year = {2025},
issn = {0263-2373},
doi = {https://doi.org/10.1016/j.emj.2024.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S0263237324001841},
author = {Yang Liu and Huizhong Wang and Younggeun Park},
keywords = {ChatGPT, Socio-technical approach, Satisfaction, Emotional attachment, Stickiness},
abstract = {Chat Generative Pre-trained Transformer, generally known as ChatGPT, has attracted the interest of students, engineers, authors, and social media users alike. ChatGPT is a versatile tool that facilitates various tasks related to natural language processing and finds applicability in computer technology analysis, socio-economic impact assessment, and trend analysis across diverse industries, as well as the formulation of marketing strategies for business management. However, empirical studies exploring consumer perspectives on ChatGPT remain scarce. This study analyzes the effects of satisfaction and emotional attachment of ChatGPT users on stickiness based on a socio-technical approach. Using structural equation modeling to analyze 349 Korean ChatGPT users, it reveals that technical attributes exert a positive influence on user satisfaction, while social attributes influence emotional attachment positively. In addition, both user satisfaction and emotional attachment were found to contribute positively to user stickiness. User satisfaction and emotional attachment indeed served as crucial mediating factors between cognitive response and user stickiness.}
}
@article{LIU2025100535,
title = {A multimodal deep learning framework for constructing a market sentiment index from stock news},
journal = {Big Data Research},
volume = {41},
pages = {100535},
year = {2025},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2025.100535},
url = {https://www.sciencedirect.com/science/article/pii/S2214579625000309},
author = {Yunting Liu and Yirong Huang},
keywords = {Sentiment analysis, Stock market, Multimodal fusion, Computer vision, Natural language processing, Financial news sentiment classification, Investor Sentiment, Indicator Construction},
abstract = {Unimodal sentiment analysis often fails to capture the complexity of financial sentiment. This paper proposes a multimodal deep learning framework that integrates text, audio, and image data from CCTV news videos on TikTok to construct a multimodal sentiment indicator for the Chinese stock market. Empirical results show that multimodal fusion enhances sentiment analysis, with text outperforming audio and image modalities. The indicator correlates weakly with stock returns but significantly with market volatility, aligns with seasonal sentiment patterns, and reflects significant events like COVID-19. Additionally, weekly sentiment trends indicate the lowest sentiment on Thursdays and the highest on Fridays. This study advances financial sentiment analysis by demonstrating the efficacy of multimodal indicators in capturing market sentiment and informing volatility forecasts.}
}
@article{ALAMSYAH2024200394,
title = {Empowering Indonesian internet users: An approach to counter online toxicity and enhance digital well-being},
journal = {Intelligent Systems with Applications},
volume = {22},
pages = {200394},
year = {2024},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2024.200394},
url = {https://www.sciencedirect.com/science/article/pii/S2667305324000693},
author = {Andry Alamsyah and Yoga Sagama},
keywords = {Online toxicity, Content moderation, Indonesian language, IndoBERTweet, Indonesian RoBERTa},
abstract = {The proliferation of online toxicity, characterized by offensive and disrespectful language, has been a pervasive issue in Indonesia’s digital environment, impacting users’ mental health and well-being. Simultaneously, the potential of Natural Language Processing (NLP) in detecting and managing toxic comments provides a promising avenue for mitigating online toxicity. This study presents a 3-stages methodology consisting of type, target audience, and topics to detect and categorize online toxicity in the Indonesian language using fine-tuned IndoBERTweet and Indonesian RoBERTa models. The results indicate that the IndoBERTweet model, with optimally adjusted hyperparameters, consistently outperforms the Indonesian RoBERTa model in all stages of our proposed methodology. These outcomes are substantiated by higher precision, recall, and F1 score metrics exhibited by the IndoBERTweet model. This model also exhibits remarkable performance in real-world applicability, accurately classifying new Indonesian language content from Twitter (now X). This research establishes a stepping stone for future work, including exploring other language models, applying the methodology to other languages, training the models on larger and more diverse datasets, and applying it to other social media platforms or forums. Our proposal contributes to create safer online spaces, and the results provide insights for the development of automated moderation tools, playing a significant role in combating online harassment and ensuring online community well-being.}
}
@article{SUN2025108627,
title = {Who is to blame for AV crashes? Public perceptions of blame attribution using text mining based on social media},
journal = {Computers in Human Behavior},
volume = {168},
pages = {108627},
year = {2025},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2025.108627},
url = {https://www.sciencedirect.com/science/article/pii/S0747563225000743},
author = {Heyuan Sun and Yutong Chen and Ying Zhang},
keywords = {Blame attribution, Perception, Crash, Bidirectional encoder representations from transformers, Latent dirichlet allocation},
abstract = {Autonomous vehicles (AVs) are transitioning from the laboratory to the market phase. However, frequent AV crashes hindered the commercialization. Since the human driver and AV systems jointly control the vehicle, responsibility is difficult to attribute. Previous research primarily investigated the blame attribution in AV crashes from an ethical or legal perspective using traditional questionnaires based on vignette-based scenarios, but reached contradictory conclusions. Public perception, as a significant source of law, is often overlooked. The research extracted 90,426 valid comments on Chinese social media platforms (Sina Weibo and TikTok) about AV crashes from April 2021 to January 2025, breaking down the limitations of traditional attribution research with novel data sources. Bidirectional Encoder Representations from Transformers (BERT) model exceeds other deep learning models in classification accuracy of blame attribution comments. The Latent Dirichlet Allocation (LDA) results revealed an interesting topic Publicity, expanding the targets of traditional research. Regarding the topic AV system, the public appeared to have difficulty in classifying AV levels, with the highest blame volume and most negative sentiments. Importantly, we found certain connections among topics that chaotic publicity results in biased public perceptions of AV systems, thereby leading to over-trust in AV technology, which ultimately increases crash rates.}
}
@article{WANG2025114401,
title = {Panoramic sales insight: Using multimodal fusion to improve the effectiveness of flash sales},
journal = {Decision Support Systems},
volume = {190},
pages = {114401},
year = {2025},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2025.114401},
url = {https://www.sciencedirect.com/science/article/pii/S0167923625000028},
author = {Haoran Wang and Zhen-Song Chen and Mingjie Fang and Yilong Wang and Feng Liu},
keywords = {Flash sales, Revenue forecasting, Deep learning, Multimodal fusion, Panoramic sales insight},
abstract = {Flash sales are a widely adopted e-commerce marketing strategy that operate over a brief period, offering limited-time discounts, special promotions, or clearance items to create a sense of urgency and promote rapid sales. This study proposes panoramic sales insight (PSI), a multimodal revenue forecasting framework designed to improve the accuracy of revenue predictions for flash sales. Using historical flash sales data from the fast fashion retailer Shein, the proposed PSI framework integrates both structured and unstructured data, utilizing a text–image fusion module to fuse features from product images and text descriptions and a deep neural network to forecast revenue. The text features are extracted using bidirectional encoder representations from transformers (BERT), the product image features are extracted using a vision transformer (ViT), and review keyword extraction is conducted using Fumeus. Multimodal fusion then integrates these features to deliver accurate revenue forecasting. Controlled experiments evaluate the performance of each module within the PSI framework, while ablation analysis confirms the robustness of PSI. This study provides valuable insights for managers, enabling more accurate revenue forecasting and improving the effectiveness of flash sales.}
}
@article{MAHMUD2023103454,
title = {Cyberbullying detection for low-resource languages and dialects: Review of the state of the art},
journal = {Information Processing & Management},
volume = {60},
number = {5},
pages = {103454},
year = {2023},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2023.103454},
url = {https://www.sciencedirect.com/science/article/pii/S0306457323001917},
author = {Tanjim Mahmud and Michal Ptaszynski and Juuso Eronen and Fumito Masui},
keywords = {Automatic cyberbullying detection, Low-resource language, Machine learning, Social media},
abstract = {The struggle of social media platforms to moderate content in a timely manner, encourages users to abuse such platforms to spread vulgar or abusive language, which, when performed repeatedly becomes cyberbullying — a social problem taking place in virtual environments, yet with real-world consequences, such as depression, withdrawal, or even suicide attempts of its victims. Systems for the automatic detection and mitigation of cyberbullying have been developed but, unfortunately, the vast majority of them are for the English language, with only a handful available for low-resource languages. To estimate the present state of research and recognize the needs for further development, in this paper we present a comprehensive systematic survey of studies done so far for automatic cyberbullying detection in low-resource languages. We analyzed all studies on this topic that were available.We investigated more than seventy published studies on automatic detection of cyberbullying or related language in low-resource languages and dialects that were published between around 2017 and January 2023. There are 23 low-resource languages and dialects covered by this paper, including Bangla, Hindi, Dravidian languages and others. In the survey, we identify some of the research gaps of previous studies, which include the lack of reliable definitions of cyberbullying and its relevant subcategories, biases in the acquisition, and annotation of data. Based on recognizing those research gaps, we provide some suggestions for improving the general research conduct in cyberbullying detection, with a primary focus on low-resource languages. Based on those proposed suggestions, we collect and release a cyberbullying dataset in the Chittagonian dialect of Bangla and propose a number of initial ML solutions trained on that dataset. In addition, pre-trained transformer-based the BanglaBERT model was also attempted. We conclude with additional discussions on ethical issues regarding such studies, highlight how our survey improves on similar surveys done in the past, and discuss the usefulness of recently popular AI-enhanced tools for streamlining such scientific surveys.}
}
@article{CHEN2025104391,
title = {TEMSA:Text enhanced modal representation learning for multimodal sentiment analysis},
journal = {Computer Vision and Image Understanding},
volume = {258},
pages = {104391},
year = {2025},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2025.104391},
url = {https://www.sciencedirect.com/science/article/pii/S1077314225001146},
author = {Jingwen Chen and Shuxiang Song and Yumei Tan and Haiying Xia},
keywords = {Representation learning, Reconstruction modules, Multimodal sentiment analysis, Text-guided pairwise cross-modal mapping modules},
abstract = {Multimodal sentiment analysis aims to identify human emotions by leveraging multimodal information, including language, visual, and audio data. Most existing models focus on extracting common features across modalities or simply integrating heterogeneous multimodal data. However, such approaches often overlook the unique representation advantages of individual modalities, as they treat all modalities equally and use bidirectional information transfer mechanisms. This can lead to information redundancy and feature conflicts. To address this challenge, we propose a Text-Enhanced Modal Representation Learning Model (TEMSA), which builds robust and unified multimodal representations through the design of text-guided pairwise cross-modal mapping modules. Specifically, TEMSA employs a text-guided multi-head cross-attention mechanism to embed linguistic information into the emotion-related representation learning of non-linguistic modalities, thereby enhancing the representations of visual and audio modalities. In addition to preserving consistent information through cross-modal mapping, TEMSA also incorporates text-guided reconstruction modules, which leverage text-enhanced non-linguistic modal features to decouple modality-specific representations from non-linguistic modalities. This dual representation learning framework captures inter-modal consistent information through cross-modal mapping, and extracts modal difference information through intra-modal decoupling, thus improving the understanding of cross-modal affective associations. The experimental results on the CMU-MOSI, CMU-MOSEI, and CH-SIMS datasets demonstrate that TEMSA achieves superior performance, highlighting the critical role of text-guided cross-modal and intra-modal representation learning in multimodal sentiment analysis.}
}
@article{JIANG2024104162,
title = {MCT-VHD: Multi-modal contrastive transformer for video highlight detection},
journal = {Journal of Visual Communication and Image Representation},
volume = {101},
pages = {104162},
year = {2024},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2024.104162},
url = {https://www.sciencedirect.com/science/article/pii/S1047320324001172},
author = {Yinhui Jiang and Sihui Luo and Lijun Guo and Rong Zhang},
keywords = {Multi-modal, Video highlight detection, Transformer, Contrastive learning},
abstract = {Autonomous highlight detection aims to identify the most captivating moments in a video, which is crucial for enhancing the efficiency of video editing and browsing on social media platforms. However, current efforts primarily focus on visual elements and often overlook other modalities, such as text information that could provide valuable semantic signals. To overcome this limitation, we propose a Multi-modal Contrastive Transformer for Video Highlight Detection (MCT-VHD). This transformer-based network mainly utilizes video and audio modalities, along with auxiliary text features (if exist) for video highlight detection. Specifically, We enhance the temporal connections within the video by integrating a convolution-based local enhancement module into the transformer blocks. Furthermore, we explore three multi-modal fusion strategies to improve highlight inference performance and employ a contrastive objective to facilitate interactions between different modalities. Comprehensive experiments conducted on three benchmark datasets validate the effectiveness of MCT-VHD, and our ablation studies provide valuable insights into its essential components.}
}
@article{MURTHY2024100160,
title = {Categorizing E-cigarette-related tweets using BERT topic modeling},
journal = {Emerging Trends in Drugs, Addictions, and Health},
volume = {4},
pages = {100160},
year = {2024},
issn = {2667-1182},
doi = {https://doi.org/10.1016/j.etdah.2024.100160},
url = {https://www.sciencedirect.com/science/article/pii/S2667118224000199},
author = {D. Murthy and S. Keshari and S. Arora and Q. Yang and A. Loukas and S.J. Schwartz and M.B. Harrell and E.T. Hébert and A.V. Wilkinson},
keywords = {Bertopic, Topic modeling, Natural language processing, Vape, E-cigarettes},
abstract = {Background
Social media platforms are critical channels for promoting e-cigarettes, particularly among youth, making analysis of their vast and diverse content essential for public health interventions. Prevalence rates of e-cigarette use are high and evidence suggests that social media are popular forums that promote e-cigarette use through direct and indirect marketing techniques. The volume and diverse nature of e-cigarette-related information on social media is challenging and may obfuscate public health prevention messaging. Traditional hand-coding methods are labor-intensive and limit scalability. In contrast, unsupervised machine learning approaches, such as topic modeling, allow for efficient analysis of large datasets, uncovering patterns and trends that manual methods cannot achieve at scale. The present study focused on ascertaining the extent to which themes and topics in tweets related to e-cigarettes can be successfully rendered into useful homogenous units using machine learning. A better understanding of current depictions and discussions around e-cigarette products and use on social media can inform public health counter messaging and policy interventions.
Methods
We used topic modeling (BERTopic) to iteratively derive vape-related tweet clusters and calculate the importance of particular words to these groupings. We conducted a qualitative content analysis to study clustered tweets. We also sought to determine the geographic locations of e-cigarette conversations using automated geoparsing methods, which translate toponyms in textual data into geographic identifiers, to attempt to infer the location of tweets.
Results
We were able to successfully identify >100,000 tweets in broad thematic categories in English and Spanish. Our correlation and inter-topic map analysis of the machine-derived topics, which examines the relationships between topics, indicated that most of the topics were unique (correlation value < 0.5) and did not overlap with each other. We identified six topics: Flavors and Disposable Vapes, Cannabis, Vape Shops and Refillable Vapes, Vape Culture, Anti-vaping and Quitting, and Spanish Tweets and Vaping Nicotine. Further analysis of these topics using qualitative methods identified themes within each topic. For example, Category 6 (Spanish Tweets and Vaping Nicotine) included four topics focused on the health risks of vaping, personal motivations for vaping, and the regulation of vaping products. Using geoparsing, which automatically detects location information, we found that the United States had the highest number of tweets related to vaping.
Discussion/conclusion
Results underscore the possibility of leveraging BERTopic modeling to reduce large quantities of data to comprehensively describe and categorize myriad e-cigarette related messages to which social media users are exposed. This data reduction approach can be applied to various social media platforms to describe and categorize e-cigarette posts and thereby triangulate and validate findings. Thematic content analysis of the topics identified through this technique requires supervision and human inputs. Our approach provides a comprehensive understanding of the evolving e-cigarette discourse, informing public health counter-messaging and policy interventions. Moreover, findings support the need for regulation, such as reducing appealing flavors and suggest that social media can be used effectively to support public health messaging (i.e., quitting messages).}
}
@article{WEI2024103776,
title = {FUMMER: A fine-grained self-supervised momentum distillation framework for multimodal recommendation},
journal = {Information Processing & Management},
volume = {61},
number = {5},
pages = {103776},
year = {2024},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103776},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324001365},
author = {Yibiao Wei and Yang Xu and Lei Zhu and Jingwei Ma and Jiangping Huang},
keywords = {Multimodal recommendation, Self-supervised learning, Contrastive learning, Momentum model, Fine-grained representation},
abstract = {The considerable semantic information contained in multimodal data is increasingly appreciated by industry and academia. To effectively leverage multimodal information, existing multimodal recommendation methods mainly build multimodal auxiliary graphs to improve the representation of users and items. However, the weak value density of multimodal data inevitably leads to serious noise issues, making it difficult to effectively exploit valuable information from the multimodal contents. To address this issue, we propose a novel Fine-grained Self-supervised Mom entum Distillation Framework (FUMMER) for multimodal recommendations. Specifically, we propose a Transformer-based Fine-grained Feature Extractor (TFFE) and a Momentum Distillation (MoD) structure that incorporates intra- and inter-modal contrastive learning to fully pre-train TFFE for fine-grained feature extraction. Additionally, we design a structure-aware fine-grained contrastive learning module to fully exploit the self-supervised signals from fine-grained structural features. Extensive experiments on three real-world datasets show that our method outperforms state-of-the-art multimodal recommendation methods. Further experiments verify that the fine-grained feature extraction method we propose can serve as a pre-trained model, enhancing the performance of recommendation methods effectively by learning the fine-grained feature representations of items. The code is publicly available at https://github.com/BIAOBIAO12138/FUMMER.}
}
@article{CHANDRA2025112743,
title = {Large language models for newspaper sentiment analysis during COVID-19: The Guardian},
journal = {Applied Soft Computing},
volume = {171},
pages = {112743},
year = {2025},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2025.112743},
url = {https://www.sciencedirect.com/science/article/pii/S1568494625000547},
author = {Rohitash Chandra and Baicheng Zhu and Qingying Fang and Eka Shinjikashvili},
keywords = {COVID-19, Sentiment analysis, BERT, RoBERTa, Social media, The Guardian},
abstract = {During the COVID-19 pandemic, news media coverage encompassed topics such as viral transmission, allocation of medical resources, and government response measures. Studies have analysed sentiment on social media platforms during COVID-19 to understand the public’s response to the rising cases and the government strategies implemented to control the spread of the virus. Sentiment analysis can enable an understanding of the dynamics in the social and psychological well-being of a group during the pandemic. Apart from social media, newspapers have played a vital role in disseminating information, including information from the government, experts, and the public about various topics. A study of sentiment analysis of newspaper sources during COVID-19 for selected countries can give an overview of how the media covered the pandemic. In this study, we selected The Guardian newspaper and conducted a sentiment analysis during various stages of COVID-19 including initial transmission, lockdowns and vaccination. We employed novel large language models (LLMs) and refined them with expert-labelled sentiment analysis data. We also provide an analysis of sentiments experienced pre-pandemic for comparison. The results indicate that during the early pandemic stages, public sentiment prioritised urgent crisis response, later shifting focus to addressing the impact on health and the economy. In comparison with related studies about social media sentiment analyses, we found a discrepancy between The Guardian, with a dominance of negative sentiments (sad, annoyed, anxious and denial), suggesting that social media offered a more diverse emotional expression during the pandemic. We found a grim narrative in The Guardian with overall dominance of negative sentiments, pre- and during COVID-19 across news sections including Australia, UK, World News, and Opinion.}
}
@article{LI2025,
title = {Application and Strategy of AIGC in Omnimedia Knowledge Dissemination Under the Background of Digital Empowerment},
journal = {International Journal of Knowledge Management},
volume = {21},
number = {1},
year = {2025},
issn = {1548-0666},
doi = {https://doi.org/10.4018/IJKM.385129},
url = {https://www.sciencedirect.com/science/article/pii/S1548066625000517},
author = {Xuelin Li},
keywords = {Digital Empowerment, Artificial Intelligence Generated Content, Knowledge Dissemination, Deep Learning, Natural Language Processing},
abstract = {ABSTRACT
In the context of digital empowerment, this paper explores the application and strategic approach of Artificial Intelligence Generated Content (AIGC) in enhancing omnimedia knowledge dissemination. By integrating deep learning and natural language processing (NLP) technologies, this study proposes an AIGC-driven omnimedia knowledge dissemination model designed to address issues such as low dissemination efficiency, insufficient personalization, and limited reach associated with traditional knowledge dissemination methods. Neural networks are employed for data processing and feature extraction, followed by the use of NLP technology to analyze the data and generate content tailored to user needs. In the omnimedia landscape, the paper presents a cross-platform mechanism for collaborative dissemination, leveraging AIGC to enable real-time updates and user feedback.}
}
@article{HE2026103632,
title = {LMVD: A large-scale multimodal vlog dataset for depression detection in the wild},
journal = {Information Fusion},
volume = {126},
pages = {103632},
year = {2026},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2025.103632},
url = {https://www.sciencedirect.com/science/article/pii/S1566253525007043},
author = {Lang He and Kai Chen and Junnan Zhao and Yimeng Wang and Ercheng Pei and Haifeng Chen and Jiewei Jiang and Shiqing Zhang and Jie Zhang and Zhongmin Wang and Tao He and Prayag Tiwari},
keywords = {Depression detection, Transformer, Vlog, Multimodal, Deep learning},
abstract = {Depression profoundly impacts multiple dimensions of an individual’s life, including personal and social functioning, academic achievement, occupational productivity, and overall quality of life. With recent advancements in affective computing, deep learning technologies have been increasingly adopted to identify patterns indicative of depression. However, due to concerns over participant privacy, data in this domain remain scarce, posing significant challenges for the development of robust discriminative models for depression detection. To address this limitation, we build a Large-scale Multimodal Vlog Dataset (LMVD) for depression recognition in real-world settings. The LMVD dataset comprises 1,823 video samples, totaling approximately 214 h of content, collected from 1,475 participants across four major multimedia platforms: Sina Weibo, Bilibili, TikTok, and YouTube. In addition, we introduce a novel architecture, MDDformer, specifically designed to capture non-verbal behavioral cues associated with depressive states. Extensive experimental evaluations conducted on LMVD demonstrate the superior performance of MDDformer in depression detection tasks. We anticipate that LMVD will become a valuable benchmark resource for the research community, facilitating progress in multimodal, real-world depression recognition. The dataset and source code will be made publicly available at: https://github.com/helang818/LMVD.}
}
@article{LI2024108013,
title = {ChatGPT in healthcare: A taxonomy and systematic review},
journal = {Computer Methods and Programs in Biomedicine},
volume = {245},
pages = {108013},
year = {2024},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2024.108013},
url = {https://www.sciencedirect.com/science/article/pii/S0169260724000087},
author = {Jianning Li and Amin Dada and Behrus Puladi and Jens Kleesiek and Jan Egger},
keywords = {ChatGPT, Healthcare, NLP, Transformer, LLM, OpenAI, Taxonomy, Bard, BERT, LLaMA},
abstract = {The recent release of ChatGPT, a chat bot research project/product of natural language processing (NLP) by OpenAI, stirs up a sensation among both the general public and medical professionals, amassing a phenomenally large user base in a short time. This is a typical example of the ‘productization’ of cutting-edge technologies, which allows the general public without a technical background to gain firsthand experience in artificial intelligence (AI), similar to the AI hype created by AlphaGo (DeepMind Technologies, UK) and self-driving cars (Google, Tesla, etc.). However, it is crucial, especially for healthcare researchers, to remain prudent amidst the hype. This work provides a systematic review of existing publications on the use of ChatGPT in healthcare, elucidating the ‘status quo’ of ChatGPT in medical applications, for general readers, healthcare professionals as well as NLP scientists. The large biomedical literature database PubMed is used to retrieve published works on this topic using the keyword ‘ChatGPT’. An inclusion criterion and a taxonomy are further proposed to filter the search results and categorize the selected publications, respectively. It is found through the review that the current release of ChatGPT has achieved only moderate or ‘passing’ performance in a variety of tests, and is unreliable for actual clinical deployment, since it is not intended for clinical applications by design. We conclude that specialized NLP models trained on (bio)medical datasets still represent the right direction to pursue for critical clinical applications.}
}
@article{BOONYARAT2024103706,
title = {Leveraging enhanced BERT models for detecting suicidal ideation in Thai social media content amidst COVID-19},
journal = {Information Processing & Management},
volume = {61},
number = {4},
pages = {103706},
year = {2024},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103706},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324000669},
author = {Panchanit Boonyarat and Di Jie Liew and Yung-Chun Chang},
keywords = {Natural language processing, Suicidal detection, Emotion analysis, COVID-19, Social media analytics, Thai Language},
abstract = {During the COVID-19 pandemic, people experienced major lifestyle changes including enforced isolation which resulted in an increase in suicidal ideation. In the face of isolation, individuals sought avenues to express themselves, and social media platforms have emerged as a primary choice. It is crucial to detect and analyze expressions of suicidal ideation and emotional distress on these platforms in order to monitor and prevent suicide. This study aims to fill the research gap on analyzing suicidal ideation and emotional distress expressed in the Thai language on social media, specifically, Twitter. We present a dataset of 2,400 manually annotated Thai tweets marked for suicidal ideation and emotions. We then designed a deep learning model using key features extracted from the tweets to predict these factors and compared its performance with other common machine learning models. Our model outperformed the baseline models, with an F1-score of 93 % for predicting suicidal ideation and an F1-score of 88 % for predicting emotions. Using this model, we analyzed 67,627 tweets from 2019 to 2020 and found a marked increase in tweets expressing suicidal thoughts and sadness, up 40.97 % and 21.28 % respectively from 2019 to 2020. The findings indicate that the pandemic had a significant impact on the mental health of the Thai population. Our study provides a tool for identifying and monitoring suicidal thinking in similar settings and offers insight into the COVID-19 impact on mental health in Thailand. The annotated dataset will serve as a valuable resource for further research in this field.}
}
@article{HAO2026128886,
title = {Susceptible-infected diffusion of food safety opinion dissemination: Infrastructure-driven spread and behavior-embedded substance},
journal = {Expert Systems with Applications},
volume = {295},
pages = {128886},
year = {2026},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.128886},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425025035},
author = {Xinyue Hao and Dapeng Dong and Chang Liu and Emrah Demir and Samuel Fosso Wamba},
keywords = {Social Media Dissemination, Agent Adaptive Behaviour, Substance-Infrastructure Framework, Public Engagement, Crisis Communication},
abstract = {This study examines how food safety information disseminates across three structurally distinct Chinese social media platforms, Weibo, TikTok, and Xiaohongshu (XHS), during crisis events. Rather than serving as neutral transmission channels, these platforms are conceptualized as dynamic Information Service Systems (ISS), in which algorithmic infrastructures and content substances co-produce public meaning, emotional salience, and trust dynamics. Drawing on the Substance–Infrastructure (S-I) model, specifically Type II logic, where infrastructure drives substance, we theorize that technical mechanisms such as feed algorithms, trending systems, and visibility logics interact with semantic features like emotional tone, media modality, and narrative framing to shape the velocity, reach, and epistemic reliability of crisis communication. Employing a mixed-methods design that combines temporal Exponential Random Graph Models (ERGM), Susceptible-Infected (SI) diffusion simulations, and BERT-based sentiment analysis, we identify how different network structures, decentralized, centralized, and hybrid, interact with conformity, homophily, and neophilia to produce platform-specific information ecologies. TikTok’s architecture enables high-speed virality with minimal deliberative anchoring, limiting the platform’s ability to support trust repair; XHS facilitates high-affinity trust ecosystems led by key opinion leaders, but is vulnerable to echo chambers and insular misinformation; Weibo, with its hybrid infrastructure, supports rapid escalation and multi-directional discourse, but suffers from volatility in trust due to inconsistent epistemic control. These distinct affordances explain the asymmetric amplification of food safety narratives and the divergent trajectories of public trust, consolidation, polarization, or collapse, across platforms. As a contribution, the study introduces the Integrated Design and Operation Management (IDOM) framework, which positions platforms as reflexive control systems that must adapt to real-time signals of uncertainty and trust decay. It further underscores the need for resilient public governance that aligns institutional interventions with platform-specific logics and user cognitive baselines, advocating for a coordinated socio-technical ecosystem capable of sustaining trustworthy, inclusive, and responsive food safety communication in the digital era.}
}
@article{TU2024102142,
title = {Face forgery video detection based on expression key sequences},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {36},
number = {7},
pages = {102142},
year = {2024},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2024.102142},
url = {https://www.sciencedirect.com/science/article/pii/S1319157824002313},
author = {Yameng Tu and Jianbin Wu and Liang Lu and Shuaikang Gao and MingHao Li},
keywords = {Deepfake video detection, Key sequences, Facial expression, Two-branch network, Transformer},
abstract = {In order to minimize additional computational costs in detecting forged videos, and enhance detection accuracy, this paper employs dynamic facial expression sequences as key sequences, replacing original video sequences as inputs for the detection model. A spatio-temporal dual-branch detection network is designed based on the visual Transformer architecture. Specifically, this process involves three steps. Firstly, dynamic facial expression sequences are localized as key sequences using optical flow difference algorithms. Subsequently, the spatial branch network employs the focal self-attention mechanism to focus on dynamic features of expression-relevant regions and uses Factorization Machines to facilitate feature interaction among multiple key sequences. Meanwhile, the temporal branch network concentrates on learning the temporal inconsistency of optical flow differences between adjacent frames. Finally, a binary classification linear SVM combines the Softmax values from the two branch networks to provide the ultimate detection outcome. Experimental results on the Faceforensics++ dataset demonstrate: (a) replacing whole video sequences with facial expression key sequences effectively reduces training and detection time by nearly 80% and 90%, respectively; (b) compared to state-of-the-art methods involving random sequence/frame extraction and key frame extraction based on video compression techniques, the proposed approach in this paper presents a more competitive detection accuracy.}
}
@article{SHAYEGAN2025100222,
title = {A micro-influencer recommender method based on brand features using deep learning approaches},
journal = {Telematics and Informatics Reports},
volume = {19},
pages = {100222},
year = {2025},
issn = {2772-5030},
doi = {https://doi.org/10.1016/j.teler.2025.100222},
url = {https://www.sciencedirect.com/science/article/pii/S2772503025000374},
author = {Mohammad Javad Shayegan and Nasim Kazem Shiroodi},
keywords = {Micro-influencer, Influencer marketing, Convolutional neural network, Engagement prediction, Recommender systems, ResNet, BERT},
abstract = {Influencer marketing has become a critical strategy for brands, with micro-influencers playing a pivotal role in shaping consumer behavior on social media. These influencers significantly impact user decision-making; however, identifying the right micro-influencers who align with a brand’s identity and can maximize engagement remains a challenging task. This study aims to develop an effective system to predict which micro-influencers will generate the highest engagement for brands. The proposed method combines ResNet for extracting visual features from Instagram posts and BERT for extracting textual features, demonstrating a high degree of accuracy in predicting engagement rates. Three machine learning algorithms—Linear Regression, Random Forest, and Extreme Gradient Boosting—were employed for model training, with performance evaluated using Root Mean Squared Error (RMSE). Among these, the Random Forest model, enhanced through feature selection and hyperparameter tuning, achieved the lowest error with an RMSE of 1.50 %. Further evaluation using metrics such as AUC, cAUC, and MedR demonstrated that the proposed approach (ResNet + BERT) outperforms other methods across all metrics, achieving an AUC of 81 %, a cAUC of 67 %, and a MedR of 2. The proposed method improved AUC by 12 % and cAUC by 8 %, confirming its effectiveness in identifying high-engagement micro-influencers.}
}
@article{FENG2025103217,
title = {Crafting user-centric prompts for UI generations based on Kansei engineering and knowledge graph},
journal = {Advanced Engineering Informatics},
volume = {65},
pages = {103217},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103217},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625001107},
author = {Xuejing Feng and Huifang Du and Jun Ma and Haofen Wang and Lijuan Zhou and Meng Wang},
keywords = {Human-centered AI, User interface generations, Prompt engineering, Knowledge graph, Kansei engineering},
abstract = {Text-to-image (T2I) models are emerging as a powerful tool for designers to create user interface (UI) prototypes from natural language inputs (i.e., prompts). However, the discrepancy between designer inputs and model-preferred prompts makes it challenging for designers to consistently deliver effective results to end users. To bridge this gap, we introduce a novel hybrid method that assists designers in crafting user-centric prompts for T2I models, ensuring that the generated UIs align with end-user expectations. First, this method merges text mining and Kansei Engineering (KE) to analyze online user reviews and construct a Knowledge Graph (KG), mapping the intricate relationships between diverse affective requirements of users, design features, and corresponding text prompts for UI generation. Then, our approach automatically transforms designer inputs into model-preferred prompts through entity mention recognition and entity linking during the human-AI collaborative design process. Finally, we validate the proposed approach with a case study on automotive human–machine interface design. Experimental results demonstrate that our approach achieves high performance in perceived efficiency, satisfaction, and expectation disconfirmation. Overall, this study represents a step forward in integrating human and AI contributions in design and innovation within engineering disciplines, enabling AI to inspire, develop, and reinforce human creativity from a human factors perspective.}
}
@article{WEI2025104089,
title = {DCCMA-Net: Disentanglement-based cross-modal clues mining and aggregation network for explainable multimodal fake news detection},
journal = {Information Processing & Management},
volume = {62},
number = {4},
pages = {104089},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2025.104089},
url = {https://www.sciencedirect.com/science/article/pii/S0306457325000317},
author = {Siqi Wei and Zheng Wang and Meiling Li and Xuanning Liu and Bin Wu},
keywords = {Multimodal fake news detection, Disentangled representation learning, Cross-modal clues exploration, Explainable machine learning},
abstract = {Multimodal fake news detection is significant in safeguarding social security. Compared with single-text news, multimodal news data contains rich cross-modal clues that can improve the detection effectiveness: modality-common semantic enhancement, modality-specific semantic complementation, and modality-specific semantic inconsistency. However, most existing studies ignore the disentanglement of modality-specific and modality-common semantics but treat them as an entangled whole. Consequently, these studies can only implicitly explore the interactions between modalities, resulting in a lack of explainability. To address that, we propose a Disentanglement-based Cross-modal Clues Mining and Aggregation Network for explainable fake news detection, called DCCMA-Net. Specifically, DCCMA-Net decomposes each modality into two distinct representations: a modality-common representation that captures shared semantics across modalities, and a modality-specific representation that captures unique semantics within each modality. Then, leveraging these disentangled representations, DCCMA-Net explicitly and comprehensively mines three cross-modal clues: modality-common semantic enhancement, modality-specific semantic complementation, and modality-specific semantic inconsistency. Since not all clues play an equal role in the decision-making process, DCCMA-Net proposes an adaptive attention aggregation module to assign contribution weights to different clues. Finally, DCCMA-Net aggregates these clues based on their contribution weights to obtain highly discriminative news representations for detection, and highlights the most contributive clues as explanations for the detection results. Extensive experiments demonstrate that DCCMA-Net outperforms existing methods, achieving detection accuracy improvements of 2.53%, 4.01%, and 3.99% on Weibo, PHEME, and Gossipcop datasets, respectively. Moreover, the explainability accuracy of DCCMA-Net exceeds that of current state-of-the-art methods on the Weibo dataset.}
}
@article{NGUYEN2023104896,
title = {Facilitating knowledge construction in informal learning: A study of TikTok scientific, educational videos},
journal = {Computers & Education},
volume = {205},
pages = {104896},
year = {2023},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2023.104896},
url = {https://www.sciencedirect.com/science/article/pii/S0360131523001732},
author = {Ha Nguyen and Morgan Diederich},
keywords = {Informal learning, Social media},
abstract = {Social media can facilitate informal learning, where individuals engage in phases of knowledge construction (sharing ideas, exploring dissonances, negotiating, synthesizing, and applying knowledge). The goal of this research is to identify key patterns of knowledge construction on TikTok, a social media platform for sharing short-form videos. We leverage machine learning classifiers to label knowledge construction phases in 259,508 comments on 114 videos under scientific, educational hashtags on TikTok. We further examine the association between knowledge construction phases and manually coded video characteristics. Overall, the classifiers suggest that comments predominantly serve social, non-knowledge-construction purposes, followed by sharing ideas, negotiating understanding, and exploring idea dissonance. Videos that focus on informational content, leverage original audio, and employ analytic language have higher proportions of comments that share and negotiate knowledge. We discuss considerations for science communicators, to develop content that encourages viewers to socially construct knowledge.}
}
@article{JUNG2024,
title = {The Normalization of Vaping on TikTok Using Computer Vision, Natural Language Processing, and Qualitative Thematic Analysis: Mixed Methods Study},
journal = {Journal of Medical Internet Research},
volume = {26},
year = {2024},
issn = {1438-8871},
doi = {https://doi.org/10.2196/55591},
url = {https://www.sciencedirect.com/science/article/pii/S1438887124005636},
author = {Sungwon Jung and Dhiraj Murthy and Bara S Bateineh and Alexandra Loukas and Anna V Wilkinson},
keywords = {electronic cigarettes, vaping, social media, natural language processing, computer vision},
abstract = {Background
Social media posts that portray vaping in positive social contexts shape people’s perceptions and serve to normalize vaping. Despite restrictions on depicting or promoting controlled substances, vape-related content is easily accessible on TikTok. There is a need to understand strategies used in promoting vaping on TikTok, especially among susceptible youth audiences.
Objective
This study seeks to comprehensively describe direct (ie, explicit promotional efforts) and indirect (ie, subtler strategies) themes promoting vaping on TikTok using a mixture of computational and qualitative thematic analyses of social media posts. In addition, we aim to describe how these themes might play a role in normalizing vaping behavior on TikTok for youth audiences, thereby informing public health communication and regulatory policies regarding vaping endorsements on TikTok.
Methods
We collected 14,002 unique TikTok posts using 50 vape-related hashtags (eg, #vapetok and #boxmod). Using the k-means unsupervised machine learning algorithm, we identified clusters and then categorized posts qualitatively based on themes. Next, we organized all videos from the posts thematically and extracted the visual features of each theme using 3 machine learning–based model architectures: residual network (ResNet) with 50 layers (ResNet50), Visual Geometry Group model with 16 layers, and vision transformer. We chose the best-performing model, ResNet50, to thoroughly analyze the image clustering output. To assess clustering accuracy, we examined 4.01% (441/10,990) of the samples from each video cluster. Finally, we randomly selected 50 videos (5% of the total videos) from each theme, which were qualitatively coded and compared with the machine-derived classification for validation.
Results
We successfully identified 5 major themes from the TikTok posts. Vape product marketing (1160/10,990, 8.28%) reflected direct marketing, while the other 4 themes reflected indirect marketing: TikTok influencer (3775/14,002, 26.96%), general vape (2741/14,002, 19.58%), vape brands (2042/14,002, 14.58%), and vaping cessation (1272/14,002, 9.08%). The ResNet50 model successfully classified clusters based on image features, achieving an average F1-score of 0.97, the highest among the 3 models. Qualitative content analyses indicated that vaping was depicted as a normal, routine part of daily life, with TikTok influencers subtly incorporating vaping into popular culture (eg, gaming, skateboarding, and tattooing) and social practices (eg, shopping sprees, driving, and grocery shopping).
Conclusions
The results from both computational and qualitative analyses of text and visual data reveal that vaping is normalized on TikTok. Our identified themes underscore how everyday conversations, promotional content, and the influence of popular figures collectively contribute to depicting vaping as a normal and accepted aspect of daily life on TikTok. Our study provides valuable insights for regulatory policies and public health initiatives aimed at tackling the normalization of vaping on social media platforms.}
}
@article{LIU2024102512,
title = {Streaming disasters on TikTok: Examining social mediated crisis communication, public engagement, and emotional responses during the 2023 Maui wildfire},
journal = {Public Relations Review},
volume = {50},
number = {5},
pages = {102512},
year = {2024},
issn = {0363-8111},
doi = {https://doi.org/10.1016/j.pubrev.2024.102512},
url = {https://www.sciencedirect.com/science/article/pii/S0363811124000912},
author = {Wenlin Liu and Xinyan Zhao and Mengqi (Monica) Zhan and Stephanie Hernandez},
keywords = {Crisis communication, Digital and visual storytelling, Disaster-specific social media engagement, Public sentiments, Computational methods},
abstract = {Short video-sharing platforms, such as TikTok, play a significant role in allowing the public to experience natural disasters vicariously, share information, and coordinate peer-led disaster relief efforts. With emerging platforms like TikTok providing experientially immersive content, the role of digital storytelling in stimulating public engagement and emotions remains underexplored. Drawing from narrative persuasion theory and visual storytelling literature, the current study proposes an integrated framework to examine the storyteller, visual frame, and digital presentation characteristics of TikTok videos in predicting social-mediated public engagement and emotional reactions surrounding a recent natural disaster, the 2023 Maui Wildfire. Using visual content analysis and computational methods, the study analyzed 526 TikTok videos and 59,950 associated comments. Results showed that audiovisual vividness consistently predicted all three types of disaster engagement. In contrast, storyteller type, storytelling frame, and other presentation characteristics inconsistently predicted information consumption intention, information sharing intention, and emotional reactions on TikTok.}
}
@article{CHINIVAR2025100147,
title = {OVALYTICS: Enhancing Offensive Video Detection with YouTube Transcriptions and Advanced Language Models},
journal = {Natural Language Processing Journal},
volume = {11},
pages = {100147},
year = {2025},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2025.100147},
url = {https://www.sciencedirect.com/science/article/pii/S2949719125000238},
author = {Sneha Chinivar and Roopa M.S. and Arunalatha J.S. and Venugopal K.R.},
keywords = {Classification head, Large language models, Offensive video detection, Text transcription, Whisper AI},
abstract = {The exponential growth of offensive content online underscores the need for robust content moderation. In response, this work presents OVALYTICS (Offensive Video Analysis Leveraging YouTube Transcriptions with Intelligent Classification System), a comprehensive framework that introduces novel integrations of advanced technologies for offensive video detection. Unlike existing approaches, OVALYTICS uniquely combines Whisper AI for accurate audio-to-text transcription with state-of-the-art large language models (LLMs) such as BERT, ALBERT, XLM-R, MPNet, and T5 for semantic analysis. The framework also features a newly curated dataset tailored for fine-grained evaluation, achieving significant improvements in accuracy and F1-scores over traditional methods and advancing the state of automated content moderation.}
}
@article{KHEDAIRIA20251267,
title = {A Co-Attention Mechanism into a Combined GNN-Based Model for Fake News Detection},
journal = {Computers, Materials and Continua},
volume = {85},
number = {1},
pages = {1267-1285},
year = {2025},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2025.066601},
url = {https://www.sciencedirect.com/science/article/pii/S1546221825007775},
author = {Soufiane Khedairia and Akram Bennour and Mouaaz Nahas and Aida Chefrour and Rashiq Rafiq Marie and Mohammed Al-Sarem},
keywords = {Fake news detection, co-attention mechanism, user preferences, GNNs},
abstract = {These days, social media has grown to be an integral part of people’s lives. However, it involves the possibility of exposure to “fake news,” which may contain information that is intentionally or inaccurately false to promote particular political or economic interests. The main objective of this work is to use the co-attention mechanism in a Combined Graph neural network model (CMCG) to capture the relationship between user profile features and user preferences in order to detect fake news and examine the influence of various social media features on fake news detection. The proposed approach includes three modules. The first one creates a Graph Neural Network (GNN) based model to learn user profile properties, while the second module encodes news content, user historical posts, and news sharing cascading on social media as user preferences GNN-based model. The inter-dependencies between user profiles and user preferences are handled through the third module using a co-attention mechanism for capturing the relationship between the two GNN-based models. We conducted several experiments on two commonly used fake news datasets, Politifact and Gossipcop, where our approach achieved 98.53% accuracy on the Gossipcop dataset and 96.77% accuracy on the Politifact dataset. These results illustrate the effectiveness of the CMCG approach for fake news detection, as it combines various information from different modalities to achieve relatively high performances.}
}
@article{ALQURASHI2024374,
title = {A data-driven multi-perspective approach to cybersecurity knowledge discovery through topic modelling},
journal = {Alexandria Engineering Journal},
volume = {107},
pages = {374-389},
year = {2024},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2024.07.044},
url = {https://www.sciencedirect.com/science/article/pii/S1110016824007658},
author = {Fahad Alqurashi and Istiak Ahmad},
keywords = {Cyber security, Knowledge discovery, Semantic analysis, Bert, Topic modelling, Natural language processing},
abstract = {Cybersecurity is crucial for protecting the privacy of digital systems, maintaining economic stability, and ensuring national security. This study presents a comprehensive approach to cybersecurity knowledge discovery through topic modelling, using a multi-perspective analysis of academic and industry sources. The datasets include 15,751 articles from the Web of Science (WoS) database and 5,831 articles from Security Magazine, spanning from 2011 to 2023. We employed BERTopic for topic modelling, UMAP for dimensionality reduction, and HDBSCAN clustering algorithm for grouping and analysing distinct article clusters to uncover latent topics, enhancing the understanding of the evolving cybersecurity landscape. This study found 12 micro-clusters and three macro-clusters, namely technology, smart city and education, from the WoS database and 12 more micro-clusters and four macro-clusters, including organization, public security, governance, and education, from magazines. This study reveals key cybersecurity research and practice trends, such as the increasing focus on malware, ransomware, and cyber-attack mitigation. Additionally, temporal analysis indicates a significant rise in cybersecurity interest around 2020, followed by a diversification of topics. The results highlight the importance of integrating diverse data sources to capture a holistic view of cybersecurity developments. Future work will aim to refine the clustering algorithms to further improve topic extraction and analysis and expand the datasets to include more diverse sources and perspectives. This approach helps discover current cybersecurity trends and provides a foundation for more targeted and effective cybersecurity strategies.}
}
@article{HUANG2024,
title = {The Impact of Short Video Live Broadcast on the Sales of Sports Machinery and Equipment},
journal = {International Journal of e-Collaboration},
volume = {20},
number = {1},
year = {2024},
issn = {1548-3673},
doi = {https://doi.org/10.4018/IJeC.344027},
url = {https://www.sciencedirect.com/science/article/pii/S1548367324000024},
author = {Chunyue Huang and Lichun Chen},
keywords = {Deep Learning, E-Commerce, Mobile Multimedia, Residual Module, Short Video Live Broadcast, Sports Machinery Sales, Transformer Model, U-Net Network},
abstract = {ABSTRACT
This paper proposes a neural network model based on deep learning, which can better analyze the impact of short video live broadcast on sports machinery and equipment. Firstly, this paper proposes a U-Net-based convolutional neural network as the backbone network of this paper, which mainly realizes the impact of short video live broadcast on sales. Secondly, this paper proposes a dense residual module based on the Transformer lightweight module, which can effectively improve the global modeling ability of the network model and improve the prediction accuracy of the network model. Finally, through a large number of experiments, it is proved that the convolutional neural network based on U-Net proposed in this paper can be better used for the task of short video live broadcast for the sales of sports machinery and equipment, and achieves better prediction accuracy and reasoning speed.}
}
@article{ZHANG2025977,
title = {A Cross Attention Transformer-Mixed Feedback Video Recommendation Algorithm Based on DIEN},
journal = {Computers, Materials and Continua},
volume = {82},
number = {1},
pages = {977-996},
year = {2025},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2024.058438},
url = {https://www.sciencedirect.com/science/article/pii/S1546221825000190},
author = {Jianwei Zhang and Zhishang Zhao and Zengyu Cai and Yuan Feng and Liang Zhu and Yahui Sun},
keywords = {Video recommendation, user interest, cross-attention, transformer},
abstract = {The rapid development of short video platforms poses new challenges for traditional recommendation systems. Recommender systems typically depend on two types of user behavior feedback to construct user interest profiles: explicit feedback (interactive behavior), which significantly influences users’ short-term interests, and implicit feedback (viewing time), which substantially affects their long-term interests. However, the previous model fails to distinguish between these two feedback methods, leading it to predict only the overall preferences of users based on extensive historical behavior sequences. Consequently, it cannot differentiate between users’ long-term and short-term interests, resulting in low accuracy in describing users’ interest states and predicting the evolution of their interests. This paper introduces a video recommendation model called CAT-MF Rec (Cross Attention Transformer-Mixed Feedback Recommendation) designed to differentiate between explicit and implicit user feedback within the DIEN (Deep Interest Evolution Network) framework. This study emphasizes the separate learning of the two types of behavioral feedback, effectively integrating them through the cross-attention mechanism. Additionally, it leverages the long sequence dependence capabilities of Transformer technology to accurately construct user interest profiles and predict the evolution of user interests. Experimental results indicate that CAT-MF Rec significantly outperforms existing recommendation methods across various performance indicators. This advancement offers new theoretical and practical insights for the development of video recommendations, particularly in addressing complex and dynamic user behavior patterns.}
}
@article{TANG2024102183,
title = {GCNT: Identify influential seed set effectively in social networks by integrating graph convolutional networks with graph transformers},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {36},
number = {8},
pages = {102183},
year = {2024},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2024.102183},
url = {https://www.sciencedirect.com/science/article/pii/S1319157824002726},
author = {Jianxin Tang and Jitao Qu and Shihui Song and Zhili Zhao and Qian Du},
keywords = {Social network analysis, Influence maximization, Graph transformers, Graph convolutional networks},
abstract = {Exploring effective and efficient strategies for identifying influential nodes from social networks as seeds to promote the propagation of influence remains a crucial challenge in the field of influence maximization (IM), which has attracted significant research efforts. Deep learning-based approaches have been adopted as an alternative promising solution to the IM problem. However, a robust model that captures the associations between network information and node influence needs to be investigated, while concurrently considering the effects of the overlapped influence on training labels. To address these challenges, a GCNT model, which integrates Graph Convolutional Networks with Graph Transformers, is introduced in this paper to capture the intricate relationships among the topology of the network, node attributes, and node influence effectively. Furthermore, an innovative method called Greedy-LIE is proposed to generate labels to alleviate the issue of overlapped influence spread. Moreover, a Mask mechanism specially tailored for the IM problem is presented along with an input embedding balancing strategy. The effectiveness of the GCNT model is demonstrated through comprehensive experiments conducted on six real-world networks, and the model shows its competitive performance in terms of both influence maximization and computational efficiency over state-of-the-art methods.}
}
@article{XU2024114277,
title = {MEMF: Multi-entity multimodal fusion framework for sales prediction in live streaming commerce},
journal = {Decision Support Systems},
volume = {184},
pages = {114277},
year = {2024},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2024.114277},
url = {https://www.sciencedirect.com/science/article/pii/S0167923624001106},
author = {Guang Xu and Ming Ren and Zhenhua Wang and Guozhi Li},
keywords = {Live streaming commerce, Sales prediction, Multimodal fusion, Entity-level fusion, QuadTransformer},
abstract = {Live streaming commerce thrives with a rich tapestry of multimodal information that intertwines with various entities, including the anchor, the commodities, and the live streaming environment. Despite the wealth of data at hand, the synthesis and analysis of this information to predict sales remains a significant challenge. This study introduces a framework for multi-entity multimodal fusion, which is characterized by the effective synthesis of multimodal data and its prioritization of entity-level fusion, thereby providing a comprehensive feature representation for improving predictive performance. In addressing the multimodal data associated with a diverse range of products, our framework improves the Transformer architecture to initially capture the intra-product modal features and subsequently integrate the inter-product features. Data experiments are conducted on a real-world dataset from Taobao Live. The framework outperforms both traditional machine learning methods and state-of-the-art multimodal fusion methods, which affirms its value as a robust decision-support tool for sales prediction, enabling more accurate pre-event predictions and strategic planning. We also examine the impact of different types of information in accurate sales prediction. It is found that harnessing a comprehensive suite of data leads to optimal performance across all evaluation metrics. Commodity-related data is primary factor in determining the prediction accuracy, followed by video data and streaming room-related data, providing insights regarding the resource allocation for collecting and analyzing multimodal data from live streaming platforms.}
}