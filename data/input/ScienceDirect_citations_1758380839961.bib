@article{ZHANG2025100253,
title = {Construction and application of knowledge graph for urban agglomeration emergency rescue collaboration in earthquake disasters},
journal = {Journal of Safety Science and Resilience},
pages = {100253},
year = {2025},
issn = {2666-4496},
doi = {https://doi.org/10.1016/j.jnlssr.2025.100253},
url = {https://www.sciencedirect.com/science/article/pii/S2666449625000878},
author = {Lin Zhang and Qichen Wang and Yanjun Guo and Xiangliang Tian and Lin Qi},
keywords = {Knowledge graph, Earthquake disaster, Urban agglomeration, Emergency rescue collaboration, Neo4j graph database},
abstract = {This study proposes an emergency rescue collaboration knowledge graph construction method for urban agglomeration in earthquake disasters. Based on the collection of 22 earthquake disaster emergency plans published on the official websites of multiple cities in the Chengdu-Chongqing urban agglomeration in China, earthquake disaster emergency rescue data from the Red Cross Society of Sichuan Province and Chongqing City, and historical rescue information from the China Blue Sky rescue team, this study defines and extracts six types of entities including rescue entities, policy documents, rescue actions, rescue information, rescue supplies, and emergency response levels. A knowledge graph pattern layer is established using a hybrid approach of top-down and bottom-up, including concept layer, relationship layer, rule layer, and instance layer. This study extracts earthquake disaster emergency rescue collaboration knowledge information from collected data sources, and YEDDA software is used for knowledge fusion, thus constructing a knowledge graph data layer. The data is stored in the Neo4j graph database as triplets (entity-relation-entity). Visual representation and retrieval are used to achieve the query, association, and inference of emergency rescue collaboration information for urban agglomeration in earthquake disasters. The 2022 Luding earthquake disaster in Ganzi Tibetan Autonomous Prefecture, China is selected as a typical case, and verified the effectiveness and reliability by inputting the case into the emergency rescue collaboration knowledge graph which constructed in this study. The results indicate that the constructed knowledge graph provides intelligent decision support for earthquake disaster emergency rescue collaboration in urban agglomeration, effectively improves the performance of earthquake disaster emergency rescue, and provides new ideas and methods for earthquake disaster rescue and reduction.}
}
@article{LI2025106657,
title = {A data-driven and knowledge graph-based research on safety risk-coupled evolution analysis and assessment in shield tunneling},
journal = {Tunnelling and Underground Space Technology},
volume = {162},
pages = {106657},
year = {2025},
issn = {0886-7798},
doi = {https://doi.org/10.1016/j.tust.2025.106657},
url = {https://www.sciencedirect.com/science/article/pii/S0886779825002950},
author = {Xuewei Li and Shuchen Li and Jingfeng Yuan and Zeen Wan and Xuan Liu},
keywords = {Shield tunneling, Risk-coupled evolution, Data driven, Knowledge graph, BBi-MA-DCGCN, Risk assessment},
abstract = {Shield tunneling encounters critical safety challenges stemming from spatiotemporal risk coupling and insufficient utilization of accident data. This study developed a hybrid entity–relationship extraction model called BBi-MA-DCGCN by integrating bidirectional encoder representations from Transformer embeddings, bidirectional long short-term memory architectures, multihead attention mechanisms, and densely connected graph convolutional networks. Subsequently, a risk-coupled evolution knowledge graph was constructed to support intelligent inference and probabilistic reasoning of accident scenarios. Furthermore, an interaction matrix framework was implemented within the graph structure to quantitatively evaluate risk interdependencies. Results showed that (1) the BBi-MA-DCGCN model achieved an F1 score of 90.33%, demonstrating robust capabilities in entity and relationship extraction. (2) The coupling evolution reasoning method enabled the rapid inference of the most probable risk evolution path and potential accident types from any given risk node. (3) Among the top 25 key risk factors obtained through quantitative risk assessment, 52% were attributed to organizational management factors, with management, supervision, and training identified as having the most substantial effects. This study provides a novel method for understanding the spatiotemporal coupling evolution mechanism of safety risks in shield tunneling and enhances the accuracy of risk identification, dynamic inference, and quantitative evaluation.}
}
@article{LIN2025103696,
title = {Multisource data-driven method for product innovation design based on knowledge graph},
journal = {Advanced Engineering Informatics},
volume = {68},
pages = {103696},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103696},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625005890},
author = {Wenguang Lin and Yu Wang and Renbin Xiao},
keywords = {Knowledge graph, Reviews data, Patent data, Product design, MacBERT-BiLSTM-CRF},
abstract = {The acquisition of sufficient data, including market data and experience data, is the key to reducing product innovation design (PID) risks. Data mined from a single channel has cannot meet the needs of existing designs, and the fusion of multi-channel data is a trend in existing research. Among them, online reviews data (ORD) are an important channel for customers to express their preferences, and they are also an important data source for manufacturers to track customer requirements (CRs). Patent data contains professional product structure descriptions, product functions, and performance. However, how to merge CRs with patent data is a challenge. First, a patent knowledge graph (PKG) is constructed, and MacBERT-BiLSTM-CRF algorithm suitable for Chinese text mining is introduced into patent entity recognition; then, through the review of the data, CRs are obtained and integrated into the PKG. Finally, knowledge reasoning (KR) algorithm is used to complete structure acquisition and product design. In order to verify the effectiveness of our method, we conducted an experiment using wheelchairs as an example. The results show that our method is superior to other methods and effectively promotes the implementation of product design improvements based on PKG.}
}
@article{SHI2025103523,
title = {A stepwise intelligence generative method for structured maintenance guidance documents based on knowledge graph augmented LLM},
journal = {Advanced Engineering Informatics},
volume = {67},
pages = {103523},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103523},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625004161},
author = {Fangcheng Shi and Liang Chen and Moshi Zhou and Yue Zhao and Yu Zheng},
keywords = {Graph retrieval augmented generation, Chain of thought, Large language model, Maintenance guidance documents, Knowledge graph},
abstract = {Maintenance guidance documents (MGDs) are the basis for engineering maintenance process. At present, application of large language models (LLMs) in the generation of industrial documents have issues with inaccurate content and structure that does not match professional requirements. Therefore, this paper proposes an enhanced method that integrates professional knowledge graph retrieval augmented generation (GraphRAG) and chain-of-thought (CoT) prompts to guide LLMs to intelligently generate structured MGDs step by step. First, the LLM prompt enhancement methods are used to assist in the construction of the professional knowledge graph. Second, a CoT prompt is constructed corresponding to the stepwise characteristics of MGDs. Finally, based on the CoT prompt, the corresponding graph entity content is retrieved step by step to construct a stepwise prompt, enhancing the generation. This method has been experimentally verified in the automatic generation task of Baosteel continuous casting equipment MGDs. Compared with methods that rely solely on prompts and examples, this method significantly improves the structural controllability and content accuracy of the generated results.}
}
@article{ZHU2025102496,
title = {Multi-Granularity History Graph Network for temporal knowledge graph reasoning},
journal = {Data & Knowledge Engineering},
volume = {160},
pages = {102496},
year = {2025},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2025.102496},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X25000916},
author = {Jun Zhu and Yan Fu and Junlin Zhou and Duanbing Chen},
keywords = {Multi-granularity history, Knowledge graph reasoning, Temporal knowledge graph},
abstract = {Reasoning on knowledge graphs (KGs) can be categorized into two main categories: predicting missing facts and predicting unknown facts in the future. However, when it comes to future prediction, it becomes crucial to incorporate temporal information and add timestamps to KGs, thereby forming temporal knowledge graphs (TKGs). The key aspect of reasoning lies in treating a TKG as a sequence of static KGs in order to effectively grasp temporal information. Additionally, it is equally important to consider the evolution of facts from various perspectives. Existing models tend to replicate the original time granularity of data while modeling TKGs, often disregarding the impact of the minimum time period in the evolution process. Furthermore, historical information is typically perceived as a single sequence of facts, with a lack of diversity in strategies (e.g., modeling sequences with varying granularities or lengths) to capture complex temporal dynamics. This unified approach may lead to the loss of critical information during the modeling process. However, the process of historical evolution often exhibits complex periodic transformation characteristics, and associated events do not necessarily follow a fixed time period. Therefore, a single granularity is insufficient to model periodic events with dynamic changes in history. Consequently, we propose the Multi-Granularity History Graph Network (MGHGN), an innovative model for TKG reasoning. MGHGN dynamically models various event evolution periods by constructing representations with multiple time granularities, and integrates various modeling methods to reason the potential facts in the future. Our model adeptly captures valuable insights from the history of multi-granularity and employs diverse approaches to model historical information. The experimental results on six benchmark datasets demonstrate that the MGHGN outperforms state-of-the-art methods.}
}
@article{DING2025115561,
title = {A failure knowledge graph learning framework for offshore wind turbines with incomplete knowledge},
journal = {Renewable and Sustainable Energy Reviews},
volume = {215},
pages = {115561},
year = {2025},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2025.115561},
url = {https://www.sciencedirect.com/science/article/pii/S1364032125002345},
author = {Yi Ding and Feng Zhu and He Li and Ajith Kumar Parlikad and Min Xie},
keywords = {Offshore wind energy, Operation and maintenance, Failure knowledge graph, Knowledge transfer, Incomplete knowledge},
abstract = {This study presents a novel framework for Failure Knowledge Graph (FKG) construction tailored for the safe operation and maintenance of offshore wind turbines. Specifically, Bidirectional Encoder Representations from Transformers (BERT) and Conditional Random Field (CRF) are combined for failure extraction, enhanced by iterative learning for failure data transfer from onshore to offshore wind turbines. Additionally, this framework incorporates a rule-based pseudo-label module and an innovative replacement-based pseudo-sample module to mitigate the impact of label errors and failure data imbalance during the iterative learning process. With the failure events extracted, the affiliate components and corresponding failure modes are identified to construct a tree-structured FKG automatically for offshore wind turbines. The feasibility and effectiveness of the proposed framework are validated by the presentation of an FKG regarding 313 offshore wind turbines recorded in the LGS-offshore dataset. Overall, the study provides the offshore wind sector with an intelligent framework for failure data analysis, presentation, and understanding and contributes to the safe operation of offshore wind turbines and wind farms.}
}
@article{WU2025338,
title = {Design of Intelligent Q&A System Based on Knowledge Graph Combined with Large Language Model},
journal = {Procedia Computer Science},
volume = {262},
pages = {338-347},
year = {2025},
note = {The 5th International Conference on Multi-modal Information Analytics (MMIA)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.05.061},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925019088},
author = {Quanquan Wu},
keywords = {Large Language Model, LLM, Knowledge Graph, KG, Intelligent Question-Answering, Q&A, System Design},
abstract = {LLM can transform natural language questions into structured queries, and the KG-based Q&A system can provide accurate and reliable answers. The combination of LLM and KG is the key technology to support the modern Q&A model. Through the two-wheel drive of structured knowledge and semantic understanding, the processing ability of complex problems is significantly improved, and the Q&A system is jointly promoted from the primary to the advanced intelligence evolution. In this paper, based on LLM and KG, knowledge distillation based LLM fusion KG technology is studied, so that the target model can absorb the advantages of both, not only have the language processing capability of large models, but also use the structured knowledge of KG to improve performance and interpretability. On this basis, the multi-layer architecture of intelligent Q&A system is designed, which is easy for developers to work together. The ClaudeKG model constructed in this paper is compared with DeepSeek and Doubao baseline models, and the function and performance are analyzed.}
}
@article{KRUIPER2024102653,
title = {A platform-based Natural Language processing-driven strategy for digitalising regulatory compliance processes for the built environment},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102653},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102653},
url = {https://www.sciencedirect.com/science/article/pii/S147403462400301X},
author = {Ruben Kruiper and Bimal Kumar and Richard Watson and Farhad Sadeghineko and Alasdair Gray and Ioannis Konstas},
keywords = {Digital Regulatory Compliance, Natural Language Processing, Semantic Web, Machine Learning, Knowledge Graph, Automated Compliance Checking},
abstract = {The digitalisation of the regulatory compliance process has been an active area of research for several decades. However, more recently the level of activities in this area has increased considerably. In the UK, the tragic incident of Grenfell fire in 2017 has been a major catalyst for this as a result of the Hackitt report’s recommendations pointing a lot of the blame on the broken regulatory regime in the country. The Hackitt report emphasises the need to overhaul the building regulations, but the approach to do so remains an open research question. Existing work in this space tends to overlook the processing of actual regulatory documents, or limits their scope to solving a relatively small subtask. This paper presents a new comprehensive platform approach to the digitalisation of the regulatory compliance processing. We present i-ReC (intelligent Regulatory Compliance), a platform approach to digitalisation of regulatory compliance that takes into consideration the enormous diversity of all the stakeholders’ activities. A historical perspective on research in this area is first presented to put things in perspective which identifies the challenges in such an endeavour and identifies the gaps in state-of-the-art. After enumerating all the challenges in implementing a platform-based approach to digitalising the regulatory compliance process, the implementation of some parts of the platform is described. Our research demonstrates that the identification and extraction of all relevant requirements from the corpus of several hundred regulatory documents is a key part of the whole process which underlies the entire process from authoring to eventually compliance checking of designs. Some of the issues that need addressing in this endeavour include ambiguous language, inconsistent use of terms, contradicting requirements and handling multi-word expressions. The implementation of these tools is driven by NLP, ML and Semantic Web technologies. A semantic search engine was developed and validated against other popular and comparable engines with a corpus of 420 (out of about 800) documents used in the UK for compliance checking of building designs. In every search scenario, our search engine performed better on all objective criteria. Limitations of the approach are discussed which includes the challenges around licensing for all the documents in the corpus. Further work includes improving the performance of SPaR.txt (the tool created to identify multi-word expressions) as well as the information retrieval engine by increasing the dataset and providing the model with examples from more diverse formats of regulations. There is also a need to develop and align strategies to collect a comprehensive set of domain vocabularies to be combined in a Knowledge Graph.}
}
@article{BORREGO2025113280,
title = {Research hypothesis generation over scientific knowledge graphs},
journal = {Knowledge-Based Systems},
volume = {315},
pages = {113280},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113280},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125003272},
author = {Agustín Borrego and Danilo Dessì and Daniel Ayala and Inma Hernández and Francesco Osborne and Diego {Reforgiato Recupero} and Davide Buscaldi and David Ruiz and Enrico Motta},
keywords = {Hypothesis generation, Knowledge graphs, Link prediction, Scholarly domain, Scientific facts, Artificial Intelligence},
abstract = {Generating research hypotheses is a crucial step in scientific investigation that involves the creation of precise, verifiable, and logically valid statements that can be empirically examined. Therefore, many efforts have been made to automate or assist this process through the use of various Artificial Intelligence solutions. However, most existing methods are tailored to very specific domains, particularly within the biomedical field. There have been recent attempts to formalize hypothesis generation as a link prediction task over knowledge graphs. This solution is potentially domain-independent and applicable across diverse disciplines. Nevertheless, current approaches for link prediction, which typically rely on embedding models or path-based methods, have shown limited success in accurately predicting new hypotheses. To address these limitations, this paper introduces ResearchLink, an innovative and domain-independent methodology for hypothesis generation over knowledge graphs. ResearchLink combines path-based features and knowledge graph embeddings with text embeddings, capturing the semantic context of entities within a given corpus, and integrates additional information from bibliometric databases to improve research collaboration predictions. To conduct a rigorous evaluation of ResearchLink, we constructed CSKG-600, a new dataset for hypothesis generation, consisting of 600 statements that were manually labeled by domain experts. ResearchLink achieved outstanding performance (78.7% P@20), significantly outperforming alternative approaches such as TransH (71.8%), TransD (71.7%), and RotatE (70.7%).}
}
@article{LIMA2025103263,
title = {ULKB Logic: A HOL-based framework for reasoning over knowledge graphs},
journal = {Science of Computer Programming},
volume = {242},
pages = {103263},
year = {2025},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2025.103263},
url = {https://www.sciencedirect.com/science/article/pii/S0167642325000024},
author = {Guilherme Lima and Alexandre Rademaker and Rosario Uceda-Sosa},
keywords = {HOL, Python, Wikidata, SPARQL, MRS, NLP},
abstract = {ULKB Logic is an open-source framework written in Python for reasoning over knowledge graphs. It provides an interactive theorem prover-like environment equipped with a higher-order language similar to the one used by HOL Light. The main goal of ULKB Logic is to ease the construction of applications that combine state-of-the-art computational logic tools with the knowledge available in knowledge graphs, such as Wikidata. To this end, the framework provides APIs for fetching statements from SPARQL endpoints and operating over the constructed theories using automated theorem provers and SMT solvers (such as the E prover and Z3). In this paper, we describe the design and implementation of ULKB Logic, present its interfaces for querying knowledge graphs and for calling external provers, and discuss a use case of commonsense reasoning in which ULKB Logic is used as the target logic for representing the semantics of English sentences.}
}
@article{ZHOU2025103142,
title = {Augmenting general-purpose large-language models with domain-specific multimodal knowledge graph for question-answering in construction project management},
journal = {Advanced Engineering Informatics},
volume = {65},
pages = {103142},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103142},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625000357},
author = {Shenghua Zhou and Keyan Liu and Dezhi Li and Chun Fu and Yan Ning and Wenying Ji and Xuefan Liu and Bo Xiao and Ran Wei},
keywords = {GLM, Construction Project Management, QA, Multimodal Knowledge Graph},
abstract = {Current studies on Question-Answering of Construction Project Management (CPM-QA) face challenges, including the small-scale CPM-related knowledge repositories, the limited effectiveness of QA methods using grammar rules or tiny machine-learning models, and the shortage of testing sets for comparing QA performance. Hence, this research augments general-purpose large-language models (GLMs) with the multimodal CPM knowledge graph (CPM-KG) for CPM-QA. It encompasses (i) building the multimodal CPM-KG covering 36 CPM subfields, (ii) combining CPM-KG and GLMs through three stages, (iii) developing a 2435-question CPM-QA testing set, and (iv) assessing and comparing CPM-QA accuracies for eight pairs of original and CPM-KG-augmented GLMs. The results demonstrate that CPM-KG-augmented GLMs’ CPM-QA accuracy rate is 30.0 % superior to original GLMs on average, and top-performing CPM-KG-augmented GLMs (e.g., ERNIE-Bot 4.0) pass CRCEEs. Within 36 CPM subfields, CPM-QA accuracy enhancements resulting from CPM-KG are between 12.2 % and 57.8 %. Furthermore, CPM-KG leads to CPM-QA accuracy enhancements of 19.6 % for single-answer, 48.0 % for multiple-answer, 30.6 % for text-only, and 20.4 % for image-embedded questions. The multimodal CPM-KG also outperforms the text-only single-modal CPM-KG in enhancing CPM-QA performance. This work contributes to unveiling the significance of CPM-specific knowledge in augmenting GLMs, sharing a reusable multimodal CPM-KG-formatted knowledge repository, and delivering a testing set of CPM-QA.}
}
@article{HUANG2025103590,
title = {Knowledge graph-driven methodology for complex product architecture solution generation and simulation verification},
journal = {Advanced Engineering Informatics},
volume = {68},
pages = {103590},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103590},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625004835},
author = {Yu Huang and Ru Wang and Yingjie Li and Guoxin Wang and Yan Yan},
keywords = {Knowledge graph, Product architecture design, Solution generation, Architecture simulation validation, Architecture optimization and decision-making},
abstract = {Complex product architecture design is a key process that determines system performance, development efficiency, and reliability. The intelligent transformation of this process is of significant importance in responding to dynamic demands and reducing R&D costs. Traditional architecture design methods face challenges such as reliance on experience, low knowledge reuse, and insufficient efficiency in solution generation and validation. To address these issues, this paper proposes a knowledge graph-driven approach for the generation and simulation validation of complex product architectures. Through structured knowledge management and application, this method enhances the intelligence of product architecture design and decision-making support capabilities. Firstly, a multi-source knowledge graph that integrates product, feature, and decision data is constructed to support semantic associations and dynamic inference of design knowledge. Secondly, an adjacency matrix is created based on graph matching, generating an initial architecture solution. This solution is then refined using multi-objective optimization algorithms and multi-attribute decision-making methods for parameter configuration and trade-off analysis. Finally, a mechanism model is employed to dynamically evaluate the solution’s performance, creating a closed-loop feedback system to iteratively optimize and shorten the design verification cycle. A case study of the separation system of a launch vehicle’s primary and second stages demonstrates the entire process of function-component matching, solution generation, and simulation validation under the knowledge graph-driven approach.}
}
@article{LI2025104148,
title = {Quality-Controllable automatic construction method of Chinese knowledge graph for medical decision-making applications},
journal = {Information Processing & Management},
volume = {62},
number = {4},
pages = {104148},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2025.104148},
url = {https://www.sciencedirect.com/science/article/pii/S0306457325000895},
author = {Xue Li and Ye Yuan and Yang Yang and Yi Guan and Haotian Wang and Jingchi Jiang and Huaizhang Shi and Xiguang Liu},
keywords = {Medical knowledge graphs, Automatic construction of knowledge graph, Knowledge quality control, Knowledge-enhanced medical large language model, Medical decision-making application},
abstract = {Medical Knowledge Graphs (KGs) store complex medical knowledge in a structured manner, increasingly becoming the foundation of medical artificial intelligence. They provide interpretable evidence for disease diagnosis and treatment, and enhance the accuracy and interpretability of medical information in large language models (LLMs), thus mitigating the hallucination issues. However, existing medical KGs lack diverse knowledge types, sufficient coverage, fine granularity, and high quality, resulting in low utilization rates. To address these issues, this paper, under the guidance of medical professionals, proposes guidelines and automated methods for constructing a Chinese medical KG, drawing from existing experience in building KGs and the requirements of medical decision systems. The construction principles include (1) universality and personalization, (2) comprehensiveness and granularity, (3) knowledge quality control. Furthermore, the automated construction method integrates a chain of thought-based knowledge mining approach and an axiom logic-based quality control module, which improves the scalability of mining and the quality of the knowledge. Based on these, a Chinese medical KG named WiMedKG has been developed. It meets the established construction guidelines by: (1) including both commonsense and experiential medical knowledge, (2) comprehensively covering 111 departments with content ranging from clinical practice to preventive medicine and rehabilitation treatments. The granularity of the knowledge is detailed, featuring 29 entity types, 128 refined relationship types, and 40 attribute types, comprising a total of 367,108 entities, 3,176,389 relational triples, and 1,021,966 attribute triples. (3) The knowledge has been validated and completed, receiving an evaluation score of 90.66% from medical professionals, which demonstrates the reliability of the quality-controlled automatic KG construction method. Finally, we constructed medical LLM WiMedLLM enhanced by WiMedKG. Experimental results on the medical test dataset show an average performance improvement of 1.51% after KG enhancement, demonstrating the necessity of KG construction and the effectiveness of the automatic construction method. The data and system resources can be found on our page: https://github.com/lx-hit/WiMedKG.}
}
@article{COLOMBO2025104082,
title = {An LLM-assisted ETL pipeline to build a high-quality knowledge graph of the Italian legislation},
journal = {Information Processing & Management},
volume = {62},
number = {4},
pages = {104082},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2025.104082},
url = {https://www.sciencedirect.com/science/article/pii/S030645732500024X},
author = {Andrea Colombo and Anna Bernasconi and Stefano Ceri},
keywords = {Law, Knowledge graph, Property graph, Large language models, Data quality},
abstract = {The increasing complexity of legislative systems, characterized by an ever-growing number of laws and their interdependencies, has highlighted the utility of Knowledge Graphs (KGs) as an effective data model for organizing such information, compared to traditional methods, often based on relational models, which struggle to efficiently represent interlinked data, such as references within laws, hindering efficient knowledge discovery. A paradigm shift in modeling legislative data is already ongoing with the adoption of common international standards, predominantly XML-based, such as Akoma Ntoso (AKN) and the Legal Knowledge Interchange Format, which aim to capture fundamental aspects of laws shared across different legislations and simplify the task of creating Knowledge Graphs through the use of XML tags and identifiers. However, to enable advanced analysis and data discovery within these KGs, it is necessary to carefully check, complement, and enrich KG nodes and edges with properties, either metadata or additional derived knowledge, that enhance the quality and utility of the model, for instance, by leveraging the capabilities of state-of-the-art Large Language Models. In this paper, we present an ETL pipeline for modeling and querying the Italian legislation in a Knowledge Graph, by adopting the property graph model and the AKN standard implemented in the Italian system. The property graph model offers a good compromise between knowledge representation and the possibility of performing graph analytics, which we consider essential for enabling advanced pattern detection. Then, we enhance the KG with valuable properties by employing carefully fine-tuned open-source LLMs, i.e., BERT and Mistral-7B models, which enrich and augment the quality of the KG, allowing in-depth analysis of legislative data.}
}
@article{SUN2025104866,
title = {A scoping review of natural language processing in addressing medically inaccurate information: Errors, misinformation, and hallucination},
journal = {Journal of Biomedical Informatics},
volume = {169},
pages = {104866},
year = {2025},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2025.104866},
url = {https://www.sciencedirect.com/science/article/pii/S1532046425000954},
author = {Zhaoyi Sun and Wen-Wai Yim and Özlem Uzuner and Fei Xia and Meliha Yetisgen},
keywords = {Natural language processing, Inaccurate information, Medical errors, Misinformation, Hallucination, Scoping review},
abstract = {Objective:
This review aims to explore the potential and challenges of using Natural Language Processing (NLP) to detect, correct, and mitigate medically inaccurate information, including errors, misinformation, and hallucination. By unifying these concepts, the review emphasizes their shared methodological foundations and their distinct implications for healthcare. Our goal is to advance patient safety, improve public health communication, and support the development of more reliable and transparent NLP applications in healthcare.
Methods:
A scoping review was conducted following PRISMA-ScR guidelines, analyzing studies from 2020 to 2024 across five databases. Studies were selected based on their use of NLP to address medically inaccurate information and were categorized by topic, tasks, document types, datasets, models, and evaluation metrics.
Results:
NLP has shown potential in addressing medically inaccurate information on the following tasks: (1) error detection (2) error correction (3) misinformation detection (4) misinformation correction (5) hallucination detection (6) hallucination mitigation. However, challenges remain with data privacy, context dependency, and evaluation standards.
Conclusion:
This review highlights the advancements in applying NLP to tackle medically inaccurate information while underscoring the need to address persistent challenges. Future efforts should focus on developing real-world datasets, refining contextual methods, and improving hallucination management to ensure reliable and transparent healthcare applications.}
}
@article{FAN2024103646,
title = {CuPe-KG: Cultural perspective–based knowledge graph construction of tourism resources via pretrained language models},
journal = {Information Processing & Management},
volume = {61},
number = {3},
pages = {103646},
year = {2024},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103646},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324000062},
author = {Zhanling Fan and Chongcheng Chen},
keywords = {Knowledge graph, Pretrained language models, Cultural tourism, Cultural type, ChatGPT, Travel intelligence},
abstract = {Tourism knowledge graphs lack cultural content, limiting their usefulness for cultural tourists.This paper presents the development of a cultural perspective-based knowledge graph (CuPe-KG). We evaluated fine-tuning ERNIE 3.0 (FT-ERNIE) and ChatGPT for cultural type recognition to strengthen the relationship between tourism resources and cultures. Our investigation used an annotated cultural tourism resource dataset containing 2,745 items across 16 cultural types. The results showed accuracy scores for FT-ERNIE and ChatGPT of 0.81 and 0.12, respectively, with FT-ERNIE achieving a micro-F1 score of 0.93, a 26 percentage point lead over ChatGPT's score of 0.67. These underscore FT-ERNIE's superior performance (the shortcoming is the need to annotate data) while highlighting ChatGPT's limitations because of insufficient Chinese training data and lower identification accuracy in professional knowledge. A novel ontology was designed to facilitate the construction of CuPe-KG, including elements such as cultural types, historical figures, events, and intangible cultural heritage. CuPe-KG effectively addresses cultural tourism visitors’ information retrieval needs.}
}
@article{LIU2025106482,
title = {Automated knowledge graph-based risk assessment for fall-from-height accidents in construction},
journal = {Automation in Construction},
volume = {179},
pages = {106482},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106482},
url = {https://www.sciencedirect.com/science/article/pii/S0926580525005229},
author = {Qiong Liu and Yuexiong Ding and Xiaowei Luo},
keywords = {Fall-from-height, Accident reports, Knowledge graph, Large language models, Risk factor analysis},
abstract = {Fall-from-height (FFH) accidents remain a leading cause of fatalities in the construction industry. To systematically extract and analyze risk factors from unstructured FFH accident reports, this paper employed large language models (LLMs) to enable zero-shot automated fall-from-height knowledge graph (FFHKG) construction. By clustering FFHKG entities to merge semantically similar factors, a weighted complex network is formed, enabling topological analysis for quantitative risk assessment. A case study on 1097 FFH accident reports validates the proposed framework. Results demonstrate that GPT-4o achieves high extraction accuracy, with an F1 score of 0.94 in named entity recognition and a precision of 0.90 in relationship extraction. Key risk factors, such as poor safety management, lack of training, insufficient edge protection, etc., are quantitatively identified across multiple perspectives. Observable unsafe behaviors are also detected, offering insights for behavior-based safety monitoring. The proposed framework provides a data-driven solution for more effective safety management on construction sites.}
}
@article{XUE202243,
title = {Structured encryption for knowledge graphs},
journal = {Information Sciences},
volume = {605},
pages = {43-70},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.05.015},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522004352},
author = {Yujie Xue and Lanxiang Chen and Yi Mu and Lingfang Zeng and Fatemeh Rezaeibagha and Robert H. Deng},
keywords = {Structured encryption, Knowledge search, Knowledge graph, Multi-relational graph, Property graph},
abstract = {We investigate the problem of structured encryption (STE) for knowledge graphs (KGs) where the knowledge of data can be efficiently and privately queried. Presently, the application of natural language processing (NLP) for knowledge-based search is gradually emerging. Compared with the traditional search based only on keywords of documents—symmetric searchable encryption (SSE), the knowledge-based search system transforms the latent knowledge contained in documents into a semantic network as a knowledge base, which greatly improves the accuracy and relevance of search results. In order to develop a knowledge-based search, the contents of documents are analyzed and extracted using KG techniques (e.g. multi-relational graph (MG) and property graph (PG)), and then all encrypted nodes and edges in a KG constitute the entire index table and database. This paper proposes the first STE for KGs with CQA2-security to search on protected knowledge, where KGs include MGs and PGs. In general, the latter is more complex than the former, but it can represent more abundant knowledge. Experimental results show that the index construction time of our schemes is about 1.9s and the query time is about 190 ms. Our sensitivity analysis shows that the performance of our proposed schemes is greatly influenced by the number of edges and nodes, but less by the number of properties.}
}
@article{ZHAO2025106875,
title = {Knowledge graph construction and knowledge discovery for porphyry copper deposits},
journal = {Ore Geology Reviews},
pages = {106875},
year = {2025},
issn = {0169-1368},
doi = {https://doi.org/10.1016/j.oregeorev.2025.106875},
url = {https://www.sciencedirect.com/science/article/pii/S0169136825004354},
author = {Mo-Lei Zhao and Zhen-Jie Zhang and Jie Yang and Zhang-Bing Zhou and Deng Zhao and Yuan-Zhi Zhou and Yu-Xin Ye and Qiu-Ming Cheng},
keywords = {Knowledge Graph, Knowledge Discovery, Knowledge Graph Embedding, Porphyry Copper Deposits},
abstract = {Porphyry Cu deposits (PCDs) are the most important deposit types for essential metals of copper, molybdenum, and gold. However, they are becoming increasingly harder to discover after decades of large-scale worldwide exploration. How to build a more convincing mineral system model, by accurately identifying all indispensable theoretical components from ore deposit formation processes, to guide the PCDs exploration during the modern era of diminishing returns on fixed exploration budgets, is the most urgent work. Knowledge graph (KG), by constructing a graph database to store the concepts and entities and the complex relationships between them, has robust knowledge discovery (KD) ability to obtain new knowledge and conclusions from existing data. With the continuous publication of massive research and literature, studies on PCDs have entered a big data era. Therefore, in this study, we try to use the KG and the following KD to develop a robust workflow for knowledge graph construction and discovery, which facilitates the identification of some indispensable and previously overlooked porphyry mineral system components from published articles, preparing for linking with spatial data and exploration budgets. We provide not only a detailed roadmap for KG construction and KD of PCDs, but also a typical example of the construction and application of KG in geosciences.}
}
@article{WANG20254141,
title = {Label-Guided Scientific Abstract Generation with a Siamese Network Using Knowledge Graphs},
journal = {Computers, Materials and Continua},
volume = {83},
number = {3},
pages = {4141-4166},
year = {2025},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2025.062806},
url = {https://www.sciencedirect.com/science/article/pii/S1546221825004369},
author = {Haotong Wang and Yves Lepage},
keywords = {Graph-to-text generation, knowledge graph, siamese network, scientific abstract},
abstract = {Knowledge graphs convey precise semantic information that can be effectively interpreted by neural networks, and generating descriptive text based on these graphs places significant emphasis on content consistency. However, knowledge graphs are inadequate for providing additional linguistic features such as paragraph structure and expressive modes, making it challenging to ensure content coherence in generating text that spans multiple sentences. This lack of coherence can further compromise the overall consistency of the content within a paragraph. In this work, we present the generation of scientific abstracts by leveraging knowledge graphs, with a focus on enhancing both content consistency and coherence. In particular, we construct the ACL Abstract Graph Dataset (ACL-AGD) which pairs knowledge graphs with text, incorporating sentence labels to guide text structure and diverse expressions. We then implement a Siamese network to complement and concretize the entities and relations based on paragraph structure by accomplishing two tasks: graph-to-text generation and entity alignment. Extensive experiments demonstrate that the logical paragraphs generated by our method exhibit entities with a uniform position distribution and appropriate frequency. In terms of content, our method accurately represents the information encoded in the knowledge graph, prevents the generation of irrelevant content, and achieves coherent and non-redundant adjacent sentences, even with a shared knowledge graph.}
}
@article{DUAN2024131010,
title = {Urban flood vulnerability Knowledge-Graph based on remote sensing and textual bimodal data fusion},
journal = {Journal of Hydrology},
volume = {633},
pages = {131010},
year = {2024},
issn = {0022-1694},
doi = {https://doi.org/10.1016/j.jhydrol.2024.131010},
url = {https://www.sciencedirect.com/science/article/pii/S0022169424004050},
author = {Chenfei Duan and Xiazhong Zheng and Rong Li and Zhixia Wu},
keywords = {Remote sensing and text bimodal data, Urban flooding, Vulnerability, Index system, Knowledge graph},
abstract = {Flood disasters inflict extensive, serious damage on cities and society, significantly constraining urban construction and development. There is an urgent demand to reduce urban flood vulnerability, to explore the evolution mechanism of urban flood vulnerability, and to guide the construction and improvement of urban flood control resilience. An objective and accurate knowledge graph of urban flood vulnerability intuitively, quantitatively, and conveniently expresses the logical relationship between vulnerability indexes. This provides a theoretical and data-based foundation for enhancing urban flood resistance and improving the safety of urban flood control and drainage systems. To address this issue, first, the fusion extraction of text and remote sensing dual-mode data is achieved through technical means such as neural networks. Second, by using multiclass natural language processing (NLP) models, we create an objective index system for urban flood vulnerability that avoids subjective human influence. Finally, we construct an objective weight model group, calculate weights, and then, we establish a vulnerability knowledge graph. The results indicate that (1) by utilizing multidimensional remote sensing images and by adopting the robotic satellite (RoboSat) semantic segmentation model, we achieve high-precision extraction of the remote sensing parameters, such as those for urban roads, terrain, and buildings. Thus, we successfully transform remote sensing data into text data (accuracy is approximately 1 m). (2) We have confirmed the effectiveness of the subjective–objective combined weight method. (3) We introduce a novel approach to create an urban flood vulnerability index system based on bimodal objective data fusion. (4) Utilizing the flood vulnerability knowledge graph, we assess vulnerability levels within the primary urban areas of Zhengzhou City, and we propose governance strategies tailored to the current vulnerability status of each district.}
}
@article{HE2025,
title = {Chinese relation extraction for constructing satellite frequency and orbit knowledge graph: a survey},
journal = {Digital Communications and Networks},
year = {2025},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2025.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S2352864825000665},
author = {Yuanzhi He and Zhiqiang Li and Zheng Dou},
keywords = {Relation extraction, Information extraction, Distant supervision, Parsing tree, Joint entity-relation extraction},
abstract = {As Satellite Frequency and Orbit (SFO) constitute scarce natural resources, constructing a Satellite Frequency and Orbit Knowledge Graph (SFO-KG) becomes crucial for optimizing their utilization. In the process of building the SFO-KG from Chinese unstructured data, extracting Chinese entity relations is the fundamental step. Although Relation Extraction (RE) methods in the English field have been extensively studied and developed earlier than their Chinese counterparts, their direct application to Chinese texts faces significant challenges due to linguistic distinctions such as unique grammar, pictographic characters, and prevalent polysemy. The absence of comprehensive reviews on Chinese RE research progress necessitates a systematic investigation. A thorough review of Chinese RE has been conducted from four methodological approaches: pipeline RE, joint entity-relation extraction, open domain RE, and multimodal RE techniques. In addition, we further analyze the essential research infrastructure, including specialized datasets, evaluation benchmarks, and competitions within Chinese RE research. Finally, the current research challenges and development trends in the field of Chinese RE were summarized and analyzed from the perspectives of ecological construction methods for datasets, open domain RE, N-ary RE, and RE based on large language models. This comprehensive review aims to facilitate SFO-KG construction and its practical applications in SFO resource management.}
}
@article{SONG2025105706,
title = {Knowledge graph-based alarm management in petrochemical enterprises: A study on fusion and analysis of multi-source heterogeneous information},
journal = {Journal of Loss Prevention in the Process Industries},
volume = {97},
pages = {105706},
year = {2025},
issn = {0950-4230},
doi = {https://doi.org/10.1016/j.jlp.2025.105706},
url = {https://www.sciencedirect.com/science/article/pii/S0950423025001640},
author = {Xiaomiao Song and Fabo Yin and Dongfeng Zhao},
keywords = {Petrochemical enterprises, Alarm management, Knowledge graph, Ontology},
abstract = {In response to the increasing emphasis on alarm management in the petrochemical industry, there has been an explosive growth in relevant information. However, this information is often scattered across different systems and databases, stored in various forms such as documents, tables, and images, making it challenging to uniformly store, share, and utilize multi-source heterogeneous information. This commonly leads to the problem of “Information Islands.” In order to effectively leverage knowledge in the field of alarm management in the petrochemical industry and overcome the challenge of non-interoperable information, a method for fusing multi-source heterogeneous information in petrochemical enterprise alarm management based on knowledge graph is proposed. This method aims to standardize the management of alarm-related information and achieve information fusion. Initially, the approach utilizes data from petrochemical enterprises and publicly available data in the field of alarm management to establish both local and global ontologies. Subsequently, mapping algorithms are designed to achieve a more accurate construction of the hybrid ontology. Based on this foundation, a knowledge graph for alarm management in the petrochemical industry is established. Additionally, corresponding modules for information storage and retrieval are developed. Through the application demonstration using real alarm management information from a petrochemical enterprise, the results indicate that the proposed method for fusing multi-source heterogeneous information in petrochemical enterprise alarm management can effectively achieve information fusion.}
}
@article{SUN2022103749,
title = {Effective design knowledge abstraction from Chinese patents based on a meta-model of the patent design knowledge graph},
journal = {Computers in Industry},
volume = {142},
pages = {103749},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2022.103749},
url = {https://www.sciencedirect.com/science/article/pii/S0166361522001464},
author = {Yindi Sun and Wei Liu and Guozhong Cao and Qingjin Peng and Jianjie Gu and Jiaming Fu},
keywords = {Design knowledge, Natural language processing, Chinese patent text, Knowledge graph, Design for patentability},
abstract = {Patent document is one of the most important sources of knowledge for engineering design. Design for patentability has become increasingly important for the success of product in the competitive market. However, language differences hinder the effective extraction of design knowledge from patent databases of different countries using existing patent analysis methods. This paper proposes a new approach to build the patent design knowledge graph to extract useful design information from Chinese patent texts by using natural language processing techniques. A meta-model of the patent design knowledge graph is built based on the language characteristics of Chinese. The proposed method is verified using randomly selected patent text for the effective extraction of Chinese patent design knowledge. The feasibility of the proposed method is further tested to support design of a new storage device for the patentability.}
}
@article{DREGER20251221,
title = {Large language models for knowledge graph extraction from tables in materials science},
journal = {Digital Discovery},
volume = {4},
number = {5},
pages = {1221-1231},
year = {2025},
issn = {2635-098X},
doi = {https://doi.org/10.1039/d4dd00362d},
url = {https://www.sciencedirect.com/science/article/pii/S2635098X25000609},
author = {Max Dreger and Kourosh Malek and Michael Eikerling},
abstract = {Research in materials science increasingly harnesses machine learning (ML) models. These models are trained with experimental or theoretical data, the quality of their output hinges on the data's quantity and quality. Improving data quality and accessibility necessitates advanced data management solutions. Today, data are often stored in non-standardized table formats that lack interoperability, accessibility and reusability. To address this issue, we present a semi-automated data ingestion pipeline that transforms R&D tables into knowledge graphs. Utilizing large language models and rule-based feedback loops, our pipeline transforms tabular data into graph structures. The proposed process consists of entity recognition and relationship extraction. It facilitates better data interoperability and accessibility, by streamlining data integration from various sources. The pipeline is integrated into a platform harboring a graph database as well as semantic search capabilities.}
}
@article{HOU2025112622,
title = {Low-resource knowledge graph completion based on knowledge distillation driven by large language models},
journal = {Applied Soft Computing},
volume = {169},
pages = {112622},
year = {2025},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.112622},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624013966},
author = {Wenlong Hou and Weidong Zhao and Ning Jia and Xianhui Liu},
keywords = {Knowledge graph completion, Knowledge reasoning, Link prediction, Large language models},
abstract = {Knowledge graph completion (KGC) refines the existing knowledge graph (KG) by predicting missing entities or relations. Existing methods are mainly based on embeddings or texts but only perform better with abundant labeled data. Hence, KGC in resource-constrained settings is a significant problem, which faces challenges of data imbalance across relations and lack of relation label semantics. Considering that Large Language Models (LLMs) demonstrate powerful reasoning and generation capabilities, this work proposes an LLM-driven Knowledge Graph Completion Distillation (KGCD) model to address low-resource KGC. A two-stage framework is developed, involving teacher-student distillation by using LLM to improve reasoning, followed by fine-tuning on real-world low-resource datasets. To deal with data imbalance, a hybrid prompt design for LLM is proposed, which includes rethink and open prompts. Furthermore, a virtual relation label generation strategy enhances the model’s understanding of triples. Extensive experiments on three benchmarks have shown that KGCD’s effectiveness for low-resource KGC, achieving improvements in Mean Reciprocal Rank (MRR) by 11% and Hits@1 by 10% on the WN18, MRR by 10% and Hits@1 by 14% on the WN18RR, and MRR by 12% and Hits@1 by 11% on the YAGO3-10.}
}
@article{KINCL2025127888,
title = {Comprehensive benchmarking of knowledge graph embeddings methods for Android malware detection},
journal = {Expert Systems with Applications},
volume = {288},
pages = {127888},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127888},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425015106},
author = {Jan Kincl and Tome Eftimov and Adam Viktorin and Roman Šenkeřík and Tanja Pavleska},
keywords = {Mobile android security, Knowledge graphs embeddings, Machine learning, Android malware detection},
abstract = {The rising popularity and open-source model of the Android operating system has made it a main target for attackers creating malware applications. With the mobile industry being an expanding device ecosystem, there is a critical need for developing effective methods to protect against mobile malware. Recognizing the latest approaches and their limitations, we have conducted a comprehensive empirical analysis on the applicability of knowledge graphs for malware detection in view of the influence of the scoring functions, the vector dimension, the stability of the obtained results, the performance of the individual classifiers, and other important time dependencies. In addition, we propose a knowledge-graph based method aimed at improving the quality of classification input data, while offering greater interfacing capabilities with external knowledge and lower computational complexity. The proposed method offers a new perspective on working with Android malware, demonstrating a unique data processing pipeline for malware sample identification and encouraging further innovation in the field. Our findings demonstrate that knowledge graph representation is not only feasible, but also provides well-performing results, remaining competitive with state-of-the-art approaches.}
}
@article{LI20254206,
title = {Knowledge Graph-Based Few-Shot Learning for Label of Medical Imaging Reports},
journal = {Academic Radiology},
volume = {32},
number = {7},
pages = {4206-4220},
year = {2025},
issn = {1076-6332},
doi = {https://doi.org/10.1016/j.acra.2025.02.045},
url = {https://www.sciencedirect.com/science/article/pii/S1076633225001898},
author = {Tiancheng Li and Yuxuan Zhang and Deyu Su and Ming Liu and Mingxin Ge and Linyu Chen and Chuanfu Li and Jin Tang},
keywords = {Few-shot learning, Knowledge graph, Radiology reports, Natural language processing},
abstract = {Background
The application of artificial intelligence (AI) in the field of automatic imaging report labeling faces the challenge of manually labeling large datasets.
Purpose
To propose a data augmentation method by using knowledge graph (KG) and few-shot learning.
Methods
A KG of lumbar spine X-ray images was constructed, and 2000 data were annotated based on the KG, which were divided into training, validation, and test sets in a ratio of 7:2:1. The training dataset was augmented based on the synonym/replacement attributes of the KG and was the augmented data was input into the BERT (Bidirectional Encoder Representations from Transformers) model for automatic annotation training. The performance of the model under different augmentation ratios (1:10, 1:100, 1:1000) and augmentation methods (synonyms only, replacements only, combination of synonyms and replacements) was evaluated using the precision and F1 scores. In addition, with the augmentation ratio was fixed, iterative experiments were performed by supplementing the data of nodes that perform poorly in the validation set to further improve model’s performance.
Results
Prior to data augmentation, the precision was 0.728 and the F1 score was 0.666. By adjusting the augmentation ratio, the precision increased from 0.912 at a 1:10 augmentation ratio to 0.932 at a 1:100 augmentation ratio (P<.05), while F1 score improved from 0.853 at a 1:10 augmentation ratio to 0.881 at a 1:100 augmentation ratio (P<.05). Additionally, the effectiveness of various augmentation methods was compared at a 1:100 augmentation ratio. The augmentation method that combined synonyms and replacements (F1=0.881) was superior to the methods that only used synonyms (F1=0.815) and only used replacements (F1=0.753) (P<.05). For nodes that exhibited suboptimal performance on the validation set, supplementing the training set with target data improved model performance, increasing the average F1 score to 0.979 (P<.05).
Conclusion
Based on the KG, this study trained an automatic labeling model of radiology reports using a few-shot data set. This method effectively reduces the workload of manual labeling, improves the efficiency and accuracy of image data labeling, and provides an important research strategy for the application of AI in the domain of automatic labeling of image reports.}
}
@article{CUI2025104861,
title = {A review on knowledge graphs for healthcare: Resources, applications, and promises},
journal = {Journal of Biomedical Informatics},
volume = {169},
pages = {104861},
year = {2025},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2025.104861},
url = {https://www.sciencedirect.com/science/article/pii/S1532046425000905},
author = {Hejie Cui and Jiaying Lu and Ran Xu and Shiyu Wang and Wenjing Ma and Yue Yu and Shaojun Yu and Xuan Kan and Chen Ling and Liang Zhao and Zhaohui S. Qin and Joyce C. Ho and Tianfan Fu and Jing Ma and Mengdi Huai and Fei Wang and Carl Yang},
keywords = {Knowledge graph, Healthcare, language models, Multimodality, Interpretable AI},
abstract = {Objective:
This comprehensive review aims to provide an overview of the current state of Healthcare Knowledge Graphs (HKGs), including their construction, utilization models, and applications across various healthcare and biomedical research domains.
Methods:
We thoroughly analyzed existing literature on HKGs, covering their construction methodologies, utilization techniques, and applications in basic science research, pharmaceutical research and development, clinical decision support, and public health. The review encompasses both model-free and model-based utilization approaches and the integration of HKGs with large language models (LLMs).
Results:
We searched Google Scholar for relevant papers on HKGs and classified them into the following topics: HKG construction, HKG utilization, and their downstream applications in various domains. We also discussed their special challenges and the promise for future work.
Discussion:
The review highlights the potential of HKGs to significantly impact biomedical research and clinical practice by integrating vast amounts of biomedical knowledge from multiple domains. The synergy between HKGs and LLMs offers promising opportunities for constructing more comprehensive knowledge graphs and improving the accuracy of healthcare applications.
Conclusions:
HKGs have emerged as a powerful tool for structuring medical knowledge, with broad applications across biomedical research, clinical decision-making, and public health. This survey serves as a roadmap for future research and development in the field of HKGs, highlighting the potential of combining knowledge graphs with advanced machine learning models for healthcare transformation.}
}
@article{HASSAN2025101845,
title = {Leveraging Hybrid Natural Language Processing Techniques for Large-Scale Pulmonary Embolism Identification},
journal = {JACC: Advances},
pages = {101845},
year = {2025},
issn = {2772-963X},
doi = {https://doi.org/10.1016/j.jacadv.2025.101845},
url = {https://www.sciencedirect.com/science/article/pii/S2772963X25002649},
author = {Syed Moin Hassan and Ruben Mylvaganam and Tekle Didebulidze and Malaika Khalid and Pietro Nardelli and Gregory Piazza and Ruben {San Jose Estepar} and Michael J. Cuttica and Shelsey Johnson and Nam Dao and Raul {San Jose Estepar} and George R. Washko and Farbod Nicholas Rahaghi},
keywords = {1, Pulmonary embolism detection, 2, Machine learning radiology classification, 3, Hybrid NLP pipeline, 4, Automated PE identification, 5, Clinical text mining, 6, Medical document classification},
abstract = {Abstract:
Background
A critical gap persists in the diagnosis and management of pulmonary embolism (PE) despite contemporary medical advances, necessitating large-scale electronic health record analysis through hybrid natural language processing methodologies to elucidate its pathophysiology and optimize clinical interventions.
Objective
To develop and validate a hybrid natural language processing (NLP) pipeline combining machine learning (ML) and rule-based (RB) techniques for accurate identification of pulmonary embolism (PE) cases from large-scale radiology report datasets.
Methods
The hybrid NLP pipeline consisted of a ML algorithm trained on 1,040 CT pulmonary angiogram (CTPA) reports from Brigham and Women's Hospital (BWH), with 80% used for training and 20% for testing. The pipeline was then validated on a larger dataset of 49,611 radiology reports from the Mass General Brigham (MGB) healthcare system. Performance was evaluated using accuracy, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV).
Results
The machine learning model achieved an accuracy of 91 % and AUC of 0.90 on the BWH testing dataset. When deployed on the larger MGB dataset, the model's accuracy decreased to 85%. Iterative application of the rule-based algorithm improved the model's accuracy to 94.8%, sensitivity to 96.4%, specificity to 93.2%, PPV to 93.0%, and NPV to 96.5% on the MGB dataset.
Conclusion
The hybrid NLP approach required less training data than pure ML models and demonstrated high performance across diverse healthcare settings. A hybrid NLP pipeline can efficiently and accurately identify PE cases from radiology reports and could be deployed for broader PE-focused research and clinical surveillance if similarly validated in external datasets.}
}
@article{ZHU2023105074,
title = {Autonomous complex knowledge mining and graph representation through natural language processing and transfer learning},
journal = {Automation in Construction},
volume = {155},
pages = {105074},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.105074},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523003345},
author = {Xiaofeng Zhu and Haijiang Li and Tengxiang Su},
keywords = {Knowledge mining, Natural language processing (NLP), Transfer learning, Knowledge modelling, Regulation document},
abstract = {Regulatory documents play a significant role in securing engineering project quality, standard process management and long-term sustainable developments. With the digitisation of knowledge in the AEC industry, the demand for automated knowledge mining has emerged when confronted with substantial regulations. However, the current interpretation approaches for regulatory documents are still mostly labour-intensive and flawed in complex knowledge. Based on transfer learning (BERT) and natural language processing (e.g., NLP-Syntactic Parsing), this paper proposes a fully automated knowledge mining framework to convert complex knowledge in textual regulations to graph-based knowledge representations. The framework uses a BERT-based engine to extract clauses from regulation documents through fine-tuning with the self-developed domain dataset. A constituent extractor is developed to process the provisions with complex knowledge and extract constituents. A knowledge modelling engine integrates the extracted constituents into a graph-based regulation knowledge model, which can be queried, visualised, and directly applied to downstream applications. The outcome has demonstrated promising performance in complex knowledge mining and knowledge graph modelling based on ISO 19650 case study. This research can effectively convert textual regulation documents to their counterpart regulatory knowledge base, contributing to automated knowledge acquisition and multi-domain knowledge fusion toward regulation digitalization.}
}
@article{TSANEVA2025104145,
title = {Knowledge graph validation by integrating LLMs and human-in-the-loop},
journal = {Information Processing & Management},
volume = {62},
number = {5},
pages = {104145},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2025.104145},
url = {https://www.sciencedirect.com/science/article/pii/S030645732500086X},
author = {Stefani Tsaneva and Danilo Dessì and Francesco Osborne and Marta Sabou},
keywords = {Knowledge graph validation, Large language models, Hybrid human-AI workflows},
abstract = {Ensuring the quality of knowledge graphs (KGs) is crucial for the success of the intelligent applications they support. Recent advances in large language models (LLMs) have demonstrated human-level performance across various tasks, raising the question of their potential for KG validation. In this work, we explore the role of LLMs in human-centric KG validation workflows, examining different collaboration strategies between LLMs and domain experts. We propose and evaluate nine distinct approaches, ranging from fully automated validation to hybrid methods that combine expert oversight with AI assistance. These workflows are tested within a real-world KG construction pipeline used to generate the Computer Science Knowledge Graph (CS-KG), a large-scale resource designed to support scientometric tasks such as trend forecasting and hypothesis generation. CS-KG comprises 41 million statements represented as 350 million triples within the Computer Science domain. Our findings show that integrating LLMs into the CS-KG verification process enhances precision by 12%, improving alignment with expert-level validation. However, this comes at the cost of recall, resulting in a 5% decrease in the overall F1 score. In contrast, a hybrid approach which involves both human-in-the-loop and LLM modules, yields the best overall results, improving F1 score by 5% with minimal human involvement.}
}
@article{PAN2025114094,
title = {Leveraging temporal validity of rules via LLMs for enhanced temporal knowledge graph reasoning},
journal = {Knowledge-Based Systems},
volume = {327},
pages = {114094},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.114094},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125011396},
author = {Qihong Pan and Limin Yao and Guojiang Shen and Xiao Han and Yichuan Chen and Xiangjie Kong},
keywords = {Temporal knowledge graph reasoning, Temporal rule learning, Large language models (LLMs), Temporal validity assessment, LLM-guided temporal reasoning},
abstract = {Temporal Knowledge Graphs (TKGs) are critical for understanding dynamic real-world scenarios. However, incorporating temporal validity into temporal knowledge graph reasoning (TKGR) is a critical yet often overlooked challenge. This is largely because most existing methods rely on static representations or predefined temporal windows, which fail to capture the dynamic and context-dependent nature of rule applicability over time. It can result in inaccurate predictions and a limited understanding of the evolving relationships within TKGs. Large Language Models (LLMs) are well-suited for addressing this challenge due to their inherent ability to capture sequential dependencies and reason over contextual relationships in temporal data. As a solution, we propose TV-LLM, a novel framework that primarily utilizes LLMs to guide the generation of temporal rules, while integrating rule-based learning and graph-based modeling to enhance temporal reasoning. The framework features LLM-guided rule generation with explicit modeling of temporal validity to dynamically determine the active time ranges of rules. Additionally, TV-LLM combines retrieved temporal facts with graph-based candidate scores to construct structured prompts, effectively leveraging both explicit knowledge and LLM reasoning capabilities. Extensive experiments on multiple benchmark datasets demonstrate that TV-LLM achieves competitive performance, with Mean Reciprocal Rank (MRR) gains of 1.2 % to 2.3 % and Hit@1 improvements of 1.9 % to 4.5 % over strong baselines on the ICEWS benchmarks. These results underline the importance of modeling temporal validity and motivate further exploration of structured knowledge integration in temporal reasoning tasks. Future work may explore real-time applications and improve the scalability of TV-LLM for large-scale temporal graphs.}
}
@article{WANG2026100223,
title = {A position-aware attention model based on double-level contrastive learning for hyper-relational knowledge graph representation in emergency management},
journal = {Journal of Safety Science and Resilience},
volume = {7},
number = {1},
pages = {100223},
year = {2026},
issn = {2666-4496},
doi = {https://doi.org/10.1016/j.jnlssr.2025.100223},
url = {https://www.sciencedirect.com/science/article/pii/S266644962500057X},
author = {Xinzhi Wang and Weijian Zhu and Jiang Kai and Xiangfeng Luo and Jianqiang Huang},
keywords = {Emergency management, Link prediction, Hyper-relational knowledge graph, Contrastive learning, Position information},
abstract = {Effective emergency management relies on timely risk identification and decision-making, wherein natural language processing plays a vital role. Hyper-relational knowledge graph (HKG) representation, which embeds entities and their complex relations into latent space, provides a strong foundation for supporting emergency responses. Existing methods consider either inter-entity or inter-fact dependencies, leading to the loss of interaction information at the unconsidered level (fact level or entity level). To address the above issue, we propose a position-aware attention model based on dual-level contrastive learning (PDCL) for HKG representation. First, the complete and co-occurrence graphs were constructed and encoded using different graph convolutional networks, generating different embedding views for entities and facts. Second, entity-level and fact-level contrastive objectives were designed to enhance information exchange between the two levels in a self-supervised manner. Finally, a linear transformation corresponding to the ordinal information of each element was used to integrate positional constraints into the representation of the HKG. Experimental results for three benchmark datasets showed that the PDCL model outperformed existing state-of-the-art methods. Especially, MRR and Hits@1 values could be improved by up to 1.8% and 3.3%, respectively.}
}
@article{YANG2025110630,
title = {A knowledge graph for the vulnerability of construction safety system in megaprojects based on accident inversion},
journal = {Engineering Applications of Artificial Intelligence},
volume = {150},
pages = {110630},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.110630},
url = {https://www.sciencedirect.com/science/article/pii/S095219762500630X},
author = {Yingliu Yang and Pengcheng Xiang},
keywords = {Text mining, Association rules, Complex network, Knowledge graph, Construction safety systems in megaprojects, Vulnerability},
abstract = {The increasing vulnerability of construction safety systems in megaprojects (CSSMs) poses significant challenges to their safety management and control. To address this obstacle, this study retrodicts the accidents based on text mining using the Bidirectional Encoder Repre-sentations from Transformers Topic (BER-Topic) model to uncover topic and topic words related to the vulnerabilities of CSSMs. The vulnerability indicator system (VIS) is established by considering the exposure, sensitivity, and adaptability of the vulnerability of CSSMs. Subsequently, an improved Decision-making Trial and Evaluation Laboratory (DEMATEL) method based on association rules is proposed to reduce the subjectivity in assigning weights to vulnerability indicators, and a topological network based on complex network is constructed to identify the characteristics of VIS. Based on this, a knowledge graph of vulnerabilities in CSSMs is developed. Finally, taking into account the occurrence probability and the actual losses incurred of vulnerability indicators, a vulnerability assessment model for CSSMs is proposed. The research findings are: 1) Based on the BER-Topic model, 32 topics and topic words related to the vulnerability of CSSMs are mined. 2) A VIS for CSSMs is constructed, including 42 indicators across three dimensions of exposure (19), sensitivity (14), and adaptability (9), involving four aspects: humans, machines, environment, and management. 3) The key points for vulnerability management and control in CSSMs are Inaccurate implementation of geological remediation plans, Rusting of connecting components, and Unlicensed personnel operating, among others, which have strong intermediary roles.}
}
@article{BAI2026102503,
title = {Time-Aware Complex Question Answering over Temporal Knowledge Graph},
journal = {Data & Knowledge Engineering},
volume = {161},
pages = {102503},
year = {2026},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2025.102503},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X25000989},
author = {Luyi Bai and Tongyue Zhang and Guangchen Feng},
keywords = {Attention mechanism, Question answering, Temporal knowledge graph, Temporal knowledge graph embedding},
abstract = {Knowledge Graph Question Answering (KGQA) is a crucial topic in Knowledge Graphs (KGs), with the objective of retrieving the corresponding facts from KGs to answer given questions. In practical applications, facts in KGs usually have time constraints, thus, question answering on Temporal Knowledge Graphs (TKGs) has attracted extensive attention. Existing Temporal Knowledge Graph Question Answering (TKGQA) methods focus on dealing with complex questions involving multiple facts, and mainly face two challenges. First, these methods only consider matching questions with facts in TKGs to identify the answer, ignoring the temporal order between different facts, which makes it challenging to solve the questions involving temporal order. Second, they usually focus on the representation of the question text while neglecting the rich semantic information within the questions, which leads to certain limitations in understanding question. To address the above challenges, this research proposes a model named Time-Aware Complex Question Answering (TA-CQA). Specifically, we extend the Temporal Knowledge Graph Embedding (TKGE) model by incorporating temporal order information into the embedding vectors, ensuring that the model can distinguish the temporal order of different facts. To enhance the semantic representation of the question, we integrate question information using attention mechanism and learnable encoder. Different from the previous TKGQA methods, we propose time relevance measurement to further enhance the accuracy of answer prediction by better capturing the correlation between question information and time information. Multiple sets of experiments on CronQuestions and TimeQuestions demonstrate our model’s superior performance across all question types. In particular, for complex questions involving multiple facts, the hit@1 values are increased by 3.2% and 3.5% respectively.}
}
@article{MISHRA2025104045,
title = {PageLLM: Incremental approach for updating a Security Knowledge Graph by using Page ranking and Large language model},
journal = {Information Processing & Management},
volume = {62},
number = {3},
pages = {104045},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.104045},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324004047},
author = {Chinmaya Mishra and Himangshu Sarma and Saravanan M.},
keywords = {Security knowledge graph, Knowledge graph, Knowledge representation learning, Page ranking, Embedding, Generative AI, Large language models (LLMs), Static knowledge graph (SKG), Incremental knowledge graph (IKG), Full knowledge graph (FKG)},
abstract = {Due to increase in cyber crime and evolution of sophisticated tools and techniques, Threat Intelligence plays a critical role. It helps defenders to stay ahead of attackers by developing the right defense mechanism to invade those attacks. In this regards security knowledge graph plays a critical role which can be used to signify complex entities and their relationship in a graphical structure. Further projecting those entities and relationships in to the lower dimension using several embedding techniques such as TransE help in many down streaming task. The learned embedding can be used to predict new cyber threat which is very helpful for defenders to stay alert and develop necessary weapons to stay ahead of an attack. One of the major challenge security knowledge graph has its dynamic nature of changing intelligence. Active learning can be used to only update the substantial portion of embedding rather than retraining the knowledge graph from scratch which has higher time and space complexity. Also given the rise in generative AI and large language models which are super rich in context, there is a scope of utilizing those for building a robust and good quality security knowledge graph. We will discuss a novel methodology called PageLLM which utilizes page ranking and LLMs to enable active learning in an incremental way and will improve the quality of knowledge graph through enriched context.}
}
@article{ZHU2025106995,
title = {Knowledge graph based question-answering model with subgraph retrieval optimization},
journal = {Computers & Operations Research},
volume = {177},
pages = {106995},
year = {2025},
issn = {0305-0548},
doi = {https://doi.org/10.1016/j.cor.2025.106995},
url = {https://www.sciencedirect.com/science/article/pii/S0305054825000231},
author = {Rui Zhu and Bo Liu and Qiuyu Tian and Ruwen Zhang and Shengxiang Zhang and Yanna Hu and Jiuxin Cao},
keywords = {Subgraph retrieval, Entity disambiguation, Intelligent question-answering},
abstract = {Knowledge graph-based question answering (QA) is a critical domain within natural language processing, aimed at delivering precise and efficient responses to user queries. Current research predominantly focuses on minimizing subgraph sizes to enhance the efficiency and compactness of the search space. However, natural language queries often exhibit ambiguities, and merely reducing subgraph sizes may overlook relevant answer entities. Additionally, redundant relationships among entities in the knowledge graph can adversely affect QA model performance. To address these limitations, this paper introduces a novel QA model that optimizes subgraph retrieval. The proposed model enhances entity linking and subgraph retrieval by leveraging contextual features from both questions and entities. It disambiguates entities using relevant contextual features and refines the search process through entity relation merging and entity ranking strategies. This methodology improves entity recognition and linking, reduces subgraph dimensions, and broadens answer coverage, resulting in substantial improvements in QA performance. Experimental results on the CCKS2019-CKBQA dataset demonstrate the modelś effectiveness, showing an average F1 score improvement of 2.99% over the leading baseline model. Furthermore, the model’s application in the field of ocean engineering underscores its practical utility and significance.}
}
@article{LAKSHIKA2025102451,
title = {ECS-KG: An event-centric semantic knowledge graph for event-related news articles},
journal = {Data & Knowledge Engineering},
volume = {159},
pages = {102451},
year = {2025},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2025.102451},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X25000461},
author = {MVPT Lakshika and HA Caldera and TNK {De Zoysa}},
keywords = {Semantic KG, Event-centric KG, Contextual embedding, Graph attention networks, Temporal graph neural networks},
abstract = {Recent advances in deep learning techniques and contextual understanding render Knowledge Graphs (KGs) valuable tools for enhancing accessibility and news comprehension. Conventional and news-specific KGs frequently lack the specificity for efficient news-related tasks, leading to limited relevance and static data representation. To fill the gap, this study proposes an Event-Centric Semantic Knowledge Graph (ECS-KG) model that combines deep learning approaches with contextual embeddings to improve the procedural and dynamic knowledge representation observed in news articles. The ECS-KG incorporates several information extraction techniques, a temporal Graph Neural Network (GNN), and a Graph Attention Network (GAT), yielding significant improvements in news representation. Several gold-standard datasets, comprising CNN/Daily Mail, TB-Dense, and ACE 2005, revealed that the proposed model outperformed the most advanced models. By integrating temporal reasoning and semantic insights, ECS-KG not only enhances user understanding of news significance but also meets the evolving demands of news consumers. This model advances the field of event-centric semantic KGs and provides valuable resources for applications in news information processing.}
}
@article{YANG2025106419,
title = {Knowledge graph and mitigation measures recommendation for safety hazards in large-scale hydropower projects using diverse heterogeneous inspection data},
journal = {Automation in Construction},
volume = {178},
pages = {106419},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106419},
url = {https://www.sciencedirect.com/science/article/pii/S0926580525004595},
author = {Yingliu Yang and Pengcheng Xiang and Dianxue Wang},
keywords = {Large-scale hydropower project (LHP), Construction safety hazards (CSHs), Knowledge graph, Sentence-BERT (SBERT), Mitigation measures, Safety management standards},
abstract = {Large-scale hydropower project (LHP) sites are fraught with numerous construction safety hazards (CSHs). When the efficiency of addressing these CSHs falls to meet safety management requirements, accidents may occur. Existing inspection records of CSHs contain a wealth of useful information, yet these unstructured texts hinder their efficient utilization. Furthermore, current mitigation measures for CSHs largely depend on human experience, leading to low efficiency. To address these issues, this paper proposes a BERT-Att-BiLSTM-CRF model based on massive daily inspection data, achieving precise extraction of CSHs entities (F1 > 95 %); construction a multi-dimensional knowledge graph with nine entity types and eight relationships; a mitigation measures recommendation based on Sentence-BERT (SBERT) model demonstrates superior performance (Pearson = 0.92, Spearman = 0.85) through semantic similarity; for novel CSHs, a safety management standards-based semantic model recommends compliant solutions. Validation confirms the research results capability to automate safety knowledge extraction from unstructured texts, establishing a replicable paradigm for infrastructure risk management.}
}
@article{HANNAH2025100843,
title = {On the legal implications of Large Language Model answers: A prompt engineering approach and a view beyond by exploiting Knowledge Graphs},
journal = {Journal of Web Semantics},
volume = {84},
pages = {100843},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100843},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000295},
author = {George Hannah and Rita T. Sousa and Ioannis Dasoulas and Claudia d’Amato},
keywords = {Knowledge Graph, Large Language Models, Prompt engineering, Legislative texts},
abstract = {With the recent surge in popularity of Large Language Models (LLMs), there is the rising risk of users blindly trusting the information in the response. Nevertheless, there are cases where the LLM recommends actions that have potential legal implications and this may put the user in danger. We provide an empirical analysis on multiple existing LLMs showing the urgency of the problem. Hence, we propose a first short-term solution, consisting in an approach for isolating these legal issues through prompt engineering. We prove that this solution is able to stem some risks related to legal implications, nonetheless we also highlight some limitations. Hence, we argue on the need for additional knowledge-intensive resources and specifically Knowledge Graphs for fully solving these limitations. For the purpose, we draw our proposal aiming at designing and developing a solution powered by a legal Knowledge Graph (KG) that, besides capturing and alerting the user on possible legal implications coming from the LLM answers, is also able to provide actual evidence for them by supplying citations of the interested laws. We conclude with a brief discussion on the issues that may be needed to solve for building a comprehensive legal Knowledge Graph}
}
@article{YAN2025101275,
title = {Adaptive multi-view learning method for enhanced drug repurposing using chemical-induced transcriptional profiles, knowledge graphs, and large language models},
journal = {Journal of Pharmaceutical Analysis},
volume = {15},
number = {6},
pages = {101275},
year = {2025},
issn = {2095-1779},
doi = {https://doi.org/10.1016/j.jpha.2025.101275},
url = {https://www.sciencedirect.com/science/article/pii/S2095177925000929},
author = {Yudong Yan and Yinqi Yang and Zhuohao Tong and Yu Wang and Fan Yang and Zupeng Pan and Chuan Liu and Mingze Bai and Yongfang Xie and Yuefei Li and Kunxian Shu and Yinghong Li},
keywords = {Drug repurposing, Multi-view learning, Chemical-induced transcriptional profile, Knowledge graph, Large language model, Heterogeneous network},
abstract = {Drug repurposing offers a promising alternative to traditional drug development and significantly reduces costs and timelines by identifying new therapeutic uses for existing drugs. However, the current approaches often rely on limited data sources and simplistic hypotheses, which restrict their ability to capture the multi-faceted nature of biological systems. This study introduces adaptive multi-view learning (AMVL), a novel methodology that integrates chemical-induced transcriptional profiles (CTPs), knowledge graph (KG) embeddings, and large language model (LLM) representations, to enhance drug repurposing predictions. AMVL incorporates an innovative similarity matrix expansion strategy and leverages multi-view learning (MVL), matrix factorization, and ensemble optimization techniques to integrate heterogeneous multi-source data. Comprehensive evaluations on benchmark datasets (Fdataset, Cdataset, and Ydataset) and the large-scale iDrug dataset demonstrate that AMVL outperforms state-of-the-art (SOTA) methods, achieving superior accuracy in predicting drug-disease associations across multiple metrics. Literature-based validation further confirmed the model's predictive capabilities, with seven out of the top ten predictions corroborated by post-2011 evidence. To promote transparency and reproducibility, all data and codes used in this study were open-sourced, providing resources for processing CTPs, KG, and LLM-based similarity calculations, along with the complete AMVL algorithm and benchmarking procedures. By unifying diverse data modalities, AMVL offers a robust and scalable solution for accelerating drug discovery, fostering advancements in translational medicine and integrating multi-omics data. We aim to inspire further innovations in multi-source data integration and support the development of more precise and efficient strategies for advancing drug discovery and translational medicine.}
}
@article{RATHOD2025113040,
title = {Knowledge graph-driven curation of heme-TLR4 interactions in inflammatory pathways},
journal = {Journal of Inorganic Biochemistry},
volume = {273},
pages = {113040},
year = {2025},
issn = {0162-0134},
doi = {https://doi.org/10.1016/j.jinorgbio.2025.113040},
url = {https://www.sciencedirect.com/science/article/pii/S016201342500220X},
author = {Dhruv C. Rathod and Negin Sadat Babaiha and Elena Kullmann and Martin Hofmann-Apitius and Diana Imhof},
keywords = {TLR4, Heme, Knowledge graph, Hemolysis, Signaling pathways},
abstract = {Heme, a vital iron-containing molecule, serves fundamental roles in oxygen transport and electron transfer but also acts as an extracellular signaling entity, significantly influencing inflammatory responses. Elevated levels of labile heme resulting from hemolytic events or therapeutic treatments may activate inflammatory signaling pathways, particularly through the Toll-like receptor 4 (TLR4). In this study, we systematically expanded the previously developed Heme Knowledge Graph (HemeKG) to comprehensively incorporate recent findings regarding heme-TLR4 interactions. By employing rigorous literature curation and validation using Biological Expression Language (BEL) standards and the e:BEL Python package, we successfully integrated newly identified molecular entities, notably activator protein 1 (AP-1), interleukin-12 (IL-12), cluster of differentiation 80 (CD80), cluster of differentiation 86 (CD86), and chemokine (C-X-C motif) ligand 1 (CXCL1), into the existing HemeKG framework. Pathway enrichment analysis across Kyoto Encyclopedia of Genes and Genomes (KEGG), Reactome, and WikiPathways databases robustly supported these integrations, consistently identifying significant enrichment of the TLR4 signaling cascade. The updated HemeKG thus provides an integrated and predictive platform, enhancing our understanding of the complex interactions between heme-driven inflammatory pathways and metabolic dysregulation.}
}
@article{XUE2025114320,
title = {HSAE: Hierarchical structure augment embedding for various knowledge graph completion},
journal = {Knowledge-Based Systems},
volume = {329},
pages = {114320},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.114320},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125013607},
author = {Yifan Xue and Wanqiang Cai and Yingyao Ma and Lotfi Senhadji and Huazhong Shu and Jiasong Wu},
keywords = {Knowledge graph completion, Knowledge hypergraph, Generative language model},
abstract = {Knowledge Graph Completion (KGC) addresses the task of reasoning over existing facts to predict missing relationships, serving as a fundamental component for downstream applications including question answering systems and personalized recommendation engines. Over the years, the KGC field has evolved into specialized tasks, including static KGC, temporal KGC, hyper KGC, and few-shot KGC, each requiring specialized methodologies. Although previous methods have utilized Generative Language Models (GLMs) to theoretically support multi task compatibility, their performance remains suboptimal compared to task-specific models. This limitation stems from their inability to effectively integrate structural and textual information, leading to a fine-grained structure-text gap. To address this challenge, we propose HSAE, a novel two-stage framework that hierarchically aligns structural and textual modalities, first at the coarse-grained entity level and then at the fine-grained token level. In the first stage, Entity-Level Structure Augment, we transform structural embeddings into tree-shaped entity classifications, enriching entity representations with explicit structural information. This augmentation provides global structural guidance during beam search, ensuring that generated sequences adhere to the underlying knowledge graph topology. In the second stage, Token-Level Structure Augment, we introduce a cross-modal alignment module that dynamically fuses structural embeddings with token-level predictions. By aligning structural and textual representations at the token level, HSAE ensures that each decoding step is informed by both structural and textual coherence. Experiments on eight benchmarks demonstrate that HSAE outperforms competitive baselines across multiple KGC tasks. The data and code are released at https://anonymous.4open.science/r/HSAE-main/README.md.}
}
@article{HAN2025146079,
title = {A method for constructing fault knowledge graphs based on an improved hidden Markov Model: A case study for papermaking industry},
journal = {Journal of Cleaner Production},
volume = {520},
pages = {146079},
year = {2025},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2025.146079},
url = {https://www.sciencedirect.com/science/article/pii/S0959652625014295},
author = {Yulin Han and Huanhuan Zhang and Yi Man},
keywords = {Fault diagnosis, Knowledge graph, Text segmentation, Knowledge extraction, Machine learning},
abstract = {As a technology and knowledge-intensive industry, the process industry, central to sustainable manufacturing goals, faces challenges with large volumes of dispersed data, high integration of production units, and complex workflows. Existing methods struggle to analyze unstructured mechanism and experience knowledge, leading to information silos. To support cleaner production through enhanced fault diagnosis and prevention, this study leverages knowledge graph theory. An improved Hidden Markov Model for industrial text segmentation is proposed, demonstrating a 3.2 % accuracy increase over general tools. By utilizing this method to effectively process unstructured data and extract valuable knowledge, a dedicated fault knowledge graph framework and ontology model for process industries is constructed. This knowledge graph is then integrated with machine learning algorithms to build an industrial status diagnosis model; crucially, it enables intelligent feature selection, bypassing complex dimensionality reduction tasks common in previous approaches. Through a case study on tissue paper break faults, the framework is demonstrated by establishing a paper break fault knowledge graph and diagnosis model. This approach provides causal reasoning for proactive interventions that reduce scrap rates and optimize resource utilization, key drivers for improving eco-efficiency and advancing green, sustainable operations within the process industries.}
}
@article{DING2024102622,
title = {Product color emotional design based on 3D knowledge graph},
journal = {Displays},
volume = {81},
pages = {102622},
year = {2024},
issn = {0141-9382},
doi = {https://doi.org/10.1016/j.displa.2023.102622},
url = {https://www.sciencedirect.com/science/article/pii/S0141938223002561},
author = {Man Ding and Mingyu Sun and Shijian Luo},
keywords = {Product color emotion design, RotatE knowledge graph, PageRank, Big data, 3D Knowledge Graph},
abstract = {To address the problem of fragmentation, integration difficulties in fuzzy front-end information, and ambiguity in color emotion knowledge representation and conversion within the current product color emotion design stage, this paper proposes a method based on 3D Knowledge Graph. The proposed approach aims to integrate product color emotion design into the “data knowledge + artificial intelligence” growth model, facilitating a beneficial knowledge output cycle driven by data. As for the proposed approach, it divides the design problem into three stages: in the first one, big data web crawler technology and natural language processing were employed to extract knowledge related to product color emotion design. In the second phase, the construction of a product color emotion imagery association model using the RotatE knowledge graph, thereby achieving knowledge fusion of product color emotion design, and all accumulated knowledge data is integrated into a 3D Knowledge Graph for visualization. Finally, in the third phase, the PageRank algorithm is applied to calculate primary and auxiliary color weight parameters, simulating the product color synergy mechanism and determining the color synergy effects. Then, realize the knowledge generation of product color emotional design. This approach combines the human visual experience feedback with extensive big data analysis, and accurately outputs the product color emotion design scheme that meets the user's emotional needs. Moreover, the effectiveness and applicability of the method are verified by an illustrative example involving modern machine tool.}
}
@article{CARTA20232224,
title = {SailGenie: SAiling expertIse to knowLedge Graph through opEN Information Extraction},
journal = {Procedia Computer Science},
volume = {225},
pages = {2224-2233},
year = {2023},
note = {27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.213},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923013716},
author = {Salvatore Carta and Pietro Fariello and Alessandro Giuliani and Leonardo Piano and Alessandro Sebastian Podda and Sandro Gabriele Tiddia},
keywords = {Artificial Intelligence, Knowledge graphs, Sailing, Domain-specific dataset},
abstract = {This work is focused on the sailing domain, for which several innovative technologies are being adopted to improve sailing efficiency, performance, and safety. In this context a knowledge graph could be used, for example, to represent information about different types of boats, sailing techniques, maritime safety, or weather conditions. Although numerous construction methods or ready-to-go knowledge graphs have been proposed in many fields, the sailing domain still needs to be explored. As the most effective methods rely on domain-specific datasets, the absence of suitable and available sailing datasets is one of the main challenges. Although several Open Information Extraction (OpenIE) methods may generate relevant triplets (the elementary units composing a knowledge graph) from arbitrary text without any additional information about its topic, such methods usually generate many incorrect triplets. In this paper, we aim (i) to address the aforementioned problem by proposing an innovative method that combines in an improved and strengthened way different OpenIE tools to generate proper triplets from domain-specific sources and, in particular, (ii) to build and release a suitable dataset for the sailing domain. Results confirm that our proposal can maximize the extracted information and infer unique information irretrievable by the classical OpenIE tools and, furthermore, that the generated dataset is significantly valuable for the sailing scenario.}
}
@article{KUMAR2025108364,
title = {Knowledge graph applications and multi-relation learning for drug repurposing: A scoping review},
journal = {Computational Biology and Chemistry},
volume = {115},
pages = {108364},
year = {2025},
issn = {1476-9271},
doi = {https://doi.org/10.1016/j.compbiolchem.2025.108364},
url = {https://www.sciencedirect.com/science/article/pii/S1476927125000246},
author = {A.Arun Kumar and Samarth Bhandary and Swathi Gopal Hegde and Jhinuk Chatterjee},
keywords = {Drug repurposing, Knowledge graph, Embedding, Multimodal frameworks},
abstract = {Objective
Development of novel drug solutions has always been an expensive endeavour, hence drug repurposing as an approach has gained popularity in recent years. In this review we intend to examine one of the most unique computational methods for drug repurposing, that being knowledge graphs.
Method
Through literature review we looked at the application of knowledge graphs in medicine, specifically at its use in drug repurposing. We also looked at literature embedding methods, integration of machine learning models and approaches to completion of knowledge graphs.
Result
After filtering 43 papers were used for analysis. Timeline, country distribution, application areas of knowledge graph was highlighted. General trends in the use of knowledge graphs for drug repurposing and any shortcomings of the approach was discussed.
Conclusion
This approach has gained popularity only very recently; hence it is in a nascent phase.}
}
@article{KHAN2025110463,
title = {KEM-IoMT: Knowledge graph embedding-enhanced accurate medical service recommendation against diabetes},
journal = {Computers in Biology and Medicine},
volume = {194},
pages = {110463},
year = {2025},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2025.110463},
url = {https://www.sciencedirect.com/science/article/pii/S0010482525008145},
author = {Nasrullah Khan and Muhammad Rafiq Mufti and Muhammad Arif and Amjad Ali and Zubair Shah},
keywords = {Knowledge graph embedding, Medical service recommendation, Sentiment analysis, Information relevance, Diabetes},
abstract = {The Internet of Medical Things (IoMT)-enhanced Recommender System (RS) acquired swift advancement in configuring diverse medical data into intelligent systems to generate personalized medical services. However, due to the heterogeneous and complex nature of the diabetes data, generating accurate and context-sensitive service recommendations remains challenging. Additionally, existing RSs do not extend their knowledge-bases by incorporating user-reviews and current updates on the given disease alongside the medical data. Thus, this paper introduces Knowledge graph Embedding-enhanced accurate Medical service recommendation (KEM) in the IoMT, aiming to enhance the precision of RS for diabetes care. The KEM mainly collects user reviews and online data about the disease, preprocesses the collected data, and transforms it into the Knowledge Graph (KG). The model embeds the KG and encapsulates the embedding representations into the independent latent factors through the Graph Neural Network. Moreover, the KEM employs Deep Matrix Factorization to compute the latent factors and obtain the required relations for recommendation. Extensive experiments on real-world data demonstrate the effectiveness of the KEM model in enhancing performance compared to baseline methods.}
}
@article{XIE20221557,
title = {GFCNet: Utilizing graph feature collection networks for coronavirus knowledge graph embeddings},
journal = {Information Sciences},
volume = {608},
pages = {1557-1571},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.07.031},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522007204},
author = {Zhiwen Xie and Runjie Zhu and Jin Liu and Guangyou Zhou and Jimmy Xiangji Huang and Xiaohui Cui},
keywords = {Natural Language Processing, Text Mining, Knowledge Graph, COVID-19},
abstract = {In response to fighting COVID-19 pandemic, researchers in machine learning and artificial intelligence have constructed some medical knowledge graphs (KG) based on existing COVID-19 datasets, however, these KGs contain a considerable amount of semantic relations which are incomplete or missing. In this paper, we focus on the task of knowledge graph embedding (KGE), which serves an important solution to infer the missing relations. In the past, there have been a collection of knowledge graph embedding models with different scoring functions to learn entity and relation embeddings published. However, these models share the same problems of rarely taking important features of KG like attribute features, other than relation triples, into account, while dealing with the heterogeneous, complex and incomplete COVID-19 medical data. To address the above issue, we propose a graph feature collection network (GFCNet) for COVID-19 KGE task, which considers both neighbor and attribute features in KGs. The extensive experiments conducted on the COVID-19 drug KG dataset show promising results and prove the effectiveness and efficiency of our proposed model. In addition, we also explain the future directions of deepening the study on COVID-19 KGE task.}
}
@article{YANG2025110285,
title = {Alzheimer's disease knowledge graph enhances knowledge discovery and disease prediction},
journal = {Computers in Biology and Medicine},
volume = {192},
pages = {110285},
year = {2025},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2025.110285},
url = {https://www.sciencedirect.com/science/article/pii/S0010482525006365},
author = {Yue Yang and Kaixian Yu and Shan Gao and Sheng Yu and Di Xiong and Chuanyang Qin and Huiyuan Chen and Jiarui Tang and Niansheng Tang and Hongtu Zhu},
keywords = {Alzheimer's disease, Disease prediction, Knowledge graph construction, Link prediction},
abstract = {Objective
To construct an Alzheimer's Disease Knowledge Graph (ADKG) by extracting and integrating relationships among Alzheimer's disease (AD), genes, variants, chemicals, drugs, and other diseases from biomedical literature, aiming to identify existing treatments, potential targets, and diagnostic methods for AD.
Methods
We annotated 800 PubMed abstracts (ADERC corpus) with 20,886 entities and 4935 relationships, augmented via GPT-4. A SpERT model (SciBERT-based) trained on this data extracted relations from PubMed abstracts, supported by biomedical databases and entity linking refined via abbreviation resolution/string matching. The resulting knowledge graph trained embedding models to predict novel relationships. ADKG's utility was validated by integrating it with UK Biobank data for predictive modeling.
Results
The ADKG contained 3,199,276 entity mentions and 633,733 triplets, linking >5K unique entities and capturing complex AD-related interactions. Its graph embedding models produced evidence-supported predictions, enabling testable hypotheses. In UK Biobank predictive modeling, ADKG-enhanced models achieved higher AUROC of 0.928 comparing to 0.903 without ADKG enhancement.
Conclusion
By synthesizing literature-derived insights into a computable framework, ADKG bridges molecular mechanisms to clinical phenotypes, advancing precision medicine in Alzheimer's research. Its structured data and predictive utility underscore its potential to accelerate therapeutic discovery and risk stratification.}
}
@article{MA2025114426,
title = {Provide explainable clues: A generative traceable method for knowledge graph completion},
journal = {Knowledge-Based Systems},
volume = {330},
pages = {114426},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.114426},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125014650},
author = {Ziqi Ma and Jinpeng Li and Hang Yu},
keywords = {Knowledge graph, Knowledge graph completion, Generative model, Information traceability, Explainability analysis},
abstract = {Improving the quality of Knowledge Graph Completion (KGC) results is an essential topic in the field of knowledge graphs. Recently, generative models (GMs) have gained widespread attention for addressing the generalization issues of traditional approaches. However, the black-box nature of generative models often leads to hallucinations, which reduce the model’s performance. Most methods attempt to mitigate this issue through retrieval enhancement and decoding constraints. However, they overlook one major cause of hallucinations–poor explainability. Based on this concept, we propose a Generative Traceable Method, namely GTM, which aims to improve the KGC capability of GMs by exploring the inhibitory effect of explainability on hallucinations. In GTM, a clue tracker is used to find contextual evidence for explainability. In addition, to measure explainability clues, we propose a context-aware analyzer, which enhances the understanding of context through group analogy. In the reasoning phase, we ensure the validity of the generated results by integrating the interpretive capability of clues. Extensive experiments have demonstrated that GTM can adapt to various KGC tasks and significantly enhance the performance of KGC models.}
}
@article{JOSHI2025291,
title = {Developing Natural Language Processing Algorithms to Fact-Check Speech or Text},
journal = {Transportation Research Procedia},
volume = {84},
pages = {291-298},
year = {2025},
note = {Smart Mobility and Logistics Ecosystems},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2025.03.075},
url = {https://www.sciencedirect.com/science/article/pii/S2352146525001231},
author = {Himanshu Sanjay Joshi and Hamed Taherdoost},
keywords = {Natural Language Processing, Question-Answering Systems, Fact-Checking, Modular Architecture},
abstract = {This paper explores the development of Natural Language Processing (NLP) systems designed to fact-check speech and text through a distributed architecture. The integration of various Question-Answering (QA) systems to improve question diversity, coverage, and adapt modular frameworks to dynamic data sources is being investigated. The efficacy of these systems enhancing vast data pools critically enhances the fact-checking process. This study proposes a new approach combining existing QA systems with innovative NLP methodologies to advance the fact-checking capabilities in mitigating misinformation.}
}
@article{TANG2023101426,
title = {Construction and application of an ontology-based domain-specific knowledge graph for petroleum exploration and development},
journal = {Geoscience Frontiers},
volume = {14},
number = {5},
pages = {101426},
year = {2023},
issn = {1674-9871},
doi = {https://doi.org/10.1016/j.gsf.2022.101426},
url = {https://www.sciencedirect.com/science/article/pii/S1674987122000792},
author = {Xianming Tang and Zhiqiang Feng and Yitian Xiao and Ming Wang and Tianrui Ye and Yujie Zhou and Jin Meng and Baosen Zhang and Dongwei Zhang},
keywords = {Knowledge Graph, Petroleum exploration and development, Natural language processing, Ontology},
abstract = {The massive amount and multi-sourced, multi-structured data in the upstream petroleum industry impose great challenge on data integration and smart application. Knowledge graph, as an emerging technology, can potentially provide a way to tackle the challenges associated with oil and gas big data. This paper proposes an engineering-based method that can improve upon traditional natural language processing to construct the domain knowledge graph based on a petroleum exploration and development ontology. The exploration and development knowledge graph is constructed by assembling Sinopec’s multi-sourced heterogeneous database, and millions of nodes. The two applications based on the constructed knowledge graph are developed and validated for effectiveness and advantages in providing better knowledge services for the oil and gas industry.}
}
@article{LI2025131182,
title = {Time-enhanced compound geometric operations for temporal knowledge graph embedding},
journal = {Neurocomputing},
volume = {654},
pages = {131182},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.131182},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225018545},
author = {Wenhao Li and Dong Zhang and Guanyu Li and Yingqi Zou},
keywords = {Temporal knowledge graph, Temporal knowledge graph embedding, Compound geometric operation, Link prediction task},
abstract = {Temporal knowledge graph embedding aims to represent entities, relations, and timestamps in a continuous vector space while preserving their temporal evolutionary patterns. TCompoundE is a recently proposed temporal knowledge graph embedding model that uses compound operations involving translation and scaling as the relation-specific and time-specific operations. However, this model has the following three problems: (1) The model only employs traditional geometric transformations and does not explore more complex transformation relationships. (2) The model fails to enhance the head entity with time-awareness, which weakens the model’s ability to model dynamic changes. (3) The model neglects the influence of the head entity and the relation in the modeling process. We propose the TTComE, a time-enhanced compound geometric temporal knowledge graph embedding model, to tackle these questions. Here are three improvement points: (1) We introduced the Spatiotemporal Rotation Operation for modeling, which enables the model to better capture complex relationships and temporal evolution patterns. (2) We performed the Temporal-aware Enhancement Operation on the head entities and relations respectively, to enhance the model’s ability to capture dynamic changes. (3) We introduced the Weight-adaptive Translation Operation, which assigns learnable weights to both the head entity and the relation, followed by a translation operation. This enables the model to adaptively adjust the contributions of the head entity and the relation in prediction tasks. Finally, we experimentally validated the model on multiple datasets, complemented by ablation studies, and the results demonstrated significant improvements over other baseline models.}
}
@article{YANG2025113503,
title = {A comprehensive survey on integrating large language models with knowledge-based methods},
journal = {Knowledge-Based Systems},
volume = {318},
pages = {113503},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113503},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125005490},
author = {Wenli Yang and Lilian Some and Michael Bain and Byeong Kang},
keywords = {LLMs, Knowledge-based, Knowledge integration, RAG, KG},
abstract = {The rapid development of artificial intelligence has led to marked progress in the field. One interesting direction for research is whether Large Language Models (LLMs) can be integrated with structured knowledge-based systems. This approach aims to combine the generative language understanding of LLMs and the precise knowledge representation systems by which they are integrated. This article surveys the relationship between LLMs and knowledge bases, looks at how they can be applied in practice, and discusses related technical, operational, and ethical challenges. Utilizing a comprehensive examination of the literature, the study both identifies important issues and assesses existing solutions. It demonstrates the merits of incorporating generative AI into structured knowledge-base systems concerning data contextualization, model accuracy, and utilization of knowledge resources. The findings give a full list of the current situation of research, point out the main gaps, and propose helpful paths to take. These insights contribute to advancing AI technologies and support their practical deployment across various sectors.}
}
@article{CHEN2024104804,
title = {Enhancing emergency decision-making with knowledge graphs and large language models},
journal = {International Journal of Disaster Risk Reduction},
volume = {113},
pages = {104804},
year = {2024},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2024.104804},
url = {https://www.sciencedirect.com/science/article/pii/S2212420924005661},
author = {Minze Chen and Zhenxiang Tao and Weitong Tang and Tingxin Qin and Rui Yang and Chunli Zhu},
keywords = {Emergency decision support, Large language model, Knowledge graph, Decision support system},
abstract = {Emergency management urgently requires comprehensive knowledge while having a high possibility to go beyond individuals’ cognitive scope. Therefore, artificial intelligence(AI) supported decision-making under that circumstance is of vital importance. Recent emerging large language models (LLM) provide a new direction for enhancing targeted machine intelligence. However, the utilization of LLM directly would inevitably introduce unreliable output for its inherent issue of hallucination and poor reasoning skills. In this work, we develop a system called Enhancing Emergency decision-making with Knowledge Graph and LLM (E-KELL), which provides evidence-based decision-making in various emergency stages. The study constructs a structured emergency knowledge graph and guides LLMs to reason over it via a prompt chain. In real-world evaluations, E-KELL demonstrates significant improvement over baseline models in various emergency response scenarios, as rated by emergency commanders and firefighters. This work introduces a novel approach to applying LLMs to enhance emergency decision-making.}
}
@article{LI2026103577,
title = {LLM supporting knowledge tracing leveraging global subject and student specific knowledge graphs},
journal = {Information Fusion},
volume = {126},
pages = {103577},
year = {2026},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2025.103577},
url = {https://www.sciencedirect.com/science/article/pii/S1566253525006499},
author = {Linqing Li and Zhifeng Wang and Joemon M. Jose and Xuri Ge},
keywords = {Knowledge tracing, Education AI, Large language model, And knowledge graph},
abstract = {In this paper, we propose a novel LLM-based KT model, called the Teacher Thinking Knowledge Tracing model (2T-KT), to solve the issue that traditional knowledge tracing methods relying on numerous student exercise records cannot make good predictions when predicting new knowledge concepts by leveraging the excellent abilities of reasoning and generation from large language model (LLM). The 2T-KT model leverages large language models (LLMs) to enrich the knowledge graph with new knowledge concepts and predict student performance on the next exercise by four key components, i.e. observation, guideline, interpretation, and cognition. In particular, there are two stages, the preprocessing stage, and the 2T-KT stage, to predict the student’s performance on the next exercise. In the preprocessing stage, two novel local and global knowledge graphs are first designed to improve the capability of evaluating new concepts. In the 2T-KT stage, a novel teacher’s thinking mode is designed to include four key components, i.e. observation, guideline, interpretation, and cognition to assist the LLM in predicting the student’s performance on the next exercise. This exercise contains new knowledge concepts. Finally, even with new concepts, the LLM ‘teacher’ can accurately predict students’ abilities through interpretable augmentation prompts. Extensive evaluations on three public educational benchmarks—the FrcSub dataset, comprising 10K student records and 8 exercises, and the Xes3g5m dataset, comprising around 522K student records and 6,641 exercises. In addition, the MOOCRadar dataset contains around 897K student records and 2510 exercise records to test our model’s performance. It demonstrates that our 2T-KT model is a strong contender in knowledge tracing, delivering both high performance and interpretability.}
}
@article{WEI2025110873,
title = {The use of knowledge graphs for drug repurposing: From classical machine learning algorithms to graph neural networks},
journal = {Computers in Biology and Medicine},
volume = {196},
pages = {110873},
year = {2025},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2025.110873},
url = {https://www.sciencedirect.com/science/article/pii/S0010482525012247},
author = {Siqi Wei and Christo Sasi and Jelle Piepenbrock and Martijn A. Huynen and Peter A.C. {’t Hoen}},
keywords = {Drug repositioning, Knowledge graph, Graph convolutional networks, Machine learning, Deep learning},
abstract = {Drug repurposing, the development of new therapeutic indications for existing drugs, is a promising strategy in drug development. Computational methods and artificial intelligence may be used to identify new drug repurposing candidates. Knowledge graph (KG) based methods have emerged as powerful tools for modeling and predicting drug–disease relationships, because of their intuitive way of exploiting biomedical knowledge and data. This review provides an overview of computational drug repurposing methods based on KGs. The motivation for adopting KG-based knowledge representations, traditional machine learning and deep learning approaches are discussed, followed by an analysis of selected tools, their construction, link prediction capabilities, and inherent advantages and limitations.}
}
@article{GENG2025126175,
title = {Prompting disentangled embeddings for knowledge graph completion with pre-trained language model},
journal = {Expert Systems with Applications},
volume = {268},
pages = {126175},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.126175},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424030422},
author = {Yuxia Geng and Jiaoyan Chen and Yuhang Zeng and Zhuo Chen and Wen Zhang and Jeff Z. Pan and Yuxiang Wang and Xiaoliang Xu},
keywords = {Knowledge graph completion, Pre-trained language model, Prompt tuning, Disentangled embedding},
abstract = {Both graph structures and textual information play a critical role in Knowledge Graph Completion (KGC). With the success of Pre-trained Language Models (PLMs) such as BERT, they have been applied for text encoding for KGC. However, the current methods mostly prefer to fine-tune PLMs, leading to huge training costs and limited scalability to larger PLMs. In contrast, we propose to utilize prompts and perform KGC on a frozen PLM with only the prompts trained. Accordingly, we propose a new KGC method named PDKGC with two prompts — a hard task prompt which is to adapt the KGC task to the PLM pre-training task of token prediction, and a disentangled structure prompt which learns disentangled graph representation so as to enable the PLM to combine more relevant structure knowledge with the text information. With the two prompts, PDKGC builds a textual predictor and a structural predictor, respectively, and their combination leads to more comprehensive entity prediction. Solid evaluation on three widely used KGC datasets has shown that PDKGC often outperforms the baselines including the state-of-the-art, and its components are all effective. Our codes and data are available at https://github.com/genggengcss/PDKGC.}
}
@article{SHEN2025112315,
title = {A self-supervised method for learning path-augmented knowledge graph embedding},
journal = {Engineering Applications of Artificial Intelligence},
volume = {162},
pages = {112315},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.112315},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625023231},
author = {Tong Shen and Fu Zhang and Jingwei Cheng},
keywords = {Knowledge graph, Knowledge graph embedding, Self-supervised learning},
abstract = {Knowledge graphs (KGs) consist of factual triples that describe relations between entities in the real world. Knowledge graph embedding (KGE) aims to map entities and relations into constantly low-dimensional vectors, which is important for lots of downstream tasks (e.g., KG completion and information retrieval). Current KGE methods primarily rely on explicit structural patterns, neglecting latent contextual semantics behind those structures and resulting in sub-optimal performance. While some methods incorporate additional data (e.g., textual descriptions), such dependencies limit applicability due to additional data requirements. Furthermore, most KGE models suffer from limited supervision with sparse labeled triples, restricting their capacity to learn comprehensive semantic features. Inspired by the simple but effective self-supervised language model word2vec, one interesting question is: Can KGE be performed as a simple self-supervised language model? To achieve this, we innovatively propose a self-supervised KGE framework that learns entity and relation embeddings by adapting word2vec’s skip-gram objective to path sequences extracted from KGs. Our framework employs separate embedding spaces for entities and relations with an entity-relation mapping mechanism for effective interaction between the two embedding spaces. Further, to enhance the training efficiency, we introduce a markov chain-based negative sampling strategy, which generates semantically meaningful negative samples by preserving the structural contexts along KG paths. Our framework, which is the first attempt to follow the context-based self-supervised idea of language models to conduct KGE tasks, addresses the constraints of label-dependent supervised KGE techniques and obviates the requirement for external information, while simultaneously enabling effective extraction of the implicit contextual semantics inherent in triple structures. Experiments on two widely-used KGE datasets show state-of-the-art performance, demonstrating our framework’s ability to learn semantically rich representations solely from graph structure.}
}
@article{CHEN2025100123,
title = {Bibliometric analysis of natural language processing using CiteSpace and VOSviewer},
journal = {Natural Language Processing Journal},
volume = {10},
pages = {100123},
year = {2025},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2024.100123},
url = {https://www.sciencedirect.com/science/article/pii/S2949719124000712},
author = {Xiuming Chen and Wenjie Tian and Haoyun Fang},
keywords = {NLP, CiteSpace, VOSviewer, Bibliometrics analysis},
abstract = {Natural Language Processing (NLP) holds a pivotal position in the domains of computer science and artificial intelligence (AI). Its focus is on exploring and developing theories and methodologies that facilitate seamless and effective communication between humans and computers through the use of natural language. First of all, In this paper, we employ the bibliometric analysis tools, namely CiteSpace and VOSviewer (Visualization of Similarities viewer) are used as the bibliometric analysis software in this paper to summarize the domain of NLP research and gain insights into its core research priorities. What is more, the Web of Science(WoS) Core Collection database serves as the primary source for data acquisition in this study. The data includes 4803 articles on NLP published from 2011 to May 15, 2024. The trends and types of articles reveal the developmental trajectory and current hotspots in NLP. Finally, the analysis covers eight aspects: volume of published articles, classification, countries, institutional collaboration, author collaboration network, cited author network, co-cited journals, and co-cited references. The applications of NLP are vast, spanning areas such as AI, electronic health records, risk, task analysis, data mining, computational modeling. The findings suggest that the emphasis of future research ought to focus on areas like AI, risk, task analysis, and computational modeling. This paper provides learners and practitioners with a comprehensive insight into the current status and emerging trends in NLP.}
}
@article{MEIJER2025654,
title = {Empowering natural product science with AI: leveraging multimodal data and knowledge graphs††In memory of Dr Othman Skiredj},
journal = {Natural Product Reports},
volume = {42},
number = {4},
pages = {654-662},
year = {2025},
issn = {0265-0568},
doi = {https://doi.org/10.1039/d4np00008k},
url = {https://www.sciencedirect.com/science/article/pii/S0265056824000862},
author = {David Meijer and Mehdi A. Beniddir and Connor W. Coley and Yassine M. Mejri and Meltem Öztürk and Justin J. J. {van der Hooft} and Marnix H. Medema and Adam Skiredj},
abstract = {Artificial intelligence (AI) is accelerating how we conduct science, from folding proteins with AlphaFold and summarizing literature findings with large language models, to annotating genomes and prioritizing newly generated molecules for screening using specialized software. However, the application of AI to emulate human cognition in natural product research and its subsequent impact has so far been limited. One reason for this limited impact is that available natural product data is multimodal, unbalanced, unstandardized, and scattered across many data repositories. This makes natural product data challenging to use with existing deep learning architectures that consume fairly standardized, often non-relational, data. It also prevents models from learning overarching patterns in natural product science. In this Viewpoint, we address this challenge and support ongoing initiatives aimed at democratizing natural product data by collating our collective knowledge into a knowledge graph. By doing so, we believe there will be an opportunity to use such a knowledge graph to develop AI models that can truly mimic natural product scientists' decision-making.}
}
@article{WANG2020,
title = {Using Natural Language Processing Techniques to Provide Personalized Educational Materials for Chronic Disease Patients in China: Development and Assessment of a Knowledge-Based Health Recommender System},
journal = {JMIR Medical Informatics},
volume = {8},
number = {4},
year = {2020},
issn = {2291-9694},
doi = {https://doi.org/10.2196/17642},
url = {https://www.sciencedirect.com/science/article/pii/S2291969420001556},
author = {Zheyu Wang and Haoce Huang and Liping Cui and Juan Chen and Jiye An and Huilong Duan and Huiqing Ge and Ning Deng},
keywords = {health education, ontology, natural language processing, chronic disease, recommender system},
abstract = {Background
Health education emerged as an important intervention for improving the awareness and self-management abilities of chronic disease patients. The development of information technologies has changed the form of patient educational materials from traditional paper materials to electronic materials. To date, the amount of patient educational materials on the internet is tremendous, with variable quality, which makes it hard to identify the most valuable materials by individuals lacking medical backgrounds.
Objective
The aim of this study was to develop a health recommender system to provide appropriate educational materials for chronic disease patients in China and evaluate the effect of this system.
Methods
A knowledge-based recommender system was implemented using ontology and several natural language processing (NLP) techniques. The development process was divided into 3 stages. In stage 1, an ontology was constructed to describe patient characteristics contained in the data. In stage 2, an algorithm was designed and implemented to generate recommendations based on the ontology. Patient data and educational materials were mapped to the ontology and converted into vectors of the same length, and then recommendations were generated according to similarity between these vectors. In stage 3, the ontology and algorithm were incorporated into an mHealth system for practical use. Keyword extraction algorithms and pretrained word embeddings were used to preprocess educational materials. Three strategies were proposed to improve the performance of keyword extraction. System evaluation was based on a manually assembled test collection for 50 patients and 100 educational documents. Recommendation performance was assessed using the macro precision of top-ranked documents and the overall mean average precision (MAP).
Results
The constructed ontology contained 40 classes, 31 object properties, 67 data properties, and 32 individuals. A total of 80 SWRL rules were defined to implement the semantic logic of mapping patient original data to the ontology vector space. The recommender system was implemented as a separate Web service connected with patients' smartphones. According to the evaluation results, our system can achieve a macro precision up to 0.970 for the top 1 recommendation and an overall MAP score up to 0.628.
Conclusions
This study demonstrated that a knowledge-based health recommender system has the potential to accurately recommend educational materials to chronic disease patients. Traditional NLP techniques combined with improvement strategies for specific language and domain proved to be effective for improving system performance. One direction for future work is to explore the effect of such systems from the perspective of patients in a practical setting.}
}
@article{SUTHAR2025453,
title = {Exploring the Landscape of Natural Language Processing for Text Analytics: A comprehensive Review},
journal = {Procedia Computer Science},
volume = {259},
pages = {453-462},
year = {2025},
note = {Sixth International Conference on Futuristic Trends in Networks and Computing Technologies (FTNCT06), held in Uttarakhand, India},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.03.347},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925010919},
author = {Om Prakash Suthar and Ankita Mishra and Shilpa Singhal},
keywords = {NLP, LDA, NMF, text analytics, text extraction},
abstract = {The amount of textual information that can be analyzed in order to look for meaningful information has become a constraint as the amount of digital content that is being produced everyday increases. When it comes to mining large text datasets for useful information, NLP methods and models are necessary and extremely effective tools. The paper’s primary objective is to present a comprehensive review of the NLP methods and models that are utilized for text analytics, sentiment analysis, topic modelling, text summarization, and text generation. In this paper, we will discuss the trending methodologies for social media analysis, consumer opinion analysis, and content creation. In addition, we will discuss the methodologies, methods, and evaluation metrics that are utilized in these types of contexts. This analysis aims to provide context for the development of natural language processing (NLP) in the context of text analytics, both historically and prospectively. This will be accomplished by providing context for the development of natural language processing in the context of text analytics.}
}
@article{MA2024128490,
title = {A review of graph neural networks and pretrained language models for knowledge graph reasoning},
journal = {Neurocomputing},
volume = {609},
pages = {128490},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128490},
url = {https://www.sciencedirect.com/science/article/pii/S092523122401261X},
author = {Jiangtao Ma and Bo Liu and Kunlin Li and Chenliang Li and Fan Zhang and Xiangyang Luo and Yaqiong Qiao},
keywords = {Knowledge graph reasoning, Graph neural networks, Pretrained language models, Logic rules},
abstract = {Knowledge Graph (KG) stores human knowledge facts in an intuitive graphical structure but faces challenges such as incomplete construction or inability to handle new knowledge. Knowledge Graph Reasoning (KGR) can make KGs more accurate, complete, and trustworthy to support various artificial intelligence applications better. Currently, the popular KGR methods are based on graph neural networks (GNNs). Recent studies have shown that hybrid logic rules and synergized pre-trained language models (PLMs) can enhance the GNN-based KGR methods. These methods mainly focus on data sparsity, insufficient knowledge evolution patterns, multi-modal fusion, and few-shot reasoning. Although many studies have been conducted, there are still few review papers that comprehensively summarize and explore KGR methods related to GNNs, logic rules, and PLMs. Therefore, this paper provides a comprehensive review of GNNs and PLMs for KGR based on a large number of high-quality papers. To present a clear overview of KGR, we propose a general framework. Specifically, we first introduce the KG preparation. Then we provide an overview of KGR methods, in which we categorize KGR methods into GNNs-based, logic rules-enhanced, and pre-trained language models-enhanced KGR methods. Furthermore, we also compare and analyze the GNN-based KGR methods in two scenarios. Moreover, we also present the application of KGR in different fields. Finally, we discuss the current challenges and future research directions for KGR.}
}
@article{HAO2025,
title = {Knowledge graph–based safety risk evaluation method for hazardous behaviors of road transport vehicles},
journal = {Traffic Injury Prevention},
year = {2025},
issn = {1538-9588},
doi = {https://doi.org/10.1080/15389588.2025.2540554},
url = {https://www.sciencedirect.com/science/article/pii/S1538958825001407},
author = {Yadi Hao and Gen Li and Jiwei Lu and Wanrong Cheng and Quan Yuan and Zhihong Yao},
keywords = {Knowledge graph, hazardous driving behaviors, risk prediction, traffic safety management},
abstract = {Objective
This study aims to develop a knowledge graph (KG)–based framework to quantify and analyze the impact of hazardous driving behaviors on road transport safety.
Method
A top-down approach was adopted to construct a multilayered KG incorporating seven categories of hazardous behavior factors (C1–C7). Multisource accident datasets were integrated to map the relationships among hazardous behavior factors, accident types, and accident causes. The Criteria Importance Through Intercriteria Correlation (CRITIC) method was applied to calculate the safety risk levels of various hazardous behaviors. Cosine similarity analysis was used to quantify correlations between hazardous behavioral factors and calculated risk metrics. Furthermore, KG-based path reasoning was used to trace causal chains linking hazardous behaviors to accidents.
Results
Dangerous driving (C5) and driver technical competency (C1) emerged as the two most influential risk factor categories, with correlation coefficients of 0.995 and 0.987, respectively. Rear-end collisions were identified as the most probable accident type caused by C5, with a conditional probability of 0.5. Fatigue and speeding were identified as the most common behavioral triggers. KG pathway analysis effectively traced risk propagation paths, highlighting key links in accident causation.
Conclusions
This study integrates the multidimensional correlation analysis of knowledge graphs with the weighting advantages of the CRITIC method, explicitly expressing the causal chain of “hazardous behavior–accident type–accident cause” through graph structures to comprehensively analyze the behavioral mechanisms of traffic accidents.}
}
@article{TRAJANOV2023714,
title = {Review of Natural Language Processing in Pharmacology},
journal = {Pharmacological Reviews},
volume = {75},
number = {4},
pages = {714-738},
year = {2023},
issn = {0031-6997},
doi = {https://doi.org/10.1124/pharmrev.122.000715},
url = {https://www.sciencedirect.com/science/article/pii/S0031699724007762},
author = {Dimitar Trajanov and Vangel Trajkovski and Makedonka Dimitrieva and Jovana Dobreva and Milos Jovanovik and Matej Klemen and Aleš Žagar and Marko Robnik-Šikonja},
abstract = {Natural language processing (NLP) is an area of artificial intelligence that applies information technologies to process the human language, understand it to a certain degree, and use it in various applications. This area has rapidly developed in the past few years and now employs modern variants of deep neural networks to extract relevant patterns from large text corpora. The main objective of this work is to survey the recent use of NLP in the field of pharmacology. As our work shows, NLP is a highly relevant information extraction and processing approach for pharmacology. It has been used extensively, from intelligent searches through thousands of medical documents to finding traces of adversarial drug interactions in social media. We split our coverage into five categories to survey modern NLP: methodology, commonly addressed tasks, relevant textual data, knowledge bases, and useful programming libraries. We split each of the five categories into appropriate subcategories, describe their main properties and ideas, and summarize them in a tabular form. The resulting survey presents a comprehensive overview of the area, useful to practitioners and interested observers.
Significance Statement
The main objective of this work is to survey the recent use of NLP in the field of pharmacology in order to provide a comprehensive overview of the current state in the area after the rapid developments that occurred in the past few years. The resulting survey will be useful to practitioners and interested observers in the domain.}
}
@article{SUNIL2024102665,
title = {The gene function prediction challenge: Large language models and knowledge graphs to the rescue},
journal = {Current Opinion in Plant Biology},
volume = {82},
pages = {102665},
year = {2024},
issn = {1369-5266},
doi = {https://doi.org/10.1016/j.pbi.2024.102665},
url = {https://www.sciencedirect.com/science/article/pii/S1369526624001560},
author = {Rohan Shawn Sunil and Shan Chun Lim and Manoj Itharajula and Marek Mutwil},
abstract = {Elucidating gene function is one of the ultimate goals of plant science. Despite this, only ∼15 % of all genes in the model plant Arabidopsis thaliana have comprehensively experimentally verified functions. While bioinformatical gene function prediction approaches can guide biologists in their experimental efforts, neither the performance of the gene function prediction methods nor the number of experimental characterization of genes has increased dramatically in recent years. In this review, we will discuss the status quo and the trajectory of gene function elucidation and outline the recent advances in gene function prediction approaches. We will then discuss how recent artificial intelligence advances in large language models and knowledge graphs can be leveraged to accelerate gene function predictions and keep us updated with scientific literature.}
}
@article{YANG2025122135,
title = {OD-Mind: An ocean drilling expert knowledge query system driven by knowledge graph},
journal = {Ocean Engineering},
volume = {339},
pages = {122135},
year = {2025},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2025.122135},
url = {https://www.sciencedirect.com/science/article/pii/S0029801825018190},
author = {Yulong Yang and Weihua Cao and Yupeng Li and Runzhou Chang and Gangcheng Yang and Chao Gan and Min Wu},
keywords = {Ocean drilling, Knowledge extraction, Knowledge graph, Large language model, Retrieval-augmented generation},
abstract = {The textual publications generated by the Scientific Ocean Drilling compile a substantial body of knowledge. However, these publications present significant analytical challenges due to their technical language and rapidly expanding volume. To address these challenges, this study presents Ocean Drilling Mind (OD-Mind), a comprehensive system designed to automatically extract key knowledge from large-scale publications and support question answering in the Scientific Ocean Drilling domain. The contributions are threefold: 1) A knowledge extraction method is adapted to capture the knowledge related to complex multi-word compound named entities in ocean drilling; 2) A knowledge graph refinement method is proposed to consolidate the weak-connected knowledge into a high-density ocean drilling knowledge graph; 3) A query system is designed to answer specialized queries in the domain of ocean drilling based on the knowledge graph. In the case study, OD-Mind demonstrated its potential by constructing a comprehensive knowledge graph on 116 publications. Preliminary results showed that the system performed better than general-purpose large language models, particularly in answering specialized ocean drilling domain queries with improved speed and accuracy.}
}
@article{GUO2024136502,
title = {Knowledge graph-guided data-driven design of ultra-high-performance concrete (UHPC) with interpretability and physicochemical reaction discovery capability},
journal = {Construction and Building Materials},
volume = {430},
pages = {136502},
year = {2024},
issn = {0950-0618},
doi = {https://doi.org/10.1016/j.conbuildmat.2024.136502},
url = {https://www.sciencedirect.com/science/article/pii/S095006182401643X},
author = {Pengwei Guo and Weina Meng and Yi Bao},
keywords = {Interpretable artificial intelligence, Machine learning, Knowledge graph, Solid wastes, Physicochemical reactions, Ultra-high-performance concrete},
abstract = {Traditional methods for designing concrete materials typically rely on labor-intensive laboratory experiments, resulting in time and cost inefficiencies. Recently, designing concrete using artificial intelligence (AI) methods has shown high efficiency, but existing AI methods often rely solely on data, which can lead to violation with scientific principles and result in models lacking reasoning abilities. To overcome these challenges, this paper presents an interpretable knowledge graph-guided data-driven design approach. By integrating advanced computing techniques with domain knowledge via knowledge graphs, this approach enables the interpretation of data-driven models and uncovers the underlying mechanisms behind predictions. This approach is applied to ultra-high-performance concrete (UHPC) involving complex physicochemical reactions. The domain knowledge about UHPC is imparted using a knowledge graph, and UHPC properties are predicted using a machine learning model considering mixing proportions, processing methods, and physiochemical properties of materials via natural language processing. The results show that the knowledge graph displays crucial design variables and their effects on UHPC properties, aiding in selecting variables for machine learning models and interpreting their results. The prediction accuracy of the machine learning model reached 0.95. The research paves the way for more transparent and scientific AI models for material design and AI-enabled discovery of scientific knowledge.}
}
@article{VARSHNEY2025110929,
title = {Med-KGMA: A novel AI-driven medical support system leveraging knowledge graphs and medical advisors},
journal = {Computers in Biology and Medicine},
volume = {197},
pages = {110929},
year = {2025},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2025.110929},
url = {https://www.sciencedirect.com/science/article/pii/S0010482525012818},
author = {Sona Varshney and Bhawna Jain and Prerna Singh and Drishti Rani and Saumya Mehra},
keywords = {Medical decision support system, Healthcare, Machine learning, Knowledge graph},
abstract = {Healthcare systems worldwide face a growing burden, struggling to provide timely diagnosis and personalized care due to resource constraints. The increasing demand for medical expertise often results in delayed interventions, making automated decision support crucial. However, existing medical question-answering (QnA) systems struggle with hallucinations, limited contextual understanding, and difficulty handling complex queries, which often results in unreliable responses. To address these challenges, this study proposes Med-KGMA, an artificial intelligence-driven medical QnA system that leverages the proposed SequentialRotatE, a knowledge graph embedding model, along with the Mixture-of-Medical-Advisors (MoMA) framework to enhance diagnostic accuracy and treatment recommendations. Unlike conventional methods, SequentialRotatE effectively captures contextual relationships between medical entities, improving the system’s reasoning capabilities. Additionally, the MoMA framework, which dynamically routes queries to specialized advisors based on complexity and relevance, ensures more precise recommendations. Experimental results demonstrate that Med-KGMA achieves 91.32% accuracy, outperforming state-of-the-art baselines. This approach advances medical knowledge representation through optimized query processing, tailored knowledge graphs, and intelligent advisor selection, providing an efficient, scalable solution. By addressing initial symptom analysis and reducing healthcare load, Med-KGMA empowers users with reliable medical insights, bridging the gap between patients and timely care.}
}
@article{DU2025106317,
title = {OFPO & KGFPO: Ontology and knowledge graph for flood process observation},
journal = {Environmental Modelling & Software},
volume = {185},
pages = {106317},
year = {2025},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2025.106317},
url = {https://www.sciencedirect.com/science/article/pii/S1364815225000015},
author = {Wenying Du and Chang Liu and Qingyun Xia and Mengtian Wen and Ying Hu and Zeqiang Chen and Lei Xu and Xiang Zhang and Berhanu Keno Terfa and Nengcheng Chen},
keywords = {Flood, Process observation, Ontology, Knowledge graph},
abstract = {Flooding is the most frequent natural disaster globally, resulting in the highest economic losses. Efficient resource retrieval is crucial for improving flood response. Constructing a knowledge graph aids in the precise discovery of flood observation resources. However, current research faces issues: phased flood process observation is neglected, and effective correlation among disaster elements, such as tasks, data, methods, and sensors, is lacking. To address this, we construct the Ontology for Flood Process Observation (OFPO) and develop the Knowledge Graph for Flood Process Observation (KGFPO), providing integrated management and decision-making support. These are validated using the “7–20 Henan Extremely Heavy Rainfall” and “7-21 Xinxiang Extremely Heavy Rainfall” cases. OFPO and KGFPO achieve integrated management of flood observation resources, improve retrieval efficiency and accuracy, facilitate decision-making, and support other natural disasters.}
}
@article{HU2024103999,
title = {LLM-TIKG: Threat intelligence knowledge graph construction utilizing large language model},
journal = {Computers & Security},
volume = {145},
pages = {103999},
year = {2024},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.103999},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824003043},
author = {Yuelin Hu and Futai Zou and Jiajia Han and Xin Sun and Yilei Wang},
keywords = {Threat intelligence, Large language model, Knowledge graph, TTP classification},
abstract = {Open-source threat intelligence is often unstructured and cannot be directly applied to the next detection and defense. By constructing a knowledge graph through open-source threat intelligence, we can better apply this information to intrusion detection. However, the current methods for constructing knowledge graphs face limitations due to the domain-specific attributes of entities and the analysis of lengthy texts, and they require large amounts of labeled data. Furthermore, there is a lack of authoritative open-source annotated threat intelligence datasets, which require significant manual effort. Moreover, it is noteworthy that current research often neglects the textual descriptions of attack behaviors, resulting in the loss of vital information to understand intricate cyber threats. To address these issues, we propose LLM-TIKG that applies the large language model to construct a knowledge graph from unstructured open-source threat intelligence. The few-shot learning capability of GPT is leveraged to achieve data annotation and augmentation, thereby creating the datasets for fine-tuning a smaller language model (7B). Using the fine-tuned model, we perform topic classification on the collected reports, extract entities and relationships, and extract TTPs from the attack description. This process results in the construction of a threat intelligence knowledge graph, enabling automated and universal analysis of textualized threat intelligence. The experimental results demonstrate improved performance in both named entity recognition and TTP classification, achieving the precision of 87.88% and 96.53%, respectively.}
}
@article{BADENESOLMEDO2023104382,
title = {Lessons learned to enable question answering on knowledge graphs extracted from scientific publications: A case study on the coronavirus literature},
journal = {Journal of Biomedical Informatics},
volume = {142},
pages = {104382},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2023.104382},
url = {https://www.sciencedirect.com/science/article/pii/S153204642300103X},
author = {Carlos Badenes-Olmedo and Oscar Corcho},
keywords = {Question-answering, Knowledge graphs, Ontology, Evidences},
abstract = {The article presents a workflow to create a question-answering system whose knowledge base combines knowledge graphs and scientific publications on coronaviruses. It is based on the experience gained in modeling evidence from research articles to provide answers to questions in natural language. The work contains best practices for acquiring scientific publications, tuning language models to identify and normalize relevant entities, creating representational models based on probabilistic topics, and formalizing an ontology that describes the associations between domain concepts supported by the scientific literature. All the resources generated in the domain of coronavirus are available openly as part of the Drugs4COVID initiative, and can be (re)-used independently or as a whole. They can be exploited by scientific communities conducting research related to SARS-CoV-2/COVID-19 and also by therapeutic communities, laboratories, etc., wishing to find and understand relationships between symptoms, drugs, active ingredients and their documentary evidence.}
}
@article{BI2024127044,
title = {Relphormer: Relational Graph Transformer for Knowledge Graph Representations},
journal = {Neurocomputing},
volume = {566},
pages = {127044},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.127044},
url = {https://www.sciencedirect.com/science/article/pii/S0925231223011670},
author = {Zhen Bi and Siyuan Cheng and Jing Chen and Xiaozhuan Liang and Feiyu Xiong and Ningyu Zhang},
keywords = {Knowledge graph, Knowledge graph representation, Transformer},
abstract = {Transformers have achieved remarkable performance in widespread fields, including natural language processing, computer vision and graph mining. However, vanilla Transformer architectures have not yielded promising improvements in the Knowledge Graph (KG) representations, where the translational distance paradigm dominates this area. Note that vanilla Transformer architectures struggle to capture the intrinsically heterogeneous structural and semantic information of knowledge graphs. To this end, we propose a new variant of Transformer for knowledge graph representations dubbed Relphormer. Specifically, we introduce Triple2Seq which can dynamically sample contextualized sub-graph sequences as the input to alleviate the heterogeneity issue. We propose a novel structure-enhanced self-attention mechanism to encode the relational information and keep the semantic information within entities and relations. Moreover, we utilize masked knowledge modeling for general knowledge graph representation learning, which can be applied to various KG-based tasks including knowledge graph completion, question answering, and recommendation. Experimental results on six datasets show that Relphormer can obtain better performance compared with baselines.22Code is available in https://github.com/zjunlp/Relphormer.}
}
@article{LIU2024123981,
title = {Graph Augmentation Networks Based on Dynamic Sentiment Knowledge and Static External Knowledge Graphs for aspect-based sentiment analysis},
journal = {Expert Systems with Applications},
volume = {251},
pages = {123981},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.123981},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424008479},
author = {Hongtao Liu and Xin Li and Wanying Lu and Kefei Cheng and Xueyan Liu},
keywords = {Natural language processing, Aspect-based sentiment analysis, Graph convolutional networks, Knowledge graph, Sentiment knowledge},
abstract = {Aspect-based sentiment analysis (ABSA) is a fine-grained activity that aims to ascertain the sentiment polarity linked to a given aspect word. However, most previous approaches ignore the dynamic fusion of sentiment information and the use of external knowledge graphs. Furthermore, the fusion mechanism between multiple representations is flawed. In this research, we introduce a novel Graph Augmentation Networks Based on Dynamic Sentiment Knowledge and Static External Knowledge Graphs (DSSK-GAN), which utilizes both dynamic and static external knowledge to aid in modeling the relationship between aspect words and their associated sentiment words. Specifically, DSSK-GAN is engineered to effectively capture and integrate information from three perspectives of representation (i.e., syntactic representation with dynamically updated sentiment nodes, semantic representation based on self-attention mechanism, and knowledge graph representation). Firstly, we have designed and developed the Bifold-GCN module, a two-channel graph encoder. A globally shared and dynamically updated sentiment enhancement matrix is used to feed the DS-SGCN to obtain the representation T1. The semantic representation T2 is produced by Sem-GCN using multi-head self-attention. Secondly, the knowledge graph embedding is concatenated with the output of Bi-LSTM to obtain the knowledge representation T3 using the aspect knowledge focus mechanism. Finally, we propose a local gating-global convolution module to fuse the representations of the three perspectives. Furthermore, we incorporate an extra mission for sentiment word categorization to enhance the model’s attention towards sentiment terms. We conduct comprehensive experiments on five publicly available ABSA datasets. The obtained findings demonstrate the effectiveness and robustness of our proposed DSSK-GAN model.}
}
@article{LIU2025103502,
title = {Digital twin-based assembly process framework utilizing STEP and knowledge graph},
journal = {Advanced Engineering Informatics},
volume = {67},
pages = {103502},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103502},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625003957},
author = {Yazui Liu and Haodong Shen and Gang Zhao and Xiaoxiao Du and Xishuang Jing},
keywords = {Digital twin-based assembly process, Assembly information model, Standard for the Exchange of Product model data (STEP), Knowledge graph},
abstract = {Effective information organization and data analysis are foundational for achieving assembly process digital twin. However, the development of the assembly digital twin technology faces significant challenges due to “information silos” resulting from ineffective data exchange among multi-source heterogeneous data involved in assembly processes. This study proposes a Digital Twin-based Assembly Process Framework (DT-APF) designed to systematically organize and manage multi-stage manufacturing data. Through object instantiation, the framework establishes a Digital Twin-based Assembly Process Information Model (DT-APIM) that enables standardized transformation of heterogeneous data. By converting the instance of the DT-APIM into a graph-based structure, the framework generates a Digital Twin-based Assembly Process Knowledge Graph (DT-APKG), achieving deep integration of multi-source data. Furthermore, the integration of Autoregressive Structured Prediction (ASP) algorithm constructs a knowledge reasoning engine, establishing a comprehensive data organization and knowledge reasoning system that spans the entire assembly lifecycle. Experimental validation was conducted using an aero-engine casing assembly case. Results demonstrate that the ASP-based reasoning mechanism validates the effectiveness in identifying assembly performance correlations, while the DT-APF framework improves data integration efficiency by 31.6% compared with traditional knowledge graph construction approaches. This research provides a systematic solution for overcoming information barriers in complex product assembly and enhances the implementation of DT technology in industrial applications.}
}
@article{YANG2025106514,
title = {Knowledge graph construction with BERT-BiLSTM-IDCNN-CRF and graph algorithms for metallogenic pattern discovery: A case study of pegmatite-type lithium deposits in China},
journal = {Ore Geology Reviews},
volume = {179},
pages = {106514},
year = {2025},
issn = {0169-1368},
doi = {https://doi.org/10.1016/j.oregeorev.2025.106514},
url = {https://www.sciencedirect.com/science/article/pii/S0169136825000745},
author = {Xin Yang and Li Sun and Mei-Ling Liu and Ke-Yan Xiao and Cheng Li and Xu-Chao Dong},
keywords = {Knowledge graph, Bert-BiLSTM-IDCNN-CRF model, Eigenvector centrality, Cosine similarity},
abstract = {Compared to traditional geological data processing methods, knowledge graphs are more effective in calculating and processing the associated information and implicit geological knowledge within the data, helping to accurately grasp the underlying patterns and relationships of geological phenomena. To further optimize the semantic representation of geological text data and extract more detailed feature information, this study introduces the dilated convolutional neural network (IDCNN) layer into the Bert-BiLSTM-CRF model, constructing the Bert-BiLSTM-IDCNN-CRF framework for the precise extraction of lithium deposit named entities.This framework is then used to construct a knowledge graph for granite (pegmatite) lithium deposits in China. Experimental results demonstrate that the Bert-BiLSTM-IDCNN-CRF model exhibits excellent performance in processing Chinese geological text data, achieving a precision of 89%, a recall rate of 87%, and an F1 score of 88%. These results confirm the model's high effectiveness in geological named entity recognition and extraction tasks. Based on this, the study further employs centrality and similarity algorithms from graph theory to deeply analyze the metallogenic characteristics and potential patterns of lithium deposits. This analysis successfully identifies key influencing factors and core nodes for each lithium belt, providing a solid scientific foundation for subsequent lithium exploration target area delineation.}
}
@article{XU2026103141,
title = {Automated multimodal process knowledge graph construction for intelligent process planning with Cross-Modal Transformers},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {98},
pages = {103141},
year = {2026},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2025.103141},
url = {https://www.sciencedirect.com/science/article/pii/S0736584525001954},
author = {Qingfeng Xu and Chao Zhang and Dongxu Ma and Jiacheng Li and Jiewu Leng and Guanghui Zhou},
keywords = {Transformer, Multimodal process knowledge graphs, Machining feature, Intelligent process planning},
abstract = {Intelligent process planning is pivotal in modern manufacturing systems, enabling efficient, precise, and flexible production by optimizing resource allocation, enhancing machining accuracy, and shortening production cycles. Knowledge graphs integrate multi-source heterogeneous data to support this process, yet traditional single-modal approaches hinder the exploration of complex relationships in multimodal data, falling short of the needs for complex part planning. This paper examines machining features, the foundational units of process planning, and introduces an automatic construction method for a Multimodal Process Knowledge Graph (MPKG) tailored to intelligent process planning, powered by Cross-Modal Transformers. We developed the MF36 dataset, encompassing 36 machining features with 3D models, engineering views, and descriptive texts. A cross-modal framework integrating LERT-CRF and PA-ViT models automates the extraction and fusion of multimodal process knowledge, with PA-ViT’s pooling attention mechanism markedly boosting machining feature recognition accuracy. Experiments demonstrate superior performance over baselines, achieving F1 scores of 0.895 in entity extraction and 0.877 in image recognition. A case study validates the method’s reliability for precise process recommendations, providing fresh insights into advancing intelligent process planning.}
}
@article{CHEN2024105873,
title = {Knowledge graph for safety management standards of water conservancy construction engineering},
journal = {Automation in Construction},
volume = {168},
pages = {105873},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105873},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524006095},
author = {Yun Chen and Gengyang Lu and Ke Wang and Shu Chen and Chenfei Duan},
keywords = {Water conservancy construction engineering, Knowledge graph, Safety management standards, ALBERT-BiLSTM-CRF, Association rules},
abstract = {With the increasing demand for water conservancy engineering (WCE), the number of safety accidents during construction has continued to rise, requiring an urgent improvement in construction safety. The existing safety management regulations for water conservancy construction engineering (WCCE) comprise a considerable amount of text, with cross-references between different standards severely reducing their use efficiency. To address this issue, this paper proposes an ALBERT-BiLSTM-CRF model based on textual data from WCCE safety management standards. ALBERT, a lightweight pretrained language model, is integrated with the BiLSTM-CRF to construct an intelligent text entity recognition method. Association rules are used to extract entity relationships, and a knowledge graph representing the WCCE safety management standards is established. The results show that the ALBERT-BiLSTM-CRF algorithm improves the precision, with a recognition accuracy exceeding 85 %. Case studies validate that the constructed knowledge graph can quickly query safety standard knowledge, aiding in the generation of safety measures.}
}
@article{LI2025100809,
title = {A review of background, methods, limitations and opportunities of knowledge graph completion},
journal = {Computer Science Review},
volume = {58},
pages = {100809},
year = {2025},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2025.100809},
url = {https://www.sciencedirect.com/science/article/pii/S1574013725000851},
author = {Daiyi Li and Yaoyao Liang and Shenyi Qian and Huaiguang Wu and Wei Jia and Yilong Fu and Yifan Sun},
keywords = {Knowledge graph completion, Embedded-based completion, Path-based completion, Neural network-based completion, Large language model-based completion},
abstract = {Knowledge graph completion (KGC), as a pivotal technology for extracting hidden knowledge from large-scale data, has evolved into a systematic research framework through the development of knowledge graph (KG) technology in recent years. To address the many challenges faced by current research, this study systematically reviews the fundamental theories and methodological systems in the field of KGC, grouping them into four categories: embedding-based, path-based, neural network-based, and large language model (LLM)-based approaches. Current research indicates that traditional closed-domain KGC relies on standard KG embedding or relational path models, which remain effective for completing structured data. However, there are notable limitations in handling unseen entities and relations in open scenarios. With breakthroughs in neural networks and LLMs, open-domain KGC has begun to emerge, although a lack of systematic analysis and classification of model architectures still persists. To address this gap, this review conducts a multi-dimensional academic investigation, clarifying the foundational research landscape and core methodological distinctions, establishing a model classification framework that spans both closed and open domains and integrating mainstream dataset resources within the field. Furthermore, the review explores the challenges and future directions of technological development, including critical issues such as complex knowledge reasoning, improvements in domain adaptability improvement, and the deep integration of LLMs with KGs, providing theoretical foundations and practical references to guide subsequent research efforts.}
}
@article{REN2025103525,
title = {Automated disassembly-oriented knowledge graph construction for retired battery packs using a candidate entity-based relational triple joint extraction method},
journal = {Advanced Engineering Informatics},
volume = {67},
pages = {103525},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103525},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625004185},
author = {Yaping Ren and Junying Wu and Cunbo Zhuang and Xiaoguang Sun and Hongfei Guo and Jianzhao Wu and Yang Chen and Jianhua Liu},
keywords = {Retired battery packs, Automated disassembly, Knowledge graph, Relational triple joint extraction, Knowledge recommendation},
abstract = {Currently, the disassembly of retired electric vehicle battery packs relies on manpower and results in high cost, low efficiency, and poor stability. With the development of artificial intelligence, automated disassembly is an efficient method to largely reduce even completely replace human disassembly. However, the various kinds of battery packs and the uncertainty on their retired numbers and types lead to frequent changes of their disassembly processes. It is necessary to provide a method that can integrate valuable disassembly knowledge to enable automated disassembly. Thus, this study proposes an automated disassembly-oriented knowledge graph for retired battery packs which considers the properties of subassemblies (entities) and explicit physical connections/implicit associations among subassemblies (relations). A large amount of unstructured data exists regarding battery packs, such as product manuals and maintenance records, whereas the knowledge that can be available to guide the disassembly process is dispersed and sparse. To solve this, a candidate entity-based relational triple joint extraction method is developed to efficiently extract the disassembly knowledge, which consists of semantic feature learning, candidate entity recognition, and explicit/implicit relational triple identification. Finally, more than 10,000 sentences collected from multi-source unstructured texts are adopted to verify the proposed method. The experimental results demonstrate that our proposed method achieves an F1-score of 93.99% in candidate entity recognition and an F1-score of 95.6% in triple extraction. Also, the information of disassembly operations, disassembly tools, and subassembly properties can be recommended by the automated disassembly-oriented knowledge graph for retired battery packs.}
}
@article{WANG2025761,
title = {MMCSD: Multi-Modal Knowledge Graph Completion Based on Super-Resolution and Detailed Description Generation},
journal = {Computers, Materials and Continua},
volume = {83},
number = {1},
pages = {761-783},
year = {2025},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2025.060395},
url = {https://www.sciencedirect.com/science/article/pii/S154622182500298X},
author = {Huansha Wang and Ruiyang Huang and Qinrang Liu and Shaomei Li and Jianpeng Zhang},
keywords = {Multi-modal knowledge graph, knowledge graph completion, multi-modal fusion},
abstract = {Multi-modal knowledge graph completion (MMKGC) aims to complete missing entities or relations in multi-modal knowledge graphs, thereby discovering more previously unknown triples. Due to the continuous growth of data and knowledge and the limitations of data sources, the visual knowledge within the knowledge graphs is generally of low quality, and some entities suffer from the issue of missing visual modality. Nevertheless, previous studies of MMKGC have primarily focused on how to facilitate modality interaction and fusion while neglecting the problems of low modality quality and modality missing. In this case, mainstream MMKGC models only use pre-trained visual encoders to extract features and transfer the semantic information to the joint embeddings through modal fusion, which inevitably suffers from problems such as error propagation and increased uncertainty. To address these problems, we propose a Multi-modal knowledge graph Completion model based on Super-resolution and Detailed Description Generation (MMCSD). Specifically, we leverage a pre-trained residual network to enhance the resolution and improve the quality of the visual modality. Moreover, we design multi-level visual semantic extraction and entity description generation, thereby further extracting entity semantics from structural triples and visual images. Meanwhile, we train a variational multi-modal auto-encoder and utilize a pre-trained multi-modal language model to complement the missing visual features. We conducted experiments on FB15K-237 and DB13K, and the results showed that MMCSD can effectively perform MMKGC and achieve state-of-the-art performance.}
}
@article{HU2026129291,
title = {Construction and application of conditional event-based knowledge graphs for electric power standards},
journal = {Expert Systems with Applications},
volume = {297},
pages = {129291},
year = {2026},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.129291},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425029070},
author = {Yizhuo Hu and Hao Wang and Ji Chen and Chongxing Zhang and Ming Ren and Ming Dong and Haibin Zhang and Zhonglin Hu and Xuan Dong},
keywords = {Conditional event-based knowledge graphs, Power standards, Hybrid knowledge extraction, Production rules},
abstract = {With the increasing demand for standardized and intelligent management in new power systems, the limitations of traditional knowledge graphs in characterizing logical rules of electric power standards (EPSs) have become prominent. This study proposes a conditional event-based knowledge graph (CEKG)-based digitization method for EPSs, which achieves structured modeling of complex logical rules in power standard texts through the introduction of conditional nodes and event logic relationships. To address the linguistic characteristics of EPSs, a hybrid knowledge extraction method targeting diverse linguistic objects is developed. Specifically, a term-enhanced Bidirectional Encoder Representations from Transformers (BERT) architecture is employed as the input layer of the entity extraction model to extract domain-specific knowledge from EPS texts, while rule-based pattern matching is adopted for extracting general knowledge. A production rule-based transformer condition assessment model combining Cypher query language and graph algorithms is constructed based on the CEKG framework, with its effectiveness demonstrated through case validation. The results demonstrate that the proposed named entity recognition model outperforms other Transformer-based baseline models on the power domain dataset, exhibiting significant performance improvement particularly in the identification of core physical entities. The constructed CEKG effectively preserves intrinsic relationships between the physical meanings of different graph nodes with high knowledge representation fidelity. The developed production rule-based assessment model accurately outputs condition evaluation results based on equipment status information and online monitoring data. This research provides extensible paradigms for digital applications of EPSs as well as the knowledge extraction in power domain, offering both methodological and practical contributions to intelligent power system management.}
}
@article{CHEN2025100259,
title = {Integrating neuro-symbolic AI and knowledge graph for enhanced geochemical prediction in copper deposits},
journal = {Applied Computing and Geosciences},
volume = {27},
pages = {100259},
year = {2025},
issn = {2590-1974},
doi = {https://doi.org/10.1016/j.acags.2025.100259},
url = {https://www.sciencedirect.com/science/article/pii/S2590197425000412},
author = {Weilin Chen and Jiyin Zhang and Wenjia Li and Xiang Que and Chenhao Li and Xiaogang Ma},
keywords = {Large language model, Knowledge graph, Neuro-symbolic AI, Mineral prediction, Copper deposit, Interpretability},
abstract = {The integration of machine learning (ML) and deep learning (DL) in geoscience has demonstrated great promise for mineral prediction. However, existing approaches are predominantly data-driven and often overlook expert geological knowledge, limiting their interpretability, accuracy, and practical applicability. This study introduces a new method that combines Large Language Models (LLMs), knowledge graphs (KGs), and Neuro-Symbolic AI (NSAI) models to predict mineralization systems in diverse copper deposits, significantly increasing the precision in prediction results. We utilize LLMs to generate KGs from geological literature, extracting symbolic rules that encode domain-specific insights about copper mineralization. These rules, derived dynamically from expert knowledge, are integrated into ML models as guidance during the training and prediction phases. By fusing symbolic reasoning with ML's computational power, our approach overcomes the limitations of black-box models, offering both improved accuracy and transparency in mineral prediction. To validate this method, we apply it to a comprehensive geochemical dataset of global copper deposits. The results show that rule-guided ML models achieve notable performance improvements, outperforming traditional ML methods in accuracy, precision, and robustness. Interpretability is further enhanced by using tools such as SHAP values, which explain the influence of individual geochemical features within the rule-based framework. This combination not only identifies critical geochemical elements like Cu, Fe, and S but also provides coherent, domain-aligned explanations for the predicted mineralization patterns. Our findings demonstrate the transformative potential of combining LLMs, KGs, and ML models for mineral prediction. This hybrid approach enables geoscientists to leverage both computational and expert knowledge, achieving a deeper understanding of mineralization systems.}
}
@article{YANG2025102868,
title = {GS-KGC: A generative subgraph-based framework for knowledge graph completion with large language models},
journal = {Information Fusion},
volume = {117},
pages = {102868},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102868},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524006468},
author = {Rui Yang and Jiahao Zhu and Jianping Man and Hongze Liu and Li Fang and Yi Zhou},
keywords = {Knowledge graph, Knowledge graph completion, Large language models, Question answer},
abstract = {Knowledge graph completion (KGC) focuses on identifying missing triples in a knowledge graph (KG) , which is crucial for many downstream applications. Given the rapid development of large language models (LLMs), some LLM-based methods are proposed for KGC task. However, most of them focus on prompt engineering while overlooking the fact that finer-grained subgraph information can aid LLMs in generating more accurate answers. In this paper, we propose a novel completion framework called Generative Subgraph-based KGC (GS-KGC), which utilizes subgraph information as contextual reasoning and employs a QA approach to achieve the KGC task. This framework primarily includes a subgraph partitioning algorithm designed to generate negatives and neighbors. Specifically, negatives can encourage LLMs to generate a broader range of answers, while neighbors provide additional contextual insights for LLM reasoning. Furthermore, we found that GS-KGC can discover potential triples within the KGs and new facts beyond the KGs. Experiments conducted on four common KGC datasets highlight the advantages of the proposed GS-KGC, e.g., it shows a 5.6% increase in Hits@3 compared to the LLM-based model CP-KGC on the FB15k-237N, and a 9.3% increase over the LLM-based model TECHS on the ICEWS14.}
}
@article{LIU2025130909,
title = {SEMKR: Joint learning of semantic and topological representations for Knowledge Graph Completion},
journal = {Neurocomputing},
volume = {653},
pages = {130909},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.130909},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225015814},
author = {Pengjie Liu and Wang Zhang and Yulong Ding and Jie Jiang and Shuang-Hua Yang},
keywords = {Knowledge Graph Completion, Continual pre-training, Relation prediction, Contrastive learning, Representation learning, Legal judgment prediction},
abstract = {Knowledge Graph Completion (KGC) methods concentrate on predicting unknown components in the (head entity)⟶[relation](tail entity) triplet. However, large-scale Knowledge Graphs (KGs), especially those in legal domains, often face the problem of imbalanced data distribution, which brings significant challenges to achieve accurate prediction. For instance, high-frequency criminal charges generally produce well-structured and densely connected sub-graphs, whereas low-frequency/confusing criminal charges are sparsely connected. Current KGC methods learn individual features, such as relation types or sub-graph structures, while overlooking joint embedding associations. To address these challenges, we propose a text-guided graph reasoning mechanism that integrates semantic and topological representations synergistically. Our model, SEMKR, SEMantic and topological representations for Knowledge gRaph completion, incorporate two key modules: (1) Entity Description Representation Learning enhances semantic features by pre-train and fine-tune BERT; (2) Relational Topological Representation Learning refines context representations by learning edge and path information in two stage pre-training and fine-tuning. Experimental results demonstrate that SEMKR outperforms state-of-the-art baselines in relation or link prediction tasks, achieving significant improvements, including over 7.8% H@1 on the NELL995 dataset. All implementations will be available at https://github.com/SUSTech-TP/SEMKR.git.}
}
@article{CIROKU2024100822,
title = {RevOnt: Reverse engineering of competency questions from knowledge graphs via language models},
journal = {Journal of Web Semantics},
volume = {82},
pages = {100822},
year = {2024},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100822},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000088},
author = {Fiorela Ciroku and Jacopo {de Berardinis} and Jongmo Kim and Albert Meroño-Peñuela and Valentina Presutti and Elena Simperl},
keywords = {Knowledge engineering, Knowledge graph, Ontology development, Competency question extraction},
abstract = {The process of developing ontologies – a formal, explicit specification of a shared conceptualisation – is addressed by well-known methodologies. As for any engineering development, its fundamental basis is the collection of requirements, which includes the elicitation of competency questions. Competency questions are defined through interacting with domain and application experts or by investigating existing datasets that may be used to populate the ontology i.e. its knowledge graph. The rise in popularity and accessibility of knowledge graphs provides an opportunity to support this phase with automatic tools. In this work, we explore the possibility of extracting competency questions from a knowledge graph. This reverses the traditional workflow in which knowledge graphs are built from ontologies, which in turn are engineered from competency questions. We describe in detail RevOnt, an approach that extracts and abstracts triples from a knowledge graph, generates questions based on triple verbalisations, and filters the resulting questions to yield a meaningful set of competency questions; the WDV dataset. This approach is implemented utilising the Wikidata knowledge graph as a use case, and contributes a set of core competency questions from 20 domains present in the WDV dataset. To evaluate RevOnt, we contribute a new dataset of manually-annotated high-quality competency questions, and compare the extracted competency questions by calculating their BLEU score against the human references. The results for the abstraction and question generation components of the approach show good to high quality. Meanwhile, the accuracy of the filtering component is above 86%, which is comparable to the state-of-the-art classifications.}
}
@article{YANG2026103587,
title = {Integrating knowledge from knowledge graphs and large language models for explainable entity alignment},
journal = {Information Fusion},
volume = {126},
pages = {103587},
year = {2026},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2025.103587},
url = {https://www.sciencedirect.com/science/article/pii/S1566253525006591},
author = {Linyao Yang and Hongyang Chen and Xiao Wang and Jing Yang and Fei-Yue Wang and Han Liu},
keywords = {Entity alignment, Knowledge fusion, Knowledge graph, Large language model},
abstract = {Entity alignment, a critical task in integrating knowledge from multiple knowledge graphs (KGs), aims to identify equivalent entities across different KGs. Traditional approaches predominantly rely on knowledge embedding models to generate entity representations and compute similarity scores for alignment. However, these methods often lack interpretability, rendering their predictions opaque to end users. Recently, large language models (LLMs) have demonstrated strong semantic reasoning capabilities and have been applied to various KG-related tasks, including entity alignment. Despite this progress, existing methods still suffer from three key limitations: inaccurate retrieval of candidate entities, noisy prompt construction, and weak interaction between the retrieval module and the LLM. To address these challenges, we propose EARAG (entity alignment-oriented retrieval-augmented generation), a novel framework that effectively integrates structured knowledge from KGs with the semantic reasoning power of LLMs. EARAG first employs a convolutional neural network (CNN)-based retriever that jointly models multiple similarity metrics and captures relative ranking information to retrieve high-quality candidate entities. It then constructs carefully designed prompts that guide the LLM to not only determine entity equivalence but also generate human-understandable explanations. Extensive experiments on benchmark datasets demonstrate that EARAG achieves state-of-the-art alignment accuracy while offering superior interpretability. These results highlight the potential of retrieval-augmented LLMs as transparent and effective solutions for real-world entity alignment tasks. Code and datasets are publicly available at: https://github.com/linyaoyang/EARAG.}
}
@article{WANG2025305,
title = {Dual-Perspective Evaluation of Knowledge Graphs for Graph-to-Text Generation},
journal = {Computers, Materials and Continua},
volume = {84},
number = {1},
pages = {305-324},
year = {2025},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2025.066351},
url = {https://www.sciencedirect.com/science/article/pii/S1546221825005867},
author = {Haotong Wang and Liyan Wang and Yves Lepage},
keywords = {Knowledge graph evaluation, graph-to-text generation, scientific abstract, large language model},
abstract = {Data curation is vital for selecting effective demonstration examples in graph-to-text generation. However, evaluating the quality of Knowledge Graphs (KGs) remains challenging. Prior research exhibits a narrow focus on structural statistics, such as the shortest path length, while the correctness of graphs in representing the associated text is rarely explored. To address this gap, we introduce a dual-perspective evaluation framework for KG-text data, based on the computation of structural adequacy and semantic alignment. From a structural perspective, we propose the Weighted Incremental Edge Method (WIEM) to quantify graph completeness by leveraging agreement between relation models to predict possible edges between entities. WIEM targets to find increments from models on “unseen links”, whose presence is inversely proportional to the structural adequacy of the original KG in representing the text. From a semantic perspective, we evaluate how well a KG aligns with the text in capturing the intended meaning. To do so, we instruct a large language model to convert KGs into natural language and measure the similarity between generated and reference texts. Based on these computations, we apply a Top-K union method, integrating the structural and semantic modules, to rank and select high-quality KGs. We evaluate our framework against various approaches for selecting few-shot examples in graph-to-text generation. Experiments on the Association for Computational Linguistics Abstract Graph Dataset (ACL-AGD) and Automatic Content Extraction 05 (ACE05) dataset demonstrate the effectiveness of our approach in distinguishing KG-text data of different qualities, evidenced by the largest performance gap between top- and bottom-ranked examples. We also find that the top examples selected through our dual-perspective framework consistently yield better performance than those selected by traditional measures. These results highlight the importance of data curation in improving graph-to-text generation.}
}
@article{KUNIG2025711,
title = {Explainable Event Extraction in Knowledge-Based Maintenance},
journal = {Procedia CIRP},
volume = {134},
pages = {711-716},
year = {2025},
note = {58th CIRP Conference on Manufacturing Systems 2025},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2025.02.177},
url = {https://www.sciencedirect.com/science/article/pii/S2212827125005682},
author = {Lukas Künig and Linus Kohl and Sareh Aghaei and Fazel Ansari},
keywords = {Industry 4.0, Knowledge-Based Maintenance, Natural Language Processing, Transformer, Event Extraction, Zero-Shot, Explainability},
abstract = {As an integral part of Industry 4.0, industrial maintenance is adopting smart methods, giving rise to knowledge-based maintenance. This transition requires the integration of advanced digital technologies to optimize maintenance strategies, where natural language processing (NLP) plays a crucial role. Transformer models, as a recent advancement in NLP, enable the extraction of relevant information—such as industrial events—from unstructured maintenance data, revealing hidden patterns and informing decision making. This paper conceptualizes the task of event extraction as a classification problem, applying transformer-based models and zero-shot learning. The proposed approach addresses the absence of labeled training data in maintenance sectors and also offers a solution to enhance transparency and explainability for the extracted events. In this paper, we present the performance of four transformer-based models in terms of accuracy and F1-score across four synthetic datasets available in English and German. Furthermore, we apply the Shapley value method to visualize the quantification of each token’s contribution within the text to the model’s prediction (i.e., the extracted event). We also design and develop an intuitive graphical user interface that not only facilitates user interaction but also promotes transparency. The experimental results demonstrate the effectiveness of transformer-based models for industrial event extraction, providing a powerful tool for the maintenance sector to extract valuable insights from unstructured data within historical maintenance logs, sensor data, and technical manuals. Moreover, the integration of explainability through the Shapley value method offers a deeper understanding of the model’s decision-making process, which is essential in real-world applications in manufacturing systems.}
}
@article{LOPEZGAZPIO2025113026,
title = {Corporate relation extraction for the construction of knowledge-bases against tax fraud},
journal = {Knowledge-Based Systems},
volume = {311},
pages = {113026},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113026},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125000747},
author = {Inigo Lopez-Gazpio and Laura Baselga-Pascual and Aitor Garmendia-Lazcano},
keywords = {Computer Vision, Information extraction, Knowledge-base generation, Natural Language Processing, Tax fraud investigation},
abstract = {Tax fraud is a criminal activity that entails significant losses for governments. Due to its clandestine nature, it is difficult to reliably estimate the amount of taxes evaded. To fight tax fraud, this investigation details the construction and evaluation of a corporate relation extraction system designed to access an unstructured knowledge-base and extract corporate relations for further validation. The system was developed in response to a need raised by the Treasury and Finance Department of the Provincial Council of Gipuzkoa (Spain). It follows a waterfall architecture that integrates Natural Language Processing (NLP) and Computer Vision (CV) components, including web scraping, optical character recognition, syntactic parsing, and information extraction. The proposed system produces a relational knowledge-base with structured data representing 23 types of corporate operations published in the Official Gazette of the Commercial Registry (e.g., incorporation of companies, terminations, capital increases and reductions, mergers and takeovers, etc.), allowing for comparison with the fiscal information available in the tax agency. Facilitating such comparison across distinct sources is key to identifying discrepancies that might be indicators of tax fraud.}
}
@article{AIRESBARBOSA20251080,
title = {A Bottom-Up Approach to Knowledge Graph Construction for Health Innovation Ecosystems},
journal = {Procedia Computer Science},
volume = {257},
pages = {1080-1085},
year = {2025},
note = {The 16th International Conference on Ambient Systems, Networks and Technologies Networks (ANT)/ the 8th International Conference on Emerging Data and Industry 4.0 (EDI40)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.03.141},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925008786},
author = {Antonio Marcos {Aires Barbosa} and Raimir Holanda Filho},
keywords = {Analytics, Knowledge Graphs, Innovation Ecosystem},
abstract = {This paper presents the initial phase of a comprehensive research project aimed at modeling and optimizing innovation ecosystems within Scientific and Technological Institutions (STIs) in the context of the Brazilian Health Industrial Economic Complex (CEIS). We propose a novel bottom-up approach to competency mapping and innovation management using initially semantic similarity analysis. The proposed solution combines natural language processing techniques with graph-based representations to identify and cluster competencies within research and development institutions. By analyzing researchers’ publications, development projects, and their relationships, our method creates a dynamic competency map that supports strategic decision-making in innovation ecosystems. The implementation uses a Neo4j graph database to store and process relationships, while employing BERT-based multilingual embeddings for semantic analysis. Experimental results demonstrate the effectiveness of our approach in identifying competency clusters and their relationships with product development needs, providing valuable insights for innovation management. The method was validated in a real-world scenario within a health innovation ecosystem, showing promising results for strategic planning and competency development.}
}
@article{LIAO2025103134,
title = {Large language model assisted fine-grained knowledge graph construction for robotic fault diagnosis},
journal = {Advanced Engineering Informatics},
volume = {65},
pages = {103134},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103134},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625000278},
author = {Xingming Liao and Chong Chen and Zhuowei Wang and Ying Liu and Tao Wang and Lianglun Cheng},
keywords = {Knowledge Graph, Large Language Model, Fault Diagnosis, Industrial Robots},
abstract = {With the rapid deployment of industrial robots in manufacturing, the demand for advanced maintenance techniques to sustain operational efficiency has become crucial. Fault diagnosis Knowledge Graph (KG) is essential as it interlinks multi-source data related to industrial robot faults, capturing multi-level semantic associations among different fault events. However, the construction and application of fine-grained fault diagnosis KG face significant challenges due to the inherent complexity of nested entities in maintenance texts and the severe scarcity of annotated industrial data. In this study, we propose a Large Language Model (LLM) assisted data augmentation approach, which handles the complex nested entities in maintenance corpora and constructs a more fine-grained fault diagnosis KG. Firstly, the fine-grained ontology is constructed via LLM Assistance in Industrial Nested Named Entity Recognition (assInNNER). Then, an Industrial Nested Label Classification Template (INCT) is designed, enabling the use of nested entities in Attention-map aware keyword selection for the Industrial Nested Language Model (ANLM) data augmentation methods. ANLM can effectively improve the model’s performance in nested entity extraction when corpora are scarce. Subsequently, a Confidence Filtering Mechanism (CFM) is introduced to evaluate and select the generated data for enhancement, and assInNNER is further deployed to recall the negative samples corpus again to further improve performance. Experimental studies based on multi-source corpora demonstrate that compared to existing algorithms, our method achieves an average F1 increase of 8.25 %, 3.31 %, and 1.96 % in 5%, 10 %, and 25 % in few-shot settings, respectively.}
}
@article{WANG2023120211,
title = {Novel medical question and answer system: Graph convolutional neural network based with knowledge graph optimization},
journal = {Expert Systems with Applications},
volume = {227},
pages = {120211},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120211},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423007133},
author = {Xu Wang and Zijin Luo and Rui He and Yixin Shao},
keywords = {Medical diagnosis, Knowledge graph, Graph neural network, Speech recognition, Disease classification},
abstract = {In order to effectively integrate medical data and alleviate the problem of uneven distribution of medical resources. In this paper, we combine the techniques of expert systems, graph neural networks, and knowledge graphs to propose a disease guidance model combining semi-supervised graph neural networks and knowledge graphs. We use the MASR speech recognition module combined with gated convolutional units for effective text processing of different types of speech; then we use the LTP module in natural language processing for semantic analysis and segmentation matching of interrogative sentences; we combine keywords with the number of diseases and divide and construct the set of nodes with knowledge graphs. And we use semi-supervised graph neural network type analysis to give treatment results and rehabilitation suggestions effectively. We optimize the Chinese and English corpora respectively, adding consideration for local dialect audiences. We performed a comprehensive comparison of the accuracy and training time of several mainstream GCN algorithms and our GCN semi-supervised (SGS) under various graphical text datasets to validate the efficiency and accuracy of our own algorithm choices. We preprocess the number of different symptoms for classification and simplify the redundant nodes to optimize the running time while taking into account the overall convergence. The operational mechanism of the model as well as the convergence and hits under different symptom parameters are explained through hit rate and convergence rate metrics to demonstrate the effectiveness and stability of the model under proprietary medical conditions.}
}
@article{LIU2022117991,
title = {Using text mining to establish knowledge graph from accident/incident reports in risk assessment},
journal = {Expert Systems with Applications},
volume = {207},
pages = {117991},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.117991},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422012179},
author = {Chang Liu and Shiwu Yang},
keywords = {Text mining, Knowledge graph, Named entity recognition, Machine learning, Risk assessment, Railway safety},
abstract = {To clarify the risk factors and propagation characteristics affecting railway safety, we learn from historical reports to build a connected network of hazards and accidents, forming a knowledge graph (KG), and apply it to railway hazard identification and risk assessment. First, the open source-British railway accident/incident reports are selected as the data source. The text augmentation algorithm in the text mining technology is introduced and optimized to achieve data enhancement. An ensemble model is constructed based on the hidden Markov model, conditional random field (CRF) algorithm, bidirectional long short-term memory (Bi-LSTM), and Bi-LSTM-CRF deep learning network, completing the named entity recognition of the reports. Then, using the random forest algorithm, the standardized classification of entities is accomplished, and the multi-dimensional knowledge graph network is established. Finally, after defining a series of safety-related feature parameters, the obtained KG is applied to the quantitative assessment of the corresponding risk level of the hazards. The results show that this approach realizes the visualization and quantitative description of the potential relationship among hazards, faults, and accidents by exploring the topological relationship of the railway accident network, further assisting the formulation of railway risk preventive measures.}
}
@article{CHEN2025120536,
title = {Risk factors extraction and analysis of Chinese ship collision accidents based on knowledge graph},
journal = {Ocean Engineering},
volume = {322},
pages = {120536},
year = {2025},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2025.120536},
url = {https://www.sciencedirect.com/science/article/pii/S0029801825002513},
author = {Jihong Chen and Chenglin Zhuang and Jia Shi and Houqiang Jiang and Jinyu Xu and Jutong Liu},
keywords = {Chinese ship collisions, Knowledge graph, Complex network, Deep learning, Factors analysis},
abstract = {Shipping is a crucial mode of transportation. The high density of ship activities in Chinese waters increases the likelihood and severity of shipping accidents, which can significantly impact the global supply chain and shipping network operations. Among various maritime accidents, collisions are the most prevalent. Knowledge graphs, using triples (entity-relation-entity) as basic units, describe real-world concepts and relationships through text information, which aid in the causal analysis of accidents. This paper analyzes text data from Chinese ship collision accident reports and employs joint triple extraction algorithms based on deep learning and CART (Classification and Regression Tree) algorithm to construct a knowledge graph of these accidents, visualized using Gephi software. Utilizing complex network theory, a series of safety-related topological indicators are defined to perform quantitative risk assessment, identify key risk factors, and propose preventive measures, offering significant reference value for preventing ship collisions and other maritime accidents in Chinese waters.}
}
@article{XIAO20251110,
title = {KGESM: A knowledge graph embedding-based similarity matching model for intelligent assembly process generation},
journal = {Journal of Manufacturing Systems},
volume = {82},
pages = {1110-1124},
year = {2025},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2025.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0278612525002018},
author = {Youzi Xiao and Shuai Zheng and Hucheng Feng and Yejia Huang and Jiewu Leng and Jun Hong},
keywords = {Assembly process, Process generation, Similarity matching, Knowledge graph embedding},
abstract = {The assembly process knowledge graph is an important carrier for assembly process knowledge management and reuse in manufacturing enterprises. Its important application scenario is to generate assembly process. However, the primary method is to perform simple retrieval on the knowledge graph using the graph database tool. This method can only retrieve the identical assembly process due to the lack of deep semantics for process knowledge, which restricts the flexibility of assembly process generation. Since there may be different representations and descriptions of identical process knowledge, it is necessary to mine deep semantic information to achieve process generation. To address these challenges, we propose a knowledge graph embedding-based similarity matching model for intelligent assembly process generation. First, we build a knowledge graph embedding-based similarity matching model called KGESM. Then, we construct a dataset consisting of a series of assembly process knowledge pairs extracted from actual electronic equipment manufacturing documents. Finally, the trained model is used to generate assembly processes according to new manufacturing needs. We conduct comprehensive experiments on the electronic equipment assembly process knowledge graph, where the mean square error of similarity matching achieves 1.200×10−3. Unlike traditional knowledge graph retrieval, similarity matching based on assembly process knowledge graph embedding has the advantage of fusing the features of assembly process nodes and assembly relations. Furthermore, examples of electronic equipment assembly processes are generated, and the highest similarity score of the generated assembly processes is 0.939, which proves the feasibility of our method in the equipment manufacturing field.}
}
@article{WU2025129999,
title = {MKGF: A multi-modal knowledge graph based RAG framework to enhance LVLMs for Medical visual question answering},
journal = {Neurocomputing},
volume = {635},
pages = {129999},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.129999},
url = {https://www.sciencedirect.com/science/article/pii/S092523122500671X},
author = {Yinan Wu and Yuming Lu and Yan Zhou and Yifan Ding and Jingping Liu and Tong Ruan},
keywords = {Multi-modal, Knowledge graph, Large language model},
abstract = {Medical visual question answering (MedVQA) is a challenging task that requires models to understand medical images and return accurate responses for the given questions. Most recent methods focus on transferring general-domain large vision–language models (LVLMs) to the medical domain by constructing medical instruction datasets and in-context learning. However, the performance of these methods are limited due to the hallucination issue of LVLMs. In addition, fine-tuning the abundant parameters of LVLMs on medical instruction datasets is high time and economic cost. Hence, we propose a MKGF framework that leverages a multi-modal medical knowledge graph (MMKG) to relieve the hallucination issue without fine-tuning the abundant parameters of LVLMs. Firstly, we employ a pre-trained text retriever to build question–knowledge relations on training set. Secondly, we train a multi-modal retriever with these relations. Finally, we use it to retrieve question-relevant knowledge and enhance the performance of LVLMs on the test set. To evaluate the effectiveness of MKGF, we conduct extensive experiments on two public datasets Slake and VQA-RAD. Our method improves the pre-trained SOTA LVLMs by 10.15% and 9.32%, respectively. The source codes are available at https://github.com/ehnal/MKGF.}
}